{"2025-04-11T00:00:00Z":{"Statistical Machine Learning (Theory)":[{"id":"http://arxiv.org/abs/2504.08721v1","updated":"2025-04-11T17:35:58Z","published":"2025-04-11T17:35:58Z","title":"Surrogate-based optimization of system architectures subject to hidden\n  constraints","summary":"  The exploration of novel architectures requires physics-based simulation due\nto a lack of prior experience to start from, which introduces two specific\nchallenges for optimization algorithms: evaluations become more expensive (in\ntime) and evaluations might fail. The former challenge is addressed by\nSurrogate-Based Optimization (SBO) algorithms, in particular Bayesian\nOptimization (BO) using Gaussian Process (GP) models. An overview is provided\nof how BO can deal with challenges specific to architecture optimization, such\nas design variable hierarchy and multiple objectives: specific measures include\nensemble infills and a hierarchical sampling algorithm. Evaluations might fail\ndue to non-convergence of underlying solvers or infeasible geometry in certain\nareas of the design space. Such failed evaluations, also known as hidden\nconstraints, pose a particular challenge to SBO/BO, as the surrogate model\ncannot be trained on empty results. This work investigates various strategies\nfor satisfying hidden constraints in BO algorithms. Three high-level strategies\nare identified: rejection of failed points from the training set, replacing\nfailed points based on viable (non-failed) points, and predicting the failure\nregion. Through investigations on a set of test problems including a jet engine\narchitecture optimization problem, it is shown that best performance is\nachieved with a mixed-discrete GP to predict the Probability of Viability\n(PoV), and by ensuring selected infill points satisfy some minimum PoV\nthreshold. This strategy is demonstrated by solving a jet engine architecture\nproblem that features at 50% failure rate and could not previously be solved by\na BO algorithm. The developed BO algorithm and used test problems are available\nin the open-source Python library SBArchOpt.\n","authors":["Jasper Bussemaker","Paul Saves","Nathalie Bartoli","Thierry Lefebvre","Björn Nagel"],"pdf_url":"https://arxiv.org/pdf/2504.08721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08712v1","updated":"2025-04-11T17:23:09Z","published":"2025-04-11T17:23:09Z","title":"Beyond Black-Box Predictions: Identifying Marginal Feature Effects in\n  Tabular Transformer Networks","summary":"  In recent years, deep neural networks have showcased their predictive power\nacross a variety of tasks. Beyond natural language processing, the transformer\narchitecture has proven efficient in addressing tabular data problems and\nchallenges the previously dominant gradient-based decision trees in these\nareas. However, this predictive power comes at the cost of intelligibility:\nMarginal feature effects are almost completely lost in the black-box nature of\ndeep tabular transformer networks. Alternative architectures that use the\nadditivity constraints of classical statistical regression models can maintain\nintelligible marginal feature effects, but often fall short in predictive power\ncompared to their more complex counterparts. To bridge the gap between\nintelligibility and performance, we propose an adaptation of tabular\ntransformer networks designed to identify marginal feature effects. We provide\ntheoretical justifications that marginal feature effects can be accurately\nidentified, and our ablation study demonstrates that the proposed model\nefficiently detects these effects, even amidst complex feature interactions. To\ndemonstrate the model's predictive capabilities, we compare it to several\ninterpretable as well as black-box models and find that it can match black-box\nperformances while maintaining intelligibility. The source code is available at\nhttps://github.com/OpenTabular/NAMpy.\n","authors":["Anton Thielmann","Arik Reuter","Benjamin Saefken"],"pdf_url":"https://arxiv.org/pdf/2504.08712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08682v1","updated":"2025-04-11T16:43:11Z","published":"2025-04-11T16:43:11Z","title":"Bayesian optimization for mixed variables using an adaptive dimension\n  reduction process: applications to aircraft design","summary":"  Multidisciplinary design optimization methods aim at adapting numerical\noptimization techniques to the design of engineering systems involving multiple\ndisciplines. In this context, a large number of mixed continuous, integer and\ncategorical variables might arise during the optimization process and practical\napplications involve a large number of design variables. Recently, there has\nbeen a growing interest in mixed variables constrained Bayesian optimization\nbut most existing approaches severely increase the number of the\nhyperparameters related to the surrogate model. In this paper, we address this\nissue by constructing surrogate models using less hyperparameters. The\nreduction process is based on the partial least squares method. An adaptive\nprocedure for choosing the number of hyperparameters is proposed. The\nperformance of the proposed approach is confirmed on analytical tests as well\nas two real applications related to aircraft design. A significant improvement\nis obtained compared to genetic algorithms.\n","authors":["Paul Saves","Nathalie Bartoli","Youssef Diouane","Thierry Lefebvre","Joseph Morlier","Christophe David","Eric Nguyen Van","Sébastien Defoort"],"pdf_url":"https://arxiv.org/pdf/2504.08682v1.pdf","comment":"AIAA SciTech 2022 Forum. arXiv admin note: substantial text overlap\n  with arXiv:2402.04711"},{"id":"http://arxiv.org/abs/2410.05898v7","updated":"2025-04-11T16:19:15Z","published":"2024-10-08T10:55:40Z","title":"Manifolds, Random Matrices and Spectral Gaps: The geometric phases of\n  generative diffusion","summary":"  In this paper, we investigate the latent geometry of generative diffusion\nmodels under the manifold hypothesis. For this purpose, we analyze the spectrum\nof eigenvalues (and singular values) of the Jacobian of the score function,\nwhose discontinuities (gaps) reveal the presence and dimensionality of distinct\nsub-manifolds. Using a statistical physics approach, we derive the spectral\ndistributions and formulas for the spectral gaps under several distributional\nassumptions, and we compare these theoretical predictions with the spectra\nestimated from trained networks. Our analysis reveals the existence of three\ndistinct qualitative phases during the generative process: a trivial phase; a\nmanifold coverage phase where the diffusion process fits the distribution\ninternal to the manifold; a consolidation phase where the score becomes\northogonal to the manifold and all particles are projected on the support of\nthe data. This `division of labor' between different timescales provides an\nelegant explanation of why generative diffusion models are not affected by the\nmanifold overfitting phenomenon that plagues likelihood-based models, since the\ninternal distribution and the manifold geometry are produced at different time\npoints during generation.\n","authors":["Enrico Ventura","Beatrice Achilli","Gianluigi Silvestri","Carlo Lucibello","Luca Ambrogioni"],"pdf_url":"https://arxiv.org/pdf/2410.05898v7.pdf","comment":"22 pages, 13 figures"},{"id":"http://arxiv.org/abs/2404.12481v2","updated":"2025-04-11T15:49:46Z","published":"2024-04-18T19:33:55Z","title":"Understanding Optimal Feature Transfer via a Fine-Grained Bias-Variance\n  Analysis","summary":"  In the transfer learning paradigm models learn useful representations (or\nfeatures) during a data-rich pretraining stage, and then use the pretrained\nrepresentation to improve model performance on data-scarce downstream tasks. In\nthis work, we explore transfer learning with the goal of optimizing downstream\nperformance. We introduce a simple linear model that takes as input an\narbitrary pretrained feature transform. We derive exact asymptotics of the\ndownstream risk and its \\textit{fine-grained} bias-variance decomposition. We\nthen identify the pretrained representation that optimizes the asymptotic\ndownstream bias and variance averaged over an ensemble of downstream tasks. Our\ntheoretical and empirical analysis uncovers the surprising phenomenon that the\noptimal featurization is naturally sparse, even in the absence of explicit\nsparsity-inducing priors or penalties. Additionally, we identify a phase\ntransition where the optimal pretrained representation shifts from hard\nselection to soft selection of relevant features.\n","authors":["Yufan Li","Subhabrata Sen","Ben Adlam"],"pdf_url":"https://arxiv.org/pdf/2404.12481v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08638v1","updated":"2025-04-11T15:39:44Z","published":"2025-04-11T15:39:44Z","title":"Transformer Learns Optimal Variable Selection in Group-Sparse\n  Classification","summary":"  Transformers have demonstrated remarkable success across various\napplications. However, the success of transformers have not been understood in\ntheory. In this work, we give a case study of how transformers can be trained\nto learn a classic statistical model with \"group sparsity\", where the input\nvariables form multiple groups, and the label only depends on the variables\nfrom one of the groups. We theoretically demonstrate that, a one-layer\ntransformer trained by gradient descent can correctly leverage the attention\nmechanism to select variables, disregarding irrelevant ones and focusing on\nthose beneficial for classification. We also demonstrate that a well-pretrained\none-layer transformer can be adapted to new downstream tasks to achieve good\nprediction accuracy with a limited number of samples. Our study sheds light on\nhow transformers effectively learn structured data.\n","authors":["Chenyang Zhang","Xuran Meng","Yuan Cao"],"pdf_url":"https://arxiv.org/pdf/2504.08638v1.pdf","comment":"63 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.04860v3","updated":"2025-04-11T15:37:00Z","published":"2024-07-05T20:45:27Z","title":"Kullback-Leibler Barycentre of Stochastic Processes","summary":"  We consider the problem where an agent aims to combine the views and insights\nof different experts' models. Specifically, each expert proposes a diffusion\nprocess over a finite time horizon. The agent then combines the experts' models\nby minimising the weighted Kullback--Leibler divergence to each of the experts'\nmodels. We show existence and uniqueness of the barycentre model and prove an\nexplicit representation of the Radon--Nikodym derivative relative to the\naverage drift model. We further allow the agent to include their own\nconstraints, resulting in an optimal model that can be seen as a distortion of\nthe experts' barycentre model to incorporate the agent's constraints. We\npropose two deep learning algorithms to approximate the optimal drift of the\ncombined model, allowing for efficient simulations. The first algorithm aims at\nlearning the optimal drift by matching the change of measure, whereas the\nsecond algorithm leverages the notion of elicitability to directly estimate the\nvalue function. The paper concludes with an extended application to combine\nimplied volatility smile models that were estimated on different datasets.\n","authors":["Sebastian Jaimungal","Silvana M. Pesenti"],"pdf_url":"https://arxiv.org/pdf/2407.04860v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08628v1","updated":"2025-04-11T15:29:55Z","published":"2025-04-11T15:29:55Z","title":"Gradient Descent Robustly Learns the Intrinsic Dimension of Data in\n  Training Convolutional Neural Networks","summary":"  Modern neural networks are usually highly over-parameterized. Behind the wide\nusage of over-parameterized networks is the belief that, if the data are\nsimple, then the trained network will be automatically equivalent to a simple\npredictor. Following this intuition, many existing works have studied different\nnotions of \"ranks\" of neural networks and their relation to the rank of data.\nIn this work, we study the rank of convolutional neural networks (CNNs) trained\nby gradient descent, with a specific focus on the robustness of the rank to\nimage background noises. Specifically, we point out that, when adding\nbackground noises to images, the rank of the CNN trained with gradient descent\nis affected far less compared with the rank of the data. We support our claim\nwith a theoretical case study, where we consider a particular data model to\ncharacterize low-rank clean images with added background noises. We prove that\nCNNs trained by gradient descent can learn the intrinsic dimension of clean\nimages, despite the presence of relatively large background noises. We also\nconduct experiments on synthetic and real datasets to further validate our\nclaim.\n","authors":["Chenyang Zhang","Peifeng Gao","Difan Zou","Yuan Cao"],"pdf_url":"https://arxiv.org/pdf/2504.08628v1.pdf","comment":"43 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.18992v3","updated":"2025-04-11T15:04:54Z","published":"2024-04-29T18:00:00Z","title":"Unifying Simulation and Inference with Normalizing Flows","summary":"  There have been many applications of deep neural networks to detector\ncalibrations and a growing number of studies that propose deep generative\nmodels as automated fast detector simulators. We show that these two tasks can\nbe unified by using maximum likelihood estimation (MLE) from conditional\ngenerative models for energy regression. Unlike direct regression techniques,\nthe MLE approach is prior-independent and non-Gaussian resolutions can be\ndetermined from the shape of the likelihood near the maximum. Using an\nATLAS-like calorimeter simulation, we demonstrate this concept in the context\nof calorimeter energy calibration.\n","authors":["Haoxing Du","Claudius Krause","Vinicius Mikuni","Benjamin Nachman","Ian Pang","David Shih"],"pdf_url":"https://arxiv.org/pdf/2404.18992v3.pdf","comment":"13 pages, 7 figures; v3: matches published version"},{"id":"http://arxiv.org/abs/2504.08544v1","updated":"2025-04-11T13:57:09Z","published":"2025-04-11T13:57:09Z","title":"Slicing the Gaussian Mixture Wasserstein Distance","summary":"  Gaussian mixture models (GMMs) are widely used in machine learning for tasks\nsuch as clustering, classification, image reconstruction, and generative\nmodeling. A key challenge in working with GMMs is defining a computationally\nefficient and geometrically meaningful metric. The mixture Wasserstein (MW)\ndistance adapts the Wasserstein metric to GMMs and has been applied in various\ndomains, including domain adaptation, dataset comparison, and reinforcement\nlearning. However, its high computational cost -- arising from repeated\nWasserstein distance computations involving matrix square root estimations and\nan expensive linear program -- limits its scalability to high-dimensional and\nlarge-scale problems. To address this, we propose multiple novel slicing-based\napproximations to the MW distance that significantly reduce computational\ncomplexity while preserving key optimal transport properties. From a\ntheoretical viewpoint, we establish several weak and strong equivalences\nbetween the introduced metrics, and show the relations to the original MW\ndistance and the well-established sliced Wasserstein distance. Furthermore, we\nvalidate the effectiveness of our approach through numerical experiments,\ndemonstrating computational efficiency and applications in clustering,\nperceptual image comparison, and GMM minimization\n","authors":["Moritz Piening","Robert Beinert"],"pdf_url":"https://arxiv.org/pdf/2504.08544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15718v2","updated":"2025-04-11T12:55:40Z","published":"2024-02-24T04:57:59Z","title":"Optimal Rates and Saturation for Noiseless Kernel Ridge Regression","summary":"  Kernel ridge regression (KRR), also known as the least-squares support vector\nmachine, is a fundamental method for learning functions from finite samples.\nWhile most existing analyses focus on the noisy setting with constant-level\nlabel noise, we present a comprehensive study of KRR in the noiseless regime --\na critical setting in scientific computing where data are often generated via\nhigh-fidelity numerical simulations.\n  We establish that, up to logarithmic factors, noiseless KRR achieves minimax\noptimal convergence rates, jointly determined by the eigenvalue decay of the\nassociated integral operator and the target function's smoothness. These rates\nare derived under Sobolev-type interpolation norms, with the $L^2$ norm as a\nspecial case. Notably, we uncover two key phenomena: an extra-smoothness\neffect, where the KRR solution exhibits higher smoothness than typical\nfunctions in the native reproducing kernel Hilbert space (RKHS), and a\nsaturation effect, where the KRR's adaptivity to the target function's\nsmoothness plateaus beyond a certain level. Leveraging these insights, we also\nderive a novel error bound for noisy KRR that is noise-level aware and achieves\nminimax optimality in both noiseless and noisy regimes. As a key technical\ncontribution, we introduce a refined notion of degrees of freedom, which we\nbelieve has broader applicability in the analysis of kernel methods. Extensive\nnumerical experiments validate our theoretical results and provide insights\nbeyond existing theory.\n","authors":["Jihao Long","Xiaojun Peng","Lei Wu"],"pdf_url":"https://arxiv.org/pdf/2402.15718v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05837v2","updated":"2025-04-11T12:46:31Z","published":"2024-10-08T09:06:32Z","title":"A noise-corrected Langevin algorithm and sampling by half-denoising","summary":"  The Langevin algorithm is a classic method for sampling from a given pdf in a\nreal space. In its basic version, it only requires knowledge of the gradient of\nthe log-density, also called the score function. However, in deep learning, it\nis often easier to learn the so-called \"noisy-data score function\", i.e. the\ngradient of the log-density of noisy data, more precisely when Gaussian noise\nis added to the data. Such an estimate is biased and complicates the use of the\nLangevin method. Here, we propose a noise-corrected version of the Langevin\nalgorithm, where the bias due to noisy data is removed, at least regarding\nfirst-order terms. Unlike diffusion models, our algorithm needs to know the\nnoisy score function for one single noise level only. We further propose a\nsimple special case which has an interesting intuitive interpretation of\niteratively adding noise the data and then attempting to remove half of that\nnoise.\n","authors":["Aapo Hyvärinen"],"pdf_url":"https://arxiv.org/pdf/2410.05837v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08489v1","updated":"2025-04-11T12:36:06Z","published":"2025-04-11T12:36:06Z","title":"Statistically guided deep learning","summary":"  We present a theoretically well-founded deep learning algorithm for\nnonparametric regression. It uses over-parametrized deep neural networks with\nlogistic activation function, which are fitted to the given data via gradient\ndescent. We propose a special topology of these networks, a special random\ninitialization of the weights, and a data-dependent choice of the learning rate\nand the number of gradient descent steps. We prove a theoretical bound on the\nexpected $L_2$ error of this estimate, and illustrate its finite sample size\nperformance by applying it to simulated data. Our results show that a\ntheoretical analysis of deep learning which takes into account simultaneously\noptimization, generalization and approximation can result in a new deep\nlearning estimate which has an improved finite sample performance.\n","authors":["Michael Kohler","Adam Krzyzak"],"pdf_url":"https://arxiv.org/pdf/2504.08489v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2504.03405"},{"id":"http://arxiv.org/abs/2410.03363v2","updated":"2025-04-11T12:20:30Z","published":"2024-10-04T12:30:03Z","title":"Minimax-optimal and Locally-adaptive Online Nonparametric Regression","summary":"  We study adversarial online nonparametric regression with general convex\nlosses and propose a parameter-free learning algorithm that achieves minimax\noptimal rates. Our approach leverages chaining trees to compete against\nH{\\\"o}lder functions and establishes optimal regret bounds. While competing\nwith nonparametric function classes can be challenging, they often exhibit\nlocal patterns - such as local H{\\\"o}lder continuity - that online algorithms\ncan exploit. Without prior knowledge, our method dynamically tracks and adapts\nto different H{\\\"o}lder profiles by pruning a core chaining tree structure,\naligning itself with local smoothness variations. This leads to the first\ncomputationally efficient algorithm with locally adaptive optimal rates for\nonline regression in an adversarial setting. Finally, we discuss how these\nnotions could be extended to a boosting framework, offering promising\ndirections for future research.\n","authors":["Paul Liautaud","Pierre Gaillard","Olivier Wintenberger"],"pdf_url":"https://arxiv.org/pdf/2410.03363v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04613v4","updated":"2025-04-11T12:00:09Z","published":"2024-02-07T06:30:39Z","title":"Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in\n  Reproducing Kernel Hilbert Spaces","summary":"  Commonly used $f$-divergences of measures, e.g., the Kullback-Leibler\ndivergence, are subject to limitations regarding the support of the involved\nmeasures. A remedy is regularizing the $f$-divergence by a squared maximum mean\ndiscrepancy (MMD) associated with a characteristic kernel $K$. We use the\nkernel mean embedding to show that this regularization can be rewritten as the\nMoreau envelope of some function on the associated reproducing kernel Hilbert\nspace. Then, we exploit well-known results on Moreau envelopes in Hilbert\nspaces to analyze the MMD-regularized $f$-divergences, particularly their\ngradients. Subsequently, we use our findings to analyze Wasserstein gradient\nflows of MMD-regularized $f$-divergences. We provide proof-of-the-concept\nnumerical examples for flows starting from empirical measures. Here, we cover\n$f$-divergences with infinite and finite recession constants. Lastly, we extend\nour results to the tight variational formulation of $f$-divergences and\nnumerically compare the resulting flows.\n","authors":["Viktor Stein","Sebastian Neumayer","Nicolaj Rux","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2402.04613v4.pdf","comment":"56 pages, 14 figures, 3 tables. Comments welcome! NEW: Incorporated\n  Reviewers' suggestions, added FISTA and tight formulation, typos fixed"},{"id":"http://arxiv.org/abs/2504.08438v1","updated":"2025-04-11T11:01:11Z","published":"2025-04-11T11:01:11Z","title":"Diffusion Models for Robotic Manipulation: A Survey","summary":"  Diffusion generative models have demonstrated remarkable success in visual\ndomains such as image and video generation. They have also recently emerged as\na promising approach in robotics, especially in robot manipulations. Diffusion\nmodels leverage a probabilistic framework, and they stand out with their\nability to model multi-modal distributions and their robustness to\nhigh-dimensional input and output spaces. This survey provides a comprehensive\nreview of state-of-the-art diffusion models in robotic manipulation, including\ngrasp learning, trajectory planning, and data augmentation. Diffusion models\nfor scene and image augmentation lie at the intersection of robotics and\ncomputer vision for vision-based tasks to enhance generalizability and data\nscarcity. This paper also presents the two main frameworks of diffusion models\nand their integration with imitation learning and reinforcement learning. In\naddition, it discusses the common architectures and benchmarks and points out\nthe challenges and advantages of current state-of-the-art diffusion-based\nmethods.\n","authors":["Rosa Wolf","Yitian Shi","Sheng Liu","Rania Rayyes"],"pdf_url":"https://arxiv.org/pdf/2504.08438v1.pdf","comment":"28 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2504.08428v1","updated":"2025-04-11T10:37:19Z","published":"2025-04-11T10:37:19Z","title":"Standardization of Weighted Ranking Correlation Coefficients","summary":"  A relevant problem in statistics is defining the correlation of two rankings\nof a list of items. Kendall's tau and Spearman's rho are two well established\ncorrelation coefficients, characterized by a symmetric form that ensures zero\nexpected value between two pairs of rankings randomly chosen with uniform\nprobability. However, in recent years, several weighted versions of the\noriginal Spearman and Kendall coefficients have emerged that take into account\nthe greater importance of top ranks compared to low ranks, which is common in\nmany contexts. The weighting schemes break the symmetry, causing a non-zero\nexpected value between two random rankings. This issue is very relevant, as it\nundermines the concept of uncorrelation between rankings. In this paper, we\naddress this problem by proposing a standardization function $g(x)$ that maps a\ncorrelation ranking coefficient $\\Gamma$ in a standard form $g(\\Gamma)$ that\nhas zero expected value, while maintaining the relevant statistical properties\nof $\\Gamma$.\n","authors":["Pierangelo Lombardo"],"pdf_url":"https://arxiv.org/pdf/2504.08428v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.20505v2","updated":"2025-04-11T10:21:06Z","published":"2025-03-26T12:45:52Z","title":"Riemannian Optimization on Relaxed Indicator Matrix Manifold","summary":"  The indicator matrix plays an important role in machine learning, but\noptimizing it is an NP-hard problem. We propose a new relaxation of the\nindicator matrix and prove that this relaxation forms a manifold, which we call\nthe Relaxed Indicator Matrix Manifold (RIM manifold). Based on Riemannian\ngeometry, we develop a Riemannian toolbox for optimization on the RIM manifold.\nSpecifically, we provide several methods of Retraction, including a fast\nRetraction method to obtain geodesics. We point out that the RIM manifold is a\ngeneralization of the double stochastic manifold, and it is much faster than\nexisting methods on the double stochastic manifold, which has a complexity of\n\\( \\mathcal{O}(n^3) \\), while RIM manifold optimization is \\( \\mathcal{O}(n) \\)\nand often yields better results. We conducted extensive experiments, including\nimage denoising, with millions of variables to support our conclusion, and\napplied the RIM manifold to Ratio Cut, we provide a rigorous convergence proof\nand achieve clustering results that outperform the state-of-the-art methods.\nOur Code in\n\\href{https://github.com/Yuan-Jinghui/Riemannian-Optimization-on-Relaxed-Indicator-Matrix-Manifold}{here}.\n","authors":["Jinghui Yuan","Fangyuan Xie","Feiping Nie","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2503.20505v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08386v1","updated":"2025-04-11T09:38:12Z","published":"2025-04-11T09:38:12Z","title":"PCA-RAG: Principal Component Analysis for Efficient Retrieval-Augmented\n  Generation","summary":"  Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\ngrounding large language models in external knowledge sources, improving the\nprecision of agents responses. However, high-dimensional language model\nembeddings, often in the range of hundreds to thousands of dimensions, can\npresent scalability challenges in terms of storage and latency, especially when\nprocessing massive financial text corpora. This paper investigates the use of\nPrincipal Component Analysis (PCA) to reduce embedding dimensionality, thereby\nmitigating computational bottlenecks without incurring large accuracy losses.\nWe experiment with a real-world dataset and compare different similarity and\ndistance metrics under both full-dimensional and PCA-compressed embeddings. Our\nresults show that reducing vectors from 3,072 to 110 dimensions provides a\nsizeable (up to $60\\times$) speedup in retrieval operations and a $\\sim\n28.6\\times$ reduction in index size, with only moderate declines in correlation\nmetrics relative to human-annotated similarity scores. These findings\ndemonstrate that PCA-based compression offers a viable balance between\nretrieval fidelity and resource efficiency, essential for real-time systems\nsuch as Zanista AI's \\textit{Newswitch} platform. Ultimately, our study\nunderscores the practicality of leveraging classical dimensionality reduction\ntechniques to scale RAG architectures for knowledge-intensive applications in\nfinance and trading, where speed, memory efficiency, and accuracy must jointly\nbe optimized.\n","authors":["Arman Khaledian","Amirreza Ghadiridehkordi","Nariman Khaledian"],"pdf_url":"https://arxiv.org/pdf/2504.08386v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2504.08377v1","updated":"2025-04-11T09:26:37Z","published":"2025-04-11T09:26:37Z","title":"Proofs as Explanations: Short Certificates for Reliable Predictions","summary":"  We consider a model for explainable AI in which an explanation for a\nprediction $h(x)=y$ consists of a subset $S'$ of the training data (if it\nexists) such that all classifiers $h' \\in H$ that make at most $b$ mistakes on\n$S'$ predict $h'(x)=y$. Such a set $S'$ serves as a proof that $x$ indeed has\nlabel $y$ under the assumption that (1) the target function $h^\\star$ belongs\nto $H$, and (2) the set $S$ contains at most $b$ corrupted points. For example,\nif $b=0$ and $H$ is the family of linear classifiers in $\\mathbb{R}^d$, and if\n$x$ lies inside the convex hull of the positive data points in $S$ (and hence\nevery consistent linear classifier labels $x$ as positive), then\nCarath\\'eodory's theorem states that $x$ lies inside the convex hull of $d+1$\nof those points. So, a set $S'$ of size $d+1$ could be released as an\nexplanation for a positive prediction, and would serve as a short proof of\ncorrectness of the prediction under the assumption of realizability.\n  In this work, we consider this problem more generally, for general hypothesis\nclasses $H$ and general values $b\\geq 0$. We define the notion of the robust\nhollow star number of $H$ (which generalizes the standard hollow star number),\nand show that it precisely characterizes the worst-case size of the smallest\ncertificate achievable, and analyze its size for natural classes. We also\nconsider worst-case distributional bounds on certificate size, as well as\ndistribution-dependent bounds that we show tightly control the sample size\nneeded to get a certificate for any given test example. In particular, we\ndefine a notion of the certificate coefficient $\\varepsilon_x$ of an example\n$x$ with respect to a data distribution $D$ and target function $h^\\star$, and\nprove matching upper and lower bounds on sample size as a function of\n$\\varepsilon_x$, $b$, and the VC dimension $d$ of $H$.\n","authors":["Avrim Blum","Steve Hanneke","Chirag Pabbaraju","Donya Saless"],"pdf_url":"https://arxiv.org/pdf/2504.08377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08335v1","updated":"2025-04-11T08:00:37Z","published":"2025-04-11T08:00:37Z","title":"Entropic bounds for conditionally Gaussian vectors and applications to\n  neural networks","summary":"  Using entropic inequalities from information theory, we provide new bounds on\nthe total variation and 2-Wasserstein distances between a conditionally\nGaussian law and a Gaussian law with invertible covariance matrix. We apply our\nresults to quantify the speed of convergence to Gaussian of a randomly\ninitialized fully connected neural network and its derivatives - evaluated in a\nfinite number of inputs - when the initialization is Gaussian and the sizes of\nthe inner layers diverge to infinity. Our results require mild assumptions on\nthe activation function, and allow one to recover optimal rates of convergence\nin a variety of distances, thus improving and extending the findings of Basteri\nand Trevisan (2023), Favaro et al. (2023), Trevisan (2024) and Apollonio et al.\n(2024). One of our main tools are the quantitative cumulant estimates\nestablished in Hanin (2024). As an illustration, we apply our results to bound\nthe total variation distance between the Bayesian posterior law of the neural\nnetwork and its derivatives, and the posterior law of the corresponding\nGaussian limit: this yields quantitative versions of a posterior CLT by Hron et\nal. (2022), and extends several estimates by Trevisan (2024) to the total\nvariation metric.\n","authors":["Lucia Celli","Giovanni Peccati"],"pdf_url":"https://arxiv.org/pdf/2504.08335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08332v1","updated":"2025-04-11T07:54:55Z","published":"2025-04-11T07:54:55Z","title":"High-dimensional Clustering and Signal Recovery under Block Signals","summary":"  This paper studies computationally efficient methods and their minimax\noptimality for high-dimensional clustering and signal recovery under block\nsignal structures. We propose two sets of methods, cross-block feature\naggregation PCA (CFA-PCA) and moving average PCA (MA-PCA), designed for sparse\nand dense block signals, respectively. Both methods adaptively utilize block\nsignal structures, applicable to non-Gaussian data with heterogeneous variances\nand non-diagonal covariance matrices. Specifically, the CFA method utilizes a\nblock-wise U-statistic to aggregate and select block signals non-parametrically\nfrom data with unknown cluster labels. We show that the proposed methods are\nconsistent for both clustering and signal recovery under mild conditions and\nweaker signal strengths than the existing methods without considering block\nstructures of signals. Furthermore, we derive both statistical and\ncomputational minimax lower bounds (SMLB and CMLB) for high-dimensional\nclustering and signal recovery under block signals, where the CMLBs are\nrestricted to algorithms with polynomial computation complexity. The minimax\nboundaries partition signals into regions of impossibility and possibility. No\nalgorithm (or no polynomial time algorithm) can achieve consistent clustering\nor signal recovery if the signals fall into the statistical (or computational)\nregion of impossibility. We show that the proposed CFA-PCA and MA-PCA methods\ncan achieve the CMLBs for the sparse and dense block signal regimes,\nrespectively, indicating the proposed methods are computationally minimax\noptimal. A tuning parameter selection method is proposed based on\npost-clustering signal recovery results. Simulation studies are conducted to\nevaluate the proposed methods. A case study on global temperature change\ndemonstrates their utility in practice.\n","authors":["Wu Su","Yumou Qiu"],"pdf_url":"https://arxiv.org/pdf/2504.08332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08324v1","updated":"2025-04-11T07:48:42Z","published":"2025-04-11T07:48:42Z","title":"An Introduction to Double/Debiased Machine Learning","summary":"  This paper provides a practical introduction to Double/Debiased Machine\nLearning (DML). DML provides a general approach to performing inference about a\ntarget parameter in the presence of nuisance parameters. The aim of DML is to\nreduce the impact of nuisance parameter estimation on estimators of the\nparameter of interest. We describe DML and its two essential components: Neyman\northogonality and cross-fitting. We highlight that DML reduces functional form\ndependence and accommodates the use of complex data types, such as text data.\nWe illustrate its application through three empirical examples that demonstrate\nDML's applicability in cross-sectional and panel settings.\n","authors":["Achim Ahrens","Victor Chernozhukov","Christian Hansen","Damian Kozbur","Mark Schaffer","Thomas Wiemann"],"pdf_url":"https://arxiv.org/pdf/2504.08324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04914v2","updated":"2025-04-11T06:21:25Z","published":"2024-12-06T10:10:47Z","title":"Achieving Group Fairness through Independence in Predictive Process\n  Monitoring","summary":"  Predictive process monitoring focuses on forecasting future states of ongoing\nprocess executions, such as predicting the outcome of a particular case. In\nrecent years, the application of machine learning models in this domain has\ngarnered significant scientific attention. When using historical execution\ndata, which may contain biases or exhibit unfair behavior, these biases may be\nencoded into the trained models. Consequently, when such models are deployed to\nmake decisions or guide interventions for new cases, they risk perpetuating\nthis unwanted behavior. This work addresses group fairness in predictive\nprocess monitoring by investigating independence, i.e. ensuring predictions are\nunaffected by sensitive group membership. We explore independence through\nmetrics for demographic parity such as $\\Delta$DP, as well as recently\nintroduced, threshold-independent distribution-based alternatives.\nAdditionally, we propose a composite loss function existing of binary\ncross-entropy and a distribution-based loss (Wasserstein) to train models that\nbalance predictive performance and fairness, and allow for customizable\ntrade-offs. The effectiveness of both the fairness metrics and the composite\nloss functions is validated through a controlled experimental setup.\n","authors":["Jari Peeperkorn","Simon De Vos"],"pdf_url":"https://arxiv.org/pdf/2412.04914v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2404.10550v3","updated":"2025-04-11T04:02:41Z","published":"2024-04-16T13:19:46Z","title":"Analytical Approximation of the ELBO Gradient in the Context of the\n  Clutter Problem","summary":"  We propose an analytical solution for approximating the gradient of the\nEvidence Lower Bound (ELBO) in variational inference problems where the\nstatistical model is a Bayesian network consisting of observations drawn from a\nmixture of a Gaussian distribution embedded in unrelated clutter, known as the\nclutter problem. The method employs the reparameterization trick to move the\ngradient operator inside the expectation and relies on the assumption that,\nbecause the likelihood factorizes over the observed data, the variational\ndistribution is generally more compactly supported than the Gaussian\ndistribution in the likelihood factors. This allows efficient local\napproximation of the individual likelihood factors, which leads to an\nanalytical solution for the integral defining the gradient expectation. We\nintegrate the proposed gradient approximation as the expectation step in an EM\n(Expectation Maximization) algorithm for maximizing ELBO and test against\nclassical deterministic approaches in Bayesian inference, such as the Laplace\napproximation, Expectation Propagation and Mean-Field Variational Inference.\nThe proposed method demonstrates good accuracy and rate of convergence together\nwith linear computational complexity.\n","authors":["Roumen Nikolaev Popov"],"pdf_url":"https://arxiv.org/pdf/2404.10550v3.pdf","comment":"19 pages, 5 figures, supporting code available at\n  https://github.com/rpopov42/elbo_gaa"},{"id":"http://arxiv.org/abs/2504.08224v1","updated":"2025-04-11T03:12:53Z","published":"2025-04-11T03:12:53Z","title":"All Optical Echo State Network Reservoir Computing","summary":"  We propose an innovative design for an all-optical Echo State Network (ESN),\nan advanced type of reservoir computer known for its universal computational\ncapabilities. Our design enables fully optical implementation of arbitrary\nESNs, featuring complete flexibility in optical matrix multiplication and\nnonlinear activation. Leveraging the nonlinear characteristics of stimulated\nBrillouin scattering (SBS), the architecture efficiently realizes\nmeasurement-free operations crucial for reservoir computing. The approach\nsignificantly reduces computational overhead and energy consumption compared to\ntraditional software-based methods. Comprehensive simulations validate the\nsystem's memory capacity, nonlinear processing strength, and polynomial algebra\ncapabilities, showcasing performance comparable to software ESNs across key\nbenchmark tasks. Our design establishes a feasible, scalable, and universally\napplicable framework for optical reservoir computing, suitable for diverse\nmachine learning applications.\n","authors":["Ishwar S Kaushik","Peter J Ehlers","Daniel Soh"],"pdf_url":"https://arxiv.org/pdf/2504.08224v1.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2504.08216v1","updated":"2025-04-11T02:47:46Z","published":"2025-04-11T02:47:46Z","title":"Local Distance-Preserving Node Embeddings and Their Performance on\n  Random Graphs","summary":"  Learning node representations is a fundamental problem in graph machine\nlearning. While existing embedding methods effectively preserve local\nsimilarity measures, they often fail to capture global functions like graph\ndistances. Inspired by Bourgain's seminal work on Hilbert space embeddings of\nmetric spaces (1985), we study the performance of local distance-preserving\nnode embeddings. Known as landmark-based algorithms, these embeddings\napproximate pairwise distances by computing shortest paths from a small subset\nof reference nodes (i.e., landmarks). Our main theoretical contribution shows\nthat random graphs, such as Erd\\H{o}s-R\\'enyi random graphs, require lower\ndimensions in landmark-based embeddings compared to worst-case graphs.\nEmpirically, we demonstrate that the GNN-based approximations for the distances\nto landmarks generalize well to larger networks, offering a scalable\nalternative for graph representation learning.\n","authors":["My Le","Luana Ruiz","Souvik Dhara"],"pdf_url":"https://arxiv.org/pdf/2504.08216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08215v1","updated":"2025-04-11T02:46:39Z","published":"2025-04-11T02:46:39Z","title":"Deep Distributional Learning with Non-crossing Quantile Network","summary":"  In this paper, we introduce a non-crossing quantile (NQ) network for\nconditional distribution learning. By leveraging non-negative activation\nfunctions, the NQ network ensures that the learned distributions remain\nmonotonic, effectively addressing the issue of quantile crossing. Furthermore,\nthe NQ network-based deep distributional learning framework is highly\nadaptable, applicable to a wide range of applications, from classical\nnon-parametric quantile regression to more advanced tasks such as causal effect\nestimation and distributional reinforcement learning (RL). We also develop a\ncomprehensive theoretical foundation for the deep NQ estimator and its\napplication to distributional RL, providing an in-depth analysis that\ndemonstrates its effectiveness across these domains. Our experimental results\nfurther highlight the robustness and versatility of the NQ network.\n","authors":["Guohao Shen","Runpeng Dai","Guojun Wu","Shikai Luo","Chengchun Shi","Hongtu Zhu"],"pdf_url":"https://arxiv.org/pdf/2504.08215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08210v1","updated":"2025-04-11T02:27:30Z","published":"2025-04-11T02:27:30Z","title":"Optimizing Power Grid Topologies with Reinforcement Learning: A Survey\n  of Methods and Challenges","summary":"  Power grid operation is becoming increasingly complex due to the rising\nintegration of renewable energy sources and the need for more adaptive control\nstrategies. Reinforcement Learning (RL) has emerged as a promising approach to\npower network control (PNC), offering the potential to enhance decision-making\nin dynamic and uncertain environments. The Learning To Run a Power Network\n(L2RPN) competitions have played a key role in accelerating research by\nproviding standardized benchmarks and problem formulations, leading to rapid\nadvancements in RL-based methods. This survey provides a comprehensive and\nstructured overview of RL applications for power grid topology optimization,\ncategorizing existing techniques, highlighting key design choices, and\nidentifying gaps in current research. Additionally, we present a comparative\nnumerical study evaluating the impact of commonly applied RL-based methods,\noffering insights into their practical effectiveness. By consolidating existing\nresearch and outlining open challenges, this survey aims to provide a\nfoundation for future advancements in RL-driven power grid optimization.\n","authors":["Erica van der Sar","Alessandro Zocca","Sandjai Bhulai"],"pdf_url":"https://arxiv.org/pdf/2504.08210v1.pdf","comment":"60 pages, 26 figures, preprint"},{"id":"http://arxiv.org/abs/2503.01869v2","updated":"2025-04-11T00:37:11Z","published":"2025-02-25T21:50:46Z","title":"From Small to Large Language Models: Revisiting the Federalist Papers","summary":"  For a long time, the authorship of the Federalist Papers had been a subject\nof inquiry and debate, not only by linguists and historians but also by\nstatisticians. In what was arguably the first Bayesian case study, Mosteller\nand Wallace (1963) provided the first statistical evidence for attributing all\ndisputed papers to Madison. Our paper revisits this historical dataset but from\na lens of modern language models, both small and large. We review some of the\nmore popular Large Language Model (LLM) tools and examine them from a\nstatistical point of view in the context of text classification. We investigate\nwhether, without any attempt to fine-tune, the general embedding constructs can\nbe useful for stylometry and attribution. We explain differences between\nvarious word/phrase embeddings and discuss how to aggregate them in a document.\nContrary to our expectations, we exemplify that dimension expansion with word\nembeddings may not always be beneficial for attribution relative to dimension\nreduction with topic embeddings. Our experiments demonstrate that default LLM\nembeddings (even after manual fine-tuning) may not consistently improve\nauthorship attribution accuracy. Instead, Bayesian analysis with topic\nembeddings trained on ``function words\" yields superior out-of-sample\nclassification performance. This suggests that traditional (small) statistical\nlanguage models, with their interpretability and solid theoretical foundation,\ncan offer significant advantages in authorship attribution tasks. The code used\nin this analysis is available at github.com/sowonjeong/slm-to-llm\n","authors":["So Won Jeong","Veronika Ročková"],"pdf_url":"https://arxiv.org/pdf/2503.01869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08178v1","updated":"2025-04-11T00:20:37Z","published":"2025-04-11T00:20:37Z","title":"A Piecewise Lyapunov Analysis of sub--quadratic SGD: Applications to\n  Robust and Quantile Regression","summary":"  Motivated by robust and quantile regression problems, {we investigate the\nstochastic gradient descent (SGD) algorithm} for minimizing an objective\nfunction $f$ that is locally strongly convex with a sub--quadratic tail. This\nsetting covers many widely used online statistical methods. We introduce a\nnovel piecewise Lyapunov function that enables us to handle functions $f$ with\nonly first-order differentiability, which includes a wide range of popular loss\nfunctions such as Huber loss. Leveraging our proposed Lyapunov function, we\nderive finite-time moment bounds under general diminishing stepsizes, as well\nas constant stepsizes. We further establish the weak convergence, central limit\ntheorem and bias characterization under constant stepsize, providing the first\ngeometrical convergence result for sub--quadratic SGD. Our results have wide\napplications, especially in online statistical methods. In particular, we\ndiscuss two applications of our results. 1) Online robust regression: We\nconsider a corrupted linear model with sub--exponential covariates and\nheavy--tailed noise. Our analysis provides convergence rates comparable to\nthose for corrupted models with Gaussian covariates and noise. 2) Online\nquantile regression: Importantly, our results relax the common assumption in\nprior work that the conditional density is continuous and provide a more\nfine-grained analysis for the moment bounds.\n","authors":["Yixuan Zhang"," Dongyan"," Huo","Yudong Chen","Qiaomin Xie"],"pdf_url":"https://arxiv.org/pdf/2504.08178v1.pdf","comment":"ACM SIGMETRICS 2025. 40 pages, 12 figures"},{"id":"http://arxiv.org/abs/2403.09604v4","updated":"2025-04-11T23:57:57Z","published":"2024-03-14T17:45:24Z","title":"Extremal graphical modeling with latent variables via convex\n  optimization","summary":"  Extremal graphical models encode the conditional independence structure of\nmultivariate extremes and provide a powerful tool for quantifying the risk of\nrare events. Prior work on learning these graphs from data has focused on the\nsetting where all relevant variables are observed. For the popular class of\nH\\\"usler-Reiss models, we propose the \\texttt{eglatent} method, a tractable\nconvex program for learning extremal graphical models in the presence of latent\nvariables. Our approach decomposes the H\\\"usler-Reiss precision matrix into a\nsparse component encoding the graphical structure among the observed variables\nafter conditioning on the latent variables, and a low-rank component encoding\nthe effect of a few latent variables on the observed variables. We provide\nfinite-sample guarantees of \\texttt{eglatent} and show that it consistently\nrecovers the conditional graph as well as the number of latent variables. We\nhighlight the improved performances of our approach on synthetic and real data.\n","authors":["Sebastian Engelke","Armeen Taeb"],"pdf_url":"https://arxiv.org/pdf/2403.09604v4.pdf","comment":"Journal of Machine Learning Research, 2025"},{"id":"http://arxiv.org/abs/2301.06259v3","updated":"2025-04-11T23:51:11Z","published":"2023-01-16T04:52:46Z","title":"Understanding Best Subset Selection: A Tale of Two C(omplex)ities","summary":"  We consider the problem of best subset selection (BSS) under high-dimensional\nsparse linear regression model. Recently, Guo et al. (2020) showed that the\nmodel selection performance of BSS depends on a certain identifiability margin,\na measure that captures the model discriminative power of BSS under a general\ncorrelation structure that is robust to the design dependence, unlike its\ncomputational surrogates such as LASSO, SCAD, MCP, etc. Expanding on this, we\nfurther broaden the theoretical understanding of best subset selection in this\npaper and show that the complexities of the residualized signals, the portion\nof the signals orthogonal to the true active features, and spurious\nprojections, describing the projection operators associated with the irrelevant\nfeatures, also play fundamental roles in characterizing the margin condition\nfor model consistency of BSS. In particular, we establish both necessary and\nsufficient margin conditions depending only on the identifiability margin and\nthe two complexity measures. We also partially extend our sufficiency result to\nthe case of high-dimensional sparse generalized linear models (GLMs).\n","authors":["Saptarshi Roy","Ambuj Tewari","Ziwei Zhu"],"pdf_url":"https://arxiv.org/pdf/2301.06259v3.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2406.01660v4","updated":"2025-04-11T23:24:37Z","published":"2024-06-03T17:53:25Z","title":"Self-Improving Robust Preference Optimization","summary":"  Online and offline RLHF methods, such as PPO and DPO, have been highly\nsuccessful in aligning AI with human preferences. Despite their success,\nhowever, these methods suffer from fundamental limitations: (a) Models trained\nwith RLHF can learn from mistakes or negative examples through RL mechanism or\ncontrastive loss during training. However, at inference time, they lack an\ninnate self-improvement mechanism for error corrections. (b) The optimal\nsolution of existing methods is highly task-dependent, making it difficult for\nthem to generalize to new tasks. To address these challenges, we propose\nSelf-Improving Robust Preference Optimization (SRPO), a practical and\nmathematically principled offline RLHF framework. The key idea behind SRPO is\nto cast the problem of learning from human preferences as a self-improvement\nprocess, mathematically formulated as a min-max objective that jointly\noptimizes a self-improvement policy and a generative policy in an adversarial\nfashion. Crucially, the solution for this optimization problem is independent\nof the training task, which makes it robust to its changes. We then show that\nthis objective can be reformulated as a non-adversarial offline loss, which can\nbe efficiently optimized using standard supervised learning techniques at\nscale. To demonstrate SRPO's effectiveness, we evaluate it using AI Win-Rate\n(WR) against human (GOLD) completions. When tested on the XSum dataset, SRPO\noutperforms DPO by a margin of 15% after 5 self revisions, achieving an\nimpressive 90% WR. Moreover, on the challenging Arena-Hard prompts, SRPO\noutperforms both DPO and IPO (by 4% without revision and 6% after a single\nrevision), reaching a 56% WR against against Llama-3.1-8B-Instruct.\n","authors":["Eugene Choi","Arash Ahmadian","Matthieu Geist","Oilvier Pietquin","Mohammad Gheshlaghi Azar"],"pdf_url":"https://arxiv.org/pdf/2406.01660v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05776v3","updated":"2025-04-11T21:17:16Z","published":"2025-02-09T04:58:33Z","title":"Dynamic Pricing in the Linear Valuation Model using Shape Constraints","summary":"  We propose a shape-constrained approach to dynamic pricing for censored data\nin the linear valuation model eliminating the need for tuning parameters\ncommonly required by existing methods. Previous works have addressed the\nchallenge of unknown market noise distribution $F_0$ using strategies ranging\nfrom kernel methods to reinforcement learning algorithms, such as bandit\ntechniques and upper confidence bounds (UCB), under the assumption that $F_0$\nsatisfies Lipschitz (or stronger) conditions. In contrast, our method relies on\nisotonic regression under the weaker assumption that $F_0$ is $\\alpha$-H\\\"older\ncontinuous for some $\\alpha \\in (0,1]$, for which we derive a regret upper\nbound. Simulations and experiments with real-world data obtained by Welltower\nInc (a major healthcare Real Estate Investment Trust) consistently demonstrate\nthat our method attains lower empirical regret in comparison to several\nexisting methods in the literature while offering the advantage of being\ntuning-parameter free.\n","authors":["Daniele Bracale","Moulinath Banerjee","Yuekai Sun","Kevin Stoll","Salam Turki"],"pdf_url":"https://arxiv.org/pdf/2502.05776v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08980v1","updated":"2025-04-11T21:12:48Z","published":"2025-04-11T21:12:48Z","title":"Perfect Clustering in Nonuniform Hypergraphs","summary":"  While there has been tremendous activity in the area of statistical network\ninference on graphs, hypergraphs have not enjoyed the same attention, on\naccount of their relative complexity and the lack of tractable statistical\nmodels. We introduce a hyper-edge-centric model for analyzing hypergraphs,\ncalled the interaction hypergraph, which models natural sampling methods for\nhypergraphs in neuroscience and communication networks, and accommodates\ninteractions involving different numbers of entities. We define latent\nembeddings for the interactions in such a network, and analyze their\nestimators. In particular, we show that a spectral estimate of the interaction\nlatent positions can achieve perfect clustering once enough interactions are\nobserved.\n","authors":["Ga-Ming Angus Chan","Zachary Lubberts"],"pdf_url":"https://arxiv.org/pdf/2504.08980v1.pdf","comment":"21 pages, 8 figures, and 1 table"},{"id":"http://arxiv.org/abs/2311.09245v2","updated":"2025-04-11T21:06:08Z","published":"2023-11-13T14:17:57Z","title":"Affine Invariance in Continuous-Domain Convolutional Neural Networks","summary":"  The notion of group invariance helps neural networks in recognizing patterns\nand features under geometric transformations. Group convolutional neural\nnetworks enhance traditional convolutional neural networks by incorporating\ngroup-based geometric structures into their design. This research studies\naffine invariance on continuous-domain convolutional neural networks. Despite\nother research considering isometric invariance or similarity invariance, we\nfocus on the full structure of affine transforms generated by the group of all\ninvertible $2 \\times 2$ real matrices (generalized linear group\n$\\mathrm{GL}_2(\\mathbb{R})$). We introduce a new criterion to assess the\ninvariance of two signals under affine transformations. The input image is\nembedded into the affine Lie group $G_2 = \\mathbb{R}^2 \\ltimes\n\\mathrm{GL}_2(\\mathbb{R})$ to facilitate group convolution operations that\nrespect affine invariance. Then, we analyze the convolution of embedded signals\nover $G_2$. In sum, our research could eventually extend the scope of\ngeometrical transformations that usual deep-learning pipelines can handle.\n","authors":["Ali Mohaddes","Johannes Lederer"],"pdf_url":"https://arxiv.org/pdf/2311.09245v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08937v1","updated":"2025-04-11T19:33:06Z","published":"2025-04-11T19:33:06Z","title":"Rethinking Few-Shot Fusion: Granular Ball Priors Enable General-Purpose\n  Deep Image Fusion","summary":"  In image fusion tasks, due to the lack of real fused images as priors, most\ndeep learning-based fusion methods obtain global weight features from original\nimages in large-scale data pairs to generate images that approximate real fused\nimages. However, unlike previous studies, this paper utilizes Granular Ball\nadaptation to extract features in the brightness space as priors for deep\nnetworks, enabling the fusion network to converge quickly and complete the\nfusion task. This leads to few-shot training for a general image fusion\nnetwork, and based on this, we propose the GBFF fusion method. According to the\ninformation expression division of pixel pairs in the original fused image, we\nclassify pixel pairs with significant performance as the positive domain and\nnon-significant pixel pairs as the boundary domain. We perform split inference\nin the brightness space using Granular Ball adaptation to compute weights for\npixels that express information to varying degrees, generating approximate\nsupervision images that provide priors for the neural network in the structural\nbrightness space. Additionally, the extracted global saliency features also\nadaptively provide priors for setting the loss function weights of each image\nin the network, guiding the network to converge quickly at both global and\npixel levels alongside the supervised images, thereby enhancing the\nexpressiveness of the fused images. Each modality only used 10 pairs of images\nas the training set, completing the fusion task with a limited number of\niterations. Experiments validate the effectiveness of the algorithm and theory,\nand qualitative and quantitative comparisons with SOTA methods show that this\napproach is highly competitive in terms of fusion time and image\nexpressiveness.\n","authors":["Minjie Deng","Yan Wei","Hao Zhai","An Wu","Yuncan Ouyang","Qianyao Peng"],"pdf_url":"https://arxiv.org/pdf/2504.08937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05075v2","updated":"2025-04-11T19:22:00Z","published":"2025-02-07T16:46:43Z","title":"Discrepancies are Virtue: Weak-to-Strong Generalization through Lens of\n  Intrinsic Dimension","summary":"  Weak-to-strong (W2S) generalization is a type of finetuning (FT) where a\nstrong (large) student model is trained on pseudo-labels generated by a weak\nteacher. Surprisingly, W2S FT often outperforms the weak teacher. We seek to\nunderstand this phenomenon through the observation that FT often occurs in\nintrinsically low-dimensional spaces. Leveraging the low intrinsic\ndimensionality of FT, we analyze W2S in the ridgeless regression setting from a\nvariance reduction perspective. For a strong student - weak teacher pair with\nsufficiently expressive low-dimensional feature subspaces $\\mathcal{V}_s,\n\\mathcal{V}_w$, we provide an exact characterization of the variance that\ndominates the generalization error of W2S. This unveils a virtue of discrepancy\nbetween the strong and weak models in W2S: the variance of the weak teacher is\ninherited by the strong student in $\\mathcal{V}_s \\cap \\mathcal{V}_w$, while\nreduced by a factor of $\\dim(\\mathcal{V}_s)/N$ in the subspace of discrepancy\n$\\mathcal{V}_w \\setminus \\mathcal{V}_s$ with $N$ pseudo-labels for W2S.\nFurther, our analysis casts light on the sample complexities and the scaling of\nperformance gap recovery in W2S. The analysis is supported with experiments on\nsynthetic regression and real vision and NLP tasks.\n","authors":["Yijun Dong","Yicheng Li","Yunai Li","Jason D. Lee","Qi Lei"],"pdf_url":"https://arxiv.org/pdf/2502.05075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04870v2","updated":"2025-04-11T19:09:04Z","published":"2025-01-08T23:03:18Z","title":"Deep Transfer $Q$-Learning for Offline Non-Stationary Reinforcement\n  Learning","summary":"  In dynamic decision-making scenarios across business and healthcare,\nleveraging sample trajectories from diverse populations can significantly\nenhance reinforcement learning (RL) performance for specific target\npopulations, especially when sample sizes are limited. While existing transfer\nlearning methods primarily focus on linear regression settings, they lack\ndirect applicability to reinforcement learning algorithms. This paper pioneers\nthe study of transfer learning for dynamic decision scenarios modeled by\nnon-stationary finite-horizon Markov decision processes, utilizing neural\nnetworks as powerful function approximators and backward inductive learning. We\ndemonstrate that naive sample pooling strategies, effective in regression\nsettings, fail in Markov decision processes.To address this challenge, we\nintroduce a novel ``re-weighted targeting procedure'' to construct\n``transferable RL samples'' and propose ``transfer deep $Q^*$-learning'',\nenabling neural network approximation with theoretical guarantees. We assume\nthat the reward functions are transferable and deal with both situations in\nwhich the transition densities are transferable or nontransferable. Our\nanalytical techniques for transfer learning in neural network approximation and\ntransition density transfers have broader implications, extending to supervised\ntransfer learning with neural networks and domain shift scenarios. Empirical\nexperiments on both synthetic and real datasets corroborate the advantages of\nour method, showcasing its potential for improving decision-making through\nstrategically constructing transferable RL samples in non-stationary\nreinforcement learning contexts.\n","authors":["Jinhang Chai","Elynn Chen","Jianqing Fan"],"pdf_url":"https://arxiv.org/pdf/2501.04870v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08916v1","updated":"2025-04-11T18:47:16Z","published":"2025-04-11T18:47:16Z","title":"Improving the evaluation of samplers on multi-modal targets","summary":"  Addressing multi-modality constitutes one of the major challenges of\nsampling. In this reflection paper, we advocate for a more systematic\nevaluation of samplers towards two sources of difficulty that are mode\nseparation and dimension. For this, we propose a synthetic experimental setting\nthat we illustrate on a selection of samplers, focusing on the challenging\ncriterion of recovery of the mode relative importance. These evaluations are\ncrucial to diagnose the potential of samplers to handle multi-modality and\ntherefore to drive progress in the field.\n","authors":["Louis Grenioux","Maxence Noble","Marylou Gabrié"],"pdf_url":"https://arxiv.org/pdf/2504.08916v1.pdf","comment":"Accepted at ICLR 2025 workshop \"Frontiers in Probabilistic Inference:\n  Learning meets Sampling\""},{"id":"http://arxiv.org/abs/2504.08867v1","updated":"2025-04-11T10:43:17Z","published":"2025-04-11T10:43:17Z","title":"In almost all shallow analytic neural network optimization landscapes,\n  efficient minimizers have strongly convex neighborhoods","summary":"  Whether or not a local minimum of a cost function has a strongly convex\nneighborhood greatly influences the asymptotic convergence rate of optimizers.\nIn this article, we rigorously analyze the prevalence of this property for the\nmean squared error induced by shallow, 1-hidden layer neural networks with\nanalytic activation functions when applied to regression problems. The\nparameter space is divided into two domains: the 'efficient domain' (all\nparameters for which the respective realization function cannot be generated by\na network having a smaller number of neurons) and the 'redundant domain' (the\nremaining parameters). In almost all regression problems on the efficient\ndomain the optimization landscape only features local minima that are strongly\nconvex. Formally, we will show that for certain randomly picked regression\nproblems the optimization landscape is almost surely a Morse function on the\nefficient domain. The redundant domain has significantly smaller dimension than\nthe efficient domain and on this domain, potential local minima are never\nisolated.\n","authors":["Felix Benning","Steffen Dereich"],"pdf_url":"https://arxiv.org/pdf/2504.08867v1.pdf","comment":null}],"Optimization":[{"id":"http://arxiv.org/abs/2504.08722v1","updated":"2025-04-11T17:36:05Z","published":"2025-04-11T17:36:05Z","title":"Deriving the Gradients of Some Popular Optimal Transport Algorithms","summary":"  In this note, I review entropy-regularized Monge-Kantorovich problem in\nOptimal Transport, and derive the gradients of several popular algorithms\npopular in Computational Optimal Transport, including the Sinkhorn algorithms,\nWasserstein Barycenter algorithms, and the Wasserstein Dictionary Learning\nalgorithms.\n","authors":["Fangzhou Xie"],"pdf_url":"https://arxiv.org/pdf/2504.08722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08721v1","updated":"2025-04-11T17:35:58Z","published":"2025-04-11T17:35:58Z","title":"Surrogate-based optimization of system architectures subject to hidden\n  constraints","summary":"  The exploration of novel architectures requires physics-based simulation due\nto a lack of prior experience to start from, which introduces two specific\nchallenges for optimization algorithms: evaluations become more expensive (in\ntime) and evaluations might fail. The former challenge is addressed by\nSurrogate-Based Optimization (SBO) algorithms, in particular Bayesian\nOptimization (BO) using Gaussian Process (GP) models. An overview is provided\nof how BO can deal with challenges specific to architecture optimization, such\nas design variable hierarchy and multiple objectives: specific measures include\nensemble infills and a hierarchical sampling algorithm. Evaluations might fail\ndue to non-convergence of underlying solvers or infeasible geometry in certain\nareas of the design space. Such failed evaluations, also known as hidden\nconstraints, pose a particular challenge to SBO/BO, as the surrogate model\ncannot be trained on empty results. This work investigates various strategies\nfor satisfying hidden constraints in BO algorithms. Three high-level strategies\nare identified: rejection of failed points from the training set, replacing\nfailed points based on viable (non-failed) points, and predicting the failure\nregion. Through investigations on a set of test problems including a jet engine\narchitecture optimization problem, it is shown that best performance is\nachieved with a mixed-discrete GP to predict the Probability of Viability\n(PoV), and by ensuring selected infill points satisfy some minimum PoV\nthreshold. This strategy is demonstrated by solving a jet engine architecture\nproblem that features at 50% failure rate and could not previously be solved by\na BO algorithm. The developed BO algorithm and used test problems are available\nin the open-source Python library SBArchOpt.\n","authors":["Jasper Bussemaker","Paul Saves","Nathalie Bartoli","Thierry Lefebvre","Björn Nagel"],"pdf_url":"https://arxiv.org/pdf/2504.08721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08682v1","updated":"2025-04-11T16:43:11Z","published":"2025-04-11T16:43:11Z","title":"Bayesian optimization for mixed variables using an adaptive dimension\n  reduction process: applications to aircraft design","summary":"  Multidisciplinary design optimization methods aim at adapting numerical\noptimization techniques to the design of engineering systems involving multiple\ndisciplines. In this context, a large number of mixed continuous, integer and\ncategorical variables might arise during the optimization process and practical\napplications involve a large number of design variables. Recently, there has\nbeen a growing interest in mixed variables constrained Bayesian optimization\nbut most existing approaches severely increase the number of the\nhyperparameters related to the surrogate model. In this paper, we address this\nissue by constructing surrogate models using less hyperparameters. The\nreduction process is based on the partial least squares method. An adaptive\nprocedure for choosing the number of hyperparameters is proposed. The\nperformance of the proposed approach is confirmed on analytical tests as well\nas two real applications related to aircraft design. A significant improvement\nis obtained compared to genetic algorithms.\n","authors":["Paul Saves","Nathalie Bartoli","Youssef Diouane","Thierry Lefebvre","Joseph Morlier","Christophe David","Eric Nguyen Van","Sébastien Defoort"],"pdf_url":"https://arxiv.org/pdf/2504.08682v1.pdf","comment":"AIAA SciTech 2022 Forum. arXiv admin note: substantial text overlap\n  with arXiv:2402.04711"},{"id":"http://arxiv.org/abs/2503.15963v5","updated":"2025-04-11T16:27:35Z","published":"2025-03-20T09:00:10Z","title":"Stability of Schrödinger bridges and Sinkhorn semigroups for\n  log-concave models","summary":"  In this article we obtain several new results and developments in the study\nof entropic optimal transport problems (a.k.a. Schr\\\"odinger problems) with\ngeneral reference distributions and log-concave target marginal measures. Our\napproach combines transportation cost inequalities\n  with the theory of Riccati matrix difference equations arising in filtering\nand optimal control theory. This methodology is partly based on a novel\nentropic stability of Schr\\\"odinger bridges and closed form expressions of a\nclass of discrete time algebraic Riccati equations. In the context of\nregularized entropic transport these techniques provide new sharp entropic map\nestimates. When applied to the stability of Sinkhorn semigroups, they also\nyield\n  a series of novel contraction estimates in terms of the fixed point of\nRiccati equations.\n  The strength of our approach is that it is applicable to a large class of\nmodels arising in machine learning and artificial intelligence algorithms. We\nillustrate the impact of our results in the context of regularized entropic\ntransport, proximal samplers and diffusion generative models as well as\ndiffusion flow matching models\n","authors":["Pierre Del Moral"],"pdf_url":"https://arxiv.org/pdf/2503.15963v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08622v1","updated":"2025-04-11T15:25:57Z","published":"2025-04-11T15:25:57Z","title":"Optimal selection of the most informative nodes for a noisy DeGroot\n  model with stubborn agents","summary":"  Finding the optimal subset of individuals to observe in order to obtain the\nbest estimate of the average opinion of a society is a crucial problem in a\nwide range of applications, including policy-making, strategic business\ndecisions, and the analysis of sociological trends. We consider the opinion\nvector X to be updated according to a DeGroot opinion dynamical model with\nstubborn agents, subject to perturbations from external random noise, which can\nbe interpreted as transmission errors. The objective function of the\noptimization problem is the variance reduction achieved by observing the\nequilibrium opinions of a subset K of agents. We demonstrate that, under this\nspecific setting, the objective function exhibits the property of\nsubmodularity. This allows us to effectively design a Greedy Algorithm to solve\nthe problem, significantly reducing its computational complexity. Simple\nexamples are provided to validate our results.\n","authors":["Roberta Raineri","Giacomo Como","Fabio Fagnani"],"pdf_url":"https://arxiv.org/pdf/2504.08622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08545v1","updated":"2025-04-11T13:57:12Z","published":"2025-04-11T13:57:12Z","title":"Optimal Mode Decomposition for Control","summary":"  We present an extension of optimal mode decomposition (OMD) for autonomous\nsystems to systems with controls. The extension is developed along the same\nlines as the extension of dynamic mode decomposition (DMD) to DMD with control\n(DMDc).\n  DMD identifies a linear dynamic system from high-dimensional snapshot data.\nDMD is often combined with a subsequent reduction by a projection to a\ntruncated basis for the space spanned by the snapshots. In OMD, the\nidentification and reduction are essentially integrated into a single\noptimization step, thus avoiding the somewhat adhoc decoupled, a posteriori\nreduction that is necessary if DMD is to be used for model reduction. DMD was\ndevised for autonomous systems and later extended to DMD for systems with\ncontrol inputs (DMDc). We present the analogous extension of OMD to OMDc, i.e.\nOMD for systems with control inputs. We illustrate the proposed method with an\napplication to coupled diffusion-equations that model the drying of a wood\nchip. Reduced models of this type are required for the efficient simulation of\nindustrial drying processes.\n","authors":["Lucas Mieg","Martin Mönnigmann"],"pdf_url":"https://arxiv.org/pdf/2504.08545v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.20264v2","updated":"2025-04-11T13:40:59Z","published":"2025-02-27T16:52:28Z","title":"Exponential convergence of general iterative proportional fitting\n  procedures","summary":"  Motivated by the success of Sinkhorn's algorithm for entropic optimal\ntransport, we study convergence properties of iterative proportional fitting\nprocedures (IPFP) used to solve more general information projection problems.\nWe establish exponential convergence guarantees for the IPFP whenever the set\nof probability measures which is projected onto is defined through constraints\narising from linear function spaces. This unifies and extends recent results\nfrom multi-marginal, adapted and martingale optimal transport. The proofs are\nbased on strong convexity arguments for the dual problem, and the key\ncontribution is to illuminate the role of the geometric interplay between the\nsubspaces defining the constraints. In this regard, we show that the larger the\nangle (in the sense of Friedrichs) between the linear function spaces, the\nbetter the rate of contraction of the IPFP.\n","authors":["Stephan Eckstein","Aziz Lakhal"],"pdf_url":"https://arxiv.org/pdf/2502.20264v2.pdf","comment":"Added Remark 4.5 and revised Section 4.2"},{"id":"http://arxiv.org/abs/2504.08506v1","updated":"2025-04-11T13:18:32Z","published":"2025-04-11T13:18:32Z","title":"Controlled stochastic processes for simulated annealing","summary":"  Simulated annealing solves global optimization problems by means of a random\nwalk in a cooling energy landscape based on the objective function and a\ntemperature parameter. However, if the temperature is decreased too quickly,\nthis procedure often gets stuck in suboptimal local minima. In this work, we\nconsider the cooling landscape as a curve of probability measures. We prove the\nexistence of a minimal norm velocity field which solves the continuity\nequation, a differential equation that governs the evolution of the\naforementioned curve. The solution is the weak gradient of an integrable\nfunction, which is in line with the interpretation of the velocity field as a\nderivative of optimal transport maps. We show that controlling stochastic\nannealing processes by superimposing this velocity field would allow them to\nfollow arbitrarily fast cooling schedules. Here we consider annealing processes\nbased on diffusions and piecewise deterministic Markov processes. Based on\nconvergent optimal transport-based approximations to this control, we design a\nnovel interacting particle--based optimization method that accelerates\nannealing. We validate this accelerating behaviour in numerical experiments.\n","authors":["Vincent Molin","Axel Ringh","Moritz Schauer","Akash Sharma"],"pdf_url":"https://arxiv.org/pdf/2504.08506v1.pdf","comment":"35 pages, 10 figures"},{"id":"http://arxiv.org/abs/2504.08503v1","updated":"2025-04-11T13:09:48Z","published":"2025-04-11T13:09:48Z","title":"Sectoral and spatial decomposition methods for multi-sector capacity\n  expansion models","summary":"  Multi-sector capacity expansion models play a crucial role in energy planning\nby providing decision support for policymaking in technology development. To\nensure reliable support, these models require high technological, spatial, and\ntemporal resolution, leading to large-scale linear programming problems that\nare often computationally intractable. To address this challenge, conventional\napproaches rely on simplifying abstractions that trade accuracy for\ncomputational efficiency. Benders decomposition has been widely explored to\nimprove computational efficiency in electricity capacity expansion models.\nSpecifically, state-of-the-art methods have primarily focused on improving\nperformance through temporal decomposition. However, multi-sector models\nintroduce additional complexity, requiring new decomposition strategies. In\nthis work, we propose a budget-based formulation to extend decomposition to the\nsectoral and spatial domains. We test the developed sectoral and spatial\nBenders decomposition algorithms on case studies of the continental United\nStates, considering different configurations in terms of spatial and temporal\nresolution. Results show that our algorithms achieve substantial performance\nimprovement compared to existing decomposition algorithms, with runtime\nreductions within 15%-70%. The proposed methods leverage the generic structure\nof multi-sector capacity expansion models, and can thus be applied to most\nexisting energy planning models, ensuring computational tractability without\nsacrificing resolution.\n","authors":["Federico Parolin","Yu Weng","Paolo Colbertaldo","Ruaridh Macdonald"],"pdf_url":"https://arxiv.org/pdf/2504.08503v1.pdf","comment":"Submitted to Elsevier for possible publication"},{"id":"http://arxiv.org/abs/2504.08493v1","updated":"2025-04-11T12:47:04Z","published":"2025-04-11T12:47:04Z","title":"SME Gender-Related Innovation: A Non-Numerical Trend Analysis Using\n  Positive, Zero, and Negative Quantities","summary":"  This paper addresses gender-related aspects of innovation processes in Small\nand Medium Enterprises (SMEs). Classical analytical and statistical approaches\noften struggle with the high complexity and insufficient data typical of\ngender-related innovation studies. We propose a trend-based modelling framework\nthat requires minimal information and uses non-numerical quantifiers:\nincreasing, constant, and decreasing. This approach enables the analysis of\nten-dimensional models including variables such as Gender, Product Innovation,\nProcess Innovation, and High-Risk Tolerance. Using trend-based artificial\nintelligence methods, we identify 13 distinct scenarios and all possible\ntransitions between them. This allows for the evaluation of queries like: Can\nexports increase while gender parameters remain constant? Two versions of the\nGASI trend model are presented: the original and an expert-modified version\naddressing critiques related to scenario transitions. The final model confirms\nstability and supports the assumption that \"no tree grows to heaven.\"\nTrend-based modelling offers a practical, interpretable alternative for\ncomplex, data-scarce systems.\n","authors":["Nina Bočková","Barbora Volná","Mirko Dohnal"],"pdf_url":"https://arxiv.org/pdf/2504.08493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11382v3","updated":"2025-04-11T12:36:35Z","published":"2025-01-20T10:19:37Z","title":"Global Regularity Estimates for Optimal Transport via Entropic\n  Regularisation","summary":"  We develop a general approach to prove global regularity estimates for\nquadratic optimal transport using the entropic regularisation of the problem.\n","authors":["Nathael Gozlan","Maxime Sylvestre"],"pdf_url":"https://arxiv.org/pdf/2501.11382v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08484v1","updated":"2025-04-11T12:19:51Z","published":"2025-04-11T12:19:51Z","title":"Physics-informed data-driven control without persistence of excitation","summary":"  We show that data that is not sufficiently informative to allow for system\nre-identification can still provide meaningful information when combined with\nexternal or physical knowledge of the system, such as bounded system matrix\nnorms. We then illustrate how this information can be leveraged for safety and\nenergy minimization problems and to enhance predictions in unmodelled dynamics.\nThis preliminary work outlines key ideas toward using limited data for\neffective control by integrating physical knowledge of the system and\nexploiting interpolation conditions.\n","authors":["Martina Vanelli","Julien M. Hendrickx"],"pdf_url":"https://arxiv.org/pdf/2504.08484v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2402.04613v4","updated":"2025-04-11T12:00:09Z","published":"2024-02-07T06:30:39Z","title":"Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in\n  Reproducing Kernel Hilbert Spaces","summary":"  Commonly used $f$-divergences of measures, e.g., the Kullback-Leibler\ndivergence, are subject to limitations regarding the support of the involved\nmeasures. A remedy is regularizing the $f$-divergence by a squared maximum mean\ndiscrepancy (MMD) associated with a characteristic kernel $K$. We use the\nkernel mean embedding to show that this regularization can be rewritten as the\nMoreau envelope of some function on the associated reproducing kernel Hilbert\nspace. Then, we exploit well-known results on Moreau envelopes in Hilbert\nspaces to analyze the MMD-regularized $f$-divergences, particularly their\ngradients. Subsequently, we use our findings to analyze Wasserstein gradient\nflows of MMD-regularized $f$-divergences. We provide proof-of-the-concept\nnumerical examples for flows starting from empirical measures. Here, we cover\n$f$-divergences with infinite and finite recession constants. Lastly, we extend\nour results to the tight variational formulation of $f$-divergences and\nnumerically compare the resulting flows.\n","authors":["Viktor Stein","Sebastian Neumayer","Nicolaj Rux","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2402.04613v4.pdf","comment":"56 pages, 14 figures, 3 tables. Comments welcome! NEW: Incorporated\n  Reviewers' suggestions, added FISTA and tight formulation, typos fixed"},{"id":"http://arxiv.org/abs/2412.02781v3","updated":"2025-04-11T10:52:32Z","published":"2024-12-03T19:20:56Z","title":"Methods with Local Steps and Random Reshuffling for Generally Smooth\n  Non-Convex Federated Optimization","summary":"  Non-convex Machine Learning problems typically do not adhere to the standard\nsmoothness assumption. Based on empirical findings, Zhang et al. (2020b)\nproposed a more realistic generalized $(L_0, L_1)$-smoothness assumption,\nthough it remains largely unexplored. Many existing algorithms designed for\nstandard smooth problems need to be revised. However, in the context of\nFederated Learning, only a few works address this problem but rely on\nadditional limiting assumptions. In this paper, we address this gap in the\nliterature: we propose and analyze new methods with local steps, partial\nparticipation of clients, and Random Reshuffling without extra restrictive\nassumptions beyond generalized smoothness. The proposed methods are based on\nthe proper interplay between clients' and server's stepsizes and gradient\nclipping. Furthermore, we perform the first analysis of these methods under the\nPolyak-{\\L} ojasiewicz condition. Our theory is consistent with the known\nresults for standard smooth problems, and our experimental results support the\ntheoretical insights.\n","authors":["Yury Demidovich","Petr Ostroukhov","Grigory Malinovsky","Samuel Horváth","Martin Takáč","Peter Richtárik","Eduard Gorbunov"],"pdf_url":"https://arxiv.org/pdf/2412.02781v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00928v3","updated":"2025-04-11T09:56:07Z","published":"2024-11-01T15:58:02Z","title":"A Bregman firmly nonexpansive proximal operator for baryconvex\n  optimization","summary":"  We present a generalization of the proximal operator defined through a convex\ncombination of convex objectives, where the coefficients are updated in a\nminimax fashion. We prove that this new operator is Bregman firmly nonexpansive\nwith respect to a Bregman divergence that combines Euclidean and information\ngeometries; and that its fixed points are given by the critical points of a\ncertain nonconvex function. Finally, we derive the associated continuous flows.\n","authors":["Mastane Achab"],"pdf_url":"https://arxiv.org/pdf/2411.00928v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18318v2","updated":"2025-04-11T09:09:42Z","published":"2024-11-27T13:15:50Z","title":"Scaled Relative Graph Analysis of Lur'e Systems and the Generalized\n  Circle Criterion","summary":"  Scaled Relative Graphs (SRGs) provide a novel graphical frequency-domain\nmethod for the analysis of nonlinear systems. However, we show that the current\nSRG analysis suffers from a pitfall that limit its applicability in analyzing\npractical nonlinear systems. We overcome this pitfall by modifying the SRG of a\nlinear time invariant operator, combining the SRG with the Nyquist criterion,\nand apply our result to Lur'e systems. We thereby obtain a generalization of\nthe celebrated circle criterion, which deals with a broader class of\nnonlinearities, and provides (incremental) $L_2$-gain performance bounds.\n","authors":["Julius P. J. Krebbekx","Roland Tóth","Amritam Das"],"pdf_url":"https://arxiv.org/pdf/2411.18318v2.pdf","comment":"6 pages, 5 figures, (to be) presented at the European Control\n  Conference 2025 and published in the corresponding proceedings"},{"id":"http://arxiv.org/abs/2504.08315v1","updated":"2025-04-11T07:36:43Z","published":"2025-04-11T07:36:43Z","title":"Annealed Mean Field Descent Is Highly Effective for Quadratic\n  Unconstrained Binary Optimization","summary":"  In recent years, formulating various combinatorial optimization problems as\nQuadratic Unconstrained Binary Optimization (QUBO) has gained significant\nattention as a promising approach for efficiently obtaining optimal or\nnear-optimal solutions. While QUBO offers a general-purpose framework, existing\nsolvers often struggle with performance variability across different problems.\n  This paper (i) theoretically analyzes Mean Field Annealing (MFA) and its\nvariants--which are representative QUBO solvers, and reveals that their\nunderlying self-consistent equations do not necessarily represent the minimum\ncondition of the Kullback-Leibler divergence between the mean-field\napproximated distribution and the exact distribution, and (ii) proposes a novel\nmethod, the Annealed Mean Field Descent (AMFD), which is designed to address\nthis limitation by directly minimizing the divergence.\n  Through extensive experiments on five benchmark combinatorial optimization\nproblems (Maximum Cut Problem, Maximum Independent Set Problem, Traveling\nSalesman Problem, Quadratic Assignment Problem, and Graph Coloring Problem), we\ndemonstrate that AMFD exhibits superior performance in many cases and reduced\nproblem dependence compared to state-of-the-art QUBO solvers and Gurobi--a\nstate-of-the-art versatile mathematical optimization solver not limited to\nQUBO.\n","authors":["Kyo Kuroki","Thiem Van Chu","Masato Motomura","Kazushi Kawamura"],"pdf_url":"https://arxiv.org/pdf/2504.08315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20237v3","updated":"2025-04-11T07:30:38Z","published":"2024-12-28T18:19:09Z","title":"Distributionally Robust Fault Detection Trade-off Design with Prior\n  Fault Information","summary":"  The robustness of fault detection algorithms against uncertainty is crucial\nin the real-world industrial environment. Recently, a new probabilistic design\nscheme called distributionally robust fault detection (DRFD) has emerged and\nreceived immense interest. Despite its robustness against unknown distributions\nin practice, current DRFD focuses on the overall detectability of all possible\nfaults rather than the detectability of critical faults that are a priori\nknown. Henceforth, a new DRFD trade-off design scheme is put forward in this\nwork by utilizing prior fault information. The key contribution includes a\nnovel distributional robustness metric of detecting a known fault and a new\nsoft distributionally robust chance constraint that ensures robust\ndetectability. Then a new trade-off design scheme of fault detection under\nunknown probability distributions is proposed, and this offers a flexible\nbalance between the robustness of detecting known critical faults and the\noverall detectability against all possible faults. To solve the resulting\nproblem, an exact reformulation is derived and a customized solution algorithm\nis developed, which includes a sequential optimization procedure and an\ninitialization strategy. Finally, case studies on a simulated three-tank system\nand a real-world battery cell are carried out to showcase the usefulness of our\nDRFD method.\n","authors":["Yulin Feng","Hailang Jin","Steven X. Ding","Hao Ye","Chao Shang"],"pdf_url":"https://arxiv.org/pdf/2412.20237v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08278v1","updated":"2025-04-11T06:18:46Z","published":"2025-04-11T06:18:46Z","title":"Interior Point Differential Dynamic Programming, Redux","summary":"  We present IPDDP2, a structure-exploiting algorithm for solving\ndiscrete-time, finite horizon optimal control problems with nonlinear\nconstraints. Inequality constraints are handled using a primal-dual interior\npoint formulation and step acceptance for equality constraints follows a\nline-search filter approach. The iterates of the algorithm are derived under\nthe Differential Dynamic Programming (DDP) framework. Our numerical experiments\nevaluate IPDDP2 on four robotic motion planning problems. IPDDP2 reliably\nconverges to low optimality error and exhibits local quadratic and global\nconvergence from remote starting points. Notably, we showcase the robustness of\nIPDDP2 by using it to solve a contact-implicit, joint limited acrobot swing-up\nproblem involving complementarity constraints from a range of initial\nconditions. We provide a full implementation of IPDDP2 in the Julia programming\nlanguage.\n","authors":["Ming Xu","Stephen Gould","Iman Shames"],"pdf_url":"https://arxiv.org/pdf/2504.08278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08265v1","updated":"2025-04-11T05:41:53Z","published":"2025-04-11T05:41:53Z","title":"Qualitative properties of solutions to a fractional pseudo-parabolic\n  equation with singular potential","summary":"  This paper investigates the initial boundary value problem for a fractional\npseudo-parabolic equation with singular potential. The global existence and\nblow-up of solutions to the initial boundary value problem are obtained at low\ninitial energy. Moreover, the exponential decay estimates for global solutions\nand energy functional are further derived, and the upper and lower bounds of\nboth blow-up time and blow-up rate for blow-up solutions are respectively\nestimated. Specifically, we extend the method for proving blow-up of solutions\nwith negative initial energy in previous literatures to cases involving\nnonnegative initial energy, which broadens the applicability of this method.\nFinally, for the corresponding stationary problem, the existence of\nground-state solutions is established, and it is proved that the global\nsolutions strongly converge to the solutions of stationary problem as time\ntends to infinity.\n","authors":["Xiang-kun Shao","Nan-jing Huang","Xue-song Li"],"pdf_url":"https://arxiv.org/pdf/2504.08265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08249v1","updated":"2025-04-11T04:14:55Z","published":"2025-04-11T04:14:55Z","title":"Neural Network-assisted Interval Reachability for Systems with Control\n  Barrier Function-Based Safe Controllers","summary":"  Control Barrier Functions (CBFs) have been widely utilized in the design of\noptimization-based controllers and filters for dynamical systems to ensure\nforward invariance of a given set of safe states. While CBF-based controllers\noffer safety guarantees, they can compromise the performance of the system,\nleading to undesirable behaviors such as unbounded trajectories and emergence\nof locally stable spurious equilibria. Computing reachable sets for systems\nwith CBF-based controllers is an effective approach for runtime performance and\nstability verification, and can potentially serve as a tool for trajectory\nre-planning. In this paper, we propose a computationally efficient interval\nreachability method for performance verification of systems with\noptimization-based controllers by: (i) approximating the optimization-based\ncontroller by a pre-trained neural network to avoid solving optimization\nproblems repeatedly, and (ii) using mixed monotone theory to construct an\nembedding system that leverages state-of-the-art neural network verification\nalgorithms for bounding the output of the neural network. Results in terms of\ncloseness of solutions of trajectories of the system with the\noptimization-based controller and the neural network are derived. Using a\nsingle trajectory of the embedding system along with our closeness of solutions\nresult, we obtain an over-approximation of the reachable set of the system with\noptimization-based controllers. Numerical results are presented to corroborate\nthe technical findings.\n","authors":["Damola Ajeyemi","Saber Jafarpour","Emiliano Dall'Anese"],"pdf_url":"https://arxiv.org/pdf/2504.08249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05865v2","updated":"2025-04-11T03:45:14Z","published":"2024-05-09T15:53:43Z","title":"Faster Linear Systems and Matrix Norm Approximation via Multi-level\n  Sketched Preconditioning","summary":"  We present a new class of preconditioned iterative methods for solving linear\nsystems of the form $Ax = b$. Our methods are based on constructing a low-rank\nNystr\\\"om approximation to $A$ using sparse random matrix sketching. This\napproximation is used to construct a preconditioner, which itself is inverted\nquickly using additional levels of random sketching and preconditioning. We\nprove that the convergence of our methods depends on a natural average\ncondition number of $A$, which improves as the rank of the Nystr\\\"om\napproximation increases. Concretely, this allows us to obtain faster runtimes\nfor a number of fundamental linear algebraic problems:\n  1. We show how to solve any $n\\times n$ linear system that is\nwell-conditioned except for $k$ outlying large singular values in\n$\\tilde{O}(n^{2.065} + k^\\omega)$ time, improving on a recent result of\n[Derezi\\'nski, Yang, STOC 2024] for all $k \\gtrsim n^{0.78}$.\n  2. We give the first $\\tilde{O}(n^2 + {d_\\lambda}^{\\omega}$) time algorithm\nfor solving a regularized linear system $(A + \\lambda I)x = b$, where $A$ is\npositive semidefinite with effective dimension\n$d_\\lambda=\\mathrm{tr}(A(A+\\lambda I)^{-1})$. This problem arises in\napplications like Gaussian process regression.\n  3. We give faster algorithms for approximating Schatten $p$-norms and other\nmatrix norms. For example, for the Schatten 1-norm (nuclear norm), we give an\nalgorithm that runs in $\\tilde{O}(n^{2.11})$ time, improving on an\n$\\tilde{O}(n^{2.18})$ method of [Musco et al., ITCS 2018]. All results are\nproven in the real RAM model of computation. Interestingly, previous\nstate-of-the-art algorithms for most of the problems above relied on stochastic\niterative methods, like stochastic coordinate and gradient descent. Our work\ntakes a completely different approach, instead leveraging tools from matrix\nsketching.\n","authors":["Michał Dereziński","Christopher Musco","Jiaming Yang"],"pdf_url":"https://arxiv.org/pdf/2405.05865v2.pdf","comment":"SODA 2025"},{"id":"http://arxiv.org/abs/2504.08223v1","updated":"2025-04-11T03:11:51Z","published":"2025-04-11T03:11:51Z","title":"Stochastic Momentum ADMM for nonconvex and nonsmooth optimization with\n  application to PnP algorithm","summary":"  This paper introduces a single-loop Stochastic Momentum Alternating Direction\nMethod of Multipliers (SMADMM) for tackling a class of nonconvex and nonsmooth\noptimization problems. We establish that SMADMM achieves an optimal oracle\ncomplexity of $\\mathcal{O}(\\epsilon^{-\\frac{3}{2}})$ in the online setting,\nwhere only stochastic first-order oracle, is available. In particular, SMADMM\nrequires only $\\mathcal{O}(1)$ stochastic gradient evaluations per iteration\nand avoids the need for restarting with large batch gradient estimates. This is\nthe first stochastic ADMM method achieving optimal oracle complexity for\nnonconvex and nonsmooth problems, requiring $\\mathcal{O}(1)$ batch size.\nFurthermore, we extend our method by integrating it with plug-and-play (PnP)\npriors, resulting in the PnP-SMADMM algorithm. Numerical experiments on\nclassification, CT image reconstruction and phase retrieve demonstrate the\npractical effectiveness of our approach and validate the theoretical findings.\n","authors":["Kangkang Deng","Shuchang Zhang","Boyu Wang","Jiachen Jin","Juan Zhou","Hongxia Wang"],"pdf_url":"https://arxiv.org/pdf/2504.08223v1.pdf","comment":"28 Pages"},{"id":"http://arxiv.org/abs/2503.09490v2","updated":"2025-04-11T02:57:02Z","published":"2025-03-12T15:49:31Z","title":"Sequential Quadratic Optimization for Solving Expectation Equality\n  Constrained Stochastic Optimization Problems","summary":"  A sequential quadratic programming method is designed for solving general\nsmooth nonlinear stochastic optimization problems subject to expectation\nequality constraints. We consider the setting where the objective and\nconstraint function values, as well as their derivatives, are not directly\navailable. The algorithm applies an adaptive step size policy and only relies\non objective gradient estimates, constraint function estimates, and constraint\nderivative estimates to update iterates. Both asymptotic and non-asymptotic\nconvergence properties of the algorithm are analyzed. Under reasonable\nassumptions, the algorithm generates a sequence of iterates whose first-order\nstationary measure diminishes in expectation. In addition, we identify the\niteration and sample complexity for obtaining a first-order\n$\\varepsilon$-stationary iterate in expectation. The results of numerical\nexperiments demonstrate the efficiency and efficacy of our proposed algorithm\ncompared to a penalty method and an augmented Lagrangian method.\n","authors":["Haoming Shen","Yang Zeng","Baoyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.09490v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02377v4","updated":"2025-04-11T01:56:37Z","published":"2023-04-05T11:30:57Z","title":"A Budget-Adaptive Allocation Rule for Optimal Computing Budget\n  Allocation","summary":"  Simulation-based ranking and selection (R&S) is a popular technique for\noptimizing discrete-event systems (DESs). It evaluates the mean performance of\nsystem designs by simulation outputs and aims to identify the best system\ndesign from a set of alternatives by intelligently allocating a limited\nsimulation budget. In R&S, the optimal computing budget allocation (OCBA) is an\nefficient budget allocation rule that asymptotically maximizes the probability\nof correct selection (PCS). In this paper, we first show the asymptotic OCBA\nrule can be recovered by considering a large-scale problem with a specific\nlarge budget. Considering a sufficiently large budget can greatly simplify\ncomputations, but it also causes the asymptotic OCBA rule ignoring the impact\nof budget. To address this, we then derive a budget-adaptive rule under the\nsetting where budget is not large enough to simplify computations. The proposed\nbudget-adaptive rule determines the ratio of total budget allocated to designs\nbased on the budget size, and its budget-adaptive property highlights the\nsignificant impact of budget on allocation strategy. Based on the proposed\nbudget-adaptive rule, two heuristic algorithms are developed. In the numerical\nexperiments, the superior efficiency of our proposed allocation rule is shown.\n","authors":["Zirui Cao","Haowei Wang","Ek Peng Chew","Haobin Li","Kok Choon Tan"],"pdf_url":"https://arxiv.org/pdf/2304.02377v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08191v1","updated":"2025-04-11T01:12:59Z","published":"2025-04-11T01:12:59Z","title":"Optimal protection and vaccination against epidemics with reinfection\n  risk","summary":"  We consider the problem of optimal allocation of vaccination and protection\nmeasures for the Susceptible-Infected-Recovered-Infected (SIRI) epidemiological\nmodel, which generalizes the classical Susceptible-Infected-Recovered (SIR) and\nSusceptible-Infected-Susceptible (SIS) epidemiological models by allowing for\nreinfection. We first introduce the controlled SIRI dynamics, and discuss the\nexistence and stability of the equilibrium points. We then formulate a\nfinite-horizon optimal control problem where the cost of vaccination and\nprotection is proportional to the mass of population that adopts it. Our main\ncontribution in this work arises from a detailed investigation into the\nexistence/non-existence of singular control inputs, and establishing optimality\nof bang-bang controls, obtained by solving an optimal control problem\nconsidering a running cost that is linear with respect to the input variables\nof limited non-pharmaceutical and medical resources, in an epidemic model with\nreinfection risk and compromised immunity. In contrast to most prior works, we\nrigorously establish the non-existence of singular controls, i.e., the\noptimality of bang-bang control. Under some reasonable conditions, we\ncharacterize the structure of both the optimal control inputs, and also that\nvaccination control input admits a bang-bang structure. Numerical results\nprovide valuable insights into the evolution of the disease spread under\noptimal control inputs.\n","authors":["Urmee Maitra","Ashish R. Hota","Rohit Gupta","Alfred O. Hero"],"pdf_url":"https://arxiv.org/pdf/2504.08191v1.pdf","comment":"20 pages, 2 figures"},{"id":"http://arxiv.org/abs/2504.08178v1","updated":"2025-04-11T00:20:37Z","published":"2025-04-11T00:20:37Z","title":"A Piecewise Lyapunov Analysis of sub--quadratic SGD: Applications to\n  Robust and Quantile Regression","summary":"  Motivated by robust and quantile regression problems, {we investigate the\nstochastic gradient descent (SGD) algorithm} for minimizing an objective\nfunction $f$ that is locally strongly convex with a sub--quadratic tail. This\nsetting covers many widely used online statistical methods. We introduce a\nnovel piecewise Lyapunov function that enables us to handle functions $f$ with\nonly first-order differentiability, which includes a wide range of popular loss\nfunctions such as Huber loss. Leveraging our proposed Lyapunov function, we\nderive finite-time moment bounds under general diminishing stepsizes, as well\nas constant stepsizes. We further establish the weak convergence, central limit\ntheorem and bias characterization under constant stepsize, providing the first\ngeometrical convergence result for sub--quadratic SGD. Our results have wide\napplications, especially in online statistical methods. In particular, we\ndiscuss two applications of our results. 1) Online robust regression: We\nconsider a corrupted linear model with sub--exponential covariates and\nheavy--tailed noise. Our analysis provides convergence rates comparable to\nthose for corrupted models with Gaussian covariates and noise. 2) Online\nquantile regression: Importantly, our results relax the common assumption in\nprior work that the conditional density is continuous and provide a more\nfine-grained analysis for the moment bounds.\n","authors":["Yixuan Zhang"," Dongyan"," Huo","Yudong Chen","Qiaomin Xie"],"pdf_url":"https://arxiv.org/pdf/2504.08178v1.pdf","comment":"ACM SIGMETRICS 2025. 40 pages, 12 figures"},{"id":"http://arxiv.org/abs/2403.19186v2","updated":"2025-04-11T23:40:04Z","published":"2024-03-28T07:30:28Z","title":"Optimization hardness constrains ecological transients","summary":"  Living systems operate far from equilibrium, yet few general frameworks\nprovide global bounds on biological transients. In high-dimensional biological\nnetworks like ecosystems, long transients arise from the separate timescales of\ninteractions within versus among subcommunities. Here, we use tools from\ncomputational complexity theory to frame equilibration in complex ecosystems as\nthe process of solving an analogue optimization problem. We show that\nfunctional redundancies among species in an ecosystem produce difficult,\nill-conditioned problems, which physically manifest as transient chaos. We find\nthat the recent success of dimensionality reduction methods in describing\necological dynamics arises due to preconditioning, in which fast relaxation\ndecouples from slow solving timescales. In evolutionary simulations, we show\nthat selection for steady-state species diversity produces ill-conditioning, an\neffect quantifiable using scaling relations originally derived for numerical\nanalysis of complex optimization problems. Our results demonstrate the physical\ntoll of computational constraints on biological dynamics.\n","authors":["William Gilpin"],"pdf_url":"https://arxiv.org/pdf/2403.19186v2.pdf","comment":"9 pages, 7 figures, plus Appendix. Accepted at PLOS Comp Biol"},{"id":"http://arxiv.org/abs/2504.09008v1","updated":"2025-04-11T23:23:49Z","published":"2025-04-11T23:23:49Z","title":"A Linear and Scalable Cutting-Plane Algorithm for Electricity Pricing","summary":"  We propose a linear cutting-plane pricing algorithm tailored for large-scale\nelectricity markets, addressing nonconvexities arising from the Alternating\nCurrent Optimal Power Flow equations. We benchmark our algorithm against a\nDirect Current (DC) approximation and the Jabr Second-Order Cone (SOC)\nrelaxation under both the Integer Programming and Convex Hull pricing rules. We\nprovide numerical results for a small (617-bus) and three large ($\\geq\n15,000$-bus) networks. Our algorithm yields price signals very close to the\nJabr SOC, with computation times comparable to DC once we allow for\nwarm-starts, including scenarios with line contingencies.\n","authors":["Matías Romero","Felipe Verástegui","Matías Villagra"],"pdf_url":"https://arxiv.org/pdf/2504.09008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08911v1","updated":"2025-04-11T18:26:40Z","published":"2025-04-11T18:26:40Z","title":"Low-Rank Tensor Recovery via Theta Nuclear p-Norm","summary":"  We investigate the low-rank tensor recovery problem using a relaxation of the\nnuclear p-norm by theta bodies.\n  We provide algebraic descriptions of the norms and compute their Gr\\\"obner\nbases.\n  Moreover, we develop geometric properties of these bodies.\n  Finally, our numerical results suggest that for\n  $n\\times\\cdots\\times n$ tensors,\n  $m\\geq O(n)$ measurements should be sufficient to recover low-rank tensors\nvia theta body relaxation.\n","authors":["Felix Röhrich","Yuhuai Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.08911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02332v2","updated":"2025-04-11T18:06:15Z","published":"2025-02-04T14:09:00Z","title":"Coreset-Based Task Selection for Sample-Efficient Meta-Reinforcement\n  Learning","summary":"  We study task selection to enhance sample efficiency in model-agnostic\nmeta-reinforcement learning (MAML-RL). Traditional meta-RL typically assumes\nthat all available tasks are equally important, which can lead to task\nredundancy when they share significant similarities. To address this, we\npropose a coreset-based task selection approach that selects a weighted subset\nof tasks based on how diverse they are in gradient space, prioritizing the most\ninformative and diverse tasks. Such task selection reduces the number of\nsamples needed to find an $\\epsilon$-close stationary solution by a factor of\nO(1/$\\epsilon$). Consequently, it guarantees a faster adaptation to unseen\ntasks while focusing training on the most relevant tasks. As a case study, we\nincorporate task selection to MAML-LQR (Toso et al., 2024b), and prove a sample\ncomplexity reduction proportional to O(log(1/$\\epsilon$)) when the task\nspecific cost also satisfy gradient dominance. Our theoretical guarantees\nunderscore task selection as a key component for scalable and sample-efficient\nmeta-RL. We numerically validate this trend across multiple RL benchmark\nproblems, illustrating the benefits of task selection beyond the LQR baseline.\n","authors":["Donglin Zhan","Leonardo F. Toso","James Anderson"],"pdf_url":"https://arxiv.org/pdf/2502.02332v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2504.08730v1","updated":"2025-04-11T17:56:52Z","published":"2025-04-11T17:56:52Z","title":"Dimension reduction for derivative-informed operator learning: An\n  analysis of approximation errors","summary":"  We study the derivative-informed learning of nonlinear operators between\ninfinite-dimensional separable Hilbert spaces by neural networks. Such\noperators can arise from the solution of partial differential equations (PDEs),\nand are used in many simulation-based outer-loop tasks in science and\nengineering, such as PDE-constrained optimization, Bayesian inverse problems,\nand optimal experimental design. In these settings, the neural network\napproximations can be used as surrogate models to accelerate the solution of\nthe outer-loop tasks. However, since outer-loop tasks in infinite dimensions\noften require knowledge of the underlying geometry, the approximation accuracy\nof the operator's derivatives can also significantly impact the performance of\nthe surrogate model. Motivated by this, we analyze the approximation errors of\nneural operators in Sobolev norms over infinite-dimensional Gaussian input\nmeasures. We focus on the reduced basis neural operator (RBNO), which uses\nlinear encoders and decoders defined on dominant input/output subspaces spanned\nby reduced sets of orthonormal bases. To this end, we study two methods for\ngenerating the bases; principal component analysis (PCA) and\nderivative-informed subspaces (DIS), which use the dominant eigenvectors of the\ncovariance of the data or the derivatives as the reduced bases, respectively.\nWe then derive bounds for errors arising from both the dimension reduction and\nthe latent neural network approximation, including the sampling errors\nassociated with the empirical estimation of the PCA/DIS. Our analysis is\nvalidated on numerical experiments with elliptic PDEs, where our results show\nthat bases informed by the map (i.e., DIS or output PCA) yield accurate\nreconstructions and generalization errors for both the operator and its\nderivatives, while input PCA may underperform unless ranks and training sample\nsizes are sufficiently large.\n","authors":["Dingcheng Luo","Thomas O'Leary-Roseberry","Peng Chen","Omar Ghattas"],"pdf_url":"https://arxiv.org/pdf/2504.08730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08729v1","updated":"2025-04-11T17:56:09Z","published":"2025-04-11T17:56:09Z","title":"Steering CLIP's vision transformer with sparse autoencoders","summary":"  While vision models are highly capable, their internal mechanisms remain\npoorly understood -- a challenge which sparse autoencoders (SAEs) have helped\naddress in language, but which remains underexplored in vision. We address this\ngap by training SAEs on CLIP's vision transformer and uncover key differences\nbetween vision and language processing, including distinct sparsity patterns\nfor SAEs trained across layers and token types. We then provide the first\nsystematic analysis on the steerability of CLIP's vision transformer by\nintroducing metrics to quantify how precisely SAE features can be steered to\naffect the model's output. We find that 10-15\\% of neurons and features are\nsteerable, with SAEs providing thousands more steerable features than the base\nmodel. Through targeted suppression of SAE features, we then demonstrate\nimproved performance on three vision disentanglement tasks (CelebA, Waterbirds,\nand typographic attacks), finding optimal disentanglement in middle model\nlayers, and achieving state-of-the-art performance on defense against\ntypographic attacks.\n","authors":["Sonia Joseph","Praneet Suresh","Ethan Goldfarb","Lorenz Hufe","Yossi Gandelsman","Robert Graham","Danilo Bzdok","Wojciech Samek","Blake Aaron Richards"],"pdf_url":"https://arxiv.org/pdf/2504.08729v1.pdf","comment":"8 pages, 7 figures. Accepted to the CVPR 2025 Workshop on Mechanistic\n  Interpretability for Vision (MIV)"},{"id":"http://arxiv.org/abs/2504.08725v1","updated":"2025-04-11T17:50:08Z","published":"2025-04-11T17:50:08Z","title":"DocAgent: A Multi-Agent System for Automated Code Documentation\n  Generation","summary":"  High-quality code documentation is crucial for software development\nespecially in the era of AI. However, generating it automatically using Large\nLanguage Models (LLMs) remains challenging, as existing approaches often\nproduce incomplete, unhelpful, or factually incorrect outputs. We introduce\nDocAgent, a novel multi-agent collaborative system using topological code\nprocessing for incremental context building. Specialized agents (Reader,\nSearcher, Writer, Verifier, Orchestrator) then collaboratively generate\ndocumentation. We also propose a multi-faceted evaluation framework assessing\nCompleteness, Helpfulness, and Truthfulness. Comprehensive experiments show\nDocAgent significantly outperforms baselines consistently. Our ablation study\nconfirms the vital role of the topological processing order. DocAgent offers a\nrobust approach for reliable code documentation generation in complex and\nproprietary repositories.\n","authors":["Dayu Yang","Antoine Simoulin","Xin Qian","Xiaoyi Liu","Yuwei Cao","Zhaopu Teng","Grey Yang"],"pdf_url":"https://arxiv.org/pdf/2504.08725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08721v1","updated":"2025-04-11T17:35:58Z","published":"2025-04-11T17:35:58Z","title":"Surrogate-based optimization of system architectures subject to hidden\n  constraints","summary":"  The exploration of novel architectures requires physics-based simulation due\nto a lack of prior experience to start from, which introduces two specific\nchallenges for optimization algorithms: evaluations become more expensive (in\ntime) and evaluations might fail. The former challenge is addressed by\nSurrogate-Based Optimization (SBO) algorithms, in particular Bayesian\nOptimization (BO) using Gaussian Process (GP) models. An overview is provided\nof how BO can deal with challenges specific to architecture optimization, such\nas design variable hierarchy and multiple objectives: specific measures include\nensemble infills and a hierarchical sampling algorithm. Evaluations might fail\ndue to non-convergence of underlying solvers or infeasible geometry in certain\nareas of the design space. Such failed evaluations, also known as hidden\nconstraints, pose a particular challenge to SBO/BO, as the surrogate model\ncannot be trained on empty results. This work investigates various strategies\nfor satisfying hidden constraints in BO algorithms. Three high-level strategies\nare identified: rejection of failed points from the training set, replacing\nfailed points based on viable (non-failed) points, and predicting the failure\nregion. Through investigations on a set of test problems including a jet engine\narchitecture optimization problem, it is shown that best performance is\nachieved with a mixed-discrete GP to predict the Probability of Viability\n(PoV), and by ensuring selected infill points satisfy some minimum PoV\nthreshold. This strategy is demonstrated by solving a jet engine architecture\nproblem that features at 50% failure rate and could not previously be solved by\na BO algorithm. The developed BO algorithm and used test problems are available\nin the open-source Python library SBArchOpt.\n","authors":["Jasper Bussemaker","Paul Saves","Nathalie Bartoli","Thierry Lefebvre","Björn Nagel"],"pdf_url":"https://arxiv.org/pdf/2504.08721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08714v1","updated":"2025-04-11T17:24:58Z","published":"2025-04-11T17:24:58Z","title":"Generating Fine Details of Entity Interactions","summary":"  Images not only depict objects but also encapsulate rich interactions between\nthem. However, generating faithful and high-fidelity images involving multiple\nentities interacting with each other, is a long-standing challenge. While\npre-trained text-to-image models are trained on large-scale datasets to follow\ndiverse text instructions, they struggle to generate accurate interactions,\nlikely due to the scarcity of training data for uncommon object interactions.\nThis paper introduces InterActing, an interaction-focused dataset with 1000\nfine-grained prompts covering three key scenarios: (1) functional and\naction-based interactions, (2) compositional spatial relationships, and (3)\nmulti-subject interactions. To address interaction generation challenges, we\npropose a decomposition-augmented refinement procedure. Our approach,\nDetailScribe, built on Stable Diffusion 3.5, leverages LLMs to decompose\ninteractions into finer-grained concepts, uses a VLM to critique generated\nimages, and applies targeted interventions within the diffusion process in\nrefinement. Automatic and human evaluations show significantly improved image\nquality, demonstrating the potential of enhanced inference strategies. Our\ndataset and code are available at https://concepts-ai.com/p/detailscribe/ to\nfacilitate future exploration of interaction-rich image generation.\n","authors":["Xinyi Gu","Jiayuan Mao"],"pdf_url":"https://arxiv.org/pdf/2504.08714v1.pdf","comment":"Project Page: https://concepts-ai.com/p/detailscribe/"},{"id":"http://arxiv.org/abs/2504.08713v1","updated":"2025-04-11T17:23:37Z","published":"2025-04-11T17:23:37Z","title":"ProtoECGNet: Case-Based Interpretable Deep Learning for Multi-Label ECG\n  Classification with Contrastive Learning","summary":"  Deep learning-based electrocardiogram (ECG) classification has shown\nimpressive performance but clinical adoption has been slowed by the lack of\ntransparent and faithful explanations. Post hoc methods such as saliency maps\nmay fail to reflect a model's true decision process. Prototype-based reasoning\noffers a more transparent alternative by grounding decisions in similarity to\nlearned representations of real ECG segments, enabling faithful, case-based\nexplanations. We introduce ProtoECGNet, a prototype-based deep learning model\nfor interpretable, multi-label ECG classification. ProtoECGNet employs a\nstructured, multi-branch architecture that reflects clinical interpretation\nworkflows: it integrates a 1D CNN with global prototypes for rhythm\nclassification, a 2D CNN with time-localized prototypes for morphology-based\nreasoning, and a 2D CNN with global prototypes for diffuse abnormalities. Each\nbranch is trained with a prototype loss designed for multi-label learning,\ncombining clustering, separation, diversity, and a novel contrastive loss that\nencourages appropriate separation between prototypes of unrelated classes while\nallowing clustering for frequently co-occurring diagnoses. We evaluate\nProtoECGNet on all 71 diagnostic labels from the PTB-XL dataset, demonstrating\ncompetitive performance relative to state-of-the-art black-box models while\nproviding structured, case-based explanations. To assess prototype quality, we\nconduct a structured clinician review of the final model's projected\nprototypes, finding that they are rated as representative and clear.\nProtoECGNet shows that prototype learning can be effectively scaled to complex,\nmulti-label time-series classification, offering a practical path toward\ntransparent and trustworthy deep learning models for clinical decision support.\n","authors":["Sahil Sethi","David Chen","Thomas Statchen","Michael C. Burkhart","Nipun Bhandari","Bashar Ramadan","Brett Beaulieu-Jones"],"pdf_url":"https://arxiv.org/pdf/2504.08713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08712v1","updated":"2025-04-11T17:23:09Z","published":"2025-04-11T17:23:09Z","title":"Beyond Black-Box Predictions: Identifying Marginal Feature Effects in\n  Tabular Transformer Networks","summary":"  In recent years, deep neural networks have showcased their predictive power\nacross a variety of tasks. Beyond natural language processing, the transformer\narchitecture has proven efficient in addressing tabular data problems and\nchallenges the previously dominant gradient-based decision trees in these\nareas. However, this predictive power comes at the cost of intelligibility:\nMarginal feature effects are almost completely lost in the black-box nature of\ndeep tabular transformer networks. Alternative architectures that use the\nadditivity constraints of classical statistical regression models can maintain\nintelligible marginal feature effects, but often fall short in predictive power\ncompared to their more complex counterparts. To bridge the gap between\nintelligibility and performance, we propose an adaptation of tabular\ntransformer networks designed to identify marginal feature effects. We provide\ntheoretical justifications that marginal feature effects can be accurately\nidentified, and our ablation study demonstrates that the proposed model\nefficiently detects these effects, even amidst complex feature interactions. To\ndemonstrate the model's predictive capabilities, we compare it to several\ninterpretable as well as black-box models and find that it can match black-box\nperformances while maintaining intelligibility. The source code is available at\nhttps://github.com/OpenTabular/NAMpy.\n","authors":["Anton Thielmann","Arik Reuter","Benjamin Saefken"],"pdf_url":"https://arxiv.org/pdf/2504.08712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08704v1","updated":"2025-04-11T17:11:21Z","published":"2025-04-11T17:11:21Z","title":"Offline Reinforcement Learning using Human-Aligned Reward Labeling for\n  Autonomous Emergency Braking in Occluded Pedestrian Crossing","summary":"  Effective leveraging of real-world driving datasets is crucial for enhancing\nthe training of autonomous driving systems. While Offline Reinforcement\nLearning enables the training of autonomous vehicles using such data, most\navailable datasets lack meaningful reward labels. Reward labeling is essential\nas it provides feedback for the learning algorithm to distinguish between\ndesirable and undesirable behaviors, thereby improving policy performance. This\npaper presents a novel pipeline for generating human-aligned reward labels. The\nproposed approach addresses the challenge of absent reward signals in\nreal-world datasets by generating labels that reflect human judgment and safety\nconsiderations. The pipeline incorporates an adaptive safety component,\nactivated by analyzing semantic segmentation maps, allowing the autonomous\nvehicle to prioritize safety over efficiency in potential collision scenarios.\nThe proposed pipeline is applied to an occluded pedestrian crossing scenario\nwith varying levels of pedestrian traffic, using synthetic and simulation data.\nThe results indicate that the generated reward labels closely match the\nsimulation reward labels. When used to train the driving policy using Behavior\nProximal Policy Optimisation, the results are competitive with other baselines.\nThis demonstrates the effectiveness of our method in producing reliable and\nhuman-aligned reward signals, facilitating the training of autonomous driving\nsystems through Reinforcement Learning outside of simulation environments and\nin alignment with human values.\n","authors":["Vinal Asodia","Zhenhua Feng","Saber Fallah"],"pdf_url":"https://arxiv.org/pdf/2504.08704v1.pdf","comment":"13 pages, 9 figures, 1 table"},{"id":"http://arxiv.org/abs/2504.08696v1","updated":"2025-04-11T17:03:58Z","published":"2025-04-11T17:03:58Z","title":"SeaView: Software Engineering Agent Visual Interface for Enhanced\n  Workflow","summary":"  Auto-regressive LLM-based software engineering (SWE) agents, henceforth SWE\nagents, have made tremendous progress (>60% on SWE-Bench Verified) on\nreal-world coding challenges including GitHub issue resolution. SWE agents use\na combination of reasoning, environment interaction and self-reflection to\nresolve issues thereby generating \"trajectories\". Analysis of SWE agent\ntrajectories is difficult, not only as they exceed LLM sequence length\n(sometimes, greater than 128k) but also because it involves a relatively\nprolonged interaction between an LLM and the environment managed by the agent.\nIn case of an agent error, it can be hard to decipher, locate and understand\nits scope. Similarly, it can be hard to track improvements or regression over\nmultiple runs or experiments. While a lot of research has gone into making\nthese SWE agents reach state-of-the-art, much less focus has been put into\ncreating tools to help analyze and visualize agent output. We propose a novel\ntool called SeaView: Software Engineering Agent Visual Interface for Enhanced\nWorkflow, with a vision to assist SWE-agent researchers to visualize and\ninspect their experiments. SeaView's novel mechanisms help compare experimental\nruns with varying hyper-parameters or LLMs, and quickly get an understanding of\nLLM or environment related problems. Based on our user study, experienced\nresearchers spend between 10 and 30 minutes to gather the information provided\nby SeaView, while researchers with little experience can spend between 30\nminutes to 1 hour to diagnose their experiment.\n","authors":["Timothy Bula","Saurabh Pujar","Luca Buratti","Mihaela Bornea","Avirup Sil"],"pdf_url":"https://arxiv.org/pdf/2504.08696v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2501.18563v2","updated":"2025-04-11T16:59:39Z","published":"2025-01-30T18:36:48Z","title":"No Equations Needed: Learning System Dynamics Without Relying on\n  Closed-Form ODEs","summary":"  Data-driven modeling of dynamical systems is a crucial area of machine\nlearning. In many scenarios, a thorough understanding of the model's behavior\nbecomes essential for practical applications. For instance, understanding the\nbehavior of a pharmacokinetic model, constructed as part of drug development,\nmay allow us to both verify its biological plausibility (e.g., the drug\nconcentration curve is non-negative and decays to zero) and to design dosing\nguidelines. Discovery of closed-form ordinary differential equations (ODEs) can\nbe employed to obtain such insights by finding a compact mathematical equation\nand then analyzing it (a two-step approach). However, its widespread use is\ncurrently hindered because the analysis process may be time-consuming,\nrequiring substantial mathematical expertise, or even impossible if the\nequation is too complex. Moreover, if the found equation's behavior does not\nsatisfy the requirements, editing it or influencing the discovery algorithms to\nrectify it is challenging as the link between the symbolic form of an ODE and\nits behavior can be elusive. This paper proposes a conceptual shift to modeling\nlow-dimensional dynamical systems by departing from the traditional two-step\nmodeling process. Instead of first discovering a closed-form equation and then\nanalyzing it, our approach, direct semantic modeling, predicts the semantic\nrepresentation of the dynamical system (i.e., description of its behavior)\ndirectly from data, bypassing the need for complex post-hoc analysis. This\ndirect approach also allows the incorporation of intuitive inductive biases\ninto the optimization algorithm and editing the model's behavior directly,\nensuring that the model meets the desired specifications. Our approach not only\nsimplifies the modeling pipeline but also enhances the transparency and\nflexibility of the resulting models compared to traditional closed-form ODEs.\n","authors":["Krzysztof Kacprzyk","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2501.18563v2.pdf","comment":"To appear in the Proceedings of the Thirteenth International\n  Conference on Learning Representations (ICLR 2025)"},{"id":"http://arxiv.org/abs/2504.03767v2","updated":"2025-04-11T16:59:05Z","published":"2025-04-02T21:46:02Z","title":"MCP Safety Audit: LLMs with the Model Context Protocol Allow Major\n  Security Exploits","summary":"  To reduce development overhead and enable seamless integration between\npotential components comprising any given generative AI application, the Model\nContext Protocol (MCP) (Anthropic, 2024) has recently been released and\nsubsequently widely adopted. The MCP is an open protocol that standardizes API\ncalls to large language models (LLMs), data sources, and agentic tools. By\nconnecting multiple MCP servers, each defined with a set of tools, resources,\nand prompts, users are able to define automated workflows fully driven by LLMs.\nHowever, we show that the current MCP design carries a wide range of security\nrisks for end users. In particular, we demonstrate that industry-leading LLMs\nmay be coerced into using MCP tools to compromise an AI developer's system\nthrough various attacks, such as malicious code execution, remote access\ncontrol, and credential theft. To proactively mitigate these and related\nattacks, we introduce a safety auditing tool, MCPSafetyScanner, the first\nagentic tool to assess the security of an arbitrary MCP server. MCPScanner uses\nseveral agents to (a) automatically determine adversarial samples given an MCP\nserver's tools and resources; (b) search for related vulnerabilities and\nremediations based on those samples; and (c) generate a security report\ndetailing all findings. Our work highlights serious security issues with\ngeneral-purpose agentic workflows while also providing a proactive tool to\naudit MCP server safety and address detected vulnerabilities before deployment.\n  The described MCP server auditing tool, MCPSafetyScanner, is freely available\nat: https://github.com/johnhalloran321/mcpSafetyScanner\n","authors":["Brandon Radosevich","John Halloran"],"pdf_url":"https://arxiv.org/pdf/2504.03767v2.pdf","comment":"27 pages, 21 figures, and 2 Tables. Cleans up the TeX source"},{"id":"http://arxiv.org/abs/2504.08682v1","updated":"2025-04-11T16:43:11Z","published":"2025-04-11T16:43:11Z","title":"Bayesian optimization for mixed variables using an adaptive dimension\n  reduction process: applications to aircraft design","summary":"  Multidisciplinary design optimization methods aim at adapting numerical\noptimization techniques to the design of engineering systems involving multiple\ndisciplines. In this context, a large number of mixed continuous, integer and\ncategorical variables might arise during the optimization process and practical\napplications involve a large number of design variables. Recently, there has\nbeen a growing interest in mixed variables constrained Bayesian optimization\nbut most existing approaches severely increase the number of the\nhyperparameters related to the surrogate model. In this paper, we address this\nissue by constructing surrogate models using less hyperparameters. The\nreduction process is based on the partial least squares method. An adaptive\nprocedure for choosing the number of hyperparameters is proposed. The\nperformance of the proposed approach is confirmed on analytical tests as well\nas two real applications related to aircraft design. A significant improvement\nis obtained compared to genetic algorithms.\n","authors":["Paul Saves","Nathalie Bartoli","Youssef Diouane","Thierry Lefebvre","Joseph Morlier","Christophe David","Eric Nguyen Van","Sébastien Defoort"],"pdf_url":"https://arxiv.org/pdf/2504.08682v1.pdf","comment":"AIAA SciTech 2022 Forum. arXiv admin note: substantial text overlap\n  with arXiv:2402.04711"},{"id":"http://arxiv.org/abs/2504.08672v1","updated":"2025-04-11T16:26:23Z","published":"2025-04-11T16:26:23Z","title":"Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning","summary":"  Advancing LLM reasoning skills has captivated wide interest. However, current\npost-training techniques rely heavily on supervisory signals, such as outcome\nsupervision or auxiliary reward models, which face the problem of scalability\nand high annotation costs. This motivates us to enhance LLM reasoning without\nthe need for external supervision. We introduce a generalizable and purely\nunsupervised self-training framework, named Genius. Without external auxiliary,\nGenius requires to seek the optimal response sequence in a stepwise manner and\noptimize the LLM. To explore the potential steps and exploit the optimal ones,\nGenius introduces a stepwise foresight re-sampling strategy to sample and\nestimate the step value by simulating future outcomes. Further, we recognize\nthat the unsupervised setting inevitably induces the intrinsic noise and\nuncertainty. To provide a robust optimization, we propose an\nadvantage-calibrated optimization (ACO) loss function to mitigate estimation\ninconsistencies. Combining these techniques together, Genius provides an\nadvanced initial step towards self-improve LLM reasoning with general queries\nand without supervision, revolutionizing reasoning scaling laws given the vast\navailability of general queries. The code will be released at\nhttps://github.com/xufangzhi/Genius.\n","authors":["Fangzhi Xu","Hang Yan","Chang Ma","Haiteng Zhao","Qiushi Sun","Kanzhi Cheng","Junxian He","Jun Liu","Zhiyong Wu"],"pdf_url":"https://arxiv.org/pdf/2504.08672v1.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.08671v1","updated":"2025-04-11T16:24:40Z","published":"2025-04-11T16:24:40Z","title":"Regularized infill criteria for multi-objective Bayesian optimization\n  with application to aircraft design","summary":"  Bayesian optimization is an advanced tool to perform ecient global\noptimization It consists on enriching iteratively surrogate Kriging models of\nthe objective and the constraints both supposed to be computationally expensive\nof the targeted optimization problem Nowadays efficient extensions of Bayesian\noptimization to solve expensive multiobjective problems are of high interest\nThe proposed method in this paper extends the super efficient global\noptimization with mixture of experts SEGOMOE to solve constrained\nmultiobjective problems To cope with the illposedness of the multiobjective\ninll criteria different enrichment procedures using regularization techniques\nare proposed The merit of the proposed approaches are shown on known\nmultiobjective benchmark problems with and without constraints The proposed\nmethods are then used to solve a biobjective application related to conceptual\naircraft design with ve unknown design variables and three nonlinear inequality\nconstraints The preliminary results show a reduction of the total cost in terms\nof function evaluations by a factor of 20 compared to the evolutionary\nalgorithm NSGA-II.\n","authors":["Robin Grapin","Youssef Diouane","Joseph Morlier","Nathalie Bartoli","Thierry Lefebvre","Paul Saves","Jasper Bussemaker"],"pdf_url":"https://arxiv.org/pdf/2504.08671v1.pdf","comment":"AIAA AVIATION 2022 Forum"},{"id":"http://arxiv.org/abs/2408.08968v4","updated":"2025-04-11T16:19:31Z","published":"2024-08-16T18:34:11Z","title":"Online SLA Decomposition: Enabling Real-Time Adaptation to Evolving\n  Network Systems","summary":"  When a network slice spans multiple technology domains, it is crucial for\neach domain to uphold the End-to-End (E2E) Service Level Agreement (SLA)\nassociated with the slice. Consequently, the E2E SLA must be properly\ndecomposed into partial SLAs that are assigned to each domain involved. In a\nnetwork slice management system with a two-level architecture, comprising an\nE2E service orchestrator and local domain controllers, we consider that the\norchestrator has access only to historical data regarding the responses of\nlocal controllers to previous requests, and this information is used to\nconstruct a risk model for each domain. In this study, we extend our previous\nwork by investigating the dynamic nature of real-world systems and introducing\nan online learning-decomposition framework to tackle the dynamicity. We propose\na framework that continuously updates the risk models based on the most recent\nfeedback. This approach leverages key components such as online gradient\ndescent and FIFO memory buffers, which enhance the stability and robustness of\nthe overall process. Our empirical study on an analytic model-based simulator\ndemonstrates that the proposed framework outperforms the state-of-the-art\nstatic approach, delivering more accurate and resilient SLA decomposition under\nvarying conditions and data limitations. Furthermore, we provide a\ncomprehensive complexity analysis of the proposed solution.\n","authors":["Cyril Shih-Huan Hsu","Danny De Vleeschauwer","Chrysa Papagianni","Paola Grosso"],"pdf_url":"https://arxiv.org/pdf/2408.08968v4.pdf","comment":"The paper has been accepted for publication at EuCNC & 6G Summit 2025"},{"id":"http://arxiv.org/abs/2410.05898v7","updated":"2025-04-11T16:19:15Z","published":"2024-10-08T10:55:40Z","title":"Manifolds, Random Matrices and Spectral Gaps: The geometric phases of\n  generative diffusion","summary":"  In this paper, we investigate the latent geometry of generative diffusion\nmodels under the manifold hypothesis. For this purpose, we analyze the spectrum\nof eigenvalues (and singular values) of the Jacobian of the score function,\nwhose discontinuities (gaps) reveal the presence and dimensionality of distinct\nsub-manifolds. Using a statistical physics approach, we derive the spectral\ndistributions and formulas for the spectral gaps under several distributional\nassumptions, and we compare these theoretical predictions with the spectra\nestimated from trained networks. Our analysis reveals the existence of three\ndistinct qualitative phases during the generative process: a trivial phase; a\nmanifold coverage phase where the diffusion process fits the distribution\ninternal to the manifold; a consolidation phase where the score becomes\northogonal to the manifold and all particles are projected on the support of\nthe data. This `division of labor' between different timescales provides an\nelegant explanation of why generative diffusion models are not affected by the\nmanifold overfitting phenomenon that plagues likelihood-based models, since the\ninternal distribution and the manifold geometry are produced at different time\npoints during generation.\n","authors":["Enrico Ventura","Beatrice Achilli","Gianluigi Silvestri","Carlo Lucibello","Luca Ambrogioni"],"pdf_url":"https://arxiv.org/pdf/2410.05898v7.pdf","comment":"22 pages, 13 figures"},{"id":"http://arxiv.org/abs/2504.08660v1","updated":"2025-04-11T16:01:17Z","published":"2025-04-11T16:01:17Z","title":"Channel Estimation by Infinite Width Convolutional Networks","summary":"  In wireless communications, estimation of channels in OFDM systems spans\nfrequency and time, which relies on sparse collections of pilot data, posing an\nill-posed inverse problem. Moreover, deep learning estimators require large\namounts of training data, computational resources, and true channels to produce\naccurate channel estimates, which are not realistic. To address this, a\nconvolutional neural tangent kernel (CNTK) is derived from an infinitely wide\nconvolutional network whose training dynamics can be expressed by a closed-form\nequation. This CNTK is used to impute the target matrix and estimate the\nmissing channel response using only the known values available at pilot\nlocations. This is a promising solution for channel estimation that does not\nrequire a large training set. Numerical results on realistic channel datasets\ndemonstrate that our strategy accurately estimates the channels without a large\ndataset and significantly outperforms deep learning methods in terms of speed,\naccuracy, and computational resources.\n","authors":["Mohammed Mallik","Guillaume Villemaud"],"pdf_url":"https://arxiv.org/pdf/2504.08660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08651v1","updated":"2025-04-11T15:55:50Z","published":"2025-04-11T15:55:50Z","title":"Application of machine learning models to predict the relationship\n  between air pollution, ecosystem degradation, and health disparities and lung\n  cancer in Vietnam","summary":"  Lung cancer is one of the major causes of death worldwide, and Vietnam is not\nan exception. This disease is the second most common type of cancer globally\nand the second most common cause of death in Vietnam, just after liver cancer,\nwith 23,797 fatal cases and 26,262 new cases, or 14.4% of the disease in 2020.\nRecently, with rising disease rates in Vietnam causing a huge public health\nburden, lung cancer continues to hold the top position in attention and care.\nEspecially together with climate change, under a variety of types of pollution,\ndeforestation, and modern lifestyles, lung cancer risks are on red alert,\nparticularly in Vietnam. To understand more about the severe disease sources in\nVietnam from a diversity of key factors, including environmental features and\nthe current health state, with a particular emphasis on Vietnam's distinct\nsocioeconomic and ecological context, we utilize large datasets such as patient\nhealth records and environmental indicators containing necessary information,\nsuch as deforestation rate, green cover rate, air pollution, and lung cancer\nrisks, that is collected from well-known governmental sharing websites. Then,\nwe process and connect them and apply analytical methods (heatmap, information\ngain, p-value, spearman correlation) to determine causal correlations\ninfluencing lung cancer risks. Moreover, we deploy machine learning (ML) models\n(Decision Tree, Random Forest, Support Vector Machine, K-mean clustering) to\ndiscover cancer risk patterns. Our experimental results, leveraged by the\naforementioned ML models to identify the disease patterns, are promising,\nparticularly, the models as Random Forest, SVM, and PCA are working well on the\ndatasets and give high accuracy (99%), however, the K means clustering has very\nlow accuracy (10%) and does not fit the datasets.\n","authors":["Ngoc Hong Tran","Lan Kim Vien","Ngoc-Thao Thi Le"],"pdf_url":"https://arxiv.org/pdf/2504.08651v1.pdf","comment":"Accepted and Published in the Proceeding of the 2nd International\n  Conference on \"Green Solutions and Emerging Technologies for Sustainability\"\n  (GSETS 2025) ISBN: 978-604-76-3087-5"},{"id":"http://arxiv.org/abs/2504.05636v2","updated":"2025-04-11T15:53:20Z","published":"2025-04-08T03:29:40Z","title":"A Multi-Modal AI System for Screening Mammography: Integrating 2D and 3D\n  Imaging to Improve Breast Cancer Detection in a Prospective Clinical Study","summary":"  Although digital breast tomosynthesis (DBT) improves diagnostic performance\nover full-field digital mammography (FFDM), false-positive recalls remain a\nconcern in breast cancer screening. We developed a multi-modal artificial\nintelligence system integrating FFDM, synthetic mammography, and DBT to provide\nbreast-level predictions and bounding-box localizations of suspicious findings.\nOur AI system, trained on approximately 500,000 mammography exams, achieved\n0.945 AUROC on an internal test set. It demonstrated capacity to reduce recalls\nby 31.7% and radiologist workload by 43.8% while maintaining 100% sensitivity,\nunderscoring its potential to improve clinical workflows. External validation\nconfirmed strong generalizability, reducing the gap to a perfect AUROC by\n35.31%-69.14% relative to strong baselines. In prospective deployment across 18\nsites, the system reduced recall rates for low-risk cases. An improved version,\ntrained on over 750,000 exams with additional labels, further reduced the gap\nby 18.86%-56.62% across large external datasets. Overall, these results\nunderscore the importance of utilizing all available imaging modalities,\ndemonstrate the potential for clinical impact, and indicate feasibility of\nfurther reduction of the test error with increased training set when using\nlarge-capacity neural networks.\n","authors":["Jungkyu Park","Jan Witowski","Yanqi Xu","Hari Trivedi","Judy Gichoya","Beatrice Brown-Mulry","Malte Westerhoff","Linda Moy","Laura Heacock","Alana Lewin","Krzysztof J. Geras"],"pdf_url":"https://arxiv.org/pdf/2504.05636v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13248v2","updated":"2025-04-11T15:50:59Z","published":"2024-11-20T12:07:19Z","title":"On lower bounds of the density of planar periodic sets without unit\n  distances","summary":"  Determining the maximal density $m_1(\\mathbb{R}^2)$ of planar sets without\nunit distances is a fundamental problem in combinatorial geometry. This paper\ninvestigates lower bounds for this quantity. We introduce a novel approach to\nestimating $m_1(\\mathbb{R}^2)$ by reformulating the problem as a Maximal\nIndependent Set (MIS) problem on graphs constructed from flat torus, focusing\non periodic sets with respect to two non-collinear vectors. Our experimental\nresults, supported by theoretical justifications of proposed method,\ndemonstrate that for a sufficiently wide range of parameters this approach does\nnot improve the known lower bound $0.22936 \\le m_1(\\mathbb{R}^2)$. The best\ndiscrete sets found are approximations of Croft's construction. In addition,\nseveral open source software packages for MIS problem are compared on this\ntask.\n","authors":["Alexander Tolmachev"],"pdf_url":"https://arxiv.org/pdf/2411.13248v2.pdf","comment":"21 pages, 9 figures; typos corrected"},{"id":"http://arxiv.org/abs/2404.12481v2","updated":"2025-04-11T15:49:46Z","published":"2024-04-18T19:33:55Z","title":"Understanding Optimal Feature Transfer via a Fine-Grained Bias-Variance\n  Analysis","summary":"  In the transfer learning paradigm models learn useful representations (or\nfeatures) during a data-rich pretraining stage, and then use the pretrained\nrepresentation to improve model performance on data-scarce downstream tasks. In\nthis work, we explore transfer learning with the goal of optimizing downstream\nperformance. We introduce a simple linear model that takes as input an\narbitrary pretrained feature transform. We derive exact asymptotics of the\ndownstream risk and its \\textit{fine-grained} bias-variance decomposition. We\nthen identify the pretrained representation that optimizes the asymptotic\ndownstream bias and variance averaged over an ensemble of downstream tasks. Our\ntheoretical and empirical analysis uncovers the surprising phenomenon that the\noptimal featurization is naturally sparse, even in the absence of explicit\nsparsity-inducing priors or penalties. Additionally, we identify a phase\ntransition where the optimal pretrained representation shifts from hard\nselection to soft selection of relevant features.\n","authors":["Yufan Li","Subhabrata Sen","Ben Adlam"],"pdf_url":"https://arxiv.org/pdf/2404.12481v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08638v1","updated":"2025-04-11T15:39:44Z","published":"2025-04-11T15:39:44Z","title":"Transformer Learns Optimal Variable Selection in Group-Sparse\n  Classification","summary":"  Transformers have demonstrated remarkable success across various\napplications. However, the success of transformers have not been understood in\ntheory. In this work, we give a case study of how transformers can be trained\nto learn a classic statistical model with \"group sparsity\", where the input\nvariables form multiple groups, and the label only depends on the variables\nfrom one of the groups. We theoretically demonstrate that, a one-layer\ntransformer trained by gradient descent can correctly leverage the attention\nmechanism to select variables, disregarding irrelevant ones and focusing on\nthose beneficial for classification. We also demonstrate that a well-pretrained\none-layer transformer can be adapted to new downstream tasks to achieve good\nprediction accuracy with a limited number of samples. Our study sheds light on\nhow transformers effectively learn structured data.\n","authors":["Chenyang Zhang","Xuran Meng","Yuan Cao"],"pdf_url":"https://arxiv.org/pdf/2504.08638v1.pdf","comment":"63 pages, 6 figures"},{"id":"http://arxiv.org/abs/2504.08632v1","updated":"2025-04-11T15:35:50Z","published":"2025-04-11T15:35:50Z","title":"Deep Learning Methods for Detecting Thermal Runaway Events in Battery\n  Production Lines","summary":"  One of the key safety considerations of battery manufacturing is thermal\nrunaway, the uncontrolled increase in temperature which can lead to fires,\nexplosions, and emissions of toxic gasses. As such, development of automated\nsystems capable of detecting such events is of considerable importance in both\nacademic and industrial contexts. In this work, we investigate the use of deep\nlearning for detecting thermal runaway in the battery production line of VDL\nNedcar, a Dutch automobile manufacturer. Specifically, we collect data from the\nproduction line to represent both baseline (non thermal runaway) and thermal\nrunaway conditions. Thermal runaway was simulated through the use of external\nheat and smoke sources. The data consisted of both optical and thermal images\nwhich were then preprocessed and fused before serving as input to our models.\nIn this regard, we evaluated three deep-learning models widely used in computer\nvision including shallow convolutional neural networks, residual neural\nnetworks, and vision transformers on two performance metrics. Furthermore, we\nevaluated these models using explainability methods to gain insight into their\nability to capture the relevant feature information from their inputs. The\nobtained results indicate that the use of deep learning is a viable approach to\nthermal runaway detection in battery production lines.\n","authors":["Athanasios Athanasopoulos","Matúš Mihalák","Marcin Pietrasik"],"pdf_url":"https://arxiv.org/pdf/2504.08632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08628v1","updated":"2025-04-11T15:29:55Z","published":"2025-04-11T15:29:55Z","title":"Gradient Descent Robustly Learns the Intrinsic Dimension of Data in\n  Training Convolutional Neural Networks","summary":"  Modern neural networks are usually highly over-parameterized. Behind the wide\nusage of over-parameterized networks is the belief that, if the data are\nsimple, then the trained network will be automatically equivalent to a simple\npredictor. Following this intuition, many existing works have studied different\nnotions of \"ranks\" of neural networks and their relation to the rank of data.\nIn this work, we study the rank of convolutional neural networks (CNNs) trained\nby gradient descent, with a specific focus on the robustness of the rank to\nimage background noises. Specifically, we point out that, when adding\nbackground noises to images, the rank of the CNN trained with gradient descent\nis affected far less compared with the rank of the data. We support our claim\nwith a theoretical case study, where we consider a particular data model to\ncharacterize low-rank clean images with added background noises. We prove that\nCNNs trained by gradient descent can learn the intrinsic dimension of clean\nimages, despite the presence of relatively large background noises. We also\nconduct experiments on synthetic and real datasets to further validate our\nclaim.\n","authors":["Chenyang Zhang","Peifeng Gao","Difan Zou","Yuan Cao"],"pdf_url":"https://arxiv.org/pdf/2504.08628v1.pdf","comment":"43 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.08626v1","updated":"2025-04-11T15:27:29Z","published":"2025-04-11T15:27:29Z","title":"Task-conditioned Ensemble of Expert Models for Continuous Learning","summary":"  One of the major challenges in machine learning is maintaining the accuracy\nof the deployed model (e.g., a classifier) in a non-stationary environment. The\nnon-stationary environment results in distribution shifts and, consequently, a\ndegradation in accuracy. Continuous learning of the deployed model with new\ndata could be one remedy. However, the question arises as to how we should\nupdate the model with new training data so that it retains its accuracy on the\nold data while adapting to the new data. In this work, we propose a\ntask-conditioned ensemble of models to maintain the performance of the existing\nmodel. The method involves an ensemble of expert models based on task\nmembership information. The in-domain models-based on the local outlier concept\n(different from the expert models) provide task membership information\ndynamically at run-time to each probe sample. To evaluate the proposed method,\nwe experiment with three setups: the first represents distribution shift\nbetween tasks (LivDet-Iris-2017), the second represents distribution shift both\nbetween and within tasks (LivDet-Iris-2020), and the third represents disjoint\ndistribution between tasks (Split MNIST). The experiments highlight the\nbenefits of the proposed method. The source code is available at\nhttps://github.com/iPRoBe-lab/Continuous_Learning_FE_DM.\n","authors":["Renu Sharma","Debasmita Pal","Arun Ross"],"pdf_url":"https://arxiv.org/pdf/2504.08626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08621v1","updated":"2025-04-11T15:25:50Z","published":"2025-04-11T15:25:50Z","title":"MooseAgent: A LLM Based Multi-agent Framework for Automating Moose\n  Simulation","summary":"  The Finite Element Method (FEM) is widely used in engineering and scientific\ncomputing, but its pre-processing, solver configuration, and post-processing\nstages are often time-consuming and require specialized knowledge. This paper\nproposes an automated solution framework, MooseAgent, for the multi-physics\nsimulation framework MOOSE, which combines large-scale pre-trained language\nmodels (LLMs) with a multi-agent system. The framework uses LLMs to understand\nuser-described simulation requirements in natural language and employs task\ndecomposition and multi-round iterative verification strategies to\nautomatically generate MOOSE input files. To improve accuracy and reduce model\nhallucinations, the system builds and utilizes a vector database containing\nannotated MOOSE input cards and function documentation. We conducted\nexperimental evaluations on several typical cases, including heat transfer,\nmechanics, phase field, and multi-physics coupling. The results show that\nMooseAgent can automate the MOOSE simulation process to a certain extent,\nespecially demonstrating a high success rate when dealing with relatively\nsimple single-physics problems. The main contribution of this research is the\nproposal of a multi-agent automated framework for MOOSE, which validates its\npotential in simplifying finite element simulation processes and lowering the\nuser barrier, providing new ideas for the development of intelligent finite\nelement simulation software. The code for the MooseAgent framework proposed in\nthis paper has been open-sourced and is available at\nhttps://github.com/taozhan18/MooseAgent\n","authors":["Tao Zhang","Zhenhai Liu","Yong Xin","Yongjun Jiao"],"pdf_url":"https://arxiv.org/pdf/2504.08621v1.pdf","comment":"7 pages, 2 Figs"},{"id":"http://arxiv.org/abs/2412.14865v3","updated":"2025-04-11T15:18:20Z","published":"2024-12-19T14:00:03Z","title":"Hierarchical Subspaces of Policies for Continual Offline Reinforcement\n  Learning","summary":"  We consider a Continual Reinforcement Learning setup, where a learning agent\nmust continuously adapt to new tasks while retaining previously acquired skill\nsets, with a focus on the challenge of avoiding forgetting past gathered\nknowledge and ensuring scalability with the growing number of tasks. Such\nissues prevail in autonomous robotics and video game simulations, notably for\nnavigation tasks prone to topological or kinematic changes. To address these\nissues, we introduce HiSPO, a novel hierarchical framework designed\nspecifically for continual learning in navigation settings from offline data.\nOur method leverages distinct policy subspaces of neural networks to enable\nflexible and efficient adaptation to new tasks while preserving existing\nknowledge. We demonstrate, through a careful experimental study, the\neffectiveness of our method in both classical MuJoCo maze environments and\ncomplex video game-like navigation simulations, showcasing competitive\nperformances and satisfying adaptability with respect to classical continual\nlearning metrics, in particular regarding the memory usage and efficiency.\n","authors":["Anthony Kobanda","Rémy Portelas","Odalric-Ambrym Maillard","Ludovic Denoyer"],"pdf_url":"https://arxiv.org/pdf/2412.14865v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08604v1","updated":"2025-04-11T15:12:12Z","published":"2025-04-11T15:12:12Z","title":"Neural Fidelity Calibration for Informative Sim-to-Real Adaptation","summary":"  Deep reinforcement learning can seamlessly transfer agile locomotion and\nnavigation skills from the simulator to real world. However, bridging the\nsim-to-real gap with domain randomization or adversarial methods often demands\nexpert physics knowledge to ensure policy robustness. Even so, cutting-edge\nsimulators may fall short of capturing every real-world detail, and the\nreconstructed environment may introduce errors due to various perception\nuncertainties. To address these challenges, we propose Neural Fidelity\nCalibration (NFC), a novel framework that employs conditional score-based\ndiffusion models to calibrate simulator physical coefficients and residual\nfidelity domains online during robot execution. Specifically, the residual\nfidelity reflects the simulation model shift relative to the real-world\ndynamics and captures the uncertainty of the perceived environment, enabling us\nto sample realistic environments under the inferred distribution for policy\nfine-tuning. Our framework is informative and adaptive in three key ways: (a)\nwe fine-tune the pretrained policy only under anomalous scenarios, (b) we build\nsequential NFC online with the pretrained NFC's proposal prior, reducing the\ndiffusion model's training burden, and (c) when NFC uncertainty is high and may\ndegrade policy improvement, we leverage optimistic exploration to enable\nhallucinated policy optimization. Our framework achieves superior simulator\ncalibration precision compared to state-of-the-art methods across diverse\nrobots with high-dimensional parametric spaces. We study the critical\ncontribution of residual fidelity to policy improvement in simulation and\nreal-world experiments. Notably, our approach demonstrates robust robot\nnavigation under challenging real-world conditions, such as a broken wheel axle\non snowy surfaces.\n","authors":["Youwei Yu","Lantao Liu"],"pdf_url":"https://arxiv.org/pdf/2504.08604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08602v1","updated":"2025-04-11T15:10:41Z","published":"2025-04-11T15:10:41Z","title":"On Background Bias of Post-Hoc Concept Embeddings in Computer Vision\n  DNNs","summary":"  The thriving research field of concept-based explainable artificial\nintelligence (C-XAI) investigates how human-interpretable semantic concepts\nembed in the latent spaces of deep neural networks (DNNs). Post-hoc approaches\ntherein use a set of examples to specify a concept, and determine its\nembeddings in DNN latent space using data driven techniques. This proved useful\nto uncover biases between different target (foreground or concept) classes.\nHowever, given that the background is mostly uncontrolled during training, an\nimportant question has been left unattended so far: Are/to what extent are\nstate-of-the-art, data-driven post-hoc C-XAI approaches themselves prone to\nbiases with respect to their backgrounds? E.g., wild animals mostly occur\nagainst vegetation backgrounds, and they seldom appear on roads. Even simple\nand robust C-XAI methods might abuse this shortcut for enhanced performance. A\ndangerous performance degradation of the concept-corner cases of animals on the\nroad could thus remain undiscovered. This work validates and thoroughly\nconfirms that established Net2Vec-based concept segmentation techniques\nfrequently capture background biases, including alarming ones, such as\nunderperformance on road scenes. For the analysis, we compare 3 established\ntechniques from the domain of background randomization on >50 concepts from 2\ndatasets, and 7 diverse DNN architectures. Our results indicate that even\nlow-cost setups can provide both valuable insight and improved background\nrobustness.\n","authors":["Gesina Schwalbe","Georgii Mikriukov","Edgar Heinert","Stavros Gerolymatos","Mert Keser","Alois Knoll","Matthias Rottmann","Annika Mütze"],"pdf_url":"https://arxiv.org/pdf/2504.08602v1.pdf","comment":"camera-ready version for 3rd World Conference on eXplainable\n  Artificial Intelligence; 5 figures, 6 tables; code available at:\n  https://github.com/gesina/bg_randomized_loce"},{"id":"http://arxiv.org/abs/2412.15302v2","updated":"2025-04-11T15:06:01Z","published":"2024-12-19T10:44:18Z","title":"Tokenphormer: Structure-aware Multi-token Graph Transformer for Node\n  Classification","summary":"  Graph Neural Networks (GNNs) are widely used in graph data mining tasks.\nTraditional GNNs follow a message passing scheme that can effectively utilize\nlocal and structural information. However, the phenomena of over-smoothing and\nover-squashing limit the receptive field in message passing processes. Graph\nTransformers were introduced to address these issues, achieving a global\nreceptive field but suffering from the noise of irrelevant nodes and loss of\nstructural information. Therefore, drawing inspiration from fine-grained\ntoken-based representation learning in Natural Language Processing (NLP), we\npropose the Structure-aware Multi-token Graph Transformer (Tokenphormer), which\ngenerates multiple tokens to effectively capture local and structural\ninformation and explore global information at different levels of granularity.\nSpecifically, we first introduce the walk-token generated by mixed walks\nconsisting of four walk types to explore the graph and capture structure and\ncontextual information flexibly. To ensure local and global information\ncoverage, we also introduce the SGPM-token (obtained through the\nSelf-supervised Graph Pre-train Model, SGPM) and the hop-token, extending the\nlength and density limit of the walk-token, respectively. Finally, these\nexpressive tokens are fed into the Transformer model to learn node\nrepresentations collaboratively. Experimental results demonstrate that the\ncapability of the proposed Tokenphormer can achieve state-of-the-art\nperformance on node classification tasks.\n","authors":["Zijie Zhou","Zhaoqi Lu","Xuekai Wei","Rongqin Chen","Shenghui Zhang","Pak Lon Ip","Leong Hou U"],"pdf_url":"https://arxiv.org/pdf/2412.15302v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2410.13373v2","updated":"2025-04-11T14:55:24Z","published":"2024-10-17T09:23:53Z","title":"Addressing Graph Heterogeneity and Heterophily from A Spectral\n  Perspective","summary":"  Graph neural networks (GNNs) have demonstrated excellent performance in\nsemi-supervised node classification tasks. Despite this, two primary challenges\npersist: heterogeneity and heterophily. Each of these two challenges can\nsignificantly hinder the performance of GNNs. Heterogeneity refers to a graph\nwith multiple types of nodes or edges, while heterophily refers to the fact\nthat connected nodes are more likely to have dissimilar attributes or labels.\nAlthough there have been few works studying heterogeneous heterophilic graphs,\nthey either only consider the heterophily of specific meta-paths and lack\nexpressiveness, or have high expressiveness but fail to exploit high-order\nneighbors. In this paper, we propose a Heterogeneous Heterophilic Spectral\nGraph Neural Network (H2SGNN), which employs two modules: local independent\nfiltering and global hybrid filtering. Local independent filtering adaptively\nlearns node representations under different homophily, while global hybrid\nfiltering exploits high-order neighbors to learn more possible meta-paths.\nExtensive experiments are conducted on four datasets to validate the\neffectiveness of the proposed H2SGNN, which achieves superior performance with\nfewer parameters and memory consumption. The code is available at the GitHub\nrepo: https://github.com/Lukangkang123/H2SGNN/.\n","authors":["Kangkang Lu","Yanhua Yu","Zhiyong Huang","Yunshan Ma","Xiao Wang","Meiyu Liang","Yuling Wang","Yimeng Ren","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.13373v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19234v2","updated":"2025-04-11T14:51:26Z","published":"2024-09-28T04:42:21Z","title":"Decoding Android Malware with a Fraction of Features: An\n  Attention-Enhanced MLP-SVM Approach","summary":"  The escalating sophistication of Android malware poses significant challenges\nto traditional detection methods, necessitating innovative approaches that can\nefficiently identify and classify threats with high precision. This paper\nintroduces a novel framework that synergistically integrates an\nattention-enhanced Multi-Layer Perceptron (MLP) with a Support Vector Machine\n(SVM) to make Android malware detection and classification more effective. By\ncarefully analyzing a mere 47 features out of over 9,760 available in the\ncomprehensive CCCS-CIC-AndMal-2020 dataset, our MLP-SVM model achieves an\nimpressive accuracy over 99% in identifying malicious applications. The MLP,\nenhanced with an attention mechanism, focuses on the most discriminative\nfeatures and further reduces the 47 features to only 14 components using Linear\nDiscriminant Analysis (LDA). Despite this significant reduction in\ndimensionality, the SVM component, equipped with an RBF kernel, excels in\nmapping these components to a high-dimensional space, facilitating precise\nclassification of malware into their respective families. Rigorous evaluations,\nencompassing accuracy, precision, recall, and F1-score metrics, confirm the\nsuperiority of our approach compared to existing state-of-the-art techniques.\nThe proposed framework not only significantly reduces the computational\ncomplexity by leveraging a compact feature set but also exhibits resilience\nagainst the evolving Android malware landscape.\n","authors":["Safayat Bin Hakim","Muhammad Adil","Kamal Acharya","Houbing Herbert Song"],"pdf_url":"https://arxiv.org/pdf/2409.19234v2.pdf","comment":"Published in NSS-SocialSec 2024, Lecture Notes in Computer Science\n  (LNCS)"},{"id":"http://arxiv.org/abs/2502.06674v2","updated":"2025-04-11T14:48:32Z","published":"2025-02-10T17:00:32Z","title":"RAILS: Risk-Aware Iterated Local Search for Joint SLA Decomposition and\n  Service Provider Management in Multi-Domain Networks","summary":"  The emergence of the fifth generation (5G) technology has transformed mobile\nnetworks into multi-service environments, necessitating efficient network\nslicing to meet diverse Service Level Agreements (SLAs). SLA decomposition\nacross multiple network domains, each potentially managed by different service\nproviders, poses a significant challenge due to limited visibility into\nreal-time underlying domain conditions. This paper introduces Risk-Aware\nIterated Local Search (RAILS), a novel risk model-driven meta-heuristic\nframework designed to jointly address SLA decomposition and service provider\nselection in multi-domain networks. By integrating online risk modeling with\niterated local search principles, RAILS effectively navigates the complex\noptimization landscape, utilizing historical feedback from domain controllers.\nWe formulate the joint problem as a Mixed-Integer Nonlinear Programming (MINLP)\nproblem and prove its NP-hardness. Extensive simulations demonstrate that RAILS\nachieves near-optimal performance, offering an efficient, real-time solution\nfor adaptive SLA management in modern multi-domain networks.\n","authors":["Cyril Shih-Huan Hsu","Chrysa Papagianni","Paola Grosso"],"pdf_url":"https://arxiv.org/pdf/2502.06674v2.pdf","comment":"The paper has been accepted for publication at the IEEE High\n  Performance Switching and Routing (HPSR) 2025 conference"},{"id":"http://arxiv.org/abs/2504.08585v1","updated":"2025-04-11T14:39:25Z","published":"2025-04-11T14:39:25Z","title":"Ready, Bid, Go! On-Demand Delivery Using Fleets of Drones with Unknown,\n  Heterogeneous Energy Storage Constraints","summary":"  Unmanned Aerial Vehicles (UAVs) are expected to transform logistics, reducing\ndelivery time, costs, and emissions. This study addresses an on-demand delivery\n, in which fleets of UAVs are deployed to fulfil orders that arrive\nstochastically. Unlike previous work, it considers UAVs with heterogeneous,\nunknown energy storage capacities and assumes no knowledge of the energy\nconsumption models. We propose a decentralised deployment strategy that\ncombines auction-based task allocation with online learning. Each UAV\nindependently decides whether to bid for orders based on its energy storage\ncharge level, the parcel mass, and delivery distance. Over time, it refines its\npolicy to bid only for orders within its capability. Simulations using\nrealistic UAV energy models reveal that, counter-intuitively, assigning orders\nto the least confident bidders reduces delivery times and increases the number\nof successfully fulfilled orders. This strategy is shown to outperform\nthreshold-based methods which require UAVs to exceed specific charge levels at\ndeployment. We propose a variant of the strategy which uses learned policies\nfor forecasting. This enables UAVs with insufficient charge levels to commit to\nfulfilling orders at specific future times, helping to prioritise early orders.\nOur work provides new insights into long-term deployment of UAV swarms,\nhighlighting the advantages of decentralised energy-aware decision-making\ncoupled with online learning in real-world dynamic environments.\n","authors":["Mohamed S. Talamali","Genki Miyauchi","Thomas Watteyne","Micael S. Couceiro","Roderich Gross"],"pdf_url":"https://arxiv.org/pdf/2504.08585v1.pdf","comment":"The 24th International Conference on Autonomous Agents and Multiagent\n  Systems (AAMAS 2025)"},{"id":"http://arxiv.org/abs/2504.08584v1","updated":"2025-04-11T14:38:09Z","published":"2025-04-11T14:38:09Z","title":"Boosting multi-demographic federated learning for chest x-ray analysis\n  using general-purpose self-supervised representations","summary":"  Reliable artificial intelligence (AI) models for medical image analysis often\ndepend on large and diverse labeled datasets. Federated learning (FL) offers a\ndecentralized and privacy-preserving approach to training but struggles in\nhighly non-independent and identically distributed (non-IID) settings, where\ninstitutions with more representative data may experience degraded performance.\nMoreover, existing large-scale FL studies have been limited to adult datasets,\nneglecting the unique challenges posed by pediatric data, which introduces\nadditional non-IID variability. To address these limitations, we analyzed\nn=398,523 adult chest radiographs from diverse institutions across multiple\ncountries and n=9,125 pediatric images, leveraging transfer learning from\ngeneral-purpose self-supervised image representations to classify pneumonia and\ncases with no abnormality. Using state-of-the-art vision transformers, we found\nthat FL improved performance only for smaller adult datasets (P<0.001) but\ndegraded performance for larger datasets (P<0.064) and pediatric cases\n(P=0.242). However, equipping FL with self-supervised weights significantly\nenhanced outcomes across pediatric cases (P=0.031) and most adult datasets\n(P<0.008), except the largest dataset (P=0.052). These findings underscore the\npotential of easily deployable general-purpose self-supervised image\nrepresentations to address non-IID challenges in clinical FL applications and\nhighlight their promise for enhancing patient outcomes and advancing pediatric\nhealthcare, where data scarcity and variability remain persistent obstacles.\n","authors":["Mahshad Lotfinia","Arash Tayebiarasteh","Samaneh Samiei","Mehdi Joodaki","Soroosh Tayebi Arasteh"],"pdf_url":"https://arxiv.org/pdf/2504.08584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08583v1","updated":"2025-04-11T14:36:31Z","published":"2025-04-11T14:36:31Z","title":"AstroLLaVA: towards the unification of astronomical data and natural\n  language","summary":"  We present AstroLLaVA, a vision language model for astronomy that enables\ninteraction with astronomical imagery through natural dialogue. By fine-tuning\nthe LLaVA model on a diverse dataset of $\\sim$30k images with captions and\nquestion-answer pairs sourced from NASA's `Astronomy Picture of the Day', the\nEuropean Southern Observatory, and the NASA/ESA Hubble Space Telescope, we\ncreate a model capable of answering open-ended questions about astronomical\nconcepts depicted visually. Our two-stage fine-tuning process adapts the model\nto both image captioning and visual question answering in the astronomy domain.\nWe demonstrate AstroLLaVA's performance on an astronomical visual question\nanswering benchmark and release the model weights, code, and training set to\nencourage further open source work in this space. Finally, we suggest a roadmap\ntowards general astronomical data alignment with pre-trained language models,\nand provide an open space for collaboration towards this end for interested\nresearchers.\n","authors":["Sharaf Zaman","Michael J. Smith","Pranav Khetarpal","Rishabh Chakrabarty","Michele Ginolfi","Marc Huertas-Company","Maja Jabłońska","Sandor Kruk","Matthieu Le Lain","Sergio José Rodríguez Méndez","Dimitrios Tanoglidis"],"pdf_url":"https://arxiv.org/pdf/2504.08583v1.pdf","comment":"8 pages, 3 figures, accepted to SCI-FM@ICLR 2025. Code at\n  https://w3id.org/UniverseTBD/AstroLLaVA"},{"id":"http://arxiv.org/abs/2503.11217v2","updated":"2025-04-11T14:32:36Z","published":"2025-03-14T09:09:21Z","title":"Deep Joint Distribution Optimal Transport for Universal Domain\n  Adaptation on Time Series","summary":"  Universal Domain Adaptation (UniDA) aims to transfer knowledge from a labeled\nsource domain to an unlabeled target domain, even when their classes are not\nfully shared. Few dedicated UniDA methods exist for Time Series (TS), which\nremains a challenging case. In general, UniDA approaches align common class\nsamples and detect unknown target samples from emerging classes. Such detection\noften results from thresholding a discriminability metric. The threshold value\nis typically either a fine-tuned hyperparameter or a fixed value, which limits\nthe ability of the model to adapt to new data. Furthermore, discriminability\nmetrics exhibit overconfidence for unknown samples, leading to\nmisclassifications. This paper introduces UniJDOT, an optimal-transport-based\nmethod that accounts for the unknown target samples in the transport cost. Our\nmethod also proposes a joint decision space to improve the discriminability of\nthe detection module. In addition, we use an auto-thresholding algorithm to\nreduce the dependence on fixed or fine-tuned thresholds. Finally, we rely on a\nFourier transform-based layer inspired by the Fourier Neural Operator for\nbetter TS representation. Experiments on TS benchmarks demonstrate the\ndiscriminability, robustness, and state-of-the-art performance of UniJDOT.\n","authors":["Romain Mussard","Fannia Pacheco","Maxime Berar","Gilles Gasso","Paul Honeine"],"pdf_url":"https://arxiv.org/pdf/2503.11217v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06232v4","updated":"2025-04-11T14:14:17Z","published":"2024-10-08T17:41:37Z","title":"Range, not Independence, Drives Modularity in Biologically Inspired\n  Representations","summary":"  Why do biological and artificial neurons sometimes modularise, each encoding\na single meaningful variable, and sometimes entangle their representation of\nmany variables? In this work, we develop a theory of when biologically inspired\nnetworks -- those that are nonnegative and energy efficient -- modularise their\nrepresentation of source variables (sources). We derive necessary and\nsufficient conditions on a sample of sources that determine whether the neurons\nin an optimal biologically-inspired linear autoencoder modularise. Our theory\napplies to any dataset, extending far beyond the case of statistical\nindependence studied in previous work. Rather we show that sources modularise\nif their support is ``sufficiently spread''. From this theory, we extract and\nvalidate predictions in a variety of empirical studies on how data distribution\naffects modularisation in nonlinear feedforward and recurrent neural networks\ntrained on supervised and unsupervised tasks. Furthermore, we apply these ideas\nto neuroscience data, showing that range independence can be used to understand\nthe mixing or modularising of spatial and reward information in entorhinal\nrecordings in seemingly conflicting experiments. Further, we use these results\nto suggest alternate origins of mixed-selectivity, beyond the predominant\ntheory of flexible nonlinear classification. In sum, our theory prescribes\nprecise conditions on when neural activities modularise, providing tools for\ninducing and elucidating modular representations in brains and machines.\n","authors":["Will Dorrell","Kyle Hsu","Luke Hollingsworth","Jin Hwa Lee","Jiajun Wu","Chelsea Finn","Peter E Latham","Tim EJ Behrens","James CR Whittington"],"pdf_url":"https://arxiv.org/pdf/2410.06232v4.pdf","comment":"37 pages, 12 figures. WD and KH contributed equally; LH and JHL\n  contributed equally"},{"id":"http://arxiv.org/abs/2504.08554v1","updated":"2025-04-11T14:03:31Z","published":"2025-04-11T14:03:31Z","title":"Boosting-inspired online learning with transfer for railway maintenance","summary":"  The integration of advanced sensor technologies with deep learning algorithms\nhas revolutionized fault diagnosis in railway systems, particularly at the\nwheel-track interface. Although numerous models have been proposed to detect\nirregularities such as wheel out-of-roundness, they often fall short in\nreal-world applications due to the dynamic and nonstationary nature of railway\noperations. This paper introduces BOLT-RM (Boosting-inspired Online Learning\nwith Transfer for Railway Maintenance), a model designed to address these\nchallenges using continual learning for predictive maintenance. By allowing the\nmodel to continuously learn and adapt as new data become available, BOLT-RM\novercomes the issue of catastrophic forgetting that often plagues traditional\nmodels. It retains past knowledge while improving predictive accuracy with each\nnew learning episode, using a boosting-like knowledge sharing mechanism to\nadapt to evolving operational conditions such as changes in speed, load, and\ntrack irregularities. The methodology is validated through comprehensive\nmulti-domain simulations of train-track dynamic interactions, which capture\nrealistic railway operating conditions. The proposed BOLT-RM model demonstrates\nsignificant improvements in identifying wheel anomalies, establishing a\nreliable sequence for maintenance interventions.\n","authors":["Diogo Risca","Afonso Lourenço","Goreti Marreiros"],"pdf_url":"https://arxiv.org/pdf/2504.08554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08553v1","updated":"2025-04-11T14:03:23Z","published":"2025-04-11T14:03:23Z","title":"Uncovering the Structure of Explanation Quality with Spectral Analysis","summary":"  As machine learning models are increasingly considered for high-stakes\ndomains, effective explanation methods are crucial to ensure that their\nprediction strategies are transparent to the user. Over the years, numerous\nmetrics have been proposed to assess quality of explanations. However, their\npractical applicability remains unclear, in particular due to a limited\nunderstanding of which specific aspects each metric rewards. In this paper we\npropose a new framework based on spectral analysis of explanation outcomes to\nsystematically capture the multifaceted properties of different explanation\ntechniques. Our analysis uncovers two distinct factors of explanation\nquality-stability and target sensitivity-that can be directly observed through\nspectral decomposition. Experiments on both MNIST and ImageNet show that\npopular evaluation techniques (e.g., pixel-flipping, entropy) partially capture\nthe trade-offs between these factors. Overall, our framework provides a\nfoundational basis for understanding explanation quality, guiding the\ndevelopment of more reliable techniques for evaluating explanations.\n","authors":["Johannes Maeß","Grégoire Montavon","Shinichi Nakajima","Klaus-Robert Müller","Thomas Schnake"],"pdf_url":"https://arxiv.org/pdf/2504.08553v1.pdf","comment":"14 pages, 5 figures, Accepted at XAI World Conference 2025"},{"id":"http://arxiv.org/abs/2504.08544v1","updated":"2025-04-11T13:57:09Z","published":"2025-04-11T13:57:09Z","title":"Slicing the Gaussian Mixture Wasserstein Distance","summary":"  Gaussian mixture models (GMMs) are widely used in machine learning for tasks\nsuch as clustering, classification, image reconstruction, and generative\nmodeling. A key challenge in working with GMMs is defining a computationally\nefficient and geometrically meaningful metric. The mixture Wasserstein (MW)\ndistance adapts the Wasserstein metric to GMMs and has been applied in various\ndomains, including domain adaptation, dataset comparison, and reinforcement\nlearning. However, its high computational cost -- arising from repeated\nWasserstein distance computations involving matrix square root estimations and\nan expensive linear program -- limits its scalability to high-dimensional and\nlarge-scale problems. To address this, we propose multiple novel slicing-based\napproximations to the MW distance that significantly reduce computational\ncomplexity while preserving key optimal transport properties. From a\ntheoretical viewpoint, we establish several weak and strong equivalences\nbetween the introduced metrics, and show the relations to the original MW\ndistance and the well-established sliced Wasserstein distance. Furthermore, we\nvalidate the effectiveness of our approach through numerical experiments,\ndemonstrating computational efficiency and applications in clustering,\nperceptual image comparison, and GMM minimization\n","authors":["Moritz Piening","Robert Beinert"],"pdf_url":"https://arxiv.org/pdf/2504.08544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08536v1","updated":"2025-04-11T13:45:55Z","published":"2025-04-11T13:45:55Z","title":"Explainability and Continual Learning meet Federated Learning at the\n  Network Edge","summary":"  As edge devices become more capable and pervasive in wireless networks, there\nis growing interest in leveraging their collective compute power for\ndistributed learning. However, optimizing learning at the network edge entails\nunique challenges, particularly when moving beyond conventional settings and\nobjectives. While Federated Learning (FL) has emerged as a key paradigm for\ndistributed model training, critical challenges persist. First, existing\napproaches often overlook the trade-off between predictive accuracy and\ninterpretability. Second, they struggle to integrate inherently explainable\nmodels such as decision trees because their non-differentiable structure makes\nthem not amenable to backpropagation-based training algorithms. Lastly, they\nlack meaningful mechanisms for continual Machine Learning (ML) model adaptation\nthrough Continual Learning (CL) in resource-limited environments. In this\npaper, we pave the way for a set of novel optimization problems that emerge in\ndistributed learning at the network edge with wirelessly interconnected edge\ndevices, and we identify key challenges and future directions. Specifically, we\ndiscuss how Multi-objective optimization (MOO) can be used to address the\ntrade-off between predictive accuracy and explainability when using complex\npredictive models. Next, we discuss the implications of integrating inherently\nexplainable tree-based models into distributed learning settings. Finally, we\ninvestigate how CL strategies can be effectively combined with FL to support\nadaptive, lifelong learning when limited-size buffers are used to store past\ndata for retraining. Our approach offers a cohesive set of tools for designing\nprivacy-preserving, adaptive, and trustworthy ML solutions tailored to the\ndemands of edge computing and intelligent services.\n","authors":["Thomas Tsouparopoulos","Iordanis Koutsopoulos"],"pdf_url":"https://arxiv.org/pdf/2504.08536v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2504.08530v1","updated":"2025-04-11T13:41:14Z","published":"2025-04-11T13:41:14Z","title":"LGRPool: Hierarchical Graph Pooling Via Local-Global Regularisation","summary":"  Hierarchical graph pooling(HGP) are designed to consider the fact that\nconventional graph neural networks(GNN) are inherently flat and are also not\nmultiscale. However, most HGP methods suffer not only from lack of considering\nglobal topology of the graph and focusing on the feature learning aspect, but\nalso they do not align local and global features since graphs should inherently\nbe analyzed in a multiscale way. LGRPool is proposed in the present paper as a\nHGP in the framework of expectation maximization in machine learning that\naligns local and global aspects of message passing with each other using a\nregularizer to force the global topological information to be inline with the\nlocal message passing at different scales through the representations at\ndifferent layers of HGP. Experimental results on some graph classification\nbenchmarks show that it slightly outperforms some baselines.\n","authors":["Farshad Noravesh","Reza Haffari","Layki Soon","Arghya Pal"],"pdf_url":"https://arxiv.org/pdf/2504.08530v1.pdf","comment":"f tables, 2 figures"},{"id":"http://arxiv.org/abs/2501.08561v2","updated":"2025-04-11T13:05:47Z","published":"2025-01-15T04:04:57Z","title":"ANSR-DT: An Adaptive Neuro-Symbolic Learning and Reasoning Framework for\n  Digital Twins","summary":"  In this paper, we propose an Adaptive Neuro-Symbolic Learning and Reasoning\nFramework for digital twin technology called ``ANSR-DT.\" Digital twins in\nindustrial environments often struggle with interpretability, real-time\nadaptation, and human input integration. Our approach addresses these\nchallenges by combining CNN-LSTM dynamic event detection with reinforcement\nlearning and symbolic reasoning to enable adaptive intelligence with\ninterpretable decision processes. This integration enhances environmental\nunderstanding while promoting continuous learning, leading to more effective\nreal-time decision-making in human-machine collaborative applications. We\nevaluated ANSR-DT on synthetic industrial data, observing significant\nimprovements over traditional approaches, with up to 99.5% accuracy for dynamic\npattern recognition. The framework demonstrated superior adaptability with\nextended reinforcement learning training, improving explained variance from\n0.447 to 0.547. Future work aims at scaling to larger datasets to test rule\nmanagement beyond the current 14 rules. Our open-source implementation promotes\nreproducibility and establishes a foundation for future research in adaptive,\ninterpretable digital twins for industrial applications.\n","authors":["Safayat Bin Hakim","Muhammad Adil","Alvaro Velasquez","Houbing Herbert Song"],"pdf_url":"https://arxiv.org/pdf/2501.08561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15718v2","updated":"2025-04-11T12:55:40Z","published":"2024-02-24T04:57:59Z","title":"Optimal Rates and Saturation for Noiseless Kernel Ridge Regression","summary":"  Kernel ridge regression (KRR), also known as the least-squares support vector\nmachine, is a fundamental method for learning functions from finite samples.\nWhile most existing analyses focus on the noisy setting with constant-level\nlabel noise, we present a comprehensive study of KRR in the noiseless regime --\na critical setting in scientific computing where data are often generated via\nhigh-fidelity numerical simulations.\n  We establish that, up to logarithmic factors, noiseless KRR achieves minimax\noptimal convergence rates, jointly determined by the eigenvalue decay of the\nassociated integral operator and the target function's smoothness. These rates\nare derived under Sobolev-type interpolation norms, with the $L^2$ norm as a\nspecial case. Notably, we uncover two key phenomena: an extra-smoothness\neffect, where the KRR solution exhibits higher smoothness than typical\nfunctions in the native reproducing kernel Hilbert space (RKHS), and a\nsaturation effect, where the KRR's adaptivity to the target function's\nsmoothness plateaus beyond a certain level. Leveraging these insights, we also\nderive a novel error bound for noisy KRR that is noise-level aware and achieves\nminimax optimality in both noiseless and noisy regimes. As a key technical\ncontribution, we introduce a refined notion of degrees of freedom, which we\nbelieve has broader applicability in the analysis of kernel methods. Extensive\nnumerical experiments validate our theoretical results and provide insights\nbeyond existing theory.\n","authors":["Jihao Long","Xiaojun Peng","Lei Wu"],"pdf_url":"https://arxiv.org/pdf/2402.15718v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10882v2","updated":"2025-04-11T12:47:12Z","published":"2022-11-20T06:31:53Z","title":"Multi-head Ensemble of Smoothed Classifiers for Certified Robustness","summary":"  Randomized Smoothing (RS) is a promising technique for certified robustness,\nand recently in RS the ensemble of multiple Deep Neural Networks (DNNs) has\nshown state-of-the-art performances due to its variance reduction effect over\nGaussian noises. However, such an ensemble brings heavy computation burdens in\nboth training and certification, and yet under-exploits individual DNNs and\ntheir mutual effects, as the communication between these classifiers is\ncommonly ignored in optimization. In this work, we consider a novel\nensemble-based training way for a single DNN with multiple augmented heads,\nnamed as SmOothed Multi-head Ensemble (SOME). In SOME, similar to the pursuit\nof variance reduction via ensemble, an ensemble of multiple heads imposed with\na cosine constraint inside a single DNN is employed with much cheaper training\nand certification computation overloads in RS. In such network structure, an\nassociated training strategy is designed by introducing a circular\ncommunication flow among those augmented heads. That is, each head teaches its\nneighbor with the self-paced learning strategy using smoothed losses, which are\nspecifically designed in relation to certified robustness. The deployed\nmulti-head structure and the circular-teaching scheme in SOME jointly\ncontribute to the diversities among multiple heads and benefit their ensemble,\nleading to a competitively stronger certifiably-robust RS-based defense than\nensembling multiple DNNs (effectiveness) at the cost of much less computational\nexpenses (efficiency), verified by extensive experiments and discussions.\n","authors":["Kun Fang","Qinghua Tao","Yingwen Wu","Tao Li","Xiaolin Huang","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2211.10882v2.pdf","comment":"Accepted by Neural Networks"},{"id":"http://arxiv.org/abs/2410.05837v2","updated":"2025-04-11T12:46:31Z","published":"2024-10-08T09:06:32Z","title":"A noise-corrected Langevin algorithm and sampling by half-denoising","summary":"  The Langevin algorithm is a classic method for sampling from a given pdf in a\nreal space. In its basic version, it only requires knowledge of the gradient of\nthe log-density, also called the score function. However, in deep learning, it\nis often easier to learn the so-called \"noisy-data score function\", i.e. the\ngradient of the log-density of noisy data, more precisely when Gaussian noise\nis added to the data. Such an estimate is biased and complicates the use of the\nLangevin method. Here, we propose a noise-corrected version of the Langevin\nalgorithm, where the bias due to noisy data is removed, at least regarding\nfirst-order terms. Unlike diffusion models, our algorithm needs to know the\nnoisy score function for one single noise level only. We further propose a\nsimple special case which has an interesting intuitive interpretation of\niteratively adding noise the data and then attempting to remove half of that\nnoise.\n","authors":["Aapo Hyvärinen"],"pdf_url":"https://arxiv.org/pdf/2410.05837v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08489v1","updated":"2025-04-11T12:36:06Z","published":"2025-04-11T12:36:06Z","title":"Statistically guided deep learning","summary":"  We present a theoretically well-founded deep learning algorithm for\nnonparametric regression. It uses over-parametrized deep neural networks with\nlogistic activation function, which are fitted to the given data via gradient\ndescent. We propose a special topology of these networks, a special random\ninitialization of the weights, and a data-dependent choice of the learning rate\nand the number of gradient descent steps. We prove a theoretical bound on the\nexpected $L_2$ error of this estimate, and illustrate its finite sample size\nperformance by applying it to simulated data. Our results show that a\ntheoretical analysis of deep learning which takes into account simultaneously\noptimization, generalization and approximation can result in a new deep\nlearning estimate which has an improved finite sample performance.\n","authors":["Michael Kohler","Adam Krzyzak"],"pdf_url":"https://arxiv.org/pdf/2504.08489v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2504.03405"},{"id":"http://arxiv.org/abs/2410.03363v2","updated":"2025-04-11T12:20:30Z","published":"2024-10-04T12:30:03Z","title":"Minimax-optimal and Locally-adaptive Online Nonparametric Regression","summary":"  We study adversarial online nonparametric regression with general convex\nlosses and propose a parameter-free learning algorithm that achieves minimax\noptimal rates. Our approach leverages chaining trees to compete against\nH{\\\"o}lder functions and establishes optimal regret bounds. While competing\nwith nonparametric function classes can be challenging, they often exhibit\nlocal patterns - such as local H{\\\"o}lder continuity - that online algorithms\ncan exploit. Without prior knowledge, our method dynamically tracks and adapts\nto different H{\\\"o}lder profiles by pruning a core chaining tree structure,\naligning itself with local smoothness variations. This leads to the first\ncomputationally efficient algorithm with locally adaptive optimal rates for\nonline regression in an adversarial setting. Finally, we discuss how these\nnotions could be extended to a boosting framework, offering promising\ndirections for future research.\n","authors":["Paul Liautaud","Pierre Gaillard","Olivier Wintenberger"],"pdf_url":"https://arxiv.org/pdf/2410.03363v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08484v1","updated":"2025-04-11T12:19:51Z","published":"2025-04-11T12:19:51Z","title":"Physics-informed data-driven control without persistence of excitation","summary":"  We show that data that is not sufficiently informative to allow for system\nre-identification can still provide meaningful information when combined with\nexternal or physical knowledge of the system, such as bounded system matrix\nnorms. We then illustrate how this information can be leveraged for safety and\nenergy minimization problems and to enhance predictions in unmodelled dynamics.\nThis preliminary work outlines key ideas toward using limited data for\neffective control by integrating physical knowledge of the system and\nexploiting interpolation conditions.\n","authors":["Martina Vanelli","Julien M. Hendrickx"],"pdf_url":"https://arxiv.org/pdf/2504.08484v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.21443v2","updated":"2025-04-11T12:13:02Z","published":"2024-10-28T18:40:06Z","title":"TACO: Adversarial Camouflage Optimization on Trucks to Fool Object\n  Detectors","summary":"  Adversarial attacks threaten the reliability of machine learning models in\ncritical applications like autonomous vehicles and defense systems. As object\ndetectors become more robust with models like YOLOv8, developing effective\nadversarial methodologies is increasingly challenging. We present Truck\nAdversarial Camouflage Optimization (TACO), a novel framework that generates\nadversarial camouflage patterns on 3D vehicle models to deceive\nstate-of-the-art object detectors. Adopting Unreal Engine 5, TACO integrates\ndifferentiable rendering with a Photorealistic Rendering Network to optimize\nadversarial textures targeted at YOLOv8. To ensure the generated textures are\nboth effective in deceiving detectors and visually plausible, we introduce the\nConvolutional Smooth Loss function, a generalized smooth loss function.\nExperimental evaluations demonstrate that TACO significantly degrades YOLOv8's\ndetection performance, achieving an AP@0.5 of 0.0099 on unseen test data.\nFurthermore, these adversarial patterns exhibit strong transferability to other\nobject detection models such as Faster R-CNN and earlier YOLO versions.\n","authors":["Adonisz Dimitriu","Tamás Michaletzky","Viktor Remeli"],"pdf_url":"https://arxiv.org/pdf/2410.21443v2.pdf","comment":"This version matches the final published version in Big Data and\n  Cognitive Computing (MDPI). Please cite the journal version when referencing\n  this work (doi: https://doi.org/10.3390/bdcc9030072)"},{"id":"http://arxiv.org/abs/2411.09111v7","updated":"2025-04-11T12:09:42Z","published":"2024-11-14T00:59:13Z","title":"Reducing Reasoning Costs: The Path of Optimization for Chain of Thought\n  via Sparse Attention Mechanism","summary":"  In order to address the chain of thought in the large language model\ninference cost surge, this research proposes to use a sparse attention\nmechanism that only focuses on a few relevant tokens. The researcher\nconstructed a new attention mechanism and used GiantRabbit trained with custom\nGPTs as an experimental tool. The experiment tested and compared the reasoning\ntime, correctness score and chain of thought length of this model and o1\nPreview in solving the linear algebra test questions of MIT OpenCourseWare. The\nresults show that GiantRabbit's reasoning time and chain of thought length are\nsignificantly lower than o1 Preview. It verifies the feasibility of sparse\nattention mechanism for optimizing chain of thought reasoning. Detailed\narchitectural details and experimental process have been uploaded to Github,\nthe link is:https://github.com/brucewang123456789/GeniusTrail.git.\n","authors":["Libo Wang"],"pdf_url":"https://arxiv.org/pdf/2411.09111v7.pdf","comment":"The main text is 5 pages, totaling 9 pages; 4 figures, 1 table. It\n  have been submitted to NeurIPS 2024 Workshop MusIML and OpenReview"},{"id":"http://arxiv.org/abs/2402.04613v4","updated":"2025-04-11T12:00:09Z","published":"2024-02-07T06:30:39Z","title":"Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in\n  Reproducing Kernel Hilbert Spaces","summary":"  Commonly used $f$-divergences of measures, e.g., the Kullback-Leibler\ndivergence, are subject to limitations regarding the support of the involved\nmeasures. A remedy is regularizing the $f$-divergence by a squared maximum mean\ndiscrepancy (MMD) associated with a characteristic kernel $K$. We use the\nkernel mean embedding to show that this regularization can be rewritten as the\nMoreau envelope of some function on the associated reproducing kernel Hilbert\nspace. Then, we exploit well-known results on Moreau envelopes in Hilbert\nspaces to analyze the MMD-regularized $f$-divergences, particularly their\ngradients. Subsequently, we use our findings to analyze Wasserstein gradient\nflows of MMD-regularized $f$-divergences. We provide proof-of-the-concept\nnumerical examples for flows starting from empirical measures. Here, we cover\n$f$-divergences with infinite and finite recession constants. Lastly, we extend\nour results to the tight variational formulation of $f$-divergences and\nnumerically compare the resulting flows.\n","authors":["Viktor Stein","Sebastian Neumayer","Nicolaj Rux","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2402.04613v4.pdf","comment":"56 pages, 14 figures, 3 tables. Comments welcome! NEW: Incorporated\n  Reviewers' suggestions, added FISTA and tight formulation, typos fixed"},{"id":"http://arxiv.org/abs/2504.08469v1","updated":"2025-04-11T11:57:06Z","published":"2025-04-11T11:57:06Z","title":"Artifact detection and localization in single-channel mobile EEG for\n  sleep research using deep learning and attention mechanisms","summary":"  Artifacts in the electroencephalogram (EEG) degrade signal quality and impact\nthe analysis of brain activity. Current methods for detecting artifacts in\nsleep EEG rely on simple threshold-based algorithms that require manual\nintervention, which is time-consuming and impractical due to the vast volume of\ndata that novel mobile recording systems generate. We propose a convolutional\nneural network (CNN) model incorporating a convolutional block attention module\n(CNN-CBAM) to detect and identify the location of artifacts in the sleep EEG\nwith attention maps. We benchmarked this model against six other machine\nlearning and signal processing approaches. We trained/tuned all models on 72\nmanually annotated EEG recordings obtained during home-based monitoring from 18\nhealthy participants with a mean (SD) age of 68.05 y ($\\pm$5.02). We tested\nthem on 26 separate recordings from 6 healthy participants with a mean (SD) age\nof 68.33 y ($\\pm$4.08), with contained artifacts in 4\\% of epochs. CNN-CBAM\nachieved the highest area under the receiver operating characteristic curve\n(0.88), sensitivity (0.81), and specificity (0.86) when compared to the other\napproaches. The attention maps from CNN-CBAM localized artifacts within the\nepoch with a sensitivity of 0.71 and specificity of 0.67. This work\ndemonstrates the feasibility of automating the detection and localization of\nartifacts in wearable sleep EEG.\n","authors":["Khrystyna Semkiv","Jia Zhang","Maria Laura Ferster","Walter Karlen"],"pdf_url":"https://arxiv.org/pdf/2504.08469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03887v2","updated":"2025-04-11T11:51:19Z","published":"2024-10-04T19:42:14Z","title":"Solving Dual Sourcing Problems with Supply Mode Dependent Failure Rates","summary":"  This paper investigates dual sourcing problems with supply mode dependent\nfailure rates, particularly relevant in managing spare parts for\ndowntime-critical assets. To enhance resilience, businesses increasingly adopt\ndual sourcing strategies using both conventional and additive manufacturing\ntechniques. This paper explores how these strategies can optimise sourcing by\naddressing variations in part properties and failure rates. A significant\nchallenge is the distinct failure characteristics of parts produced by these\nmethods, which influence future demand. To tackle this, we propose a new\niterative heuristic and several reinforcement learning techniques combined with\nan endogenous parameterised learning (EPL) approach. This EPL approach -\ncompatible with any learning method - allows a single policy to handle various\ninput parameters for multiple items. In a stylised setting, our best policy\nachieves an average optimality gap of 0.4%. In a case study within the energy\nsector, our policies outperform the baseline in 91.1% of instances, yielding\naverage cost savings up to 22.6%.\n","authors":["Fabian Akkerman","Nils Knofius","Matthieu van der Heijden","Martijn Mes"],"pdf_url":"https://arxiv.org/pdf/2410.03887v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14695v3","updated":"2025-04-11T11:15:41Z","published":"2024-11-22T03:05:06Z","title":"Anti-Forgetting Adaptation for Unsupervised Person Re-identification","summary":"  Regular unsupervised domain adaptive person re-identification (ReID) focuses\non adapting a model from a source domain to a fixed target domain. However, an\nadapted ReID model can hardly retain previously-acquired knowledge and\ngeneralize to unseen data. In this paper, we propose a Dual-level Joint\nAdaptation and Anti-forgetting (DJAA) framework, which incrementally adapts a\nmodel to new domains without forgetting source domain and each adapted target\ndomain. We explore the possibility of using prototype and instance-level\nconsistency to mitigate the forgetting during the adaptation. Specifically, we\nstore a small number of representative image samples and corresponding cluster\nprototypes in a memory buffer, which is updated at each adaptation step. With\nthe buffered images and prototypes, we regularize the image-to-image similarity\nand image-to-prototype similarity to rehearse old knowledge. After the\nmulti-step adaptation, the model is tested on all seen domains and several\nunseen domains to validate the generalization ability of our method. Extensive\nexperiments demonstrate that our proposed method significantly improves the\nanti-forgetting, generalization and backward-compatible ability of an\nunsupervised person ReID model.\n","authors":["Hao Chen","Francois Bremond","Nicu Sebe","Shiliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14695v3.pdf","comment":"Accepted to TPAMI"},{"id":"http://arxiv.org/abs/2504.08445v1","updated":"2025-04-11T11:11:35Z","published":"2025-04-11T11:11:35Z","title":"A Systematic Evaluation of Knowledge Graph Embeddings for Gene-Disease\n  Association Prediction","summary":"  Discovery gene-disease links is important in biology and medicine areas,\nenabling disease identification and drug repurposing. Machine learning\napproaches accelerate this process by leveraging biological knowledge\nrepresented in ontologies and the structure of knowledge graphs. Still, many\nexisting works overlook ontologies explicitly representing diseases, missing\ncausal and semantic relationships between them. The gene-disease association\nproblem naturally frames itself as a link prediction task, where embedding\nalgorithms directly predict associations by exploring the structure and\nproperties of the knowledge graph. Some works frame it as a node-pair\nclassification task, combining embedding algorithms with traditional machine\nlearning algorithms. This strategy aligns with the logic of a machine learning\npipeline. However, the use of negative examples and the lack of validated\ngene-disease associations to train embedding models may constrain its\neffectiveness. This work introduces a novel framework for comparing the\nperformance of link prediction versus node-pair classification tasks, analyses\nthe performance of state of the art gene-disease association approaches, and\ncompares the different order-based formalizations of gene-disease association\nprediction. It also evaluates the impact of the semantic richness through a\ndisease-specific ontology and additional links between ontologies. The\nframework involves five steps: data splitting, knowledge graph integration,\nembedding, modeling and prediction, and method evaluation. Results show that\nenriching the semantic representation of diseases slightly improves\nperformance, while additional links generate a greater impact. Link prediction\nmethods better explore the semantic richness encoded in knowledge graphs.\nAlthough node-pair classification methods identify all true positives, link\nprediction methods outperform overall.\n","authors":["Catarina Canastra","Cátia Pesquita"],"pdf_url":"https://arxiv.org/pdf/2504.08445v1.pdf","comment":"48 pages, 2 figures, 18 tables"},{"id":"http://arxiv.org/abs/2504.08437v1","updated":"2025-04-11T10:55:01Z","published":"2025-04-11T10:55:01Z","title":"Customizing Spider Silk: Generative Models with Mechanical Property\n  Conditioning for Protein Engineering","summary":"  The remarkable mechanical properties of spider silk, including its tensile\nstrength and extensibility, are primarily governed by the repetitive regions of\nthe proteins that constitute the fiber, the major ampullate spidroins (MaSps).\nHowever, establishing correlations between mechanical characteristics and\nrepeat sequences is challenging due to the intricate\nsequence-structure-function relationships of MaSps and the limited availability\nof annotated datasets. In this study, we present a novel computational\nframework for designing MaSp repeat sequences with customizable mechanical\nproperties. To achieve this, we developed a lightweight GPT-based generative\nmodel by distilling the pre-trained ProtGPT2 protein language model. The\ndistilled model was subjected to multilevel fine-tuning using curated subsets\nof the Spider Silkome dataset. Specifically, we adapt the model for MaSp repeat\ngeneration using 6,000 MaSp repeat sequences and further refine it with 572\nrepeats associated with experimentally determined fiber-level mechanical\nproperties. Our model generates biologically plausible MaSp repeat regions\ntailored to specific mechanical properties while also predicting those\nproperties for given sequences. Validation includes sequence-level analysis,\nassessing physicochemical attributes and expected distribution of key motifs as\nwell as secondary structure compositions. A correlation study using BLAST on\nthe Spider Silkome dataset and a test set of MaSp repeats with known mechanical\nproperties further confirmed the predictive accuracy of the model. This\nframework advances the rational design of spider silk-inspired biomaterials,\noffering a versatile tool for engineering protein sequences with tailored\nmechanical attributes.\n","authors":["Neeru Dubey","Elin Karlsson","Miguel Angel Redondo","Johan Reimegård","Anna Rising","Hedvig Kjellström"],"pdf_url":"https://arxiv.org/pdf/2504.08437v1.pdf","comment":"23 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.02781v3","updated":"2025-04-11T10:52:32Z","published":"2024-12-03T19:20:56Z","title":"Methods with Local Steps and Random Reshuffling for Generally Smooth\n  Non-Convex Federated Optimization","summary":"  Non-convex Machine Learning problems typically do not adhere to the standard\nsmoothness assumption. Based on empirical findings, Zhang et al. (2020b)\nproposed a more realistic generalized $(L_0, L_1)$-smoothness assumption,\nthough it remains largely unexplored. Many existing algorithms designed for\nstandard smooth problems need to be revised. However, in the context of\nFederated Learning, only a few works address this problem but rely on\nadditional limiting assumptions. In this paper, we address this gap in the\nliterature: we propose and analyze new methods with local steps, partial\nparticipation of clients, and Random Reshuffling without extra restrictive\nassumptions beyond generalized smoothness. The proposed methods are based on\nthe proper interplay between clients' and server's stepsizes and gradient\nclipping. Furthermore, we perform the first analysis of these methods under the\nPolyak-{\\L} ojasiewicz condition. Our theory is consistent with the known\nresults for standard smooth problems, and our experimental results support the\ntheoretical insights.\n","authors":["Yury Demidovich","Petr Ostroukhov","Grigory Malinovsky","Samuel Horváth","Martin Takáč","Peter Richtárik","Eduard Gorbunov"],"pdf_url":"https://arxiv.org/pdf/2412.02781v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08428v1","updated":"2025-04-11T10:37:19Z","published":"2025-04-11T10:37:19Z","title":"Standardization of Weighted Ranking Correlation Coefficients","summary":"  A relevant problem in statistics is defining the correlation of two rankings\nof a list of items. Kendall's tau and Spearman's rho are two well established\ncorrelation coefficients, characterized by a symmetric form that ensures zero\nexpected value between two pairs of rankings randomly chosen with uniform\nprobability. However, in recent years, several weighted versions of the\noriginal Spearman and Kendall coefficients have emerged that take into account\nthe greater importance of top ranks compared to low ranks, which is common in\nmany contexts. The weighting schemes break the symmetry, causing a non-zero\nexpected value between two random rankings. This issue is very relevant, as it\nundermines the concept of uncorrelation between rankings. In this paper, we\naddress this problem by proposing a standardization function $g(x)$ that maps a\ncorrelation ranking coefficient $\\Gamma$ in a standard form $g(\\Gamma)$ that\nhas zero expected value, while maintaining the relevant statistical properties\nof $\\Gamma$.\n","authors":["Pierangelo Lombardo"],"pdf_url":"https://arxiv.org/pdf/2504.08428v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.08418v1","updated":"2025-04-11T10:23:10Z","published":"2025-04-11T10:23:10Z","title":"seeBias: A Comprehensive Tool for Assessing and Visualizing AI Fairness","summary":"  Fairness in artificial intelligence (AI) prediction models is increasingly\nemphasized to support responsible adoption in high-stakes domains such as\nhealth care and criminal justice. Guidelines and implementation frameworks\nhighlight the importance of both predictive accuracy and equitable outcomes.\nHowever, current fairness toolkits often evaluate classification performance\ndisparities in isolation, with limited attention to other critical aspects such\nas calibration. To address these gaps, we present seeBias, an R package for\ncomprehensive evaluation of model fairness and predictive performance. seeBias\noffers an integrated evaluation across classification, calibration, and other\nperformance domains, providing a more complete view of model behavior. It\nincludes customizable visualizations to support transparent reporting and\nresponsible AI implementation. Using public datasets from criminal justice and\nhealthcare, we demonstrate how seeBias supports fairness evaluations, and\nuncovers disparities that conventional fairness metrics may overlook. The R\npackage is available on GitHub, and a Python version is under development.\n","authors":["Yilin Ning","Yian Ma","Mingxuan Liu","Xin Li","Nan Liu"],"pdf_url":"https://arxiv.org/pdf/2504.08418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08417v1","updated":"2025-04-11T10:21:58Z","published":"2025-04-11T10:21:58Z","title":"Belief States for Cooperative Multi-Agent Reinforcement Learning under\n  Partial Observability","summary":"  Reinforcement learning in partially observable environments is typically\nchallenging, as it requires agents to learn an estimate of the underlying\nsystem state. These challenges are exacerbated in multi-agent settings, where\nagents learn simultaneously and influence the underlying state as well as each\nothers' observations. We propose the use of learned beliefs on the underlying\nstate of the system to overcome these challenges and enable reinforcement\nlearning with fully decentralized training and execution. Our approach\nleverages state information to pre-train a probabilistic belief model in a\nself-supervised fashion. The resulting belief states, which capture both\ninferred state information as well as uncertainty over this information, are\nthen used in a state-based reinforcement learning algorithm to create an\nend-to-end model for cooperative multi-agent reinforcement learning under\npartial observability. By separating the belief and reinforcement learning\ntasks, we are able to significantly simplify the policy and value function\nlearning tasks and improve both the convergence speed and the final\nperformance. We evaluate our proposed method on diverse partially observable\nmulti-agent tasks designed to exhibit different variants of partial\nobservability.\n","authors":["Paul J. Pritz","Kin K. Leung"],"pdf_url":"https://arxiv.org/pdf/2504.08417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20505v2","updated":"2025-04-11T10:21:06Z","published":"2025-03-26T12:45:52Z","title":"Riemannian Optimization on Relaxed Indicator Matrix Manifold","summary":"  The indicator matrix plays an important role in machine learning, but\noptimizing it is an NP-hard problem. We propose a new relaxation of the\nindicator matrix and prove that this relaxation forms a manifold, which we call\nthe Relaxed Indicator Matrix Manifold (RIM manifold). Based on Riemannian\ngeometry, we develop a Riemannian toolbox for optimization on the RIM manifold.\nSpecifically, we provide several methods of Retraction, including a fast\nRetraction method to obtain geodesics. We point out that the RIM manifold is a\ngeneralization of the double stochastic manifold, and it is much faster than\nexisting methods on the double stochastic manifold, which has a complexity of\n\\( \\mathcal{O}(n^3) \\), while RIM manifold optimization is \\( \\mathcal{O}(n) \\)\nand often yields better results. We conducted extensive experiments, including\nimage denoising, with millions of variables to support our conclusion, and\napplied the RIM manifold to Ratio Cut, we provide a rigorous convergence proof\nand achieve clustering results that outperform the state-of-the-art methods.\nOur Code in\n\\href{https://github.com/Yuan-Jinghui/Riemannian-Optimization-on-Relaxed-Indicator-Matrix-Manifold}{here}.\n","authors":["Jinghui Yuan","Fangyuan Xie","Feiping Nie","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2503.20505v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08415v1","updated":"2025-04-11T10:19:49Z","published":"2025-04-11T10:19:49Z","title":"Constrained Machine Learning Through Hyperspherical Representation","summary":"  The problem of ensuring constraints satisfaction on the output of machine\nlearning models is critical for many applications, especially in\nsafety-critical domains. Modern approaches rely on penalty-based methods at\ntraining time, which do not guarantee to avoid constraints violations; or\nconstraint-specific model architectures (e.g., for monotonocity); or on output\nprojection, which requires to solve an optimization problem that might be\ncomputationally demanding. We present the Hypersherical Constrained\nRepresentation, a novel method to enforce constraints in the output space for\nconvex and bounded feasibility regions (generalizable to star domains). Our\nmethod operates on a different representation system, where Euclidean\ncoordinates are converted into hyperspherical coordinates relative to the\nconstrained region, which can only inherently represent feasible points.\nExperiments on a synthetic and a real-world dataset show that our method has\npredictive performance comparable to the other approaches, can guarantee 100%\nconstraint satisfaction, and has a minimal computational cost at inference\ntime.\n","authors":["Gaetano Signorelli","Michele Lombardi"],"pdf_url":"https://arxiv.org/pdf/2504.08415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08408v1","updated":"2025-04-11T10:14:40Z","published":"2025-04-11T10:14:40Z","title":"BOISHOMMO: Holistic Approach for Bangla Hate Speech","summary":"  One of the most alarming issues in digital society is hate speech (HS) on\nsocial media. The severity is so high that researchers across the globe are\ncaptivated by this domain. A notable amount of work has been conducted to\naddress the identification and alarm system. However, a noticeable gap exists,\nespecially for low-resource languages. Comprehensive datasets are the main\nproblem among the constrained resource languages, such as Bangla.\nInterestingly, hate speech or any particular speech has no single\ndimensionality. Similarly, the hate component can simultaneously have multiple\nabusive attributes, which seems to be missed in the existing datasets. Thus, a\nmulti-label Bangla hate speech dataset named BOISHOMMO has been compiled and\nevaluated in this work. That includes categories of HS across race, gender,\nreligion, politics, and more. With over two thousand annotated examples,\nBOISHOMMO provides a nuanced understanding of hate speech in Bangla and\nhighlights the complexities of processing non-Latin scripts. Apart from\nevaluating with multiple algorithmic approaches, it also highlights the\ncomplexities of processing Bangla text and assesses model performance. This\nunique multi-label approach enriches future hate speech detection and analysis\nstudies for low-resource languages by providing a more nuanced, diverse\ndataset.\n","authors":["Md Abdullah Al Kafi","Sumit Kumar Banshal","Md Sadman Shakib","Showrov Azam","Tamanna Alam Tabashom"],"pdf_url":"https://arxiv.org/pdf/2504.08408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07199v2","updated":"2025-04-11T10:14:39Z","published":"2025-04-09T18:26:46Z","title":"SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject\n  Tagging for a National Technical Library's Open-Access Catalog","summary":"  We present SemEval-2025 Task 5: LLMs4Subjects, a shared task on automated\nsubject tagging for scientific and technical records in English and German\nusing the GND taxonomy. Participants developed LLM-based systems to recommend\ntop-k subjects, evaluated through quantitative metrics (precision, recall,\nF1-score) and qualitative assessments by subject specialists. Results highlight\nthe effectiveness of LLM ensembles, synthetic data generation, and multilingual\nprocessing, offering insights into applying LLMs for digital library\nclassification.\n","authors":["Jennifer D'Souza","Sameer Sadruddin","Holger Israel","Mathias Begoin","Diana Slawig"],"pdf_url":"https://arxiv.org/pdf/2504.07199v2.pdf","comment":"10 pages, 4 figures, Accepted as SemEval 2025 Task 5 description\n  paper"},{"id":"http://arxiv.org/abs/2504.08401v1","updated":"2025-04-11T10:08:38Z","published":"2025-04-11T10:08:38Z","title":"Graph Reduction with Unsupervised Learning in Column Generation: A\n  Routing Application","summary":"  Column Generation (CG) is a popular method dedicated to enhancing\ncomputational efficiency in large scale Combinatorial Optimization (CO)\nproblems. It reduces the number of decision variables in a problem by solving a\npricing problem. For many CO problems, the pricing problem is an Elementary\nShortest Path Problem with Resource Constraints (ESPPRC). Large ESPPRC\ninstances are difficult to solve to near-optimality. Consequently, we use a\nGraph neural Network (GNN) to reduces the size of the ESPPRC such that it\nbecomes computationally tractable with standard solving techniques. Our GNN is\ntrained by Unsupervised Learning and outputs a distribution for the arcs to be\nretained in the reduced PP. The reduced PP is solved by a local search that\nfinds columns with large reduced costs and speeds up convergence. We apply our\nmethod on a set of Capacitated Vehicle Routing Problems with Time Windows and\nshow significant improvements in convergence compared to simple reduction\ntechniques from the literature. For a fixed computational budget, we improve\nthe objective values by over 9\\% for larger instances. We also analyze the\nperformance of our CG algorithm and test the generalization of our method to\ndifferent classes of instances than the training data.\n","authors":["Abdo Abouelrous","Laurens Bliea","Adriana F. Gabor","Yaoxin Wu","Yingqian Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.08401v1.pdf","comment":"22 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2504.08398v1","updated":"2025-04-11T10:03:06Z","published":"2025-04-11T10:03:06Z","title":"MixDiT: Accelerating Image Diffusion Transformer Inference with\n  Mixed-Precision MX Quantization","summary":"  Diffusion Transformer (DiT) has driven significant progress in image\ngeneration tasks. However, DiT inferencing is notoriously compute-intensive and\nincurs long latency even on datacenter-scale GPUs, primarily due to its\niterative nature and heavy reliance on GEMM operations inherent to its\nencoder-based structure. To address the challenge, prior work has explored\nquantization, but achieving low-precision quantization for DiT inferencing with\nboth high accuracy and substantial speedup remains an open problem. To this\nend, this paper proposes MixDiT, an algorithm-hardware co-designed acceleration\nsolution that exploits mixed Microscaling (MX) formats to quantize DiT\nactivation values. MixDiT quantizes the DiT activation tensors by selectively\napplying higher precision to magnitude-based outliers, which produce\nmixed-precision GEMM operations. To achieve tangible speedup from the\nmixed-precision arithmetic, we design a MixDiT accelerator that enables\nprecision-flexible multiplications and efficient MX precision conversions. Our\nexperimental results show that MixDiT delivers a speedup of 2.10-5.32 times\nover RTX 3090, with no loss in FID.\n","authors":["Daeun Kim","Jinwoo Hwang","Changhun Oh","Jongse Park"],"pdf_url":"https://arxiv.org/pdf/2504.08398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00928v3","updated":"2025-04-11T09:56:07Z","published":"2024-11-01T15:58:02Z","title":"A Bregman firmly nonexpansive proximal operator for baryconvex\n  optimization","summary":"  We present a generalization of the proximal operator defined through a convex\ncombination of convex objectives, where the coefficients are updated in a\nminimax fashion. We prove that this new operator is Bregman firmly nonexpansive\nwith respect to a Bregman divergence that combines Euclidean and information\ngeometries; and that its fixed points are given by the critical points of a\ncertain nonconvex function. Finally, we derive the associated continuous flows.\n","authors":["Mastane Achab"],"pdf_url":"https://arxiv.org/pdf/2411.00928v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07557v2","updated":"2025-04-11T09:54:57Z","published":"2025-04-10T08:38:39Z","title":"Using LLMs for Analyzing AIS Data","summary":"  Recent research in Large Language Models (LLMs), has had a profound impact\nacross various fields, including mobility data science. This paper explores the\nand experiment with different approaches to using LLMs for analyzing AIS data.\nWe propose a set of carefully designed queries to assess the reasoning\ncapabilities of LLMs in this kind of tasks. Further, we experiment with four\ndifferent methods: (1) using LLMs as a natural language interface to a spatial\ndatabase, (2) reasoning on raw data, (3) reasoning on compressed trajectories,\nand (4) reasoning on semantic trajectories. We investigate the strengths and\nweaknesses for the four methods, and discuss the findings. The goal is to\nprovide valuable insights for both researchers and practitioners on selecting\nthe most appropriate LLM-based method depending on their specific data analysis\nobjectives.\n","authors":["Gaspard Merten","Gilles Dejaegere","Mahmoud Sakr"],"pdf_url":"https://arxiv.org/pdf/2504.07557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08576v2","updated":"2025-04-11T09:41:45Z","published":"2025-02-12T17:10:34Z","title":"Mapping the Landscape of Generative AI in Network Monitoring and\n  Management","summary":"  Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and\nDiffusion Models have recently gained widespread attention from both the\nresearch and the industrial communities. This survey explores their application\nin network monitoring and management, focusing on prominent use cases, as well\nas challenges and opportunities. We discuss how network traffic generation and\nclassification, network intrusion detection, networked system log analysis, and\nnetwork digital assistance can benefit from the use of GenAI models.\nAdditionally, we provide an overview of the available GenAI models, datasets\nfor large-scale training phases, and platforms for the development of such\nmodels. Finally, we discuss research directions that potentially mitigate the\nroadblocks to the adoption of GenAI for network monitoring and management. Our\ninvestigation aims to map the current landscape and pave the way for future\nresearch in leveraging GenAI for network monitoring and management.\n","authors":["Giampaolo Bovenzi","Francesco Cerasuolo","Domenico Ciuonzo","Davide Di Monda","Idio Guarino","Antonio Montieri","Valerio Persico","Antonio Pescapè"],"pdf_url":"https://arxiv.org/pdf/2502.08576v2.pdf","comment":"32 pages, 9 figure, 10 tables"},{"id":"http://arxiv.org/abs/2504.08386v1","updated":"2025-04-11T09:38:12Z","published":"2025-04-11T09:38:12Z","title":"PCA-RAG: Principal Component Analysis for Efficient Retrieval-Augmented\n  Generation","summary":"  Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\ngrounding large language models in external knowledge sources, improving the\nprecision of agents responses. However, high-dimensional language model\nembeddings, often in the range of hundreds to thousands of dimensions, can\npresent scalability challenges in terms of storage and latency, especially when\nprocessing massive financial text corpora. This paper investigates the use of\nPrincipal Component Analysis (PCA) to reduce embedding dimensionality, thereby\nmitigating computational bottlenecks without incurring large accuracy losses.\nWe experiment with a real-world dataset and compare different similarity and\ndistance metrics under both full-dimensional and PCA-compressed embeddings. Our\nresults show that reducing vectors from 3,072 to 110 dimensions provides a\nsizeable (up to $60\\times$) speedup in retrieval operations and a $\\sim\n28.6\\times$ reduction in index size, with only moderate declines in correlation\nmetrics relative to human-annotated similarity scores. These findings\ndemonstrate that PCA-based compression offers a viable balance between\nretrieval fidelity and resource efficiency, essential for real-time systems\nsuch as Zanista AI's \\textit{Newswitch} platform. Ultimately, our study\nunderscores the practicality of leveraging classical dimensionality reduction\ntechniques to scale RAG architectures for knowledge-intensive applications in\nfinance and trading, where speed, memory efficiency, and accuracy must jointly\nbe optimized.\n","authors":["Arman Khaledian","Amirreza Ghadiridehkordi","Nariman Khaledian"],"pdf_url":"https://arxiv.org/pdf/2504.08386v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2504.08381v1","updated":"2025-04-11T09:33:11Z","published":"2025-04-11T09:33:11Z","title":"An Empirical Investigation of Reconstruction-Based Models for Seizure\n  Prediction from ECG Signals","summary":"  Epileptic seizures are sudden neurological disorders characterized by\nabnormal, excessive neuronal activity in the brain, which is often associated\nwith changes in cardiovascular activity. These disruptions can pose significant\nphysical and psychological challenges for patients. Therefore, accurate seizure\nprediction can help mitigate these risks by enabling timely interventions,\nultimately improving patients' quality of life. Traditionally, EEG signals have\nbeen the primary standard for seizure prediction due to their precision in\ncapturing brain activity. However, their high cost, susceptibility to noise,\nand logistical constraints limit their practicality, restricting their use to\nclinical settings. In order to overcome these limitations, this study focuses\non leveraging ECG signals as an alternative for seizure prediction. In this\npaper, we present a novel method for predicting seizures based on detecting\nanomalies in ECG signals during their reconstruction. By extracting\ntime-frequency features and leveraging various advanced deep learning\narchitectures, the proposed method identifies deviations in heart rate dynamics\nassociated with seizure onset. The proposed approach was evaluated using the\nSiena database and could achieve specificity of 99.16\\%, accuracy of 76.05\\%,\nand false positive rate (FPR) of 0.01/h, with an average prediction time of 45\nminutes before seizure onset. These results highlight the potential of\nECG-based seizure prediction as a patient-friendly alternative to traditional\nEEG-based methods.\n","authors":["Mohammad Reza Chopannavaz","Foad Ghaderi"],"pdf_url":"https://arxiv.org/pdf/2504.08381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08378v1","updated":"2025-04-11T09:26:47Z","published":"2025-04-11T09:26:47Z","title":"Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash","summary":"  Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.\n","authors":["Fucheng Jia","Zewen Wu","Shiqi Jiang","Huiqiang Jiang","Qianxi Zhang","Yuqing Yang","Yunxin Liu","Ju Ren","Deyu Zhang","Ting Cao"],"pdf_url":"https://arxiv.org/pdf/2504.08378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08377v1","updated":"2025-04-11T09:26:37Z","published":"2025-04-11T09:26:37Z","title":"Proofs as Explanations: Short Certificates for Reliable Predictions","summary":"  We consider a model for explainable AI in which an explanation for a\nprediction $h(x)=y$ consists of a subset $S'$ of the training data (if it\nexists) such that all classifiers $h' \\in H$ that make at most $b$ mistakes on\n$S'$ predict $h'(x)=y$. Such a set $S'$ serves as a proof that $x$ indeed has\nlabel $y$ under the assumption that (1) the target function $h^\\star$ belongs\nto $H$, and (2) the set $S$ contains at most $b$ corrupted points. For example,\nif $b=0$ and $H$ is the family of linear classifiers in $\\mathbb{R}^d$, and if\n$x$ lies inside the convex hull of the positive data points in $S$ (and hence\nevery consistent linear classifier labels $x$ as positive), then\nCarath\\'eodory's theorem states that $x$ lies inside the convex hull of $d+1$\nof those points. So, a set $S'$ of size $d+1$ could be released as an\nexplanation for a positive prediction, and would serve as a short proof of\ncorrectness of the prediction under the assumption of realizability.\n  In this work, we consider this problem more generally, for general hypothesis\nclasses $H$ and general values $b\\geq 0$. We define the notion of the robust\nhollow star number of $H$ (which generalizes the standard hollow star number),\nand show that it precisely characterizes the worst-case size of the smallest\ncertificate achievable, and analyze its size for natural classes. We also\nconsider worst-case distributional bounds on certificate size, as well as\ndistribution-dependent bounds that we show tightly control the sample size\nneeded to get a certificate for any given test example. In particular, we\ndefine a notion of the certificate coefficient $\\varepsilon_x$ of an example\n$x$ with respect to a data distribution $D$ and target function $h^\\star$, and\nprove matching upper and lower bounds on sample size as a function of\n$\\varepsilon_x$, $b$, and the VC dimension $d$ of $H$.\n","authors":["Avrim Blum","Steve Hanneke","Chirag Pabbaraju","Donya Saless"],"pdf_url":"https://arxiv.org/pdf/2504.08377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17365v2","updated":"2025-04-11T09:18:53Z","published":"2025-02-01T06:58:27Z","title":"How Effective Is Constitutional AI in Small LLMs? A Study on DeepSeek-R1\n  and Its Peers","summary":"  Recent incidents highlight safety risks in Large Language Models (LLMs),\nmotivating research into alignment methods like Constitutional AI (CAI). This\npaper explores CAI's self-critique mechanism on small, uncensored 7-9B\nparameter models: DeepSeek-R1-8B, Gemma-2-9B, Llama 3.1-8B, and Qwen2.5-7B. We\nshow that while Llama-based models exhibited significant harm reduction through\nself-critique, other architectures demonstrated less improvement in harm\ndetection after abliteration. These results suggest CAI's effectiveness may\nvary depending on model architecture and reasoning capabilities.\n","authors":["Antonio-Gabriel Chacón Menke","Phan Xuan Tan"],"pdf_url":"https://arxiv.org/pdf/2503.17365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08368v1","updated":"2025-04-11T09:07:05Z","published":"2025-04-11T09:07:05Z","title":"FocalLens: Instruction Tuning Enables Zero-Shot Conditional Image\n  Representations","summary":"  Visual understanding is inherently contextual -- what we focus on in an image\ndepends on the task at hand. For instance, given an image of a person holding a\nbouquet of flowers, we may focus on either the person such as their clothing,\nor the type of flowers, depending on the context of interest. Yet, most\nexisting image encoding paradigms represent an image as a fixed, generic\nfeature vector, overlooking the potential needs of prioritizing varying visual\ninformation for different downstream use cases. In this work, we introduce\nFocalLens, a conditional visual encoding method that produces different\nrepresentations for the same image based on the context of interest, expressed\nflexibly through natural language. We leverage vision instruction tuning data\nand contrastively finetune a pretrained vision encoder to take natural language\ninstructions as additional inputs for producing conditional image\nrepresentations. Extensive experiments validate that conditional image\nrepresentation from FocalLens better pronounce the visual features of interest\ncompared to generic features produced by standard vision encoders like CLIP. In\naddition, we show FocalLens further leads to performance improvements on a\nrange of downstream tasks including image-image retrieval, image\nclassification, and image-text retrieval, with an average gain of 5 and 10\npoints on the challenging SugarCrepe and MMVP-VLM benchmarks, respectively.\n","authors":["Cheng-Yu Hsieh","Pavan Kumar Anasosalu Vasu","Fartash Faghri","Raviteja Vemulapalli","Chun-Liang Li","Ranjay Krishna","Oncel Tuzel","Hadi Pouransari"],"pdf_url":"https://arxiv.org/pdf/2504.08368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08364v1","updated":"2025-04-11T09:00:49Z","published":"2025-04-11T09:00:49Z","title":"DRIP: DRop unImportant data Points -- Enhancing Machine Learning\n  Efficiency with Grad-CAM-Based Real-Time Data Prioritization for On-Device\n  Training","summary":"  Selecting data points for model training is critical in machine learning.\nEffective selection methods can reduce the labeling effort, optimize on-device\ntraining for embedded systems with limited data storage, and enhance the model\nperformance. This paper introduces a novel algorithm that uses Grad-CAM to make\nonline decisions about retaining or discarding data points. Optimized for\nembedded devices, the algorithm computes a unique DRIP Score to quantify the\nimportance of each data point. This enables dynamic decision-making on whether\na data point should be stored for potential retraining or discarded without\ncompromising model performance. Experimental evaluations on four benchmark\ndatasets demonstrate that our approach can match or even surpass the accuracy\nof models trained on the entire dataset, all while achieving storage savings of\nup to 39\\%. To our knowledge, this is the first algorithm that makes online\ndecisions about data point retention without requiring access to the entire\ndataset.\n","authors":["Marcus Rüb","Daniel Konegen","Axel Sikora","Daniel Mueller-Gritschneder"],"pdf_url":"https://arxiv.org/pdf/2504.08364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08359v1","updated":"2025-04-11T08:48:54Z","published":"2025-04-11T08:48:54Z","title":"Kernel-Level Energy-Efficient Neural Architecture Search for Tabular\n  Dataset","summary":"  Many studies estimate energy consumption using proxy metrics like memory\nusage, FLOPs, and inference latency, with the assumption that reducing these\nmetrics will also lower energy consumption in neural networks. This paper,\nhowever, takes a different approach by introducing an energy-efficient Neural\nArchitecture Search (NAS) method that directly focuses on identifying\narchitectures that minimize energy consumption while maintaining acceptable\naccuracy. Unlike previous methods that primarily target vision and language\ntasks, the approach proposed here specifically addresses tabular datasets.\nRemarkably, the optimal architecture suggested by this method can reduce energy\nconsumption by up to 92% compared to architectures recommended by conventional\nNAS.\n","authors":["Hoang-Loc La","Phuong Hoai Ha"],"pdf_url":"https://arxiv.org/pdf/2504.08359v1.pdf","comment":"ACIIDS 2025 Conference"},{"id":"http://arxiv.org/abs/2504.08356v1","updated":"2025-04-11T08:43:12Z","published":"2025-04-11T08:43:12Z","title":"An Adaptive Clustering Scheme for Client Selections in\n  Communication-Efficient Federated Learning","summary":"  Federated learning is a novel decentralized learning architecture. During the\ntraining process, the client and server must continuously upload and receive\nmodel parameters, which consumes a lot of network transmission resources. Some\nmethods use clustering to find more representative customers, select only a\npart of them for training, and at the same time ensure the accuracy of\ntraining. However, in federated learning, it is not trivial to know what the\nnumber of clusters can bring the best training result. Therefore, we propose to\ndynamically adjust the number of clusters to find the most ideal grouping\nresults. It may reduce the number of users participating in the training to\nachieve the effect of reducing communication costs without affecting the model\nperformance. We verify its experimental results on the non-IID handwritten\ndigit recognition dataset and reduce the cost of communication and transmission\nby almost 50% compared with traditional federated learning without affecting\nthe accuracy of the model.\n","authors":["Yan-Ann Chen","Guan-Lin Chen"],"pdf_url":"https://arxiv.org/pdf/2504.08356v1.pdf","comment":"Published in the Proceedings of IEEE VTS Asia Pacific Wireless\n  Communications Symposium (APWCS), 2023"},{"id":"http://arxiv.org/abs/2406.00004v4","updated":"2025-04-11T08:41:07Z","published":"2024-05-12T04:15:05Z","title":"Navigating the Future of Federated Recommendation Systems with\n  Foundation Models","summary":"  Federated Recommendation Systems (FRSs) offer a privacy-preserving\nalternative to traditional centralized approaches by decentralizing data\nstorage. However, they face persistent challenges such as data sparsity and\nheterogeneity, largely due to isolated client environments. Recent advances in\nFoundation Models (FMs), particularly large language models like ChatGPT,\npresent an opportunity to surmount these issues through powerful, cross-task\nknowledge transfer. In this position paper, we systematically examine the\nconvergence of FRSs and FMs, illustrating how FM-enhanced frameworks can\nsubstantially improve client-side personalization, communication efficiency,\nand server-side aggregation. We also delve into pivotal challenges introduced\nby this integration, including privacy-security trade-offs, non-IID data, and\nresource constraints in federated setups, and propose prospective research\ndirections in areas such as multimodal recommendation, real-time FM adaptation,\nand explainable federated reasoning. By unifying FRSs with FMs, our position\npaper provides a forward-looking roadmap for advancing privacy-preserving,\nhigh-performance recommendation systems that fully leverage large-scale\npre-trained knowledge to enhance local performance.\n","authors":["Zhiwei Li","Guodong Long","Chunxu Zhang","Honglei Zhang","Jing Jiang","Chengqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.00004v4.pdf","comment":"11 pages, position paper, survey"},{"id":"http://arxiv.org/abs/2504.08353v1","updated":"2025-04-11T08:39:18Z","published":"2025-04-11T08:39:18Z","title":"Single View Garment Reconstruction Using Diffusion Mapping Via Pattern\n  Coordinates","summary":"  Reconstructing 3D clothed humans from images is fundamental to applications\nlike virtual try-on, avatar creation, and mixed reality. While recent advances\nhave enhanced human body recovery, accurate reconstruction of garment geometry\n-- especially for loose-fitting clothing -- remains an open challenge. We\npresent a novel method for high-fidelity 3D garment reconstruction from single\nimages that bridges 2D and 3D representations. Our approach combines Implicit\nSewing Patterns (ISP) with a generative diffusion model to learn rich garment\nshape priors in a 2D UV space. A key innovation is our mapping model that\nestablishes correspondences between 2D image pixels, UV pattern coordinates,\nand 3D geometry, enabling joint optimization of both 3D garment meshes and the\ncorresponding 2D patterns by aligning learned priors with image observations.\nDespite training exclusively on synthetically simulated cloth data, our method\ngeneralizes effectively to real-world images, outperforming existing approaches\non both tight- and loose-fitting garments. The reconstructed garments maintain\nphysical plausibility while capturing fine geometric details, enabling\ndownstream applications including garment retargeting and texture manipulation.\n","authors":["Ren Li","Cong Cao","Corentin Dumery","Yingxuan You","Hao Li","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2504.08353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07583v2","updated":"2025-04-11T08:22:54Z","published":"2025-04-10T09:24:54Z","title":"Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with\n  Question Answering","summary":"  Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa\n","authors":["Patrick Fernandes","Sweta Agrawal","Emmanouil Zaranis","André F. T. Martins","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2504.07583v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.06176v2","updated":"2025-04-11T08:14:37Z","published":"2025-04-08T16:19:19Z","title":"A Self-Supervised Framework for Space Object Behaviour Characterisation","summary":"  Foundation Models, pre-trained on large unlabelled datasets before\ntask-specific fine-tuning, are increasingly being applied to specialised\ndomains. Recent examples include ClimaX for climate and Clay for satellite\nEarth observation, but a Foundation Model for Space Object Behavioural Analysis\nhas not yet been developed. As orbital populations grow, automated methods for\ncharacterising space object behaviour are crucial for space safety. We present\na Space Safety and Sustainability Foundation Model focusing on space object\nbehavioural analysis using light curves (LCs). We implemented a\nPerceiver-Variational Autoencoder (VAE) architecture, pre-trained with\nself-supervised reconstruction and masked reconstruction on 227,000 LCs from\nthe MMT-9 observatory. The VAE enables anomaly detection, motion prediction,\nand LC generation. We fine-tuned the model for anomaly detection & motion\nprediction using two independent LC simulators (CASSANDRA and GRIAL\nrespectively), using CAD models of boxwing, Sentinel-3, SMOS, and Starlink\nplatforms. Our pre-trained model achieved a reconstruction error of 0.01%,\nidentifying potentially anomalous light curves through reconstruction\ndifficulty. After fine-tuning, the model scored 88% and 82% accuracy, with 0.90\nand 0.95 ROC AUC scores respectively in both anomaly detection and motion mode\nprediction (sun-pointing, spin, etc.). Analysis of high-confidence anomaly\npredictions on real data revealed distinct patterns including characteristic\nobject profiles and satellite glinting. Here, we demonstrate how\nself-supervised learning can simultaneously enable anomaly detection, motion\nprediction, and synthetic data generation from rich representations learned in\npre-training. Our work therefore supports space safety and sustainability\nthrough automated monitoring and simulation capabilities.\n","authors":["Ian Groves","Andrew Campbell","James Fernandes","Diego Ramírez Rodríguez","Paul Murray","Massimiliano Vasile","Victoria Nockles"],"pdf_url":"https://arxiv.org/pdf/2504.06176v2.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.14566v3","updated":"2025-04-11T08:10:03Z","published":"2024-12-19T06:35:54Z","title":"AIArena: A Blockchain-Based Decentralized AI Training Platform","summary":"  The rapid advancement of AI has underscored critical challenges in its\ndevelopment and implementation, largely due to centralized control by a few\nmajor corporations. This concentration of power intensifies biases within AI\nmodels, resulting from inadequate governance and oversight mechanisms.\nAdditionally, it limits public involvement and heightens concerns about the\nintegrity of model generation. Such monopolistic control over data and AI\noutputs threatens both innovation and fair data usage, as users inadvertently\ncontribute data that primarily benefits these corporations. In this work, we\npropose AIArena, a blockchain-based decentralized AI training platform designed\nto democratize AI development and alignment through on-chain incentive\nmechanisms. AIArena fosters an open and collaborative environment where\nparticipants can contribute models and computing resources. Its on-chain\nconsensus mechanism ensures fair rewards for participants based on their\ncontributions. We instantiate and implement AIArena on the public Base\nblockchain Sepolia testnet, and the evaluation results demonstrate the\nfeasibility of AIArena in real-world applications.\n","authors":["Zhipeng Wang","Rui Sun","Elizabeth Lui","Tuo Zhou","Yizhe Wen","Jiahao Sun"],"pdf_url":"https://arxiv.org/pdf/2412.14566v3.pdf","comment":"Camera ready version. Accepted by the ACM Web Conference (WWW) Short\n  Paper Track, 2025"},{"id":"http://arxiv.org/abs/2504.08335v1","updated":"2025-04-11T08:00:37Z","published":"2025-04-11T08:00:37Z","title":"Entropic bounds for conditionally Gaussian vectors and applications to\n  neural networks","summary":"  Using entropic inequalities from information theory, we provide new bounds on\nthe total variation and 2-Wasserstein distances between a conditionally\nGaussian law and a Gaussian law with invertible covariance matrix. We apply our\nresults to quantify the speed of convergence to Gaussian of a randomly\ninitialized fully connected neural network and its derivatives - evaluated in a\nfinite number of inputs - when the initialization is Gaussian and the sizes of\nthe inner layers diverge to infinity. Our results require mild assumptions on\nthe activation function, and allow one to recover optimal rates of convergence\nin a variety of distances, thus improving and extending the findings of Basteri\nand Trevisan (2023), Favaro et al. (2023), Trevisan (2024) and Apollonio et al.\n(2024). One of our main tools are the quantitative cumulant estimates\nestablished in Hanin (2024). As an illustration, we apply our results to bound\nthe total variation distance between the Bayesian posterior law of the neural\nnetwork and its derivatives, and the posterior law of the corresponding\nGaussian limit: this yields quantitative versions of a posterior CLT by Hron et\nal. (2022), and extends several estimates by Trevisan (2024) to the total\nvariation metric.\n","authors":["Lucia Celli","Giovanni Peccati"],"pdf_url":"https://arxiv.org/pdf/2504.08335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08329v1","updated":"2025-04-11T07:51:58Z","published":"2025-04-11T07:51:58Z","title":"MedRep: Medical Concept Representation for General Electronic Health\n  Record Foundation Models","summary":"  Electronic health record (EHR) foundation models have been an area ripe for\nexploration with their improved performance in various medical tasks. Despite\nthe rapid advances, there exists a fundamental limitation: Processing unseen\nmedical codes out of the vocabulary. This problem limits the generality of EHR\nfoundation models and the integration of models trained with different\nvocabularies. To deal with this problem, we propose MedRep for EHR foundation\nmodels based on the observational medical outcome partnership (OMOP) common\ndata model (CDM), providing the integrated medical concept representations and\nthe basic data augmentation strategy for patient trajectories. For concept\nrepresentation learning, we enrich the information of each concept with a\nminimal definition through large language model (LLM) prompts and enhance the\ntext-based representations through graph ontology of OMOP vocabulary.\nTrajectory augmentation randomly replaces selected concepts with other similar\nconcepts that have closely related representations to let the model practice\nwith the concepts out-of-vocabulary. Finally, we demonstrate that EHR\nfoundation models trained with MedRep better maintain the prediction\nperformance in external datasets. Our code implementation is publicly available\nat https://github.com/kicarussays/MedRep.\n","authors":["Junmo Kim","Namkyeong Lee","Jiwon Kim","Kwangsoo Kim"],"pdf_url":"https://arxiv.org/pdf/2504.08329v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2504.08328v1","updated":"2025-04-11T07:51:33Z","published":"2025-04-11T07:51:33Z","title":"Towards generalizable single-cell perturbation modeling via the\n  Conditional Monge Gap","summary":"  Learning the response of single-cells to various treatments offers great\npotential to enable targeted therapies. In this context, neural optimal\ntransport (OT) has emerged as a principled methodological framework because it\ninherently accommodates the challenges of unpaired data induced by cell\ndestruction during data acquisition. However, most existing OT approaches are\nincapable of conditioning on different treatment contexts (e.g., time, drug\ntreatment, drug dosage, or cell type) and we still lack methods that\nunanimously show promising generalization performance to unseen treatments.\nHere, we propose the Conditional Monge Gap which learns OT maps conditionally\non arbitrary covariates. We demonstrate its value in predicting single-cell\nperturbation responses conditional to one or multiple drugs, a drug dosage, or\ncombinations thereof. We find that our conditional models achieve results\ncomparable and sometimes even superior to the condition-specific\nstate-of-the-art on scRNA-seq as well as multiplexed protein imaging data.\nNotably, by aggregating data across conditions we perform cross-task learning\nwhich unlocks remarkable generalization abilities to unseen drugs or drug\ndosages, widely outperforming other conditional models in capturing\nheterogeneity (i.e., higher moments) in the perturbed population. Finally, by\nscaling to hundreds of conditions and testing on unseen drugs, we narrow the\ngap between structure-based and effect-based drug representations, suggesting a\npromising path to the successful prediction of perturbation effects for unseen\ntreatments.\n","authors":["Alice Driessen","Benedek Harsanyi","Marianna Rapsomaniki","Jannis Born"],"pdf_url":"https://arxiv.org/pdf/2504.08328v1.pdf","comment":"Main text, 15 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2504.08323v1","updated":"2025-04-11T07:47:39Z","published":"2025-04-11T07:47:39Z","title":"Academic Network Representation via Prediction-Sampling Incorporated\n  Tensor Factorization","summary":"  Accurate representation to an academic network is of great significance to\nacademic relationship mining like predicting scientific impact. A Latent\nFactorization of Tensors (LFT) model is one of the most effective models for\nlearning the representation of a target network. However, an academic network\nis often High-Dimensional and Incomplete (HDI) because the relationships among\nnumerous network entities are impossible to be fully explored, making it\ndifficult for an LFT model to learn accurate representation of the academic\nnetwork. To address this issue, this paper proposes a Prediction-sampling-based\nLatent Factorization of Tensors (PLFT) model with two ideas: 1) constructing a\ncascade LFT architecture to enhance model representation learning ability via\nlearning academic network hierarchical features, and 2) introducing a nonlinear\nactivation-incorporated predicting-sampling strategy to more accurately learn\nthe network representation via generating new academic network data layer by\nlayer. Experimental results from the three real-world academic network datasets\nshow that the PLFT model outperforms existing models when predicting the\nunexplored relationships among network entities.\n","authors":["Chunyang Zhang","Xin Liao","Hao Wu"],"pdf_url":"https://arxiv.org/pdf/2504.08323v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08312v1","updated":"2025-04-11T07:29:56Z","published":"2025-04-11T07:29:56Z","title":"SortBench: Benchmarking LLMs based on their ability to sort lists","summary":"  Sorting is a tedious but simple task for human intelligence and can be solved\nfairly easily algorithmically. However, for Large Language Models (LLMs) this\ntask is surprisingly hard, as some properties of sorting are among known\nweaknesses of LLMs: being faithful to the input data, logical comparisons\nbetween values, and strictly differentiating between syntax (used for sorting)\nand semantics (typically learned by embeddings). Within this paper, we describe\nthe new SortBench benchmark for LLMs that comes with different difficulties and\nthat can be easily scaled in terms of difficulty. We apply this benchmark to\nseven state-of-the-art LLMs, including current test-time reasoning models. Our\nresults show that while the o3-mini model is very capable at sorting in\ngeneral, even this can be fooled if strings are defined to mix syntactical and\nsemantical aspects, e.g., by asking to sort numbers written-out as word.\nFurthermore, all models have problems with the faithfulness to the input of\nlong lists, i.e., they drop items and add new ones. Our results also show that\ntest-time reasoning has a tendency to overthink problems which leads to\nperformance degradation. Finally, models without test-time reasoning like\nGPT-4o are not much worse than reasoning models.\n","authors":["Steffen Herbold"],"pdf_url":"https://arxiv.org/pdf/2504.08312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14390v7","updated":"2025-04-11T07:26:39Z","published":"2024-11-21T18:24:06Z","title":"Persistent Homology for Structural Characterization in Disordered\n  Systems","summary":"  We propose a unified framework based on persistent homology (PH) to\ncharacterize both local and global structures in disordered systems. It can\nsimultaneously generate local and global descriptors using the same algorithm\nand data structure, and has shown to be highly effective and interpretable in\npredicting particle rearrangements and classifying global phases. We also\ndemonstrated that using a single variable enables a linear SVM to achieve\nnearly perfect three-phase classification. Inspired by this discovery, we\ndefine a non-parametric metric, the Separation Index (SI), which not only\nachieves this classification without sacrificing significant performance but\nalso establishes a connection between particle environments and the global\nphase structure. Our methods provide an effective framework for understanding\nand analyzing the properties of disordered materials, with broad potential\napplications in materials science and even wider studies of complex systems.\n","authors":["An Wang","Li Zou"],"pdf_url":"https://arxiv.org/pdf/2411.14390v7.pdf","comment":"21 pages, 19 figures"},{"id":"http://arxiv.org/abs/2411.10212v2","updated":"2025-04-11T07:21:18Z","published":"2024-11-15T14:17:19Z","title":"Embedding Byzantine Fault Tolerance into Federated Learning via\n  Consistency Scoring","summary":"  Given sufficient data from multiple edge devices, federated learning (FL)\nenables training a shared model without transmitting private data to a central\nserver. However, FL is generally vulnerable to Byzantine attacks from\ncompromised edge devices, which can significantly degrade the model\nperformance. In this paper, we propose a intuitive plugin that can be\nintegrated into existing FL techniques to achieve Byzantine-Resilience. Key\nidea is to generate virtual data samples and evaluate model consistency scores\nacross local updates to effectively filter out compromised edge devices. By\nutilizing this scoring mechanism before the aggregation phase, the proposed\nplugin enables existing FL techniques to become robust against Byzantine\nattacks while maintaining their original benefits. Numerical results on medical\nimage classification task validate that plugging the proposed approach into\nrepresentative FL algorithms, effectively achieves Byzantine resilience.\nFurthermore, the proposed plugin maintains the original convergence properties\nof the base FL algorithms when no Byzantine attacks are present.\n","authors":["Youngjoon Lee","Jinu Gong","Joonhyuk Kang"],"pdf_url":"https://arxiv.org/pdf/2411.10212v2.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2502.06891v2","updated":"2025-04-11T07:15:26Z","published":"2025-02-09T10:36:33Z","title":"ScaffoldGPT: A Scaffold-based GPT Model for Drug Optimization","summary":"  Drug optimization has become increasingly crucial in light of fast-mutating\nvirus strains and drug-resistant cancer cells. Nevertheless, it remains\nchallenging as it necessitates retaining the beneficial properties of the\noriginal drug while simultaneously enhancing desired attributes beyond its\nscope. In this work, we aim to tackle this challenge by introducing\nScaffoldGPT, a novel Generative Pretrained Transformer (GPT) designed for drug\noptimization based on molecular scaffolds. Our work comprises three key\ncomponents: (1) A three-stage drug optimization approach that integrates\npretraining, finetuning, and decoding optimization. (2) A uniquely designed\ntwo-phase incremental training approach for pre-training the drug optimization\nGPT on molecule scaffold with enhanced performance. (3) A token-level decoding\noptimization strategy, TOP-N, that enabling controlled, reward-guided\ngeneration using pretrained/finetuned GPT. We demonstrate via a comprehensive\nevaluation on COVID and cancer benchmarks that ScaffoldGPT outperforms the\ncompeting baselines in drug optimization benchmarks, while excelling in\npreserving original functional scaffold and enhancing desired properties.\n","authors":["Xuefeng Liu","Songhao Jiang","Ian Foster","Jinbo Xu","Rick Stevens"],"pdf_url":"https://arxiv.org/pdf/2502.06891v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08281v1","updated":"2025-04-11T06:30:16Z","published":"2025-04-11T06:30:16Z","title":"ELSA: A Style Aligned Dataset for Emotionally Intelligent Language\n  Generation","summary":"  Advancements in emotion aware language processing increasingly shape vital\nNLP applications ranging from conversational AI and affective computing to\ncomputational psychology and creative content generation. Existing emotion\ndatasets either lack emotional granularity or fail to capture necessary\nstylistic diversity, limiting the advancement of effective emotion conditioned\ntext generation systems. Seeking to bridge this crucial gap between granularity\nand style diversity, this paper introduces a novel systematically constructed\ndataset named ELSA Emotion and Language Style Alignment Dataset leveraging fine\ngrained emotion taxonomies adapted from existing sources such as dair ai\nemotion dataset and GoEmotions taxonomy. This dataset comprises multiple\nemotionally nuanced variations of original sentences regenerated across\ndistinct contextual styles such as conversational, formal, poetic, and\nnarrative, using advanced Large Language Models LLMs. Rigorous computational\nevaluation using metrics such as perplexity, embedding variance, readability,\nlexical diversity, and semantic coherence measures validates the datasets\nemotional authenticity, linguistic fluency, and textual diversity.\nComprehensive metric analyses affirm its potential to support deeper\nexplorations into emotion conditioned style adaptive text generation. By\nenabling precision tuned emotionally nuanced language modeling, our dataset\ncreates fertile ground for research on fine grained emotional control, prompt\ndriven explanation, interpretability, and style adaptive expressive language\ngeneration with LLMs.\n","authors":["Vishal Gandhi","Sagar Gandhi"],"pdf_url":"https://arxiv.org/pdf/2504.08281v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2504.05343v2","updated":"2025-04-11T06:26:08Z","published":"2025-04-06T09:14:43Z","title":"AROMA: Autonomous Rank-one Matrix Adaptation","summary":"  As large language models continue to grow in size, parameter-efficient\nfine-tuning (PEFT) has become increasingly crucial. While low-rank adaptation\n(LoRA) offers a solution through low-rank updates, its static rank allocation\nmay yield suboptimal results. Adaptive low-rank adaptation (AdaLoRA) improves\nthis with dynamic allocation but remains sensitive to initial and target rank\nconfigurations. We introduce AROMA, a framework that automatically constructs\nlayer-specific updates by iteratively building up rank-one components with very\nfew trainable parameters that gradually diminish to zero. Unlike existing\nmethods that employ rank reduction mechanisms, AROMA introduces a dual-loop\narchitecture for rank growth. The inner loop extracts information from each\nrank-one subspace, while the outer loop determines the number of rank-one\nsubspaces, i.e., the optimal rank. We reset optimizer states to maintain\nsubspace independence. AROMA significantly reduces parameters compared to LoRA\nand AdaLoRA while achieving superior performance on natural language\nunderstanding and commonsense reasoning tasks, offering new insights into\nadaptive PEFT. The code is available at\n\\href{https://github.com/ShuDun23/AROMA}{AROMA}.\n","authors":["Hao Nan Sheng","Zhi-yong Wang","Mingrui Yang","Hing Cheung So"],"pdf_url":"https://arxiv.org/pdf/2504.05343v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04914v2","updated":"2025-04-11T06:21:25Z","published":"2024-12-06T10:10:47Z","title":"Achieving Group Fairness through Independence in Predictive Process\n  Monitoring","summary":"  Predictive process monitoring focuses on forecasting future states of ongoing\nprocess executions, such as predicting the outcome of a particular case. In\nrecent years, the application of machine learning models in this domain has\ngarnered significant scientific attention. When using historical execution\ndata, which may contain biases or exhibit unfair behavior, these biases may be\nencoded into the trained models. Consequently, when such models are deployed to\nmake decisions or guide interventions for new cases, they risk perpetuating\nthis unwanted behavior. This work addresses group fairness in predictive\nprocess monitoring by investigating independence, i.e. ensuring predictions are\nunaffected by sensitive group membership. We explore independence through\nmetrics for demographic parity such as $\\Delta$DP, as well as recently\nintroduced, threshold-independent distribution-based alternatives.\nAdditionally, we propose a composite loss function existing of binary\ncross-entropy and a distribution-based loss (Wasserstein) to train models that\nbalance predictive performance and fairness, and allow for customizable\ntrade-offs. The effectiveness of both the fairness metrics and the composite\nloss functions is validated through a controlled experimental setup.\n","authors":["Jari Peeperkorn","Simon De Vos"],"pdf_url":"https://arxiv.org/pdf/2412.04914v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2504.08277v1","updated":"2025-04-11T06:16:30Z","published":"2025-04-11T06:16:30Z","title":"Enabling Automatic Differentiation with Mollified Graph Neural Operators","summary":"  Physics-informed neural operators offer a powerful framework for learning\nsolution operators of partial differential equations (PDEs) by combining data\nand physics losses. However, these physics losses rely on derivatives.\nComputing these derivatives remains challenging, with spectral and finite\ndifference methods introducing approximation errors due to finite resolution.\nHere, we propose the mollified graph neural operator (mGNO), the first method\nto leverage automatic differentiation and compute \\emph{exact} gradients on\narbitrary geometries. This enhancement enables efficient training on irregular\ngrids and varying geometries while allowing seamless evaluation of physics\nlosses at randomly sampled points for improved generalization. For a PDE\nexample on regular grids, mGNO paired with autograd reduced the L2 relative\ndata error by 20x compared to finite differences, although training was slower.\nIt can also solve PDEs on unstructured point clouds seamlessly, using physics\nlosses only, at resolutions vastly lower than those needed for finite\ndifferences to be accurate enough. On these unstructured point clouds, mGNO\nleads to errors that are consistently 2 orders of magnitude lower than machine\nlearning baselines (Meta-PDE) for comparable runtimes, and also delivers\nspeedups from 1 to 3 orders of magnitude compared to the numerical solver for\nsimilar accuracy. mGNOs can also be used to solve inverse design and shape\noptimization problems on complex geometries.\n","authors":["Ryan Y. Lin","Julius Berner","Valentin Duruisseaux","David Pitt","Daniel Leibovici","Jean Kossaifi","Kamyar Azizzadenesheli","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2504.08277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15344v3","updated":"2025-04-11T05:53:17Z","published":"2024-09-10T07:04:48Z","title":"Video-Driven Graph Network-Based Simulators","summary":"  Lifelike visualizations in design, cinematography, and gaming rely on precise\nphysics simulations, typically requiring extensive computational resources and\ndetailed physical input. This paper presents a method that can infer a system's\nphysical properties from a short video, eliminating the need for explicit\nparameter input, provided it is close to the training condition. The learned\nrepresentation is then used within a Graph Network-based Simulator to emulate\nthe trajectories of physical systems. We demonstrate that the video-derived\nencodings effectively capture the physical properties of the system and\nshowcase a linear dependence between some of the encodings and the system's\nmotion.\n","authors":["Franciszek Szewczyk","Gilles Louppe","Matthia Sabatelli"],"pdf_url":"https://arxiv.org/pdf/2409.15344v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10261v2","updated":"2025-04-11T05:20:50Z","published":"2025-01-17T15:42:42Z","title":"Logarithmic Regret for Nonlinear Control","summary":"  We address the problem of learning to control an unknown nonlinear dynamical\nsystem through sequential interactions. Motivated by high-stakes applications\nin which mistakes can be catastrophic, such as robotics and healthcare, we\nstudy situations where it is possible for fast sequential learning to occur.\nFast sequential learning is characterized by the ability of the learning agent\nto incur logarithmic regret relative to a fully-informed baseline. We\ndemonstrate that fast sequential learning is achievable in a diverse class of\ncontinuous control problems where the system dynamics depend smoothly on\nunknown parameters, provided the optimal control policy is persistently\nexciting. Additionally, we derive a regret bound which grows with the square\nroot of the number of interactions for cases where the optimal policy is not\npersistently exciting. Our results provide the first regret bounds for\ncontrolling nonlinear dynamical systems depending nonlinearly on unknown\nparameters. We validate the trends our theory predicts in simulation on a\nsimple dynamical system.\n","authors":["James Wang","Bruce D. Lee","Ingvar Ziemann","Nikolai Matni"],"pdf_url":"https://arxiv.org/pdf/2501.10261v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08258v1","updated":"2025-04-11T05:10:18Z","published":"2025-04-11T05:10:18Z","title":"Accelerating Multi-Objective Collaborative Optimization of Doped\n  Thermoelectric Materials via Artificial Intelligence","summary":"  The thermoelectric performance of materials exhibits complex nonlinear\ndependencies on both elemental types and their proportions, rendering\ntraditional trial-and-error approaches inefficient and time-consuming for\nmaterial discovery. In this work, we present a deep learning model capable of\naccurately predicting thermoelectric properties of doped materials directly\nfrom their chemical formulas, achieving state-of-the-art performance. To\nenhance interpretability, we further incorporate sensitivity analysis\ntechniques to elucidate how physical descriptors affect the thermoelectric\nfigure of merit (zT). Moreover, we establish a coupled framework that\nintegrates a surrogate model with a multi-objective genetic algorithm to\nefficiently explore the vast compositional space for high-performance\ncandidates. Experimental validation confirms the discovery of a novel\nthermoelectric material with superior $zT$ values in the medium-temperature\nregime.\n","authors":["Yuxuan Zeng","Wenhao Xie","Wei Cao","Tan Peng","Yue Hou","Ziyu Wang","Jing Shi"],"pdf_url":"https://arxiv.org/pdf/2504.08258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07137v2","updated":"2025-04-11T04:58:42Z","published":"2025-03-10T10:08:55Z","title":"A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and\n  Applications","summary":"  Artificial intelligence (AI) has achieved astonishing successes in many\ndomains, especially with the recent breakthroughs in the development of\nfoundational large models. These large models, leveraging their extensive\ntraining data, provide versatile solutions for a wide range of downstream\ntasks. However, as modern datasets become increasingly diverse and complex, the\ndevelopment of large AI models faces two major challenges: (1) the enormous\nconsumption of computational resources and deployment difficulties, and (2) the\ndifficulty in fitting heterogeneous and complex data, which limits the\nusability of the models. Mixture of Experts (MoE) models has recently attracted\nmuch attention in addressing these challenges, by dynamically selecting and\nactivating the most relevant sub-models to process input data. It has been\nshown that MoEs can significantly improve model performance and efficiency with\nfewer resources, particularly excelling in handling large-scale, multimodal\ndata. Given the tremendous potential MoE has demonstrated across various\ndomains, it is urgent to provide a comprehensive summary of recent advancements\nof MoEs in many important fields. Existing surveys on MoE have their\nlimitations, e.g., being outdated or lacking discussion on certain key areas,\nand we aim to address these gaps. In this paper, we first introduce the basic\ndesign of MoE, including gating functions, expert networks, routing mechanisms,\ntraining strategies, and system design. We then explore the algorithm design of\nMoE in important machine learning paradigms such as continual learning,\nmeta-learning, multi-task learning, and reinforcement learning. Additionally,\nwe summarize theoretical studies aimed at understanding MoE and review its\napplications in computer vision and natural language processing. Finally, we\ndiscuss promising future research directions.\n","authors":["Siyuan Mu","Sen Lin"],"pdf_url":"https://arxiv.org/pdf/2503.07137v2.pdf","comment":"29 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.15279v2","updated":"2025-04-11T04:39:07Z","published":"2024-12-18T03:46:30Z","title":"Functional connectomes of neural networks","summary":"  The human brain is a complex system, and understanding its mechanisms has\nbeen a long-standing challenge in neuroscience. The study of the functional\nconnectome, which maps the functional connections between different brain\nregions, has provided valuable insights through various advanced analysis\ntechniques developed over the years. Similarly, neural networks, inspired by\nthe brain's architecture, have achieved notable success in diverse applications\nbut are often noted for their lack of interpretability. In this paper, we\npropose a novel approach that bridges neural networks and human brain functions\nby leveraging brain-inspired techniques. Our approach, grounded in the insights\nfrom the functional connectome, offers scalable ways to characterize topology\nof large neural networks using stable statistical and machine learning\ntechniques. Our empirical analysis demonstrates its capability to enhance the\ninterpretability of neural networks, providing a deeper understanding of their\nunderlying mechanisms.\n","authors":["Tananun Songdechakraiwut","Yutong Wu"],"pdf_url":"https://arxiv.org/pdf/2412.15279v2.pdf","comment":"Published at the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2406.18892v2","updated":"2025-04-11T04:35:34Z","published":"2024-06-27T05:08:09Z","title":"LearnedKV: Integrating LSM and Learned Index for Superior Performance on\n  Storage","summary":"  We present LearnedKV, a novel tiered key-value store that seamlessly\nintegrates a Log-Structured Merge (LSM) tree with a Learned Index to achieve\nsuperior read and write performance on storage systems. While existing\napproaches use learned indexes primarily as auxiliary components within LSM\ntrees, LearnedKV employs a two-tier design where the LSM tree handles recent\nwrite operations while a separate Learned Index accelerates read performance.\nOur design includes a non-blocking conversion mechanism that efficiently\ntransforms LSM data into a Learned Index during garbage collection, maintaining\nhigh performance without interrupting operations. LearnedKV dramatically\nreduces LSM size through this tiered approach, leading to significant\nperformance gains in both reads and writes. Extensive evaluations across\ndiverse workloads show that LearnedKV outperforms state-of-the-art LSM-based\nsolutions by up to 4.32x for read operations and 1.43x for writes. The system\ndemonstrates robust performance across different data distributions, access\npatterns, and storage media including both SSDs and HDDs.\n","authors":["Wenlong Wang","David Hung-Chang Du"],"pdf_url":"https://arxiv.org/pdf/2406.18892v2.pdf","comment":"14 pages, 15 figures"},{"id":"http://arxiv.org/abs/2504.08254v1","updated":"2025-04-11T04:35:24Z","published":"2025-04-11T04:35:24Z","title":"Understanding the Impact of Data Domain Extraction on Synthetic Data\n  Privacy","summary":"  Privacy attacks, particularly membership inference attacks (MIAs), are widely\nused to assess the privacy of generative models for tabular synthetic data,\nincluding those with Differential Privacy (DP) guarantees. These attacks often\nexploit outliers, which are especially vulnerable due to their position at the\nboundaries of the data domain (e.g., at the minimum and maximum values).\nHowever, the role of data domain extraction in generative models and its impact\non privacy attacks have been overlooked. In this paper, we examine three\nstrategies for defining the data domain: assuming it is externally provided\n(ideally from public data), extracting it directly from the input data, and\nextracting it with DP mechanisms. While common in popular implementations and\nlibraries, we show that the second approach breaks end-to-end DP guarantees and\nleaves models vulnerable. While using a provided domain (if representative) is\npreferable, extracting it with DP can also defend against popular MIAs, even at\nhigh privacy budgets.\n","authors":["Georgi Ganev","Meenatchi Sundaram Muthu Selva Annamalai","Sofiane Mahiou","Emiliano De Cristofaro"],"pdf_url":"https://arxiv.org/pdf/2504.08254v1.pdf","comment":"Accepted to the Synthetic Data x Data Access Problem workshop\n  (SynthData), part of ICLR 2025"},{"id":"http://arxiv.org/abs/2501.07601v5","updated":"2025-04-11T04:22:39Z","published":"2025-01-10T22:31:53Z","title":"Real-Time Decision-Making for Digital Twin in Additive Manufacturing\n  with Model Predictive Control using Time-Series Deep Neural Networks","summary":"  Digital Twin -- a virtual replica of a physical system enabling real-time\nmonitoring, model updating, prediction, and decision-making -- combined with\nrecent advances in machine learning, offers new opportunities for proactive\ncontrol strategies in autonomous manufacturing. However, achieving real-time\ndecision-making with Digital Twins requires efficient optimization driven by\naccurate predictions of highly nonlinear manufacturing systems. This paper\npresents a simultaneous multi-step Model Predictive Control (MPC) framework for\nreal-time decision-making, using a multivariate deep neural network, named\nTime-Series Dense Encoder (TiDE), as the surrogate model. Unlike conventional\nMPC models which only provide one-step ahead prediction, TiDE is capable of\npredicting future states within the prediction horizon in one shot\n(multi-step), significantly accelerating the MPC. Using Directed Energy\nDeposition (DED) additive manufacturing as a case study, we demonstrate the\neffectiveness of the proposed MPC in achieving melt pool temperature tracking\nto ensure part quality, while reducing porosity defects by regulating laser\npower to maintain melt pool depth constraints. In this work, we first show that\nTiDE is capable of accurately predicting melt pool temperature and depth.\nSecond, we demonstrate that the proposed MPC achieves precise temperature\ntracking while satisfying melt pool depth constraints within a targeted\ndilution range (10\\%-30\\%), reducing potential porosity defects. Compared to\nPID controller, the MPC results in smoother and less fluctuating laser power\nprofiles with competitive or superior melt pool temperature control\nperformance. This demonstrates the MPC's proactive control capabilities,\nleveraging time-series prediction and real-time optimization, positioning it as\na powerful tool for future Digital Twin applications and real-time process\noptimization in manufacturing.\n","authors":["Yi-Ping Chen","Vispi Karkaria","Ying-Kuan Tsai","Faith Rolark","Daniel Quispe","Robert X. Gao","Jian Cao","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2501.07601v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08249v1","updated":"2025-04-11T04:14:55Z","published":"2025-04-11T04:14:55Z","title":"Neural Network-assisted Interval Reachability for Systems with Control\n  Barrier Function-Based Safe Controllers","summary":"  Control Barrier Functions (CBFs) have been widely utilized in the design of\noptimization-based controllers and filters for dynamical systems to ensure\nforward invariance of a given set of safe states. While CBF-based controllers\noffer safety guarantees, they can compromise the performance of the system,\nleading to undesirable behaviors such as unbounded trajectories and emergence\nof locally stable spurious equilibria. Computing reachable sets for systems\nwith CBF-based controllers is an effective approach for runtime performance and\nstability verification, and can potentially serve as a tool for trajectory\nre-planning. In this paper, we propose a computationally efficient interval\nreachability method for performance verification of systems with\noptimization-based controllers by: (i) approximating the optimization-based\ncontroller by a pre-trained neural network to avoid solving optimization\nproblems repeatedly, and (ii) using mixed monotone theory to construct an\nembedding system that leverages state-of-the-art neural network verification\nalgorithms for bounding the output of the neural network. Results in terms of\ncloseness of solutions of trajectories of the system with the\noptimization-based controller and the neural network are derived. Using a\nsingle trajectory of the embedding system along with our closeness of solutions\nresult, we obtain an over-approximation of the reachable set of the system with\noptimization-based controllers. Numerical results are presented to corroborate\nthe technical findings.\n","authors":["Damola Ajeyemi","Saber Jafarpour","Emiliano Dall'Anese"],"pdf_url":"https://arxiv.org/pdf/2504.08249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08247v1","updated":"2025-04-11T04:14:32Z","published":"2025-04-11T04:14:32Z","title":"Millions of States: Designing a Scalable MoE Architecture with RWKV-7\n  Meta-learner","summary":"  State-based sequence models like RWKV-7 offer a compelling alternative to\nTransformer architectures, achieving linear complexity while demonstrating\ngreater expressive power in short-context scenarios and enabling state tracking\nbeyond the \\(\\text{TC}^0\\) complexity class. However, RWKV-7 lacks mechanisms\nfor token-parameter interactions and native scalability, limiting its\nadaptability and growth without retraining. In this paper, we propose\n\\textbf{Meta-State}, a novel extension to RWKV-7 that replaces attention\nmechanisms with a fully state-driven approach, integrating token-parameter\ninteractions through a \\textbf{Self-State Encoder} (SSE) mechanism. The SSE\nrepurposes a portion of the RWKV-7 Weighted Key-Value (WKV) state as\ntransformation weights to encode token-parameter interactions in a linear,\nstate-driven manner without introducing new trainable matrices or softmax\noperations, while preserving the autoregressive property of token processing.\nMeta-State supports progressive model scaling by expanding the WKV state and\nparameter tokens, reusing existing parameters without retraining. Our approach\nbridges the gap between state-based modeling, token-parameter interactions, and\nscalable architectures, offering a flexible framework for efficient and\nadaptable sequence modeling with linear complexity and constant memory usage.\n","authors":["Liu Xiao","Li Zhiyuan","Lin Yueyu"],"pdf_url":"https://arxiv.org/pdf/2504.08247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08246v1","updated":"2025-04-11T04:12:15Z","published":"2025-04-11T04:12:15Z","title":"Spectral Normalization for Lipschitz-Constrained Policies on Learning\n  Humanoid Locomotion","summary":"  Reinforcement learning (RL) has shown great potential in training agile and\nadaptable controllers for legged robots, enabling them to learn complex\nlocomotion behaviors directly from experience. However, policies trained in\nsimulation often fail to transfer to real-world robots due to unrealistic\nassumptions such as infinite actuator bandwidth and the absence of torque\nlimits. These conditions allow policies to rely on abrupt, high-frequency\ntorque changes, which are infeasible for real actuators with finite bandwidth.\n  Traditional methods address this issue by penalizing aggressive motions\nthrough regularization rewards, such as joint velocities, accelerations, and\nenergy consumption, but they require extensive hyperparameter tuning.\nAlternatively, Lipschitz-Constrained Policies (LCP) enforce finite bandwidth\naction control by penalizing policy gradients, but their reliance on gradient\ncalculations introduces significant GPU memory overhead. To overcome this\nlimitation, this work proposes Spectral Normalization (SN) as an efficient\nreplacement for enforcing Lipschitz continuity. By constraining the spectral\nnorm of network weights, SN effectively limits high-frequency policy\nfluctuations while significantly reducing GPU memory usage. Experimental\nevaluations in both simulation and real-world humanoid robot show that SN\nachieves performance comparable to gradient penalty methods while enabling more\nefficient parallel training.\n","authors":["Jaeyong Shin","Woohyun Cha","Donghyeon Kim","Junhyeok Cha","Jaeheung Park"],"pdf_url":"https://arxiv.org/pdf/2504.08246v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2404.10550v3","updated":"2025-04-11T04:02:41Z","published":"2024-04-16T13:19:46Z","title":"Analytical Approximation of the ELBO Gradient in the Context of the\n  Clutter Problem","summary":"  We propose an analytical solution for approximating the gradient of the\nEvidence Lower Bound (ELBO) in variational inference problems where the\nstatistical model is a Bayesian network consisting of observations drawn from a\nmixture of a Gaussian distribution embedded in unrelated clutter, known as the\nclutter problem. The method employs the reparameterization trick to move the\ngradient operator inside the expectation and relies on the assumption that,\nbecause the likelihood factorizes over the observed data, the variational\ndistribution is generally more compactly supported than the Gaussian\ndistribution in the likelihood factors. This allows efficient local\napproximation of the individual likelihood factors, which leads to an\nanalytical solution for the integral defining the gradient expectation. We\nintegrate the proposed gradient approximation as the expectation step in an EM\n(Expectation Maximization) algorithm for maximizing ELBO and test against\nclassical deterministic approaches in Bayesian inference, such as the Laplace\napproximation, Expectation Propagation and Mean-Field Variational Inference.\nThe proposed method demonstrates good accuracy and rate of convergence together\nwith linear computational complexity.\n","authors":["Roumen Nikolaev Popov"],"pdf_url":"https://arxiv.org/pdf/2404.10550v3.pdf","comment":"19 pages, 5 figures, supporting code available at\n  https://github.com/rpopov42/elbo_gaa"},{"id":"http://arxiv.org/abs/2405.05865v2","updated":"2025-04-11T03:45:14Z","published":"2024-05-09T15:53:43Z","title":"Faster Linear Systems and Matrix Norm Approximation via Multi-level\n  Sketched Preconditioning","summary":"  We present a new class of preconditioned iterative methods for solving linear\nsystems of the form $Ax = b$. Our methods are based on constructing a low-rank\nNystr\\\"om approximation to $A$ using sparse random matrix sketching. This\napproximation is used to construct a preconditioner, which itself is inverted\nquickly using additional levels of random sketching and preconditioning. We\nprove that the convergence of our methods depends on a natural average\ncondition number of $A$, which improves as the rank of the Nystr\\\"om\napproximation increases. Concretely, this allows us to obtain faster runtimes\nfor a number of fundamental linear algebraic problems:\n  1. We show how to solve any $n\\times n$ linear system that is\nwell-conditioned except for $k$ outlying large singular values in\n$\\tilde{O}(n^{2.065} + k^\\omega)$ time, improving on a recent result of\n[Derezi\\'nski, Yang, STOC 2024] for all $k \\gtrsim n^{0.78}$.\n  2. We give the first $\\tilde{O}(n^2 + {d_\\lambda}^{\\omega}$) time algorithm\nfor solving a regularized linear system $(A + \\lambda I)x = b$, where $A$ is\npositive semidefinite with effective dimension\n$d_\\lambda=\\mathrm{tr}(A(A+\\lambda I)^{-1})$. This problem arises in\napplications like Gaussian process regression.\n  3. We give faster algorithms for approximating Schatten $p$-norms and other\nmatrix norms. For example, for the Schatten 1-norm (nuclear norm), we give an\nalgorithm that runs in $\\tilde{O}(n^{2.11})$ time, improving on an\n$\\tilde{O}(n^{2.18})$ method of [Musco et al., ITCS 2018]. All results are\nproven in the real RAM model of computation. Interestingly, previous\nstate-of-the-art algorithms for most of the problems above relied on stochastic\niterative methods, like stochastic coordinate and gradient descent. Our work\ntakes a completely different approach, instead leveraging tools from matrix\nsketching.\n","authors":["Michał Dereziński","Christopher Musco","Jiaming Yang"],"pdf_url":"https://arxiv.org/pdf/2405.05865v2.pdf","comment":"SODA 2025"},{"id":"http://arxiv.org/abs/2504.08234v1","updated":"2025-04-11T03:43:46Z","published":"2025-04-11T03:43:46Z","title":"Bringing Structure to Naturalness: On the Naturalness of ASTs","summary":"  Source code comes in different shapes and forms. Previous research has\nalready shown code to be more predictable than natural language as well as\nhighlighted its statistical predictability at the token level: source code can\nbe natural. More recently, the structure of code -- control flow, syntax\ngraphs, abstract syntax trees etc. -- has been successfully used to improve the\nstate-of-the-art on numerous tasks: code suggestion, code summarisation, method\nnaming etc. This body of work implicitly assumes that structured\nrepresentations of code are similarly statistically predictable, i.e. that a\nstructured view of code is also natural. We consider that this view should be\nmade explicit and propose directly studying the Structured Naturalness\nHypothesis. Beyond just naming existing research that assumes this hypothesis\nand formulating it, we also provide evidence in the case of trees: TreeLSTM\nmodels over ASTs for some languages, such as Ruby, are competitive with\n$n$-gram models while handling the syntax token issue highlighted by previous\nresearch 'for free'. For other languages, such as Java or Python, we find tree\nmodels to perform worse, suggesting that downstream task improvement is\nuncorrelated to the language modelling task. Further, we show how such\nnaturalness signals can be employed for near state-of-the-art results on\njust-in-time defect prediction while forgoing manual feature engineering work.\n","authors":["Profir-Petru Pârţachi","Mahito Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2504.08234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14249v6","updated":"2025-04-11T03:34:49Z","published":"2025-01-24T05:27:46Z","title":"Humanity's Last Exam","summary":"  Benchmarks are important tools for tracking the rapid advancements in large\nlanguage model (LLM) capabilities. However, benchmarks are not keeping pace in\ndifficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like\nMMLU, limiting informed measurement of state-of-the-art LLM capabilities. In\nresponse, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at\nthe frontier of human knowledge, designed to be the final closed-ended academic\nbenchmark of its kind with broad subject coverage. HLE consists of 2,700\nquestions across dozens of subjects, including mathematics, humanities, and the\nnatural sciences. HLE is developed globally by subject-matter experts and\nconsists of multiple-choice and short-answer questions suitable for automated\ngrading. Each question has a known solution that is unambiguous and easily\nverifiable, but cannot be quickly answered via internet retrieval.\nState-of-the-art LLMs demonstrate low accuracy and calibration on HLE,\nhighlighting a significant gap between current LLM capabilities and the expert\nhuman frontier on closed-ended academic questions. To inform research and\npolicymaking upon a clear understanding of model capabilities, we publicly\nrelease HLE at https://lastexam.ai.\n","authors":["Long Phan","Alice Gatti","Ziwen Han","Nathaniel Li","Josephina Hu","Hugh Zhang","Chen Bo Calvin Zhang","Mohamed Shaaban","John Ling","Sean Shi","Michael Choi","Anish Agrawal","Arnav Chopra","Adam Khoja","Ryan Kim","Richard Ren","Jason Hausenloy","Oliver Zhang","Mantas Mazeika","Dmitry Dodonov","Tung Nguyen","Jaeho Lee","Daron Anderson","Mikhail Doroshenko","Alun Cennyth Stokes","Mobeen Mahmood","Oleksandr Pokutnyi","Oleg Iskra","Jessica P. Wang","John-Clark Levin","Mstyslav Kazakov","Fiona Feng","Steven Y. Feng","Haoran Zhao","Michael Yu","Varun Gangal","Chelsea Zou","Zihan Wang","Serguei Popov","Robert Gerbicz","Geoff Galgon","Johannes Schmitt","Will Yeadon","Yongki Lee","Scott Sauers","Alvaro Sanchez","Fabian Giska","Marc Roth","Søren Riis","Saiteja Utpala","Noah Burns","Gashaw M. Goshu","Mohinder Maheshbhai Naiya","Chidozie Agu","Zachary Giboney","Antrell Cheatom","Francesco Fournier-Facio","Sarah-Jane Crowson","Lennart Finke","Zerui Cheng","Jennifer Zampese","Ryan G. Hoerr","Mark Nandor","Hyunwoo Park","Tim Gehrunger","Jiaqi Cai","Ben McCarty","Alexis C Garretson","Edwin Taylor","Damien Sileo","Qiuyu Ren","Usman Qazi","Lianghui Li","Jungbae Nam","John B. Wydallis","Pavel Arkhipov","Jack Wei Lun Shi","Aras Bacho","Chris G. Willcocks","Hangrui Cao","Sumeet Motwani","Emily de Oliveira Santos","Johannes Veith","Edward Vendrow","Doru Cojoc","Kengo Zenitani","Joshua Robinson","Longke Tang","Yuqi Li","Joshua Vendrow","Natanael Wildner Fraga","Vladyslav Kuchkin","Andrey Pupasov Maksimov","Pierre Marion","Denis Efremov","Jayson Lynch","Kaiqu Liang","Aleksandar Mikov","Andrew Gritsevskiy","Julien Guillod","Gözdenur Demir","Dakotah Martinez","Ben Pageler","Kevin Zhou","Saeed Soori","Ori Press","Henry Tang","Paolo Rissone","Sean R. Green","Lina Brüssel","Moon Twayana","Aymeric Dieuleveut","Joseph Marvin Imperial","Ameya Prabhu","Jinzhou Yang","Nick Crispino","Arun Rao","Dimitri Zvonkine","Gabriel Loiseau","Mikhail Kalinin","Marco Lukas","Ciprian Manolescu","Nate Stambaugh","Subrata Mishra","Tad Hogg","Carlo Bosio","Brian P Coppola","Julian Salazar","Jaehyeok Jin","Rafael Sayous","Stefan Ivanov","Philippe Schwaller","Shaipranesh Senthilkuma","Andres M Bran","Andres Algaba","Kelsey Van den Houte","Lynn Van Der Sypt","Brecht Verbeken","David Noever","Alexei Kopylov","Benjamin Myklebust","Bikun Li","Lisa Schut","Evgenii Zheltonozhskii","Qiaochu Yuan","Derek Lim","Richard Stanley","Tong Yang","John Maar","Julian Wykowski","Martí Oller","Anmol Sahu","Cesare Giulio Ardito","Yuzheng Hu","Ariel Ghislain Kemogne Kamdoum","Alvin Jin","Tobias Garcia Vilchis","Yuexuan Zu","Martin Lackner","James Koppel","Gongbo Sun","Daniil S. Antonenko","Steffi Chern","Bingchen Zhao","Pierrot Arsene","Joseph M Cavanagh","Daofeng Li","Jiawei Shen","Donato Crisostomi","Wenjin Zhang","Ali Dehghan","Sergey Ivanov","David Perrella","Nurdin Kaparov","Allen Zang","Ilia Sucholutsky","Arina Kharlamova","Daniil Orel","Vladislav Poritski","Shalev Ben-David","Zachary Berger","Parker Whitfill","Michael Foster","Daniel Munro","Linh Ho","Shankar Sivarajan","Dan Bar Hava","Aleksey Kuchkin","David Holmes","Alexandra Rodriguez-Romero","Frank Sommerhage","Anji Zhang","Richard Moat","Keith Schneider","Zakayo Kazibwe","Don Clarke","Dae Hyun Kim","Felipe Meneguitti Dias","Sara Fish","Veit Elser","Tobias Kreiman","Victor Efren Guadarrama Vilchis","Immo Klose","Ujjwala Anantheswaran","Adam Zweiger","Kaivalya Rawal","Jeffery Li","Jeremy Nguyen","Nicolas Daans","Haline Heidinger","Maksim Radionov","Václav Rozhoň","Vincent Ginis","Christian Stump","Niv Cohen","Rafał Poświata","Josef Tkadlec","Alan Goldfarb","Chenguang Wang","Piotr Padlewski","Stanislaw Barzowski","Kyle Montgomery","Ryan Stendall","Jamie Tucker-Foltz","Jack Stade","T. Ryan Rogers","Tom Goertzen","Declan Grabb","Abhishek Shukla","Alan Givré","John Arnold Ambay","Archan Sen","Muhammad Fayez Aziz","Mark H Inlow","Hao He","Ling Zhang","Younesse Kaddar","Ivar Ängquist","Yanxu Chen","Harrison K Wang","Kalyan Ramakrishnan","Elliott Thornley","Antonio Terpin","Hailey Schoelkopf","Eric Zheng","Avishy Carmi","Ethan D. L. Brown","Kelin Zhu","Max Bartolo","Richard Wheeler","Martin Stehberger","Peter Bradshaw","JP Heimonen","Kaustubh Sridhar","Ido Akov","Jennifer Sandlin","Yury Makarychev","Joanna Tam","Hieu Hoang","David M. Cunningham","Vladimir Goryachev","Demosthenes Patramanis","Michael Krause","Andrew Redenti","David Aldous","Jesyin Lai","Shannon Coleman","Jiangnan Xu","Sangwon Lee","Ilias Magoulas","Sandy Zhao","Ning Tang","Michael K. Cohen","Orr Paradise","Jan Hendrik Kirchner","Maksym Ovchynnikov","Jason O. Matos","Adithya Shenoy","Michael Wang","Yuzhou Nie","Anna Sztyber-Betley","Paolo Faraboschi","Robin Riblet","Jonathan Crozier","Shiv Halasyamani","Shreyas Verma","Prashant Joshi","Eli Meril","Ziqiao Ma","Jérémy Andréoletti","Raghav Singhal","Jacob Platnick","Volodymyr Nevirkovets","Luke Basler","Alexander Ivanov","Seri Khoury","Nils Gustafsson","Marco Piccardo","Hamid Mostaghimi","Qijia Chen","Virendra Singh","Tran Quoc Khánh","Paul Rosu","Hannah Szlyk","Zachary Brown","Himanshu Narayan","Aline Menezes","Jonathan Roberts","William Alley","Kunyang Sun","Arkil Patel","Max Lamparth","Anka Reuel","Linwei Xin","Hanmeng Xu","Jacob Loader","Freddie Martin","Zixuan Wang","Andrea Achilleos","Thomas Preu","Tomek Korbak","Ida Bosio","Fereshteh Kazemi","Ziye Chen","Biró Bálint","Eve J. Y. Lo","Jiaqi Wang","Maria Inês S. Nunes","Jeremiah Milbauer","M Saiful Bari","Zihao Wang","Behzad Ansarinejad","Yewen Sun","Stephane Durand","Hossam Elgnainy","Guillaume Douville","Daniel Tordera","George Balabanian","Hew Wolff","Lynna Kvistad","Hsiaoyun Milliron","Ahmad Sakor","Murat Eron","Andrew Favre D. O.","Shailesh Shah","Xiaoxiang Zhou","Firuz Kamalov","Sherwin Abdoli","Tim Santens","Shaul Barkan","Allison Tee","Robin Zhang","Alessandro Tomasiello","G. Bruno De Luca","Shi-Zhuo Looi","Vinh-Kha Le","Noam Kolt","Jiayi Pan","Emma Rodman","Jacob Drori","Carl J Fossum","Niklas Muennighoff","Milind Jagota","Ronak Pradeep","Honglu Fan","Jonathan Eicher","Michael Chen","Kushal Thaman","William Merrill","Moritz Firsching","Carter Harris","Stefan Ciobâcă","Jason Gross","Rohan Pandey","Ilya Gusev","Adam Jones","Shashank Agnihotri","Pavel Zhelnov","Mohammadreza Mofayezi","Alexander Piperski","David K. Zhang","Kostiantyn Dobarskyi","Roman Leventov","Ignat Soroko","Joshua Duersch","Vage Taamazyan","Andrew Ho","Wenjie Ma","William Held","Ruicheng Xian","Armel Randy Zebaze","Mohanad Mohamed","Julian Noah Leser","Michelle X Yuan","Laila Yacar","Johannes Lengler","Katarzyna Olszewska","Claudio Di Fratta","Edson Oliveira","Joseph W. Jackson","Andy Zou","Muthu Chidambaram","Timothy Manik","Hector Haffenden","Dashiell Stander","Ali Dasouqi","Alexander Shen","Bita Golshani","David Stap","Egor Kretov","Mikalai Uzhou","Alina Borisovna Zhidkovskaya","Nick Winter","Miguel Orbegozo Rodriguez","Robert Lauff","Dustin Wehr","Colin Tang","Zaki Hossain","Shaun Phillips","Fortuna Samuele","Fredrik Ekström","Angela Hammon","Oam Patel","Faraz Farhidi","George Medley","Forough Mohammadzadeh","Madellene Peñaflor","Haile Kassahun","Alena Friedrich","Rayner Hernandez Perez","Daniel Pyda","Taom Sakal","Omkar Dhamane","Ali Khajegili Mirabadi","Eric Hallman","Kenchi Okutsu","Mike Battaglia","Mohammad Maghsoudimehrabani","Alon Amit","Dave Hulbert","Roberto Pereira","Simon Weber"," Handoko","Anton Peristyy","Stephen Malina","Mustafa Mehkary","Rami Aly","Frank Reidegeld","Anna-Katharina Dick","Cary Friday","Mukhwinder Singh","Hassan Shapourian","Wanyoung Kim","Mariana Costa","Hubeyb Gurdogan","Harsh Kumar","Chiara Ceconello","Chao Zhuang","Haon Park","Micah Carroll","Andrew R. Tawfeek","Stefan Steinerberger","Daattavya Aggarwal","Michael Kirchhof","Linjie Dai","Evan Kim","Johan Ferret","Jainam Shah","Yuzhou Wang","Minghao Yan","Krzysztof Burdzy","Lixin Zhang","Antonio Franca","Diana T. Pham","Kang Yong Loh","Joshua Robinson","Abram Jackson","Paolo Giordano","Philipp Petersen","Adrian Cosma","Jesus Colino","Colin White","Jacob Votava","Vladimir Vinnikov","Ethan Delaney","Petr Spelda","Vit Stritecky","Syed M. Shahid","Jean-Christophe Mourrat","Lavr Vetoshkin","Koen Sponselee","Renas Bacho","Zheng-Xin Yong","Florencia de la Rosa","Nathan Cho","Xiuyu Li","Guillaume Malod","Orion Weller","Guglielmo Albani","Leon Lang","Julien Laurendeau","Dmitry Kazakov","Fatimah Adesanya","Julien Portier","Lawrence Hollom","Victor Souza","Yuchen Anna Zhou","Julien Degorre","Yiğit Yalın","Gbenga Daniel Obikoya"," Rai","Filippo Bigi","M. C. Boscá","Oleg Shumar","Kaniuar Bacho","Gabriel Recchia","Mara Popescu","Nikita Shulga","Ngefor Mildred Tanwie","Thomas C. H. Lux","Ben Rank","Colin Ni","Matthew Brooks","Alesia Yakimchyk"," Huanxu"," Liu","Stefano Cavalleri","Olle Häggström","Emil Verkama","Joshua Newbould","Hans Gundlach","Leonor Brito-Santana","Brian Amaro","Vivek Vajipey","Rynaa Grover","Ting Wang","Yosi Kratish","Wen-Ding Li","Sivakanth Gopi","Andrea Caciolai","Christian Schroeder de Witt","Pablo Hernández-Cámara","Emanuele Rodolà","Jules Robins","Dominic Williamson","Vincent Cheng","Brad Raynor","Hao Qi","Ben Segev","Jingxuan Fan","Sarah Martinson","Erik Y. Wang","Kaylie Hausknecht","Michael P. Brenner","Mao Mao","Christoph Demian","Peyman Kassani","Xinyu Zhang","David Avagian","Eshawn Jessica Scipio","Alon Ragoler","Justin Tan","Blake Sims","Rebeka Plecnik","Aaron Kirtland","Omer Faruk Bodur","D. P. Shinde","Yan Carlos Leyva Labrador","Zahra Adoul","Mohamed Zekry","Ali Karakoc","Tania C. B. Santos","Samir Shamseldeen","Loukmane Karim","Anna Liakhovitskaia","Nate Resman","Nicholas Farina","Juan Carlos Gonzalez","Gabe Maayan","Earth Anderson","Rodrigo De Oliveira Pena","Elizabeth Kelley","Hodjat Mariji","Rasoul Pouriamanesh","Wentao Wu","Ross Finocchio","Ismail Alarab","Joshua Cole","Danyelle Ferreira","Bryan Johnson","Mohammad Safdari","Liangti Dai","Siriphan Arthornthurasuk","Isaac C. McAlister","Alejandro José Moyano","Alexey Pronin","Jing Fan","Angel Ramirez-Trinidad","Yana Malysheva","Daphiny Pottmaier","Omid Taheri","Stanley Stepanic","Samuel Perry","Luke Askew","Raúl Adrián Huerta Rodríguez","Ali M. R. Minissi","Ricardo Lorena","Krishnamurthy Iyer","Arshad Anil Fasiludeen","Ronald Clark","Josh Ducey","Matheus Piza","Maja Somrak","Eric Vergo","Juehang Qin","Benjámin Borbás","Eric Chu","Jack Lindsey","Antoine Jallon","I. M. J. McInnis","Evan Chen","Avi Semler","Luk Gloor","Tej Shah","Marc Carauleanu","Pascal Lauer","Tran Đuc Huy","Hossein Shahrtash","Emilien Duc","Lukas Lewark","Assaf Brown","Samuel Albanie","Brian Weber","Warren S. Vaz","Pierre Clavier","Yiyang Fan","Gabriel Poesia Reis e Silva"," Long"," Lian","Marcus Abramovitch","Xi Jiang","Sandra Mendoza","Murat Islam","Juan Gonzalez","Vasilios Mavroudis","Justin Xu","Pawan Kumar","Laxman Prasad Goswami","Daniel Bugas","Nasser Heydari","Ferenc Jeanplong","Thorben Jansen","Antonella Pinto","Archimedes Apronti","Abdallah Galal","Ng Ze-An","Ankit Singh","Tong Jiang","Joan of Arc Xavier","Kanu Priya Agarwal","Mohammed Berkani","Gang Zhang","Zhehang Du","Benedito Alves de Oliveira Junior","Dmitry Malishev","Nicolas Remy","Taylor D. Hartman","Tim Tarver","Stephen Mensah","Gautier Abou Loume","Wiktor Morak","Farzad Habibi","Sarah Hoback","Will Cai","Javier Gimenez","Roselynn Grace Montecillo","Jakub Łucki","Russell Campbell","Asankhaya Sharma","Khalida Meer","Shreen Gul","Daniel Espinosa Gonzalez","Xavier Alapont","Alex Hoover","Gunjan Chhablani","Freddie Vargus","Arunim Agarwal","Yibo Jiang","Deepakkumar Patil","David Outevsky","Kevin Joseph Scaria","Rajat Maheshwari","Abdelkader Dendane","Priti Shukla","Ashley Cartwright","Sergei Bogdanov","Niels Mündler","Sören Möller","Luca Arnaboldi","Kunvar Thaman","Muhammad Rehan Siddiqi","Prajvi Saxena","Himanshu Gupta","Tony Fruhauff","Glen Sherman","Mátyás Vincze","Siranut Usawasutsakorn","Dylan Ler","Anil Radhakrishnan","Innocent Enyekwe","Sk Md Salauddin","Jiang Muzhen","Aleksandr Maksapetyan","Vivien Rossbach","Chris Harjadi","Mohsen Bahaloohoreh","Claire Sparrow","Jasdeep Sidhu","Sam Ali","Song Bian","John Lai","Eric Singer","Justine Leon Uro","Greg Bateman","Mohamed Sayed","Ahmed Menshawy","Darling Duclosel","Dario Bezzi","Yashaswini Jain","Ashley Aaron","Murat Tiryakioglu","Sheeshram Siddh","Keith Krenek","Imad Ali Shah","Jun Jin","Scott Creighton","Denis Peskoff","Zienab EL-Wasif","Ragavendran P V","Michael Richmond","Joseph McGowan","Tejal Patwardhan","Hao-Yu Sun","Ting Sun","Nikola Zubić","Samuele Sala","Stephen Ebert","Jean Kaddour","Manuel Schottdorf","Dianzhuo Wang","Gerol Petruzella","Alex Meiburg","Tilen Medved","Ali ElSheikh","S Ashwin Hebbar","Lorenzo Vaquero","Xianjun Yang","Jason Poulos","Vilém Zouhar","Sergey Bogdanik","Mingfang Zhang","Jorge Sanz-Ros","David Anugraha","Yinwei Dai","Anh N. Nhu","Xue Wang","Ali Anil Demircali","Zhibai Jia","Yuyin Zhou","Juncheng Wu","Mike He","Nitin Chandok","Aarush Sinha","Gaoxiang Luo","Long Le","Mickaël Noyé","Ioannis Pantidis","Tianbo Qi","Soham Sachin Purohit","Letitia Parcalabescu","Thai-Hoa Nguyen","Genta Indra Winata","Edoardo M. Ponti","Hanchen Li","Kaustubh Dhole","Jongee Park","Dario Abbondanza","Yuanli Wang","Anupam Nayak","Diogo M. Caetano","Antonio A. W. L. Wong","Maria del Rio-Chanona","Dániel Kondor","Pieter Francois","Ed Chalstrey","Jakob Zsambok","Dan Hoyer","Jenny Reddish","Jakob Hauser","Francisco-Javier Rodrigo-Ginés","Suchandra Datta","Maxwell Shepherd","Thom Kamphuis","Qizheng Zhang","Hyunjun Kim","Ruiji Sun","Jianzhu Yao","Franck Dernoncourt","Satyapriya Krishna","Sina Rismanchian","Bonan Pu","Francesco Pinto","Yingheng Wang","Kumar Shridhar","Kalon J. Overholt","Glib Briia","Hieu Nguyen"," David","Soler Bartomeu","Tony CY Pang","Adam Wecker","Yifan Xiong","Fanfei Li","Lukas S. Huber","Joshua Jaeger","Romano De Maddalena","Xing Han Lù","Yuhui Zhang","Claas Beger","Patrick Tser Jern Kon","Sean Li","Vivek Sanker","Ming Yin","Yihao Liang","Xinlu Zhang","Ankit Agrawal","Li S. Yifei","Zechen Zhang","Mu Cai","Yasin Sonmez","Costin Cozianu","Changhao Li","Alex Slen","Shoubin Yu","Hyun Kyu Park","Gabriele Sarti","Marcin Briański","Alessandro Stolfo","Truong An Nguyen","Mike Zhang","Yotam Perlitz","Jose Hernandez-Orallo","Runjia Li","Amin Shabani","Felix Juefei-Xu","Shikhar Dhingra","Orr Zohar","My Chiffon Nguyen","Alexander Pondaven","Abdurrahim Yilmaz","Xuandong Zhao","Chuanyang Jin","Muyan Jiang","Stefan Todoran","Xinyao Han","Jules Kreuer","Brian Rabern","Anna Plassart","Martino Maggetti","Luther Yap","Robert Geirhos","Jonathon Kean","Dingsu Wang","Sina Mollaei","Chenkai Sun","Yifan Yin","Shiqi Wang","Rui Li","Yaowen Chang","Anjiang Wei","Alice Bizeul","Xiaohan Wang","Alexandre Oliveira Arrais","Kushin Mukherjee","Jorge Chamorro-Padial","Jiachen Liu","Xingyu Qu","Junyi Guan","Adam Bouyamourn","Shuyu Wu","Martyna Plomecka","Junda Chen","Mengze Tang","Jiaqi Deng","Shreyas Subramanian","Haocheng Xi","Haoxuan Chen","Weizhi Zhang","Yinuo Ren","Haoqin Tu","Sejong Kim","Yushun Chen","Sara Vera Marjanović","Junwoo Ha","Grzegorz Luczyna","Jeff J. Ma","Zewen Shen","Dawn Song","Cedegao E. Zhang","Zhun Wang","Gaël Gendron","Yunze Xiao","Leo Smucker","Erica Weng","Kwok Hao Lee","Zhe Ye","Stefano Ermon","Ignacio D. Lopez-Miguel","Theo Knights","Anthony Gitter","Namkyu Park","Boyi Wei","Hongzheng Chen","Kunal Pai","Ahmed Elkhanany","Han Lin","Philipp D. Siedler","Jichao Fang","Ritwik Mishra","Károly Zsolnai-Fehér","Xilin Jiang","Shadab Khan","Jun Yuan","Rishab Kumar Jain","Xi Lin","Mike Peterson","Zhe Wang","Aditya Malusare","Maosen Tang","Isha Gupta","Ivan Fosin","Timothy Kang","Barbara Dworakowska","Kazuki Matsumoto","Guangyao Zheng","Gerben Sewuster","Jorge Pretel Villanueva","Ivan Rannev","Igor Chernyavsky","Jiale Chen","Deepayan Banik","Ben Racz","Wenchao Dong","Jianxin Wang","Laila Bashmal","Duarte V. Gonçalves","Wei Hu","Kaushik Bar","Ondrej Bohdal","Atharv Singh Patlan","Shehzaad Dhuliawala","Caroline Geirhos","Julien Wist","Yuval Kansal","Bingsen Chen","Kutay Tire","Atak Talay Yücel","Brandon Christof","Veerupaksh Singla","Zijian Song","Sanxing Chen","Jiaxin Ge","Kaustubh Ponkshe","Isaac Park","Tianneng Shi","Martin Q. Ma","Joshua Mak","Sherwin Lai","Antoine Moulin","Zhuo Cheng","Zhanda Zhu","Ziyi Zhang","Vaidehi Patil","Ketan Jha","Qiutong Men","Jiaxuan Wu","Tianchi Zhang","Bruno Hebling Vieira","Alham Fikri Aji","Jae-Won Chung","Mohammed Mahfoud","Ha Thi Hoang","Marc Sperzel","Wei Hao","Kristof Meding","Sihan Xu","Vassilis Kostakos","Davide Manini","Yueying Liu","Christopher Toukmaji","Jay Paek","Eunmi Yu","Arif Engin Demircali","Zhiyi Sun","Ivan Dewerpe","Hongsen Qin","Roman Pflugfelder","James Bailey","Johnathan Morris","Ville Heilala","Sybille Rosset","Zishun Yu","Peter E. Chen","Woongyeong Yeo","Eeshaan Jain","Ryan Yang","Sreekar Chigurupati","Julia Chernyavsky","Sai Prajwal Reddy","Subhashini Venugopalan","Hunar Batra","Core Francisco Park","Hieu Tran","Guilherme Maximiano","Genghan Zhang","Yizhuo Liang","Hu Shiyu","Rongwu Xu","Rui Pan","Siddharth Suresh","Ziqi Liu","Samaksh Gulati","Songyang Zhang","Peter Turchin","Christopher W. Bartlett","Christopher R. Scotese","Phuong M. Cao","Aakaash Nattanmai","Gordon McKellips","Anish Cheraku","Asim Suhail","Ethan Luo","Marvin Deng","Jason Luo","Ashley Zhang","Kavin Jindel","Jay Paek","Kasper Halevy","Allen Baranov","Michael Liu","Advaith Avadhanam","David Zhang","Vincent Cheng","Brad Ma","Evan Fu","Liam Do","Joshua Lass","Hubert Yang","Surya Sunkari","Vishruth Bharath","Violet Ai","James Leung","Rishit Agrawal","Alan Zhou","Kevin Chen","Tejas Kalpathi","Ziqi Xu","Gavin Wang","Tyler Xiao","Erik Maung","Sam Lee","Ryan Yang","Roy Yue","Ben Zhao","Julia Yoon","Sunny Sun","Aryan Singh","Ethan Luo","Clark Peng","Tyler Osbey","Taozhi Wang","Daryl Echeazu","Hubert Yang","Timothy Wu","Spandan Patel","Vidhi Kulkarni","Vijaykaarti Sundarapandiyan","Ashley Zhang","Andrew Le","Zafir Nasim","Srikar Yalam","Ritesh Kasamsetty","Soham Samal","Hubert Yang","David Sun","Nihar Shah","Abhijeet Saha","Alex Zhang","Leon Nguyen","Laasya Nagumalli","Kaixin Wang","Alan Zhou","Aidan Wu","Jason Luo","Anwith Telluri","Summer Yue","Alexandr Wang","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2501.14249v6.pdf","comment":"29 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.01248v3","updated":"2025-04-11T03:23:52Z","published":"2025-03-03T07:23:56Z","title":"Comprehensive Evaluation of OCT-based Automated Segmentation of Retinal\n  Layer, Fluid and Hyper-Reflective Foci: Impact on Diabetic Retinopathy\n  Severity Assessment","summary":"  Diabetic retinopathy (DR) is a leading cause of vision loss, requiring early\nand accurate assessment to prevent irreversible damage. Spectral Domain Optical\nCoherence Tomography (SD-OCT) enables high-resolution retinal imaging, but\nautomated segmentation performance varies, especially in cases with complex\nfluid and hyperreflective foci (HRF) patterns. This study proposes an\nactive-learning-based deep learning pipeline for automated segmentation of\nretinal layers, fluid, and HRF, using four state-of-the-art models: U-Net,\nSegFormer, SwinUNETR, and VM-UNet, trained on expert-annotated SD-OCT volumes.\nSegmentation accuracy was evaluated with five-fold cross-validation, and\nretinal thickness was quantified using a K-nearest neighbors algorithm and\nvisualized with Early Treatment Diabetic Retinopathy Study (ETDRS) maps.\nSwinUNETR achieved the highest overall accuracy (DSC = 0.7719; NSD = 0.8149),\nwhile VM-UNet excelled in specific layers. Structural differences were observed\nbetween non-proliferative and proliferative DR, with layer-specific thickening\ncorrelating with visual acuity impairment. The proposed framework enables\nrobust, clinically relevant DR assessment while reducing the need for manual\nannotation, supporting improved disease monitoring and treatment planning.\n","authors":["S. Chen","D. Ma","M. Raviselvan","S. Sundaramoorthy","K. Popuri","M. J. Ju","M. V. Sarunic","D. Ratra","M. F. Beg"],"pdf_url":"https://arxiv.org/pdf/2503.01248v3.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2504.05150v2","updated":"2025-04-11T03:14:53Z","published":"2025-04-07T14:56:43Z","title":"A Reinforcement Learning Method for Environments with Stochastic\n  Variables: Post-Decision Proximal Policy Optimization with Dual Critic\n  Networks","summary":"  This paper presents Post-Decision Proximal Policy Optimization (PDPPO), a\nnovel variation of the leading deep reinforcement learning method, Proximal\nPolicy Optimization (PPO). The PDPPO state transition process is divided into\ntwo steps: a deterministic step resulting in the post-decision state and a\nstochastic step leading to the next state. Our approach incorporates\npost-decision states and dual critics to reduce the problem's dimensionality\nand enhance the accuracy of value function estimation. Lot-sizing is a mixed\ninteger programming problem for which we exemplify such dynamics. The objective\nof lot-sizing is to optimize production, delivery fulfillment, and inventory\nlevels in uncertain demand and cost parameters. This paper evaluates the\nperformance of PDPPO across various environments and configurations. Notably,\nPDPPO with a dual critic architecture achieves nearly double the maximum reward\nof vanilla PPO in specific scenarios, requiring fewer episode iterations and\ndemonstrating faster and more consistent learning across different\ninitializations. On average, PDPPO outperforms PPO in environments with a\nstochastic component in the state transition. These results support the\nbenefits of using a post-decision state. Integrating this post-decision state\nin the value function approximation leads to more informed and efficient\nlearning in high-dimensional and stochastic environments.\n","authors":["Leonardo Kanashiro Felizardo","Edoardo Fadda","Paolo Brandimarte","Emilio Del-Moral-Hernandez","Mariá Cristina Vasconcelos Nascimento"],"pdf_url":"https://arxiv.org/pdf/2504.05150v2.pdf","comment":"12 pages, 4 figures. Accepted for presentation at IJCNN 2025"},{"id":"http://arxiv.org/abs/2504.08224v1","updated":"2025-04-11T03:12:53Z","published":"2025-04-11T03:12:53Z","title":"All Optical Echo State Network Reservoir Computing","summary":"  We propose an innovative design for an all-optical Echo State Network (ESN),\nan advanced type of reservoir computer known for its universal computational\ncapabilities. Our design enables fully optical implementation of arbitrary\nESNs, featuring complete flexibility in optical matrix multiplication and\nnonlinear activation. Leveraging the nonlinear characteristics of stimulated\nBrillouin scattering (SBS), the architecture efficiently realizes\nmeasurement-free operations crucial for reservoir computing. The approach\nsignificantly reduces computational overhead and energy consumption compared to\ntraditional software-based methods. Comprehensive simulations validate the\nsystem's memory capacity, nonlinear processing strength, and polynomial algebra\ncapabilities, showcasing performance comparable to software ESNs across key\nbenchmark tasks. Our design establishes a feasible, scalable, and universally\napplicable framework for optical reservoir computing, suitable for diverse\nmachine learning applications.\n","authors":["Ishwar S Kaushik","Peter J Ehlers","Daniel Soh"],"pdf_url":"https://arxiv.org/pdf/2504.08224v1.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2504.04120v2","updated":"2025-04-11T03:05:17Z","published":"2025-04-05T09:31:39Z","title":"Transformer representation learning is necessary for dynamic multi-modal\n  physiological data on small-cohort patients","summary":"  Postoperative delirium (POD), a severe neuropsychiatric complication\naffecting nearly 50% of high-risk surgical patients, is defined as an acute\ndisorder of attention and cognition, It remains significantly underdiagnosed in\nthe intensive care units (ICUs) due to subjective monitoring methods. Early and\naccurate diagnosis of POD is critical and achievable. Here, we propose a POD\nprediction framework comprising a Transformer representation model followed by\ntraditional machine learning algorithms. Our approaches utilizes multi-modal\nphysiological data, including amplitude-integrated electroencephalography\n(aEEG), vital signs, electrocardiographic monitor data as well as hemodynamic\nparameters. We curated the first multi-modal POD dataset encompassing two\npatient types and evaluated the various Transformer architectures for\nrepresentation learning. Empirical results indicate a consistent improvements\nof sensitivity and Youden index in patient TYPE I using Transformer\nrepresentations, particularly our fusion adaptation of Pathformer. By enabling\neffective delirium diagnosis from postoperative day 1 to 3, our extensive\nexperimental findings emphasize the potential of multi-modal physiological data\nand highlight the necessity of representation learning via multi-modal\nTransformer architecture in clinical diagnosis.\n","authors":["Bingxu Wang","Yapeng Wang","Kunzhi Cai","Yuqi Zhang","Zeyi Zhou","Yachong Guo","Wei Wang","Qing Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.04120v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08217v1","updated":"2025-04-11T02:50:38Z","published":"2025-04-11T02:50:38Z","title":"DrivAer Transformer: A high-precision and fast prediction method for\n  vehicle aerodynamic drag coefficient based on the DrivAerNet++ dataset","summary":"  At the current stage, deep learning-based methods have demonstrated excellent\ncapabilities in evaluating aerodynamic performance, significantly reducing the\ntime and cost required for traditional computational fluid dynamics (CFD)\nsimulations. However, when faced with the task of processing extremely complex\nthree-dimensional (3D) vehicle models, the lack of large-scale datasets and\ntraining resources, coupled with the inherent diversity and complexity of the\ngeometry of different vehicle models, means that the prediction accuracy and\nversatility of these networks are still not up to the level required for\ncurrent production. In view of the remarkable success of Transformer models in\nthe field of natural language processing and their strong potential in the\nfield of image processing, this study innovatively proposes a point cloud\nlearning framework called DrivAer Transformer (DAT). The DAT structure uses the\nDrivAerNet++ dataset, which contains high-fidelity CFD data of\nindustrial-standard 3D vehicle shapes. enabling accurate estimation of air drag\ndirectly from 3D meshes, thus avoiding the limitations of traditional methods\nsuch as 2D image rendering or signed distance fields (SDF). DAT enables fast\nand accurate drag prediction, driving the evolution of the aerodynamic\nevaluation process and laying the critical foundation for introducing a\ndata-driven approach to automotive design. The framework is expected to\naccelerate the vehicle design process and improve development efficiency.\n","authors":["Jiaqi He","Xiangwen Luo","Yiping Wang"],"pdf_url":"https://arxiv.org/pdf/2504.08217v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2504.08216v1","updated":"2025-04-11T02:47:46Z","published":"2025-04-11T02:47:46Z","title":"Local Distance-Preserving Node Embeddings and Their Performance on\n  Random Graphs","summary":"  Learning node representations is a fundamental problem in graph machine\nlearning. While existing embedding methods effectively preserve local\nsimilarity measures, they often fail to capture global functions like graph\ndistances. Inspired by Bourgain's seminal work on Hilbert space embeddings of\nmetric spaces (1985), we study the performance of local distance-preserving\nnode embeddings. Known as landmark-based algorithms, these embeddings\napproximate pairwise distances by computing shortest paths from a small subset\nof reference nodes (i.e., landmarks). Our main theoretical contribution shows\nthat random graphs, such as Erd\\H{o}s-R\\'enyi random graphs, require lower\ndimensions in landmark-based embeddings compared to worst-case graphs.\nEmpirically, we demonstrate that the GNN-based approximations for the distances\nto landmarks generalize well to larger networks, offering a scalable\nalternative for graph representation learning.\n","authors":["My Le","Luana Ruiz","Souvik Dhara"],"pdf_url":"https://arxiv.org/pdf/2504.08216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07158v2","updated":"2025-04-11T02:47:17Z","published":"2025-04-09T11:24:32Z","title":"Holistic Capability Preservation: Towards Compact Yet Comprehensive\n  Reasoning Models","summary":"  This technical report presents Ring-Lite-Distill, a lightweight reasoning\nmodel derived from our open-source Mixture-of-Experts (MoE) Large Language\nModels (LLMs) Ling-Lite. This study demonstrates that through meticulous\nhigh-quality data curation and ingenious training paradigms, the compact MoE\nmodel Ling-Lite can be further trained to achieve exceptional reasoning\ncapabilities, while maintaining its parameter-efficient architecture with only\n2.75 billion activated parameters, establishing an efficient lightweight\nreasoning architecture. In particular, in constructing this model, we have not\nmerely focused on enhancing advanced reasoning capabilities, exemplified by\nhigh-difficulty mathematical problem solving, but rather aimed to develop a\nreasoning model with more comprehensive competency coverage. Our approach\nensures coverage across reasoning tasks of varying difficulty levels while\npreserving generic capabilities, such as instruction following, tool use, and\nknowledge retention. We show that, Ring-Lite-Distill's reasoning ability\nreaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general\ncapabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The\nmodels are accessible at https://huggingface.co/inclusionAI\n","authors":[" Ling Team","Caizhi Tang","Chilin Fu","Chunwei Wu","Jia Guo","Jianwen Wang","Jingyu Hu","Liang Jiang","Meng Li","Peng Jiao","Pingping Liu","Shaomian Zheng","Shiwei Liang","Shuaicheng Li","Yalin Zhang","Yingting Wu","Yongkang Liu","Zhenyu Huang"],"pdf_url":"https://arxiv.org/pdf/2504.07158v2.pdf","comment":"Based on the further discussion of the working group, the current\n  version is deemed unsuitable for release. We are currently undertaking\n  further work that is expected to involve significant revisions, but this\n  process will require some additional time. We plan to proceed with the\n  release once these updates have been fully implemented"},{"id":"http://arxiv.org/abs/2504.08215v1","updated":"2025-04-11T02:46:39Z","published":"2025-04-11T02:46:39Z","title":"Deep Distributional Learning with Non-crossing Quantile Network","summary":"  In this paper, we introduce a non-crossing quantile (NQ) network for\nconditional distribution learning. By leveraging non-negative activation\nfunctions, the NQ network ensures that the learned distributions remain\nmonotonic, effectively addressing the issue of quantile crossing. Furthermore,\nthe NQ network-based deep distributional learning framework is highly\nadaptable, applicable to a wide range of applications, from classical\nnon-parametric quantile regression to more advanced tasks such as causal effect\nestimation and distributional reinforcement learning (RL). We also develop a\ncomprehensive theoretical foundation for the deep NQ estimator and its\napplication to distributional RL, providing an in-depth analysis that\ndemonstrates its effectiveness across these domains. Our experimental results\nfurther highlight the robustness and versatility of the NQ network.\n","authors":["Guohao Shen","Runpeng Dai","Guojun Wu","Shikai Luo","Chengchun Shi","Hongtu Zhu"],"pdf_url":"https://arxiv.org/pdf/2504.08215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.06514v2","updated":"2025-04-11T02:36:28Z","published":"2025-04-09T01:25:27Z","title":"Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?","summary":"  We find that the response length of reasoning LLMs, whether trained by\nreinforcement learning or supervised learning, drastically increases for\nill-posed questions with missing premises (MiP), ending up with redundant and\nineffective thinking. This newly introduced scenario exacerbates the general\noverthinking issue to a large extent, which we name as the MiP-Overthinking.\nSuch failures are against the ``test-time scaling law'' but have been widely\nobserved on multiple datasets we curated with MiP, indicating the harm of cheap\noverthinking and a lack of critical thinking. Surprisingly, LLMs not\nspecifically trained for reasoning exhibit much better performance on the MiP\nscenario, producing much shorter responses that quickly identify ill-posed\nqueries. This implies a critical flaw of the current training recipe for\nreasoning LLMs, which does not encourage efficient thinking adequately, leading\nto the abuse of thinking patterns. To further investigate the reasons behind\nsuch failures, we conduct fine-grained analyses of the reasoning length,\noverthinking patterns, and location of critical thinking on different types of\nLLMs. Moreover, our extended ablation study reveals that the overthinking is\ncontagious through the distillation of reasoning models' responses. These\nresults improve the understanding of overthinking and shed novel insights into\nmitigating the problem.\n","authors":["Chenrui Fan","Ming Li","Lichao Sun","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.06514v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08210v1","updated":"2025-04-11T02:27:30Z","published":"2025-04-11T02:27:30Z","title":"Optimizing Power Grid Topologies with Reinforcement Learning: A Survey\n  of Methods and Challenges","summary":"  Power grid operation is becoming increasingly complex due to the rising\nintegration of renewable energy sources and the need for more adaptive control\nstrategies. Reinforcement Learning (RL) has emerged as a promising approach to\npower network control (PNC), offering the potential to enhance decision-making\nin dynamic and uncertain environments. The Learning To Run a Power Network\n(L2RPN) competitions have played a key role in accelerating research by\nproviding standardized benchmarks and problem formulations, leading to rapid\nadvancements in RL-based methods. This survey provides a comprehensive and\nstructured overview of RL applications for power grid topology optimization,\ncategorizing existing techniques, highlighting key design choices, and\nidentifying gaps in current research. Additionally, we present a comparative\nnumerical study evaluating the impact of commonly applied RL-based methods,\noffering insights into their practical effectiveness. By consolidating existing\nresearch and outlining open challenges, this survey aims to provide a\nfoundation for future advancements in RL-driven power grid optimization.\n","authors":["Erica van der Sar","Alessandro Zocca","Sandjai Bhulai"],"pdf_url":"https://arxiv.org/pdf/2504.08210v1.pdf","comment":"60 pages, 26 figures, preprint"},{"id":"http://arxiv.org/abs/2504.08207v1","updated":"2025-04-11T02:19:01Z","published":"2025-04-11T02:19:01Z","title":"DRAFT-ing Architectural Design Decisions using LLMs","summary":"  Architectural Knowledge Management (AKM) is crucial for software development\nbut remains challenging due to the lack of standardization and high manual\neffort. Architecture Decision Records (ADRs) provide a structured approach to\ncapture Architecture Design Decisions (ADDs), but their adoption is limited due\nto the manual effort involved and insufficient tool support. Our previous work\nhas shown that Large Language Models (LLMs) can assist in generating ADDs.\nHowever, simply prompting the LLM does not produce quality ADDs. Moreover,\nusing third-party LLMs raises privacy concerns, while self-hosting them poses\nresource challenges.\n  To this end, we experimented with different approaches like few-shot,\nretrieval-augmented generation (RAG) and fine-tuning to enhance LLM's ability\nto generate ADDs. Our results show that both techniques improve effectiveness.\nBuilding on this, we propose Domain Specific Retreival Augumented Few Shot Fine\nTuninng, DRAFT, which combines the strengths of all these three approaches for\nmore effective ADD generation. DRAFT operates in two phases: an offline phase\nthat fine-tunes an LLM on generating ADDs augmented with retrieved examples and\nan online phase that generates ADDs by leveraging retrieved ADRs and the\nfine-tuned model.\n  We evaluated DRAFT against existing approaches on a dataset of 4,911 ADRs and\nvarious LLMs and analyzed them using automated metrics and human evaluations.\nResults show DRAFT outperforms all other approaches in effectiveness while\nmaintaining efficiency. Our findings indicate that DRAFT can aid architects in\ndrafting ADDs while addressing privacy and resource constraints.\n","authors":["Rudra Dhar","Adyansh Kakran","Amey Karan","Karthik Vaidhyanathan","Vasudeva Varma"],"pdf_url":"https://arxiv.org/pdf/2504.08207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08201v1","updated":"2025-04-11T02:06:20Z","published":"2025-04-11T02:06:20Z","title":"Neural Encoding and Decoding at Scale","summary":"  Recent work has demonstrated that large-scale, multi-animal models are\npowerful tools for characterizing the relationship between neural activity and\nbehavior. Current large-scale approaches, however, focus exclusively on either\npredicting neural activity from behavior (encoding) or predicting behavior from\nneural activity (decoding), limiting their ability to capture the bidirectional\nrelationship between neural activity and behavior. To bridge this gap, we\nintroduce a multimodal, multi-task model that enables simultaneous Neural\nEncoding and Decoding at Scale (NEDS). Central to our approach is a novel\nmulti-task-masking strategy, which alternates between neural, behavioral,\nwithin-modality, and cross-modality masking. We pretrain our method on the\nInternational Brain Laboratory (IBL) repeated site dataset, which includes\nrecordings from 83 animals performing the same visual decision-making task. In\ncomparison to other large-scale models, we demonstrate that NEDS achieves\nstate-of-the-art performance for both encoding and decoding when pretrained on\nmulti-animal data and then fine-tuned on new animals. Surprisingly, NEDS's\nlearned embeddings exhibit emergent properties: even without explicit training,\nthey are highly predictive of the brain regions in each recording. Altogether,\nour approach is a step towards a foundation model of the brain that enables\nseamless translation between neural activity and behavior.\n","authors":["Yizi Zhang","Yanchen Wang","Mehdi Azabou","Alexandre Andre","Zixuan Wang","Hanrui Lyu","The International Brain Laboratory","Eva Dyer","Liam Paninski","Cole Hurwitz"],"pdf_url":"https://arxiv.org/pdf/2504.08201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08200v1","updated":"2025-04-11T02:05:51Z","published":"2025-04-11T02:05:51Z","title":"Influential Bandits: Pulling an Arm May Change the Environment","summary":"  While classical formulations of multi-armed bandit problems assume that each\narm's reward is independent and stationary, real-world applications often\ninvolve non-stationary environments and interdependencies between arms. In\nparticular, selecting one arm may influence the future rewards of other arms, a\nscenario not adequately captured by existing models such as rotting bandits or\nrestless bandits. To address this limitation, we propose the influential bandit\nproblem, which models inter-arm interactions through an unknown, symmetric,\npositive semi-definite interaction matrix that governs the dynamics of arm\nlosses. We formally define this problem and establish two regret lower bounds,\nincluding a superlinear $\\Omega(T^2 / \\log^2 T)$ bound for the standard UCB\nalgorithm and an algorithm-independent $\\Omega(T)$ bound, which highlight the\ninherent difficulty of the setting. We then introduce a new algorithm based on\na lower confidence bound (LCB) estimator tailored to the structure of the loss\ndynamics. Under mild assumptions, our algorithm achieves a regret of $O(KT \\log\nT)$, which is nearly optimal in terms of its dependence on the time horizon.\nThe algorithm is simple to implement and computationally efficient. Empirical\nevaluations on both synthetic and real-world datasets demonstrate the presence\nof inter-arm influence and confirm the superior performance of our method\ncompared to conventional bandit algorithms.\n","authors":["Ryoma Sato","Shinji Ito"],"pdf_url":"https://arxiv.org/pdf/2504.08200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08198v1","updated":"2025-04-11T02:01:38Z","published":"2025-04-11T02:01:38Z","title":"The More is not the Merrier: Investigating the Effect of Client Size on\n  Federated Learning","summary":"  Federated Learning (FL) has been introduced as a way to keep data local to\nclients while training a shared machine learning model, as clients train on\ntheir local data and send trained models to a central aggregator. It is\nexpected that FL will have a huge implication on Mobile Edge Computing, the\nInternet of Things, and Cross-Silo FL. In this paper, we focus on the widely\nused FedAvg algorithm to explore the effect of the number of clients in FL. We\nfind a significant deterioration of learning accuracy for FedAvg as the number\nof clients increases. To address this issue for a general application, we\npropose a method called Knowledgeable Client Insertion (KCI) that introduces a\nvery small number of knowledgeable clients to the MEC setting. These\nknowledgeable clients are expected to have accumulated a large set of data\nsamples to help with training. With the help of KCI, the learning accuracy of\nFL increases much faster even with a normal FedAvg aggregation technique. We\nexpect this approach to be able to provide great privacy protection for clients\nagainst security attacks such as model inversion attacks. Our code is available\nat https://github.com/Eleanor-W/KCI_for_FL.\n","authors":["Eleanor Wallach","Sage Siler","Jing Deng"],"pdf_url":"https://arxiv.org/pdf/2504.08198v1.pdf","comment":"6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.08797v3","updated":"2025-04-11T01:47:22Z","published":"2024-07-11T18:13:38Z","title":"Deep Inverse Design for High-Level Synthesis","summary":"  High-level synthesis (HLS) has significantly advanced the automation of\ndigital circuits design, yet the need for expertise and time in pragma tuning\nremains challenging. Existing solutions for the design space exploration (DSE)\nadopt either heuristic methods, lacking essential information for further\noptimization potential, or predictive models, missing sufficient generalization\ndue to the time-consuming nature of HLS and the exponential growth of the\ndesign space. To address these challenges, we propose Deep Inverse Design for\nHLS (DID4HLS), a novel approach that integrates graph neural networks and\ngenerative models. DID4HLS iteratively optimizes hardware designs aimed at\ncompute-intensive algorithms by learning conditional distributions of design\nfeatures from post-HLS data. Compared to four state-of-the-art DSE baselines,\nour method achieved an average improvement of 42.8% on average distance to\nreference set (ADRS) compared to the best-performing baselines across six\nbenchmarks, while demonstrating high robustness and efficiency. The code is\navailable at https://github.com/PingChang818/DID4HLS.\n","authors":["Ping Chang","Tosiron Adegbija","Yuchao Liao","Claudio Talarico","Ao Li","Janet Roveda"],"pdf_url":"https://arxiv.org/pdf/2407.08797v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07481v2","updated":"2025-04-11T01:42:47Z","published":"2025-04-10T06:19:01Z","title":"A Mechanism-Learning Deeply Coupled Model for Remote Sensing Retrieval\n  of Global Land Surface Temperature","summary":"  Land surface temperature (LST) retrieval from remote sensing data is pivotal\nfor analyzing climate processes and surface energy budgets. However, LST\nretrieval is an ill-posed inverse problem, which becomes particularly severe\nwhen only a single band is available. In this paper, we propose a deeply\ncoupled framework integrating mechanistic modeling and machine learning to\nenhance the accuracy and generalizability of single-channel LST retrieval.\nTraining samples are generated using a physically-based radiative transfer\nmodel and a global collection of 5810 atmospheric profiles. A physics-informed\nmachine learning framework is proposed to systematically incorporate the first\nprinciples from classical physical inversion models into the learning workflow,\nwith optimization constrained by radiative transfer equations. Global\nvalidation demonstrated a 30% reduction in root-mean-square error versus\nstandalone methods. Under extreme humidity, the mean absolute error decreased\nfrom 4.87 K to 2.29 K (53% improvement). Continental-scale tests across five\ncontinents confirmed the superior generalizability of this model.\n","authors":["Tian Xie","Menghui Jiang","Huanfeng Shen","Huifang Li","Chao Zeng","Xiaobin Guan","Jun Ma","Guanhao Zhang","Liangpei Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.07481v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06736v5","updated":"2025-04-11T01:35:36Z","published":"2024-11-11T06:04:53Z","title":"MrSteve: Instruction-Following Agents in Minecraft with What-Where-When\n  Memory","summary":"  Significant advances have been made in developing general-purpose embodied AI\nin environments like Minecraft through the adoption of LLM-augmented\nhierarchical approaches. While these approaches, which combine high-level\nplanners with low-level controllers, show promise, low-level controllers\nfrequently become performance bottlenecks due to repeated failures. In this\npaper, we argue that the primary cause of failure in many low-level controllers\nis the absence of an episodic memory system. To address this, we introduce\nMrSteve (Memory Recall Steve), a novel low-level controller equipped with Place\nEvent Memory (PEM), a form of episodic memory that captures what, where, and\nwhen information from episodes. This directly addresses the main limitation of\nthe popular low-level controller, Steve-1. Unlike previous models that rely on\nshort-term memory, PEM organizes spatial and event-based data, enabling\nefficient recall and navigation in long-horizon tasks. Additionally, we propose\nan Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing\nagents to alternate between exploration and task-solving based on recalled\nevents. Our approach significantly improves task-solving and exploration\nefficiency compared to existing methods. We will release our code and demos on\nthe project page: https://sites.google.com/view/mr-steve.\n","authors":["Junyeong Park","Junmo Cho","Sungjin Ahn"],"pdf_url":"https://arxiv.org/pdf/2411.06736v5.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2409.05657v3","updated":"2025-04-11T01:34:44Z","published":"2024-09-09T14:23:19Z","title":"Adversarial Attacks on Data Attribution","summary":"  Data attribution aims to quantify the contribution of individual training\ndata points to the outputs of an AI model, which has been used to measure the\nvalue of training data and compensate data providers. Given the impact on\nfinancial decisions and compensation mechanisms, a critical question arises\nconcerning the adversarial robustness of data attribution methods. However,\nthere has been little to no systematic research addressing this issue. In this\nwork, we aim to bridge this gap by detailing a threat model with clear\nassumptions about the adversary's goal and capabilities and proposing\nprincipled adversarial attack methods on data attribution. We present two\nmethods, Shadow Attack and Outlier Attack, which generate manipulated datasets\nto inflate the compensation adversarially. The Shadow Attack leverages\nknowledge about the data distribution in the AI applications, and derives\nadversarial perturbations through \"shadow training\", a technique commonly used\nin membership inference attacks. In contrast, the Outlier Attack does not\nassume any knowledge about the data distribution and relies solely on black-box\nqueries to the target model's predictions. It exploits an inductive bias\npresent in many data attribution methods - outlier data points are more likely\nto be influential - and employs adversarial examples to generate manipulated\ndatasets. Empirically, in image classification and text generation tasks, the\nShadow Attack can inflate the data-attribution-based compensation by at least\n200%, while the Outlier Attack achieves compensation inflation ranging from\n185% to as much as 643%. Our implementation is ready at\nhttps://github.com/TRAIS-Lab/adversarial-attack-data-attribution.\n","authors":["Xinhe Wang","Pingbang Hu","Junwei Deng","Jiaqi W. Ma"],"pdf_url":"https://arxiv.org/pdf/2409.05657v3.pdf","comment":"Accepted at the 13th International Conference on Learning\n  Representations (ICLR 2025)"},{"id":"http://arxiv.org/abs/2504.08192v1","updated":"2025-04-11T01:24:03Z","published":"2025-04-11T01:24:03Z","title":"SAEs $\\textit{Can}$ Improve Unlearning: Dynamic Sparse Autoencoder\n  Guardrails for Precision Unlearning in LLMs","summary":"  Machine unlearning is a promising approach to improve LLM safety by removing\nunwanted knowledge from the model. However, prevailing gradient-based\nunlearning methods suffer from issues such as high computational costs,\nhyperparameter instability, poor sequential unlearning capability,\nvulnerability to relearning attacks, low data efficiency, and lack of\ninterpretability. While Sparse Autoencoders are well-suited to improve these\naspects by enabling targeted activation-based unlearning, prior approaches\nunderperform gradient-based methods. This work demonstrates that, contrary to\nthese earlier findings, SAEs can significantly improve unlearning when employed\ndynamically. We introduce $\\textbf{Dynamic DAE Guardrails}$ (DSG), a novel\nmethod for precision unlearning that leverages principled feature selection and\na dynamic classifier. Our experiments show DSG substantially outperforms\nleading unlearning methods, achieving superior forget-utility trade-offs. DSG\naddresses key drawbacks of gradient-based approaches for unlearning -- offering\nenhanced computational efficiency and stability, robust performance in\nsequential unlearning, stronger resistance to relearning attacks, better data\nefficiency including zero-shot settings, and more interpretable unlearning.\n","authors":["Aashiq Muhamed","Jacopo Bonato","Mona Diab","Virginia Smith"],"pdf_url":"https://arxiv.org/pdf/2504.08192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08183v1","updated":"2025-04-11T00:53:53Z","published":"2025-04-11T00:53:53Z","title":"Detecting Credit Card Fraud via Heterogeneous Graph Neural Networks with\n  Graph Attention","summary":"  This study proposes a credit card fraud detection method based on\nHeterogeneous Graph Neural Network (HGNN) to address fraud in complex\ntransaction networks. Unlike traditional machine learning methods that rely\nsolely on numerical features of transaction records, this approach constructs\nheterogeneous transaction graphs. These graphs incorporate multiple node types,\nincluding users, merchants, and transactions. By leveraging graph neural\nnetworks, the model captures higher-order transaction relationships. A Graph\nAttention Mechanism is employed to dynamically assign weights to different\ntransaction relationships. Additionally, a Temporal Decay Mechanism is\nintegrated to enhance the model's sensitivity to time-related fraud patterns.\nTo address the scarcity of fraudulent transaction samples, this study applies\nSMOTE oversampling and Cost-sensitive Learning. These techniques strengthen the\nmodel's ability to identify fraudulent transactions. Experimental results\ndemonstrate that the proposed method outperforms existing GNN models, including\nGCN, GAT, and GraphSAGE, on the IEEE-CIS Fraud Detection dataset. The model\nachieves notable improvements in both accuracy and OC-ROC. Future research may\nexplore the integration of dynamic graph neural networks and reinforcement\nlearning. Such advancements could enhance the real-time adaptability of fraud\ndetection systems and provide more intelligent solutions for financial risk\ncontrol.\n","authors":["Qiuwu Sha","Tengda Tang","Xinyu Du","Jie Liu","Yixian Wang","Yuan Sheng"],"pdf_url":"https://arxiv.org/pdf/2504.08183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08182v1","updated":"2025-04-11T00:46:57Z","published":"2025-04-11T00:46:57Z","title":"Particle Hit Clustering and Identification Using Point Set Transformers\n  in Liquid Argon Time Projection Chambers","summary":"  Liquid argon time projection chambers are often used in neutrino physics and\ndark-matter searches because of their high spatial resolution. The images\ngenerated by these detectors are extremely sparse, as the energy values\ndetected by most of the detector are equal to 0, meaning that despite their\nhigh resolution, most of the detector is unused in a particular interaction.\nInstead of representing all of the empty detections, the interaction is usually\nstored as a sparse matrix, a list of detection locations paired with their\nenergy values. Traditional machine learning methods that have been applied to\nparticle reconstruction such as convolutional neural networks (CNNs), however,\ncannot operate over data stored in this way and therefore must have the matrix\nfully instantiated as a dense matrix. Operating on dense matrices requires a\nlot of memory and computation time, in contrast to directly operating on the\nsparse matrix. We propose a machine learning model using a point set neural\nnetwork that operates over a sparse matrix, greatly improving both processing\nspeed and accuracy over methods that instantiate the dense matrix, as well as\nover other methods that operate over sparse matrices. Compared to competing\nstate-of-the-art methods, our method improves classification performance by\n14%, segmentation performance by more than 22%, while taking 80% less time and\nusing 66% less memory. Compared to state-of-the-art CNN methods, our method\nimproves classification performance by more than 86%, segmentation performance\nby more than 71%, while reducing runtime by 91% and reducing memory usage by\n61%.\n","authors":["Edgar E. Robles","Alejando Yankelevich","Wenjie Wu","Jianming Bian","Pierre Baldi"],"pdf_url":"https://arxiv.org/pdf/2504.08182v1.pdf","comment":"12 pages, 5 figures, 2 tables, for submission at JINST"},{"id":"http://arxiv.org/abs/2405.08190v2","updated":"2025-04-11T00:42:39Z","published":"2024-05-13T21:12:31Z","title":"Barren plateaus are amplified by the dimension of qudits","summary":"  Variational Quantum Algorithms (VQAs) have emerged as pivotal strategies for\nattaining quantum advantage in diverse scientific and technological domains,\nnotably within Quantum Neural Networks. However, despite their potential, VQAs\nencounter significant obstacles, chief among them being the vanishing gradient\nproblem, commonly referred to as barren plateaus. In this article, through\nmeticulous analysis, we demonstrate that existing literature implicitly\nsuggests the intrinsic influence of qudit dimensionality on barren plateaus. To\ninstantiate these findings, we present numerical results that exemplify the\nimpact of qudit dimensionality on barren plateaus. Therefore, despite the\nproposition of various error mitigation techniques, our results call for\nfurther scrutiny about their efficacy in the context of VQAs with qudits.\n","authors":["Lucas Friedrich","Tiago de Souza Farias","Jonas Maziero"],"pdf_url":"https://arxiv.org/pdf/2405.08190v2.pdf","comment":"Accepted for publication in Quantum Machine Intelligence"},{"id":"http://arxiv.org/abs/2503.01869v2","updated":"2025-04-11T00:37:11Z","published":"2025-02-25T21:50:46Z","title":"From Small to Large Language Models: Revisiting the Federalist Papers","summary":"  For a long time, the authorship of the Federalist Papers had been a subject\nof inquiry and debate, not only by linguists and historians but also by\nstatisticians. In what was arguably the first Bayesian case study, Mosteller\nand Wallace (1963) provided the first statistical evidence for attributing all\ndisputed papers to Madison. Our paper revisits this historical dataset but from\na lens of modern language models, both small and large. We review some of the\nmore popular Large Language Model (LLM) tools and examine them from a\nstatistical point of view in the context of text classification. We investigate\nwhether, without any attempt to fine-tune, the general embedding constructs can\nbe useful for stylometry and attribution. We explain differences between\nvarious word/phrase embeddings and discuss how to aggregate them in a document.\nContrary to our expectations, we exemplify that dimension expansion with word\nembeddings may not always be beneficial for attribution relative to dimension\nreduction with topic embeddings. Our experiments demonstrate that default LLM\nembeddings (even after manual fine-tuning) may not consistently improve\nauthorship attribution accuracy. Instead, Bayesian analysis with topic\nembeddings trained on ``function words\" yields superior out-of-sample\nclassification performance. This suggests that traditional (small) statistical\nlanguage models, with their interpretability and solid theoretical foundation,\ncan offer significant advantages in authorship attribution tasks. The code used\nin this analysis is available at github.com/sowonjeong/slm-to-llm\n","authors":["So Won Jeong","Veronika Ročková"],"pdf_url":"https://arxiv.org/pdf/2503.01869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08178v1","updated":"2025-04-11T00:20:37Z","published":"2025-04-11T00:20:37Z","title":"A Piecewise Lyapunov Analysis of sub--quadratic SGD: Applications to\n  Robust and Quantile Regression","summary":"  Motivated by robust and quantile regression problems, {we investigate the\nstochastic gradient descent (SGD) algorithm} for minimizing an objective\nfunction $f$ that is locally strongly convex with a sub--quadratic tail. This\nsetting covers many widely used online statistical methods. We introduce a\nnovel piecewise Lyapunov function that enables us to handle functions $f$ with\nonly first-order differentiability, which includes a wide range of popular loss\nfunctions such as Huber loss. Leveraging our proposed Lyapunov function, we\nderive finite-time moment bounds under general diminishing stepsizes, as well\nas constant stepsizes. We further establish the weak convergence, central limit\ntheorem and bias characterization under constant stepsize, providing the first\ngeometrical convergence result for sub--quadratic SGD. Our results have wide\napplications, especially in online statistical methods. In particular, we\ndiscuss two applications of our results. 1) Online robust regression: We\nconsider a corrupted linear model with sub--exponential covariates and\nheavy--tailed noise. Our analysis provides convergence rates comparable to\nthose for corrupted models with Gaussian covariates and noise. 2) Online\nquantile regression: Importantly, our results relax the common assumption in\nprior work that the conditional density is continuous and provide a more\nfine-grained analysis for the moment bounds.\n","authors":["Yixuan Zhang"," Dongyan"," Huo","Yudong Chen","Qiaomin Xie"],"pdf_url":"https://arxiv.org/pdf/2504.08178v1.pdf","comment":"ACM SIGMETRICS 2025. 40 pages, 12 figures"},{"id":"http://arxiv.org/abs/2502.07202v2","updated":"2025-04-11T00:14:32Z","published":"2025-02-11T02:51:42Z","title":"Monte Carlo Tree Diffusion for System 2 Planning","summary":"  Diffusion models have recently emerged as a powerful tool for planning.\nHowever, unlike Monte Carlo Tree Search (MCTS)-whose performance naturally\nimproves with additional test-time computation (TTC), standard diffusion-based\nplanners offer only limited avenues for TTC scalability. In this paper, we\nintroduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates\nthe generative strength of diffusion models with the adaptive search\ncapabilities of MCTS. Our method reconceptualizes denoising as a\ntree-structured process, allowing partially denoised plans to be iteratively\nevaluated, pruned, and refined. By selectively expanding promising trajectories\nwhile retaining the flexibility to revisit and improve suboptimal branches,\nMCTD achieves the benefits of MCTS such as controlling exploration-exploitation\ntrade-offs within the diffusion framework. Empirical results on challenging\nlong-horizon tasks show that MCTD outperforms diffusion baselines, yielding\nhigher-quality solutions as TTC increases.\n","authors":["Jaesik Yoon","Hyeonseo Cho","Doojin Baek","Yoshua Bengio","Sungjin Ahn"],"pdf_url":"https://arxiv.org/pdf/2502.07202v2.pdf","comment":"20 pages, 7 figures"}],"Computation and Language":[{"id":"http://arxiv.org/abs/2504.08734v1","updated":"2025-04-11T17:59:53Z","published":"2025-04-11T17:59:53Z","title":"Towards an Understanding of Context Utilization in Code Intelligence","summary":"  Code intelligence is an emerging domain in software engineering, aiming to\nimprove the effectiveness and efficiency of various code-related tasks. Recent\nresearch suggests that incorporating contextual information beyond the basic\noriginal task inputs (i.e., source code) can substantially enhance model\nperformance. Such contextual signals may be obtained directly or indirectly\nfrom sources such as API documentation or intermediate representations like\nabstract syntax trees can significantly improve the effectiveness of code\nintelligence. Despite growing academic interest, there is a lack of systematic\nanalysis of context in code intelligence. To address this gap, we conduct an\nextensive literature review of 146 relevant studies published between September\n2007 and August 2024. Our investigation yields four main contributions. (1) A\nquantitative analysis of the research landscape, including publication trends,\nvenues, and the explored domains; (2) A novel taxonomy of context types used in\ncode intelligence; (3) A task-oriented analysis investigating context\nintegration strategies across diverse code intelligence tasks; (4) A critical\nevaluation of evaluation methodologies for context-aware methods. Based on\nthese findings, we identify fundamental challenges in context utilization in\ncurrent code intelligence systems and propose a research roadmap that outlines\nkey opportunities for future research.\n","authors":["Yanlin Wang","Kefeng Duan","Dewu Zheng","Ensheng Shi","Fengji Zhang","Yanli Wang","Jiachi Chen","Xilin Liu","Yuchi Ma","Hongyu Zhang","Qianxiang Wang","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2504.08734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08725v1","updated":"2025-04-11T17:50:08Z","published":"2025-04-11T17:50:08Z","title":"DocAgent: A Multi-Agent System for Automated Code Documentation\n  Generation","summary":"  High-quality code documentation is crucial for software development\nespecially in the era of AI. However, generating it automatically using Large\nLanguage Models (LLMs) remains challenging, as existing approaches often\nproduce incomplete, unhelpful, or factually incorrect outputs. We introduce\nDocAgent, a novel multi-agent collaborative system using topological code\nprocessing for incremental context building. Specialized agents (Reader,\nSearcher, Writer, Verifier, Orchestrator) then collaboratively generate\ndocumentation. We also propose a multi-faceted evaluation framework assessing\nCompleteness, Helpfulness, and Truthfulness. Comprehensive experiments show\nDocAgent significantly outperforms baselines consistently. Our ablation study\nconfirms the vital role of the topological processing order. DocAgent offers a\nrobust approach for reliable code documentation generation in complex and\nproprietary repositories.\n","authors":["Dayu Yang","Antoine Simoulin","Xin Qian","Xiaoyi Liu","Yuwei Cao","Zhaopu Teng","Grey Yang"],"pdf_url":"https://arxiv.org/pdf/2504.08725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08719v1","updated":"2025-04-11T17:33:32Z","published":"2025-04-11T17:33:32Z","title":"SWAN-GPT: An Efficient and Scalable Approach for Long-Context Language\n  Modeling","summary":"  We present a decoder-only Transformer architecture that robustly generalizes\nto sequence lengths substantially longer than those seen during training. Our\nmodel, SWAN-GPT, interleaves layers without positional encodings (NoPE) and\nsliding-window attention layers equipped with rotary positional encodings\n(SWA-RoPE). Experiments demonstrate strong performance on sequence lengths\nsignificantly longer than the training length without the need for additional\nlong-context training. This robust length extrapolation is achieved through our\nnovel architecture, enhanced by a straightforward dynamic scaling of attention\nscores during inference. In addition, SWAN-GPT is more computationally\nefficient than standard GPT architectures, resulting in cheaper training and\nhigher throughput. Further, we demonstrate that existing pre-trained\ndecoder-only models can be efficiently converted to the SWAN architecture with\nminimal continued training, enabling longer contexts. Overall, our work\npresents an effective approach for scaling language models to longer contexts\nin a robust and efficient manner.\n","authors":["Krishna C. Puvvada","Faisal Ladhak","Santiago Akle Serrano","Cheng-Ping Hsieh","Shantanu Acharya","Somshubra Majumdar","Fei Jia","Samuel Kriman","Simeng Sun","Dima Rekesh","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2504.08719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08716v1","updated":"2025-04-11T17:29:35Z","published":"2025-04-11T17:29:35Z","title":"ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on\n  Transformer Encoder Models Performance","summary":"  Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce\narchitectural advancements aimed at improving efficiency and performance.\nAlthough the authors of ModernBERT report improved performance over DeBERTaV3\non several benchmarks, the lack of disclosed training data and the absence of\ncomparisons using a shared dataset make it difficult to determine whether these\ngains are due to architectural improvements or differences in training data. In\nthis work, we conduct a controlled study by pretraining ModernBERT on the same\ndataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of\nmodel design. Our results show that the previous model generation remains\nsuperior in sample efficiency and overall benchmark performance, with\nModernBERT's primary advantage being faster training and inference speed.\nHowever, the new proposed model still provides meaningful architectural\nimprovements compared to earlier models such as BERT and RoBERTa. Additionally,\nwe observe that high-quality pre-training data accelerates convergence but does\nnot significantly improve final performance, suggesting potential benchmark\nsaturation. These findings show the importance of disentangling pretraining\ndata from architectural innovations when evaluating transformer models.\n","authors":["Wissam Antoun","Benoît Sagot","Djamé Seddah"],"pdf_url":"https://arxiv.org/pdf/2504.08716v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2504.08714v1","updated":"2025-04-11T17:24:58Z","published":"2025-04-11T17:24:58Z","title":"Generating Fine Details of Entity Interactions","summary":"  Images not only depict objects but also encapsulate rich interactions between\nthem. However, generating faithful and high-fidelity images involving multiple\nentities interacting with each other, is a long-standing challenge. While\npre-trained text-to-image models are trained on large-scale datasets to follow\ndiverse text instructions, they struggle to generate accurate interactions,\nlikely due to the scarcity of training data for uncommon object interactions.\nThis paper introduces InterActing, an interaction-focused dataset with 1000\nfine-grained prompts covering three key scenarios: (1) functional and\naction-based interactions, (2) compositional spatial relationships, and (3)\nmulti-subject interactions. To address interaction generation challenges, we\npropose a decomposition-augmented refinement procedure. Our approach,\nDetailScribe, built on Stable Diffusion 3.5, leverages LLMs to decompose\ninteractions into finer-grained concepts, uses a VLM to critique generated\nimages, and applies targeted interventions within the diffusion process in\nrefinement. Automatic and human evaluations show significantly improved image\nquality, demonstrating the potential of enhanced inference strategies. Our\ndataset and code are available at https://concepts-ai.com/p/detailscribe/ to\nfacilitate future exploration of interaction-rich image generation.\n","authors":["Xinyi Gu","Jiayuan Mao"],"pdf_url":"https://arxiv.org/pdf/2504.08714v1.pdf","comment":"Project Page: https://concepts-ai.com/p/detailscribe/"},{"id":"http://arxiv.org/abs/2504.08697v1","updated":"2025-04-11T17:04:51Z","published":"2025-04-11T17:04:51Z","title":"Large Language Models as Span Annotators","summary":"  For high-quality texts, single-score metrics seldom provide actionable\nfeedback. In contrast, span annotation - pointing out issues in the text by\nannotating their spans - can guide improvements and provide insights. Until\nrecently, span annotation was limited to human annotators or fine-tuned encoder\nmodels. In this study, we automate span annotation with large language models\n(LLMs). We compare expert or skilled crowdworker annotators with open and\nproprietary LLMs on three tasks: data-to-text generation evaluation, machine\ntranslation evaluation, and propaganda detection in human-written texts. In our\nexperiments, we show that LLMs as span annotators are straightforward to\nimplement and notably more cost-efficient than human annotators. The LLMs\nachieve moderate agreement with skilled human annotators, in some scenarios\ncomparable to the average agreement among the annotators themselves.\nQualitative analysis shows that reasoning models outperform their\ninstruction-tuned counterparts and provide more valid explanations for\nannotations. We release the dataset of more than 40k model and human\nannotations for further research.\n","authors":["Zdeněk Kasner","Vilém Zouhar","Patrícia Schmidtová","Ivan Kartáč","Kristýna Onderková","Ondřej Plátek","Dimitra Gkatzia","Saad Mahamood","Ondřej Dušek","Simone Balloccu"],"pdf_url":"https://arxiv.org/pdf/2504.08697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05058v3","updated":"2025-04-11T17:03:04Z","published":"2025-04-07T13:29:02Z","title":"Not All Data Are Unlearned Equally","summary":"  Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account.\n","authors":["Aravind Krishnan","Siva Reddy","Marius Mosbach"],"pdf_url":"https://arxiv.org/pdf/2504.05058v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08694v1","updated":"2025-04-11T17:02:40Z","published":"2025-04-11T17:02:40Z","title":"TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for\n  Spatiotemporal-Aware Travel Planning","summary":"  Large language models (LLMs) have shown promise in automating travel\nplanning, yet they often fall short in addressing nuanced spatiotemporal\nrationality. While existing benchmarks focus on basic plan validity, they\nneglect critical aspects such as route efficiency, POI appeal, and real-time\nadaptability. This paper introduces TP-RAG, the first benchmark tailored for\nretrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes\n2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784\nhigh-quality travel trajectory references sourced from online tourist\ndocuments, enabling dynamic and context-aware planning. Through extensive\nexperiments, we reveal that integrating reference trajectories significantly\nimproves spatial efficiency and POI rationality of the travel plan, while\nchallenges persist in universality and robustness due to conflicting references\nand noisy data. To address these issues, we propose EvoRAG, an evolutionary\nframework that potently synergizes diverse retrieved trajectories with LLMs'\nintrinsic reasoning. EvoRAG achieves state-of-the-art performance, improving\nspatiotemporal compliance and reducing commonsense violation compared to\nground-up and retrieval-augmented baselines. Our work underscores the potential\nof hybridizing Web knowledge with LLM-driven optimization, paving the way for\nmore reliable and adaptive travel planning agents.\n","authors":["Hang Ni","Fan Liu","Xinyu Ma","Lixin Su","Shuaiqiang Wang","Dawei Yin","Hui Xiong","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2504.08694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08690v1","updated":"2025-04-11T16:57:36Z","published":"2025-04-11T16:57:36Z","title":"Fast-Slow-Thinking: Complex Task Solving with Large Language Models","summary":"  Nowadays, Large Language Models (LLMs) have been gradually employed to solve\ncomplex tasks. To face the challenge, task decomposition has become an\neffective way, which proposes to divide a complex task into multiple simpler\nsubtasks and then solve them separately so that the difficulty of the original\ntask can be reduced. However, the performance of existing task decomposition\nmethods can be suboptimal when the task contains overly complex logic and\nconstraints. In this situation, the solution generated by LLMs may deviate from\nthe original purpose of the task, or contain redundant or even erroneous\ncontent. Therefore, inspired by the fact that humans possess two thinking\nsystems including fast thinking and slow thinking, this paper introduces a new\ntask decomposition method termed ``Fast-Slow-Thinking'' (FST), which stimulates\nLLMs to solve tasks through the cooperation of Fast Thinking (FT) and Slow\nThinking (ST) steps. Here FT focuses more on the general and concise aspect of\nthe task, and ST focuses more on the details of the task. In FT, LLMs are\nprompted to remove the constraints of the original task, therefore simplifying\nit to a general and concise one. In ST, we recall the constraints removed in\nFT, so that LLMs can improve the answer generated in FT to meet the\nrequirements of the original task. Therefore, our FST method enables LLMs to\nconsider a complex problem via a human-like cognition process from coarse to\nfine, the effectiveness of which has been well demonstrated by the experiments\non three types of tasks.\n","authors":["Yiliu Sun","Yanfang Zhang","Zicheng Zhao","Sheng Wan","Dacheng Tao","Chen Gong"],"pdf_url":"https://arxiv.org/pdf/2504.08690v1.pdf","comment":"37 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.08672v1","updated":"2025-04-11T16:26:23Z","published":"2025-04-11T16:26:23Z","title":"Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning","summary":"  Advancing LLM reasoning skills has captivated wide interest. However, current\npost-training techniques rely heavily on supervisory signals, such as outcome\nsupervision or auxiliary reward models, which face the problem of scalability\nand high annotation costs. This motivates us to enhance LLM reasoning without\nthe need for external supervision. We introduce a generalizable and purely\nunsupervised self-training framework, named Genius. Without external auxiliary,\nGenius requires to seek the optimal response sequence in a stepwise manner and\noptimize the LLM. To explore the potential steps and exploit the optimal ones,\nGenius introduces a stepwise foresight re-sampling strategy to sample and\nestimate the step value by simulating future outcomes. Further, we recognize\nthat the unsupervised setting inevitably induces the intrinsic noise and\nuncertainty. To provide a robust optimization, we propose an\nadvantage-calibrated optimization (ACO) loss function to mitigate estimation\ninconsistencies. Combining these techniques together, Genius provides an\nadvanced initial step towards self-improve LLM reasoning with general queries\nand without supervision, revolutionizing reasoning scaling laws given the vast\navailability of general queries. The code will be released at\nhttps://github.com/xufangzhi/Genius.\n","authors":["Fangzhi Xu","Hang Yan","Chang Ma","Haiteng Zhao","Qiushi Sun","Kanzhi Cheng","Junxian He","Jun Liu","Zhiyong Wu"],"pdf_url":"https://arxiv.org/pdf/2504.08672v1.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.11930v4","updated":"2025-04-11T15:46:07Z","published":"2024-07-16T17:23:16Z","title":"Localizing and Mitigating Errors in Long-form Question Answering","summary":"  Long-form question answering (LFQA) aims to provide thorough and in-depth\nanswers to complex questions, enhancing comprehension. However, such detailed\nresponses are prone to hallucinations and factual inconsistencies, challenging\ntheir faithful evaluation. This work introduces HaluQuestQA, the first\nhallucination dataset with localized error annotations for human-written and\nmodel-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 1.8k\nspan-level error annotations for five different error types by expert\nannotators, along with preference judgments. Using our collected data, we\nthoroughly analyze the shortcomings of long-form answers and find that they\nlack comprehensiveness and provide unhelpful references. We train an automatic\nfeedback model on this dataset that predicts error spans with incomplete\ninformation and provides associated explanations. Finally, we propose a\nprompt-based approach, Error-informed refinement, that uses signals from the\nlearned feedback model to refine generated answers, which we show reduces\nerrors and improves answer quality across multiple models. Furthermore, humans\nfind answers generated by our approach comprehensive and highly prefer them\n(84%) over the baseline answers.\n","authors":["Rachneet Sachdeva","Yixiao Song","Mohit Iyyer","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2407.11930v4.pdf","comment":"Code and data are available:\n  https://github.com/UKPLab/arxiv2024-lfqa-hallucination"},{"id":"http://arxiv.org/abs/2504.08641v1","updated":"2025-04-11T15:41:43Z","published":"2025-04-11T15:41:43Z","title":"Training-free Guidance in Text-to-Video Generation via Multimodal\n  Planning and Structured Noise Initialization","summary":"  Recent advancements in text-to-video (T2V) diffusion models have\nsignificantly enhanced the visual quality of the generated videos. However,\neven recent T2V models find it challenging to follow text descriptions\naccurately, especially when the prompt requires accurate control of spatial\nlayouts or object trajectories. A recent line of research uses layout guidance\nfor T2V models that require fine-tuning or iterative manipulation of the\nattention map during inference time. This significantly increases the memory\nrequirement, making it difficult to adopt a large T2V model as a backbone. To\naddress this, we introduce Video-MSG, a training-free Guidance method for T2V\ngeneration based on Multimodal planning and Structured noise initialization.\nVideo-MSG consists of three steps, where in the first two steps, Video-MSG\ncreates Video Sketch, a fine-grained spatio-temporal plan for the final video,\nspecifying background, foreground, and object trajectories, in the form of\ndraft video frames. In the last step, Video-MSG guides a downstream T2V\ndiffusion model with Video Sketch through noise inversion and denoising.\nNotably, Video-MSG does not need fine-tuning or attention manipulation with\nadditional memory during inference time, making it easier to adopt large T2V\nmodels. Video-MSG demonstrates its effectiveness in enhancing text alignment\nwith multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V\ngeneration benchmarks (T2VCompBench and VBench). We provide comprehensive\nablation studies about noise inversion ratio, different background generators,\nbackground object detection, and foreground object segmentation.\n","authors":["Jialu Li","Shoubin Yu","Han Lin","Jaemin Cho","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2504.08641v1.pdf","comment":"Website: https://video-msg.github.io; The first three authors\n  contributed equally"},{"id":"http://arxiv.org/abs/2503.05439v2","updated":"2025-04-11T15:33:14Z","published":"2025-03-07T14:10:10Z","title":"An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for\n  Robust Reasoning","summary":"  In this paper, we examine the use of Conformal Language Modelling (CLM)\nalongside Answer Set Programming (ASP) to enhance the performance of standard\nopen-weight LLMs on complex multi-step reasoning tasks. Using the StepGame\ndataset, which requires spatial reasoning, we apply CLM to generate sets of ASP\nprograms from an LLM, providing statistical guarantees on the correctness of\nthe outputs. Experimental results show that CLM significantly outperforms\nbaseline models that use standard sampling methods, achieving substantial\naccuracy improvements across different levels of reasoning complexity.\nAdditionally, the LLM-as-Judge metric enhances CLM's performance, especially in\nassessing structurally and logically correct ASP outputs. However, calibrating\nCLM with diverse calibration sets did not improve generalizability for tasks\nrequiring much longer reasoning steps, indicating limitations in handling more\ncomplex tasks.\n","authors":["Navdeep Kaur","Lachlan McPheat","Alessandra Russo","Anthony G Cohn","Pranava Madhyastha"],"pdf_url":"https://arxiv.org/pdf/2503.05439v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08619v1","updated":"2025-04-11T15:24:23Z","published":"2025-04-11T15:24:23Z","title":"Analyzing 16,193 LLM Papers for Fun and Profits","summary":"  Large Language Models (LLMs) are reshaping the landscape of computer science\nresearch, driving significant shifts in research priorities across diverse\nconferences and fields. This study provides a comprehensive analysis of the\npublication trend of LLM-related papers in 77 top-tier computer science\nconferences over the past six years (2019-2024). We approach this analysis from\nfour distinct perspectives: (1) We investigate how LLM research is driving\ntopic shifts within major conferences. (2) We adopt a topic modeling approach\nto identify various areas of LLM-related topic growth and reveal the topics of\nconcern at different conferences. (3) We explore distinct contribution patterns\nof academic and industrial institutions. (4) We study the influence of national\norigins on LLM development trajectories. Synthesizing the findings from these\ndiverse analytical angles, we derive ten key insights that illuminate the\ndynamics and evolution of the LLM research ecosystem.\n","authors":["Zhiqiu Xia","Lang Zhu","Bingzhe Li","Feng Chen","Qiannan Li","Hang Liu"],"pdf_url":"https://arxiv.org/pdf/2504.08619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08609v1","updated":"2025-04-11T15:16:31Z","published":"2025-04-11T15:16:31Z","title":"A Survey of Machine Learning Models and Datasets for the Multi-label\n  Classification of Textual Hate Speech in English","summary":"  The dissemination of online hate speech can have serious negative\nconsequences for individuals, online communities, and entire societies. This\nand the large volume of hateful online content prompted both practitioners',\ni.e., in content moderation or law enforcement, and researchers' interest in\nmachine learning models to automatically classify instances of hate speech.\nWhereas most scientific works address hate speech classification as a binary\ntask, practice often requires a differentiation into sub-types, e.g., according\nto target, severity, or legality, which may overlap for individual content.\nHence, researchers created datasets and machine learning models that approach\nhate speech classification in textual data as a multi-label problem. This work\npresents the first systematic and comprehensive survey of scientific literature\non this emerging research landscape in English (N=46). We contribute with a\nconcise overview of 28 datasets suited for training multi-label classification\nmodels that reveals significant heterogeneity regarding label-set, size,\nmeta-concept, annotation process, and inter-annotator agreement. Our analysis\nof 24 publications proposing suitable classification models further establishes\ninconsistency in evaluation and a preference for architectures based on\nBidirectional Encoder Representation from Transformers (BERT) and Recurrent\nNeural Networks (RNNs). We identify imbalanced training data, reliance on\ncrowdsourcing platforms, small and sparse datasets, and missing methodological\nalignment as critical open issues and formulate ten recommendations for\nresearch.\n","authors":["Julian Bäumler","Louis Blöcher","Lars-Joel Frey","Xian Chen","Markus Bayer","Christian Reuter"],"pdf_url":"https://arxiv.org/pdf/2504.08609v1.pdf","comment":"35 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2504.08596v1","updated":"2025-04-11T14:55:15Z","published":"2025-04-11T14:55:15Z","title":"MedHal: An Evaluation Dataset for Medical Hallucination Detection","summary":"  We present MedHal, a novel large-scale dataset specifically designed to\nevaluate if models can detect hallucinations in medical texts. Current\nhallucination detection methods face significant limitations when applied to\nspecialized domains like medicine, where they can have disastrous consequences.\nExisting medical datasets are either too small, containing only a few hundred\nsamples, or focus on a single task like Question Answering or Natural Language\nInference. MedHal addresses these gaps by: (1) incorporating diverse medical\ntext sources and tasks; (2) providing a substantial volume of annotated samples\nsuitable for training medical hallucination detection models; and (3) including\nexplanations for factual inconsistencies to guide model learning. We\ndemonstrate MedHal's utility by training and evaluating a baseline medical\nhallucination detection model, showing improvements over general-purpose\nhallucination detection approaches. This resource enables more efficient\nevaluation of medical text generation systems while reducing reliance on costly\nexpert review, potentially accelerating the development of medical AI research.\n","authors":["Gaya Mehenni","Amal Zouaq"],"pdf_url":"https://arxiv.org/pdf/2504.08596v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08590v1","updated":"2025-04-11T14:49:33Z","published":"2025-04-11T14:49:33Z","title":"Playpen: An Environment for Exploring Learning Through Conversational\n  Interaction","summary":"  Are we running out of learning signal? Predicting the next word in an\nexisting text has turned out to be a powerful signal, at least at scale. But\nthere are signs that we are running out of this resource. In recent months,\ninteraction between learner and feedback-giver has come into focus, both for\n\"alignment\" (with a reward model judging the quality of instruction following\nattempts) and for improving \"reasoning\" (process- and outcome-based verifiers\njudging reasoning steps). In this paper, we explore to what extent synthetic\ninteraction in what we call Dialogue Games -- goal-directed and rule-governed\nactivities driven predominantly by verbal actions -- can provide a learning\nsignal, and how this signal can be used. We introduce an environment for\nproducing such interaction data (with the help of a Large Language Model as\ncounterpart to the learner model), both offline and online. We investigate the\neffects of supervised fine-tuning on this data, as well as reinforcement\nlearning setups such as DPO, and GRPO; showing that all of these approaches\nachieve some improvements in in-domain games, but only GRPO demonstrates the\nability to generalise to out-of-domain games as well as retain competitive\nperformance in reference-based tasks. We release the framework and the baseline\ntraining setups in the hope that this can foster research in this promising new\ndirection.\n","authors":["Nicola Horst","Davide Mazzaccara","Antonia Schmidt","Michael Sullivan","Filippo Momentè","Luca Franceschetti","Philipp Sadler","Sherzod Hakimov","Alberto Testoni","Raffaella Bernardi","Raquel Fernández","Alexander Koller","Oliver Lemon","David Schlangen","Mario Giulianelli","Alessandro Suglia"],"pdf_url":"https://arxiv.org/pdf/2504.08590v1.pdf","comment":"Source code: https://github.com/lm-playpen/playpen Please send\n  correspodence to: lm-playschool@googlegroups.com"},{"id":"http://arxiv.org/abs/2409.19075v4","updated":"2025-04-11T14:38:25Z","published":"2024-09-27T18:22:22Z","title":"Meta-RTL: Reinforcement-Based Meta-Transfer Learning for Low-Resource\n  Commonsense Reasoning","summary":"  Meta learning has been widely used to exploit rich-resource source tasks to\nimprove the performance of low-resource target tasks. Unfortunately, most\nexisting meta learning approaches treat different source tasks equally,\nignoring the relatedness of source tasks to the target task in knowledge\ntransfer. To mitigate this issue, we propose a reinforcement-based multi-source\nmeta-transfer learning framework (Meta-RTL) for low-resource commonsense\nreasoning. In this framework, we present a reinforcement-based approach to\ndynamically estimating source task weights that measure the contribution of the\ncorresponding tasks to the target task in the meta-transfer learning. The\ndifferences between the general loss of the meta model and task-specific losses\nof source-specific temporal meta models on sampled target data are fed into the\npolicy network of the reinforcement learning module as rewards. The policy\nnetwork is built upon LSTMs that capture long-term dependencies on source task\nweight estimation across meta learning iterations. We evaluate the proposed\nMeta-RTL using both BERT and ALBERT as the backbone of the meta model on three\ncommonsense reasoning benchmark datasets. Experimental results demonstrate that\nMeta-RTL substantially outperforms strong baselines and previous task selection\nstrategies and achieves larger improvements on extremely low-resource settings.\n","authors":["Yu Fu","Jie He","Yifan Yang","Qun Liu","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2409.19075v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15932v5","updated":"2025-04-11T14:36:16Z","published":"2023-05-25T10:59:47Z","title":"BUCA: A Binary Classification Approach to Unsupervised Commonsense\n  Question Answering","summary":"  Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as\nthe construction of commonsense reasoning datasets is expensive, and they are\ninevitably limited in their scope. A popular approach to UCR is to fine-tune\nlanguage models with external knowledge (e.g., knowledge graphs), but this\nusually requires a large number of training examples. In this paper, we propose\nto transform the downstream multiple choice question answering task into a\nsimpler binary classification task by ranking all candidate answers according\nto their reasonableness. To this end, for training the model, we convert the\nknowledge graph triples into reasonable and unreasonable texts. Extensive\nexperimental results show the effectiveness of our approach on various multiple\nchoice question answering benchmarks. Furthermore, compared with existing UCR\napproaches using KGs, ours is less data hungry. Our code is available at\nhttps://github.com/probe2/BUCA.\n","authors":["Jie He","Simon Chi Lok U","Víctor Gutiérrez-Basulto","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2305.15932v5.pdf","comment":"ACL 2023"},{"id":"http://arxiv.org/abs/2409.07703v3","updated":"2025-04-11T14:12:58Z","published":"2024-09-12T02:08:00Z","title":"DSBench: How Far Are Data Science Agents from Becoming Data Science\n  Experts?","summary":"  Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have\ndemonstrated impressive language/vision reasoning abilities, igniting the\nrecent trend of building agents for targeted applications such as shopping\nassistants or AI software engineers. Recently, many data science benchmarks\nhave been proposed to investigate their performance in the data science domain.\nHowever, existing data science benchmarks still fall short when compared to\nreal-world data science applications due to their simplified settings. To\nbridge this gap, we introduce DSBench, a comprehensive benchmark designed to\nevaluate data science agents with realistic tasks. This benchmark includes 466\ndata analysis tasks and 74 data modeling tasks, sourced from Eloquence and\nKaggle competitions. DSBench offers a realistic setting by encompassing long\ncontexts, multimodal task backgrounds, reasoning with large data files and\nmulti-table structures, and performing end-to-end data modeling tasks. Our\nevaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle\nwith most tasks, with the best agent solving only 34.12% of data analysis tasks\nand achieving a 34.74% Relative Performance Gap (RPG). These findings\nunderscore the need for further advancements in developing more practical,\nintelligent, and autonomous data science agents.\n","authors":["Liqiang Jing","Zhehui Huang","Xiaoyang Wang","Wenlin Yao","Wenhao Yu","Kaixin Ma","Hongming Zhang","Xinya Du","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2409.07703v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08543v1","updated":"2025-04-11T13:56:44Z","published":"2025-04-11T13:56:44Z","title":"UoB-NLP at SemEval-2025 Task 11: Leveraging Adapters for Multilingual\n  and Cross-Lingual Emotion Detection","summary":"  Emotion detection in natural language processing is a challenging task due to\nthe complexity of human emotions and linguistic diversity. While significant\nprogress has been made in high-resource languages, emotion detection in\nlow-resource languages remains underexplored. In this work, we address\nmultilingual and cross-lingual emotion detection by leveraging adapter-based\nfine-tuning with multilingual pre-trained language models. Adapters introduce a\nsmall number of trainable parameters while keeping the pre-trained model\nweights fixed, offering a parameter-efficient approach to adaptation. We\nexperiment with different adapter tuning strategies, including task-only\nadapters, target-language-ready task adapters, and language-family-based\nadapters. Our results show that target-language-ready task adapters achieve the\nbest overall performance, particularly for low-resource African languages with\nour team ranking 7th for Tigrinya, and 8th for Kinyarwanda in Track A. In Track\nC, our system ranked 3rd for Amharic, and 4th for Oromo, Tigrinya, Kinyarwanda,\nHausa, and Igbo. Our approach outperforms large language models in 11 languages\nand matches their performance in four others, despite our models having\nsignificantly fewer parameters. Furthermore, we find that adapter-based models\nretain cross-linguistic transfer capabilities while requiring fewer\ncomputational resources compared to full fine-tuning for each language.\n","authors":["Frances Laureano De Leon","Yixiao Wang","Yue Feng","Mark G. Lee"],"pdf_url":"https://arxiv.org/pdf/2504.08543v1.pdf","comment":"Accepted to appear in Proceedings of the 19th International Workshop\n  on Semantic Evaluation (SemEval-2025)"},{"id":"http://arxiv.org/abs/2504.08537v1","updated":"2025-04-11T13:47:56Z","published":"2025-04-11T13:47:56Z","title":"Lexical Bundle Frequency as a Construct-Relevant Candidate Feature in\n  Automated Scoring of L2 Academic Writing","summary":"  Automated scoring (AS) systems are increasingly used for evaluating L2\nwriting, but require ongoing refinement for construct validity. While prior\nwork suggested lexical bundles (LBs) - recurrent multi-word sequences\nsatisfying certain frequency criteria - could inform assessment, their\nempirical integration into AS models needs further investigation. This study\ntested the impact of incorporating LB frequency features into an AS model for\nTOEFL independent writing tasks. Analyzing a sampled subcorpus (N=1,225 essays,\n9 L1s) from the TOEFL11 corpus, scored by ETS-trained raters (Low, Medium,\nHigh), 3- to 9-word LBs were extracted, distinguishing prompt-specific from\nnon-prompt types. A baseline Support Vector Machine (SVM) scoring model using\nestablished linguistic features (e.g., mechanics, cohesion, sophistication) was\ncompared against an extended model including three aggregate LB frequency\nfeatures (total prompt, total non-prompt, overall total). Results revealed\nsignificant, though generally small-effect, relationships between LB frequency\n(especially non-prompt bundles) and proficiency (p < .05). Mean frequencies\nsuggested lower proficiency essays used more LBs overall. Critically, the\nLB-enhanced model improved agreement with human raters (Quadratic Cohen's Kappa\n+2.05%, overall Cohen's Kappa +5.63%), with notable gains for low (+10.1% exact\nagreement) and medium (+14.3% Cohen's Kappa) proficiency essays. These findings\ndemonstrate that integrating aggregate LB frequency offers potential for\ndeveloping more linguistically informed and accurate AS systems, particularly\nfor differentiating developing L2 writers.\n","authors":["Burak Senel"],"pdf_url":"https://arxiv.org/pdf/2504.08537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08528v1","updated":"2025-04-11T13:40:53Z","published":"2025-04-11T13:40:53Z","title":"On The Landscape of Spoken Language Models: A Comprehensive Survey","summary":"  The field of spoken language processing is undergoing a shift from training\ncustom-built, task-specific models toward using and optimizing spoken language\nmodels (SLMs) which act as universal speech processing systems. This trend is\nsimilar to the progression toward universal language models that has taken\nplace in the field of (text) natural language processing. SLMs include both\n\"pure\" language models of speech -- models of the distribution of tokenized\nspeech sequences -- and models that combine speech encoders with text language\nmodels, often including both spoken and written input or output. Work in this\narea is very diverse, with a range of terminology and evaluation settings. This\npaper aims to contribute an improved understanding of SLMs via a unifying\nliterature survey of recent work in the context of the evolution of the field.\nOur survey categorizes the work in this area by model architecture, training,\nand evaluation choices, and describes some key challenges and directions for\nfuture work.\n","authors":["Siddhant Arora","Kai-Wei Chang","Chung-Ming Chien","Yifan Peng","Haibin Wu","Yossi Adi","Emmanuel Dupoux","Hung-Yi Lee","Karen Livescu","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2504.08528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08527v1","updated":"2025-04-11T13:40:50Z","published":"2025-04-11T13:40:50Z","title":"Integrated ensemble of BERT- and features-based models for authorship\n  attribution in Japanese literary works","summary":"  Traditionally, authorship attribution (AA) tasks relied on statistical data\nanalysis and classification based on stylistic features extracted from texts.\nIn recent years, pre-trained language models (PLMs) have attracted significant\nattention in text classification tasks. However, although they demonstrate\nexcellent performance on large-scale short-text datasets, their effectiveness\nremains under-explored for small samples, particularly in AA tasks.\nAdditionally, a key challenge is how to effectively leverage PLMs in\nconjunction with traditional feature-based methods to advance AA research. In\nthis study, we aimed to significantly improve performance using an integrated\nintegrative ensemble of traditional feature-based and modern PLM-based methods\non an AA task in a small sample. For the experiment, we used two corpora of\nliterary works to classify 10 authors each. The results indicate that BERT is\neffective, even for small-sample AA tasks. Both BERT-based and classifier\nensembles outperformed their respective stand-alone models, and the integrated\nensemble approach further improved the scores significantly. For the corpus\nthat was not included in the pre-training data, the integrated ensemble\nimproved the F1 score by approximately 14 points, compared to the\nbest-performing single model. Our methodology provides a viable solution for\nthe efficient use of the ever-expanding array of data processing tools in the\nforeseeable future.\n","authors":["Taisei Kanda","Mingzhe Jin","Wataru Zaitsu"],"pdf_url":"https://arxiv.org/pdf/2504.08527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08525v1","updated":"2025-04-11T13:38:36Z","published":"2025-04-11T13:38:36Z","title":"Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM\n  Agent Tasks","summary":"  Large Language Models (LLMs) are increasingly used as autonomous agents for\nmulti-step tasks. However, most existing frameworks fail to maintain a\nstructured understanding of the task state, often relying on linear prompt\nconcatenation or shallow memory buffers. This leads to brittle performance,\nfrequent hallucinations, and poor long-range coherence. In this work, we\npropose the Task Memory Engine (TME), a lightweight and structured memory\nmodule that tracks task execution using a hierarchical Task Memory Tree (TMT).\nEach node in the tree corresponds to a task step, storing relevant input,\noutput, status, and sub-task relationships. We introduce a prompt synthesis\nmethod that dynamically generates LLM prompts based on the active node path,\nsignificantly improving execution consistency and contextual grounding. Through\ncase studies and comparative experiments on multi-step agent tasks, we\ndemonstrate that TME leads to better task completion accuracy and more\ninterpretable behavior with minimal implementation overhead. The full\nimplementation of TME is available at\nhttps://github.com/biubiutomato/TME-Agent.\n","authors":["Ye Ye"],"pdf_url":"https://arxiv.org/pdf/2504.08525v1.pdf","comment":"14 pages, 5 figures. Preprint prepared for future submission.\n  Includes implementation and token-efficiency analysis. Code at\n  https://github.com/biubiutomato/TME-Agent"},{"id":"http://arxiv.org/abs/2307.11170v2","updated":"2025-04-11T11:58:33Z","published":"2023-07-20T18:08:34Z","title":"UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for\n  Biomedical Entity Recognition","summary":"  Pre-trained transformer language models (LMs) have in recent years become the\ndominant paradigm in applied NLP. These models have achieved state-of-the-art\nperformance on tasks such as information extraction, question answering,\nsentiment analysis, document classification and many others. In the biomedical\ndomain, significant progress has been made in adapting this paradigm to NLP\ntasks that require the integration of domain-specific knowledge as well as\nstatistical modelling of language. In particular, research in this area has\nfocused on the question of how best to construct LMs that take into account not\nonly the patterns of token distribution in medical text, but also the wealth of\nstructured information contained in terminology resources such as the UMLS.\nThis work contributes a data-centric paradigm for enriching the language\nrepresentations of biomedical transformer-encoder LMs by extracting text\nsequences from the UMLS. This allows for graph-based learning objectives to be\ncombined with masked-language pre-training. Preliminary results from\nexperiments in the extension of pre-trained LMs as well as training from\nscratch show that this framework improves downstream performance on multiple\nbiomedical and clinical Named Entity Recognition (NER) tasks.\n","authors":["Aidan Mannion","Thierry Chevalier","Didier Schwab","Lorraine Geouriot"],"pdf_url":"https://arxiv.org/pdf/2307.11170v2.pdf","comment":"Addition of v2 experiments"},{"id":"http://arxiv.org/abs/2412.04942v2","updated":"2025-04-11T11:34:34Z","published":"2024-12-06T11:00:05Z","title":"A Federated Approach to Few-Shot Hate Speech Detection for Marginalized\n  Communities","summary":"  Hate speech online remains an understudied issue for marginalized\ncommunities, particularly in the Global South, which includes developing\nsocieties with increasing internet penetration. In this paper, we aim to\nprovide marginalized communities in societies where the dominant language is\nlow-resource with a privacy-preserving tool to protect themselves from online\nhate speech by filtering offensive content in their native languages. Our\ncontributions are twofold: 1) we release REACT (REsponsive hate speech datasets\nAcross ConTexts), a collection of high-quality, culture-specific hate speech\ndetection datasets comprising multiple target groups and low-resource\nlanguages, curated by experienced data collectors; 2) we propose a few-shot\nhate speech detection approach based on federated learning (FL), a\nprivacy-preserving method for collaboratively training a central model that\nexhibits robustness when tackling different target groups and languages. By\nkeeping training local to user devices, we ensure data privacy while leveraging\nthe collective learning benefits of FL. Furthermore, we explore personalized\nclient models tailored to specific target groups and evaluate their\nperformance. Our findings indicate the overall effectiveness of FL across\ndifferent target groups, and point to personalization as a promising direction.\n","authors":["Haotian Ye","Axel Wisiorek","Antonis Maronikolakis","Özge Alaçam","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2412.04942v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08408v1","updated":"2025-04-11T10:14:40Z","published":"2025-04-11T10:14:40Z","title":"BOISHOMMO: Holistic Approach for Bangla Hate Speech","summary":"  One of the most alarming issues in digital society is hate speech (HS) on\nsocial media. The severity is so high that researchers across the globe are\ncaptivated by this domain. A notable amount of work has been conducted to\naddress the identification and alarm system. However, a noticeable gap exists,\nespecially for low-resource languages. Comprehensive datasets are the main\nproblem among the constrained resource languages, such as Bangla.\nInterestingly, hate speech or any particular speech has no single\ndimensionality. Similarly, the hate component can simultaneously have multiple\nabusive attributes, which seems to be missed in the existing datasets. Thus, a\nmulti-label Bangla hate speech dataset named BOISHOMMO has been compiled and\nevaluated in this work. That includes categories of HS across race, gender,\nreligion, politics, and more. With over two thousand annotated examples,\nBOISHOMMO provides a nuanced understanding of hate speech in Bangla and\nhighlights the complexities of processing non-Latin scripts. Apart from\nevaluating with multiple algorithmic approaches, it also highlights the\ncomplexities of processing Bangla text and assesses model performance. This\nunique multi-label approach enriches future hate speech detection and analysis\nstudies for low-resource languages by providing a more nuanced, diverse\ndataset.\n","authors":["Md Abdullah Al Kafi","Sumit Kumar Banshal","Md Sadman Shakib","Showrov Azam","Tamanna Alam Tabashom"],"pdf_url":"https://arxiv.org/pdf/2504.08408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07199v2","updated":"2025-04-11T10:14:39Z","published":"2025-04-09T18:26:46Z","title":"SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject\n  Tagging for a National Technical Library's Open-Access Catalog","summary":"  We present SemEval-2025 Task 5: LLMs4Subjects, a shared task on automated\nsubject tagging for scientific and technical records in English and German\nusing the GND taxonomy. Participants developed LLM-based systems to recommend\ntop-k subjects, evaluated through quantitative metrics (precision, recall,\nF1-score) and qualitative assessments by subject specialists. Results highlight\nthe effectiveness of LLM ensembles, synthetic data generation, and multilingual\nprocessing, offering insights into applying LLMs for digital library\nclassification.\n","authors":["Jennifer D'Souza","Sameer Sadruddin","Holger Israel","Mathias Begoin","Diana Slawig"],"pdf_url":"https://arxiv.org/pdf/2504.07199v2.pdf","comment":"10 pages, 4 figures, Accepted as SemEval 2025 Task 5 description\n  paper"},{"id":"http://arxiv.org/abs/2504.08399v1","updated":"2025-04-11T10:03:55Z","published":"2025-04-11T10:03:55Z","title":"Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in\n  Large Language Models","summary":"  There is a growing interest in assessing the personality traits of Large\nlanguage models (LLMs). However, traditional personality assessments based on\nself-report questionnaires may fail to capture their true behavioral nuances\ndue to inherent biases and meta-knowledge contamination. This paper introduces\na novel multi-observer framework for LLM personality assessment that draws\ninspiration from informant-report methods in psychology. Instead of relying\nsolely on self-assessments, our approach employs multiple observer agents\nconfigured with a specific relationship context (e.g., family, friend, or\nworkplace) to simulate interactive scenarios with a subject LLM. These\nobservers engage in dialogues and subsequently provide ratings across the Big\nFive personality dimensions. Our experiments reveal that LLMs possess\nsystematic biases in self-report personality ratings. Moreover, aggregating\nobserver ratings effectively reduces non-systematic biases and achieves optimal\nreliability with 5-7 observers. The findings highlight the significant impact\nof relationship context on personality perception and demonstrate that a\nmulti-observer paradigm yields a more robust and context-sensitive evaluation\nof LLM personality traits.\n","authors":["Yin Jou Huang","Rafik Hadfi"],"pdf_url":"https://arxiv.org/pdf/2504.08399v1.pdf","comment":"13 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2504.08385v1","updated":"2025-04-11T09:37:48Z","published":"2025-04-11T09:37:48Z","title":"Scholar Inbox: Personalized Paper Recommendations for Scientists","summary":"  Scholar Inbox is a new open-access platform designed to address the\nchallenges researchers face in staying current with the rapidly expanding\nvolume of scientific literature. We provide personalized recommendations,\ncontinuous updates from open-access archives (arXiv, bioRxiv, etc.), visual\npaper summaries, semantic search, and a range of tools to streamline research\nworkflows and promote open research access. The platform's personalized\nrecommendation system is trained on user ratings, ensuring that recommendations\nare tailored to individual researchers' interests. To further enhance the user\nexperience, Scholar Inbox also offers a map of science that provides an\noverview of research across domains, enabling users to easily explore specific\ntopics. We use this map to address the cold start problem common in recommender\nsystems, as well as an active learning strategy that iteratively prompts users\nto rate a selection of papers, allowing the system to learn user preferences\nquickly. We evaluate the quality of our recommendation system on a novel\ndataset of 800k user ratings, which we make publicly available, as well as via\nan extensive user study. https://www.scholar-inbox.com/\n","authors":["Markus Flicke","Glenn Angrabeit","Madhav Iyengar","Vitalii Protsenko","Illia Shakun","Jovan Cicvaric","Bora Kargi","Haoyu He","Lukas Schuler","Lewin Scholz","Kavyanjali Agnihotri","Yong Cao","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2504.08385v1.pdf","comment":"https://www.scholar-inbox.com/"},{"id":"http://arxiv.org/abs/2504.08368v1","updated":"2025-04-11T09:07:05Z","published":"2025-04-11T09:07:05Z","title":"FocalLens: Instruction Tuning Enables Zero-Shot Conditional Image\n  Representations","summary":"  Visual understanding is inherently contextual -- what we focus on in an image\ndepends on the task at hand. For instance, given an image of a person holding a\nbouquet of flowers, we may focus on either the person such as their clothing,\nor the type of flowers, depending on the context of interest. Yet, most\nexisting image encoding paradigms represent an image as a fixed, generic\nfeature vector, overlooking the potential needs of prioritizing varying visual\ninformation for different downstream use cases. In this work, we introduce\nFocalLens, a conditional visual encoding method that produces different\nrepresentations for the same image based on the context of interest, expressed\nflexibly through natural language. We leverage vision instruction tuning data\nand contrastively finetune a pretrained vision encoder to take natural language\ninstructions as additional inputs for producing conditional image\nrepresentations. Extensive experiments validate that conditional image\nrepresentation from FocalLens better pronounce the visual features of interest\ncompared to generic features produced by standard vision encoders like CLIP. In\naddition, we show FocalLens further leads to performance improvements on a\nrange of downstream tasks including image-image retrieval, image\nclassification, and image-text retrieval, with an average gain of 5 and 10\npoints on the challenging SugarCrepe and MMVP-VLM benchmarks, respectively.\n","authors":["Cheng-Yu Hsieh","Pavan Kumar Anasosalu Vasu","Fartash Faghri","Raviteja Vemulapalli","Chun-Liang Li","Ranjay Krishna","Oncel Tuzel","Hadi Pouransari"],"pdf_url":"https://arxiv.org/pdf/2504.08368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07583v2","updated":"2025-04-11T08:22:54Z","published":"2025-04-10T09:24:54Z","title":"Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with\n  Question Answering","summary":"  Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa\n","authors":["Patrick Fernandes","Sweta Agrawal","Emmanouil Zaranis","André F. T. Martins","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2504.07583v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08329v1","updated":"2025-04-11T07:51:58Z","published":"2025-04-11T07:51:58Z","title":"MedRep: Medical Concept Representation for General Electronic Health\n  Record Foundation Models","summary":"  Electronic health record (EHR) foundation models have been an area ripe for\nexploration with their improved performance in various medical tasks. Despite\nthe rapid advances, there exists a fundamental limitation: Processing unseen\nmedical codes out of the vocabulary. This problem limits the generality of EHR\nfoundation models and the integration of models trained with different\nvocabularies. To deal with this problem, we propose MedRep for EHR foundation\nmodels based on the observational medical outcome partnership (OMOP) common\ndata model (CDM), providing the integrated medical concept representations and\nthe basic data augmentation strategy for patient trajectories. For concept\nrepresentation learning, we enrich the information of each concept with a\nminimal definition through large language model (LLM) prompts and enhance the\ntext-based representations through graph ontology of OMOP vocabulary.\nTrajectory augmentation randomly replaces selected concepts with other similar\nconcepts that have closely related representations to let the model practice\nwith the concepts out-of-vocabulary. Finally, we demonstrate that EHR\nfoundation models trained with MedRep better maintain the prediction\nperformance in external datasets. Our code implementation is publicly available\nat https://github.com/kicarussays/MedRep.\n","authors":["Junmo Kim","Namkyeong Lee","Jiwon Kim","Kwangsoo Kim"],"pdf_url":"https://arxiv.org/pdf/2504.08329v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2504.07866v2","updated":"2025-04-11T07:47:04Z","published":"2025-04-10T15:41:51Z","title":"Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend\n  NPUs","summary":"  We present Pangu Ultra, a Large Language Model (LLM) with 135 billion\nparameters and dense Transformer modules trained on Ascend Neural Processing\nUnits (NPUs). Although the field of LLM has been witnessing unprecedented\nadvances in pushing the scale and capability of LLM in recent years, training\nsuch a large-scale model still involves significant optimization and system\nchallenges. To stabilize the training process, we propose depth-scaled sandwich\nnormalization, which effectively eliminates loss spikes during the training\nprocess of deep models. We pre-train our model on 13.2 trillion diverse and\nhigh-quality tokens and further enhance its reasoning capabilities during\npost-training. To perform such large-scale training efficiently, we utilize\n8,192 Ascend NPUs with a series of system optimizations. Evaluations on\nmultiple diverse benchmarks indicate that Pangu Ultra significantly advances\nthe state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral\nLarge 2, and even achieves competitive results with DeepSeek-R1, whose sparse\nmodel structure contains much more parameters. Our exploration demonstrates\nthat Ascend NPUs are capable of efficiently and effectively training dense\nmodels with more than 100 billion parameters. Our model and system will be\navailable for our commercial customers.\n","authors":["Yichun Yin","Wenyong Huang","Kaikai Song","Yehui Tang","Xueyu Wu","Wei Guo","Peng Guo","Yaoyuan Wang","Xiaojun Meng","Yasheng Wang","Dong Li","Can Chen","Dandan Tu","Yin Li","Fisher Yu","Ruiming Tang","Yunhe Wang","Baojun Wang","Bin Wang","Bo Wang","Boxiao Liu","Changzheng Zhang","Duyu Tang","Fei Mi","Hui Jin","Jiansheng Wei","Jiarui Qin","Jinpeng Li","Jun Zhao","Liqun Deng","Lin Li","Minghui Xu","Naifu Zhang","Nianzu Zheng","Qiang Li","Rongju Ruan","Shengjun Cheng","Tianyu Guo","Wei He","Wei Li","Weiwen Liu","Wulong Liu","Xinyi Dai","Yonghan Dong","Yu Pan","Yue Li","Yufei Wang","Yujun Li","Yunsheng Ni","Zhe Liu","Zhenhe Zhang","Zhicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2504.07866v2.pdf","comment":"fix conflicts of latex pacakges"},{"id":"http://arxiv.org/abs/2305.14985v2","updated":"2025-04-11T07:26:47Z","published":"2023-05-24T10:19:57Z","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via\n  Large Language Models","summary":"  The field of vision-and-language (VL) understanding has made unprecedented\nprogress with end-to-end large pre-trained VL models (VLMs). However, they\nstill fall short in zero-shot reasoning tasks that require multi-step\ninferencing. To achieve this goal, previous works resort to a\ndivide-and-conquer pipeline. In this paper, we argue that previous efforts have\nseveral inherent shortcomings: 1) They rely on domain-specific sub-question\ndecomposing models. 2) They force models to predict the final answer even if\nthe sub-questions or sub-answers provide insufficient information. We address\nthese limitations via IdealGPT, a framework that iteratively decomposes VL\nreasoning using large language models (LLMs). Specifically, IdealGPT utilizes\nan LLM to generate sub-questions, a VLM to provide corresponding sub-answers,\nand another LLM to reason to achieve the final answer. These three modules\nperform the divide-and-conquer procedure iteratively until the model is\nconfident about the final answer to the main question. We evaluate IdealGPT on\nmultiple challenging VL reasoning tasks under a zero-shot setting. In\nparticular, our IdealGPT outperforms the best existing GPT-4-like models by an\nabsolute 10% on VCR and 15% on SNLI-VE. Code is available at\nhttps://github.com/Hxyou/IdealGPT\n","authors":["Haoxuan You","Zhecan Wang","Rui Sun","Long Chen","Gengyu Wang","Hammad A. Ayyubi","Kai-Wei Chang","Shih-Fu Chang"],"pdf_url":"https://arxiv.org/pdf/2305.14985v2.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2405.02228v3","updated":"2025-04-11T07:20:47Z","published":"2024-05-03T16:38:51Z","title":"Attribution in Scientific Literature: New Benchmark and Methods","summary":"  Large language models (LLMs) present a promising yet challenging frontier for\nautomated source citation in scientific communication. Previous approaches to\ncitation generation have been limited by citation ambiguity and LLM\novergeneralization. We introduce REASONS, a novel dataset with sentence-level\nannotations across 12 scientific domains from arXiv. Our evaluation framework\ncovers two key citation scenarios: indirect queries (matching sentences to\npaper titles) and direct queries (author attribution), both enhanced with\ncontextual metadata. We conduct extensive experiments with models such as\nGPT-O1, GPT-4O, GPT-3.5, DeepSeek, and other smaller models like Perplexity AI\n(7B). While top-tier LLMs achieve high performance in sentence attribution,\nthey struggle with high hallucination rates, a key metric for scientific\nreliability. Our metadata-augmented approach reduces hallucination rates across\nall tasks, offering a promising direction for improvement. Retrieval-augmented\ngeneration (RAG) with Mistral improves performance in indirect queries,\nreducing hallucination rates by 42% and maintaining competitive precision with\nlarger models. However, adversarial testing highlights challenges in linking\npaper titles to abstracts, revealing fundamental limitations in current LLMs.\nREASONS provides a challenging benchmark for developing reliable and\ntrustworthy LLMs in scientific applications\n","authors":["Yash Saxena","Deepa Tilwani","Ali Mohammadi","Edward Raff","Amit Sheth","Srinivasan Parthasarathy","Manas Gaur"],"pdf_url":"https://arxiv.org/pdf/2405.02228v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2502.06891v2","updated":"2025-04-11T07:15:26Z","published":"2025-02-09T10:36:33Z","title":"ScaffoldGPT: A Scaffold-based GPT Model for Drug Optimization","summary":"  Drug optimization has become increasingly crucial in light of fast-mutating\nvirus strains and drug-resistant cancer cells. Nevertheless, it remains\nchallenging as it necessitates retaining the beneficial properties of the\noriginal drug while simultaneously enhancing desired attributes beyond its\nscope. In this work, we aim to tackle this challenge by introducing\nScaffoldGPT, a novel Generative Pretrained Transformer (GPT) designed for drug\noptimization based on molecular scaffolds. Our work comprises three key\ncomponents: (1) A three-stage drug optimization approach that integrates\npretraining, finetuning, and decoding optimization. (2) A uniquely designed\ntwo-phase incremental training approach for pre-training the drug optimization\nGPT on molecule scaffold with enhanced performance. (3) A token-level decoding\noptimization strategy, TOP-N, that enabling controlled, reward-guided\ngeneration using pretrained/finetuned GPT. We demonstrate via a comprehensive\nevaluation on COVID and cancer benchmarks that ScaffoldGPT outperforms the\ncompeting baselines in drug optimization benchmarks, while excelling in\npreserving original functional scaffold and enhancing desired properties.\n","authors":["Xuefeng Liu","Songhao Jiang","Ian Foster","Jinbo Xu","Rick Stevens"],"pdf_url":"https://arxiv.org/pdf/2502.06891v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08300v1","updated":"2025-04-11T07:04:44Z","published":"2025-04-11T07:04:44Z","title":"Large language models could be rote learners","summary":"  Multiple-choice question (MCQ) benchmarks are widely used for evaluating\nLarge Language Models (LLMs), yet their reliability is undermined by benchmark\ncontamination. In this study, we reframe contamination as an inherent aspect of\nlearning and seek to disentangle genuine capability acquisition from\nsuperficial memorization in LLM evaluation. First, by analyzing model\nperformance under different memorization conditions, we uncover a\ncounterintuitive trend: LLMs perform worse on memorized MCQs than on\nnon-memorized ones, indicating the coexistence of two distinct learning\nphenomena, i.e., rote memorization and genuine capability learning. To\ndisentangle them, we propose TrinEval, a novel evaluation framework that\nreformulates MCQs into an alternative trinity format, reducing memorization\nwhile preserving knowledge assessment. Experiments validate TrinEval's\neffectiveness in reformulation, and its evaluation reveals that common LLMs may\nmemorize by rote 20.5% of knowledge points (in MMLU on average).\n","authors":["Yuyang Xu","Renjun Hu","Haochao Ying","Jian Wu","Xing Shi","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2504.08300v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2504.08281v1","updated":"2025-04-11T06:30:16Z","published":"2025-04-11T06:30:16Z","title":"ELSA: A Style Aligned Dataset for Emotionally Intelligent Language\n  Generation","summary":"  Advancements in emotion aware language processing increasingly shape vital\nNLP applications ranging from conversational AI and affective computing to\ncomputational psychology and creative content generation. Existing emotion\ndatasets either lack emotional granularity or fail to capture necessary\nstylistic diversity, limiting the advancement of effective emotion conditioned\ntext generation systems. Seeking to bridge this crucial gap between granularity\nand style diversity, this paper introduces a novel systematically constructed\ndataset named ELSA Emotion and Language Style Alignment Dataset leveraging fine\ngrained emotion taxonomies adapted from existing sources such as dair ai\nemotion dataset and GoEmotions taxonomy. This dataset comprises multiple\nemotionally nuanced variations of original sentences regenerated across\ndistinct contextual styles such as conversational, formal, poetic, and\nnarrative, using advanced Large Language Models LLMs. Rigorous computational\nevaluation using metrics such as perplexity, embedding variance, readability,\nlexical diversity, and semantic coherence measures validates the datasets\nemotional authenticity, linguistic fluency, and textual diversity.\nComprehensive metric analyses affirm its potential to support deeper\nexplorations into emotion conditioned style adaptive text generation. By\nenabling precision tuned emotionally nuanced language modeling, our dataset\ncreates fertile ground for research on fine grained emotional control, prompt\ndriven explanation, interpretability, and style adaptive expressive language\ngeneration with LLMs.\n","authors":["Vishal Gandhi","Sagar Gandhi"],"pdf_url":"https://arxiv.org/pdf/2504.08281v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2504.08274v1","updated":"2025-04-11T06:12:57Z","published":"2025-04-11T06:12:57Z","title":"Generalized Multilingual Text-to-Speech Generation with Language-Aware\n  Style Adaptation","summary":"  Text-to-Speech (TTS) models can generate natural, human-like speech across\nmultiple languages by transforming phonemes into waveforms. However,\nmultilingual TTS remains challenging due to discrepancies in phoneme\nvocabularies and variations in prosody and speaking style across languages.\nExisting approaches either train separate models for each language, which\nachieve high performance at the cost of increased computational resources, or\nuse a unified model for multiple languages that struggles to capture\nfine-grained, language-specific style variations. In this work, we propose\nLanStyleTTS, a non-autoregressive, language-aware style adaptive TTS framework\nthat standardizes phoneme representations and enables fine-grained,\nphoneme-level style control across languages. This design supports a unified\nmultilingual TTS model capable of producing accurate and high-quality speech\nwithout the need to train language-specific models. We evaluate LanStyleTTS by\nintegrating it with several state-of-the-art non-autoregressive TTS\narchitectures. Results show consistent performance improvements across\ndifferent model backbones. Furthermore, we investigate a range of acoustic\nfeature representations, including mel-spectrograms and autoencoder-derived\nlatent features. Our experiments demonstrate that latent encodings can\nsignificantly reduce model size and computational cost while preserving\nhigh-quality speech generation.\n","authors":["Haowei Lou","Hye-young Paik","Sheng Li","Wen Hu","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2504.08274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08269v1","updated":"2025-04-11T05:51:44Z","published":"2025-04-11T05:51:44Z","title":"VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop\n  Question Answering","summary":"  The increasing availability of multimodal data across text, tables, and\nimages presents new challenges for developing models capable of complex\ncross-modal reasoning. Existing methods for Multimodal Multi-hop Question\nAnswering (MMQA) often suffer from limited reasoning capabilities, reliance on\nmodality conversion, and inadequate alignment between visual and textual\nrepresentations. To address these limitations, this paper introduces\nVision-Language Multimodal Transformer (VLMT), a unified architecture that\nintegrates a transformer-based vision encoder with a sequence-to-sequence\nlanguage model. VLMT employs a direct token-level injection mechanism to fuse\nvisual and textual inputs within a shared embedding space, eliminating the need\nfor intermediate projection layers. To enhance cross-modal alignment and\nreasoning, a three-stage pretraining strategy is proposed to progressively\nalign vision-language representations and improve the model's capacity for\nmultimodal understanding. Based on the pretrained backbone, two task-specific\nmodules are instantiated to form a two-stage MMQA framework: a multimodal\nreranker that predicts document relevance scores and utilizes a relative\nthreshold with top-k strategy for context retrieval, and a multimodal question\nanswering model that generates contextually grounded answers based on the\nretrieved evidence. Comprehensive experiments on two benchmark datasets\ndemonstrate the effectiveness of the proposed approach. On MultimodalQA\nvalidation set, VLMT-Large achieves 76.5% Exact Match and 80.1% F1,\noutperforming the previous state-of-the-art by +9.1% in Exact Match and +8.8%\nin F1. On WebQA, it attains a QA score of 47.6, surpassing prior models such as\nPERQA by +3.2. These results highlight VLMT's strong capabilities in multimodal\nreasoning and its potential to advance real-world information retrieval and\nquestion answering systems.\n","authors":["Qi Zhi Lim","Chin Poo Lee","Kian Ming Lim","Kalaiarasi Sonai Muthu Anbananthen"],"pdf_url":"https://arxiv.org/pdf/2504.08269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08864v3","updated":"2025-04-11T05:27:08Z","published":"2024-12-12T01:52:25Z","title":"A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning\n  Instructions","summary":"  Synthesizing high-quality reasoning data for continual training has been\nproven to be effective in enhancing the performance of Large Language Models\n(LLMs). However, previous synthetic approaches struggle to easily scale up data\nand incur high costs in the pursuit of high quality. In this paper, we propose\nthe Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable\nframework for high-quality reasoning data synthesis. Inspired by knowledge\ngraphs, we extracted knowledge points from seed data and constructed a\nknowledge point relationships graph to explore their interconnections. By\nexploring the implicit relationships among knowledge, our method achieves\n$\\times$255 data expansion. Furthermore, GSDP led by open-source models,\nachieves synthesis quality comparable to GPT-4-0613 while maintaining\n$\\times$100 lower costs. To tackle the most challenging mathematical reasoning\ntask, we present the GSDP-MATH dataset comprising over 1.91 million pairs of\nmath problems and answers. After fine-tuning on GSDP-MATH, GSDP-7B based on\nMistral-7B achieves 37.7% accuracy on MATH and 78.4% on GSM8K, demonstrating\nthe effectiveness of our method. The dataset and models will be released at\nhttps://github.com/Jayce1kk/GSDP.\n","authors":["Jiankang Wang","Jianjun Xu","Xiaorui Wang","Yuxin Wang","Mengting Xing","Shancheng Fang","Zhineng Chen","Hongtao Xie","Yongdong Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.08864v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08260v1","updated":"2025-04-11T05:11:40Z","published":"2025-04-11T05:11:40Z","title":"Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in\n  Healthcare","summary":"  Generative agents have been increasingly used to simulate human behaviour in\nsilico, driven by large language models (LLMs). These simulacra serve as\nsandboxes for studying human behaviour without compromising privacy or safety.\nHowever, it remains unclear whether such agents can truly represent real\nindividuals. This work compares survey data from the Understanding America\nStudy (UAS) on healthcare decision-making with simulated responses from\ngenerative agents. Using demographic-based prompt engineering, we create\ndigital twins of survey respondents and analyse how well different LLMs\nreproduce real-world behaviours. Our findings show that some LLMs fail to\nreflect realistic decision-making, such as predicting universal vaccine\nacceptance. However, Llama 3 captures variations across race and Income more\naccurately but also introduces biases not present in the UAS data. This study\nhighlights the potential of generative agents for behavioural research while\nunderscoring the risks of bias from both LLMs and prompting strategies.\n","authors":["Yonchanok Khaokaew","Flora D. Salim","Andreas Züfle","Hao Xue","Taylor Anderson","Matthew Scotch","David J Heslop"],"pdf_url":"https://arxiv.org/pdf/2504.08260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11858v2","updated":"2025-04-11T04:26:42Z","published":"2025-01-21T03:22:10Z","title":"EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents","summary":"  Multimodal Large Language Models (MLLMs) have shown significant advancements,\nproviding a promising future for embodied agents. Existing benchmarks for\nevaluating MLLMs primarily utilize static images or videos, limiting\nassessments to non-interactive scenarios. Meanwhile, existing embodied AI\nbenchmarks are task-specific and not diverse enough, which do not adequately\nevaluate the embodied capabilities of MLLMs. To address this, we propose\nEmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs\nwith embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied\n3D scenes, each of which is rigorously selected and annotated. It covers a\nbroad spectrum of existing embodied AI tasks with significantly enhanced\ndiversity, all within a unified simulation and evaluation framework tailored\nfor MLLMs. The tasks are organized into five categories: navigation, object\ninteraction, social interaction, attribute question answering, and spatial\nquestion answering to assess different capabilities of the agents. We evaluated\nthe state-of-the-art MLLMs on EmbodiedEval and found that they have a\nsignificant shortfall compared to human level on embodied tasks. Our analysis\ndemonstrates the limitations of existing MLLMs in embodied capabilities,\nproviding insights for their future development. We open-source all evaluation\ndata and simulation framework at https://github.com/thunlp/EmbodiedEval.\n","authors":["Zhili Cheng","Yuge Tu","Ran Li","Shiqi Dai","Jinyi Hu","Shengding Hu","Jiahao Li","Yang Shi","Tianyu Yu","Weize Chen","Lei Shi","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2501.11858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15655v3","updated":"2025-04-11T04:17:44Z","published":"2024-12-20T08:13:05Z","title":"MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical\n  Speech-to-Formula","summary":"  In various academic and professional settings, such as mathematics lectures\nor research presentations, it is often necessary to convey mathematical\nexpressions orally. However, reading mathematical expressions aloud without\naccompanying visuals can significantly hinder comprehension, especially for\nthose who are hearing-impaired or rely on subtitles due to language barriers.\nFor instance, when a presenter reads Euler's Formula, current Automatic Speech\nRecognition (ASR) models often produce a verbose and error-prone textual\ndescription (e.g., e to the power of i x equals cosine of x plus i\n$\\textit{side}$ of x), instead of the concise $\\LaTeX{}$ format (i.e., $ e^{ix}\n= \\cos(x) + i\\sin(x) $), which hampers clear understanding and communication.\nTo address this issue, we introduce MathSpeech, a novel pipeline that\nintegrates ASR models with small Language Models (sLMs) to correct errors in\nmathematical expressions and accurately convert spoken expressions into\nstructured $\\LaTeX{}$ representations. Evaluated on a new dataset derived from\nlecture recordings, MathSpeech demonstrates $\\LaTeX{}$ generation capabilities\ncomparable to leading commercial Large Language Models (LLMs), while leveraging\nfine-tuned small language models of only 120M parameters. Specifically, in\nterms of CER, BLEU, and ROUGE scores for $\\LaTeX{}$ translation, MathSpeech\ndemonstrated significantly superior capabilities compared to GPT-4o. We\nobserved a decrease in CER from 0.390 to 0.298, and higher ROUGE/BLEU scores\ncompared to GPT-4o.\n","authors":["Sieun Hyeon","Kyudan Jung","Jaehee Won","Nam-Joon Kim","Hyun Gon Ryu","Hyuk-Jae Lee","Jaeyoung Do"],"pdf_url":"https://arxiv.org/pdf/2412.15655v3.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2504.08247v1","updated":"2025-04-11T04:14:32Z","published":"2025-04-11T04:14:32Z","title":"Millions of States: Designing a Scalable MoE Architecture with RWKV-7\n  Meta-learner","summary":"  State-based sequence models like RWKV-7 offer a compelling alternative to\nTransformer architectures, achieving linear complexity while demonstrating\ngreater expressive power in short-context scenarios and enabling state tracking\nbeyond the \\(\\text{TC}^0\\) complexity class. However, RWKV-7 lacks mechanisms\nfor token-parameter interactions and native scalability, limiting its\nadaptability and growth without retraining. In this paper, we propose\n\\textbf{Meta-State}, a novel extension to RWKV-7 that replaces attention\nmechanisms with a fully state-driven approach, integrating token-parameter\ninteractions through a \\textbf{Self-State Encoder} (SSE) mechanism. The SSE\nrepurposes a portion of the RWKV-7 Weighted Key-Value (WKV) state as\ntransformation weights to encode token-parameter interactions in a linear,\nstate-driven manner without introducing new trainable matrices or softmax\noperations, while preserving the autoregressive property of token processing.\nMeta-State supports progressive model scaling by expanding the WKV state and\nparameter tokens, reusing existing parameters without retraining. Our approach\nbridges the gap between state-based modeling, token-parameter interactions, and\nscalable architectures, offering a flexible framework for efficient and\nadaptable sequence modeling with linear complexity and constant memory usage.\n","authors":["Liu Xiao","Li Zhiyuan","Lin Yueyu"],"pdf_url":"https://arxiv.org/pdf/2504.08247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06631v3","updated":"2025-04-11T03:53:03Z","published":"2024-08-13T04:36:18Z","title":"IFShip: Interpretable Fine-grained Ship Classification with Domain\n  Knowledge-Enhanced Vision-Language Models","summary":"  End-to-end interpretation currently dominates the remote sensing fine-grained\nship classification (RS-FGSC) task. However, the inference process remains\nuninterpretable, leading to criticisms of these models as \"black box\" systems.\nTo address this issue, we propose a domain knowledge-enhanced Chain-of-Thought\n(CoT) prompt generation mechanism, which is used to semi-automatically\nconstruct a task-specific instruction-following dataset, TITANIC-FGS. By\ntraining on TITANIC-FGS, we adapt general-domain vision-language models (VLMs)\nto the FGSC task, resulting in a model named IFShip. Building upon IFShip, we\ndevelop an FGSC visual chatbot that redefines the FGSC problem as a\nstep-by-step reasoning task and conveys the reasoning process in natural\nlanguage. Experimental results show that IFShip outperforms state-of-the-art\nFGSC algorithms in both interpretability and classification accuracy.\nFurthermore, compared to VLMs such as LLaVA and MiniGPT-4, IFShip demonstrates\nsuperior performance on the FGSC task. It provides an accurate chain of\nreasoning when fine-grained ship types are recognizable to the human eye and\noffers interpretable explanations when they are not.\n","authors":["Mingning Guo","Mengwei Wu","Yuxiang Shen","Haifeng Li","Chao Tao"],"pdf_url":"https://arxiv.org/pdf/2408.06631v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14249v6","updated":"2025-04-11T03:34:49Z","published":"2025-01-24T05:27:46Z","title":"Humanity's Last Exam","summary":"  Benchmarks are important tools for tracking the rapid advancements in large\nlanguage model (LLM) capabilities. However, benchmarks are not keeping pace in\ndifficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like\nMMLU, limiting informed measurement of state-of-the-art LLM capabilities. In\nresponse, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at\nthe frontier of human knowledge, designed to be the final closed-ended academic\nbenchmark of its kind with broad subject coverage. HLE consists of 2,700\nquestions across dozens of subjects, including mathematics, humanities, and the\nnatural sciences. HLE is developed globally by subject-matter experts and\nconsists of multiple-choice and short-answer questions suitable for automated\ngrading. Each question has a known solution that is unambiguous and easily\nverifiable, but cannot be quickly answered via internet retrieval.\nState-of-the-art LLMs demonstrate low accuracy and calibration on HLE,\nhighlighting a significant gap between current LLM capabilities and the expert\nhuman frontier on closed-ended academic questions. To inform research and\npolicymaking upon a clear understanding of model capabilities, we publicly\nrelease HLE at https://lastexam.ai.\n","authors":["Long Phan","Alice Gatti","Ziwen Han","Nathaniel Li","Josephina Hu","Hugh Zhang","Chen Bo Calvin Zhang","Mohamed Shaaban","John Ling","Sean Shi","Michael Choi","Anish Agrawal","Arnav Chopra","Adam Khoja","Ryan Kim","Richard Ren","Jason Hausenloy","Oliver Zhang","Mantas Mazeika","Dmitry Dodonov","Tung Nguyen","Jaeho Lee","Daron Anderson","Mikhail Doroshenko","Alun Cennyth Stokes","Mobeen Mahmood","Oleksandr Pokutnyi","Oleg Iskra","Jessica P. Wang","John-Clark Levin","Mstyslav Kazakov","Fiona Feng","Steven Y. Feng","Haoran Zhao","Michael Yu","Varun Gangal","Chelsea Zou","Zihan Wang","Serguei Popov","Robert Gerbicz","Geoff Galgon","Johannes Schmitt","Will Yeadon","Yongki Lee","Scott Sauers","Alvaro Sanchez","Fabian Giska","Marc Roth","Søren Riis","Saiteja Utpala","Noah Burns","Gashaw M. Goshu","Mohinder Maheshbhai Naiya","Chidozie Agu","Zachary Giboney","Antrell Cheatom","Francesco Fournier-Facio","Sarah-Jane Crowson","Lennart Finke","Zerui Cheng","Jennifer Zampese","Ryan G. Hoerr","Mark Nandor","Hyunwoo Park","Tim Gehrunger","Jiaqi Cai","Ben McCarty","Alexis C Garretson","Edwin Taylor","Damien Sileo","Qiuyu Ren","Usman Qazi","Lianghui Li","Jungbae Nam","John B. Wydallis","Pavel Arkhipov","Jack Wei Lun Shi","Aras Bacho","Chris G. Willcocks","Hangrui Cao","Sumeet Motwani","Emily de Oliveira Santos","Johannes Veith","Edward Vendrow","Doru Cojoc","Kengo Zenitani","Joshua Robinson","Longke Tang","Yuqi Li","Joshua Vendrow","Natanael Wildner Fraga","Vladyslav Kuchkin","Andrey Pupasov Maksimov","Pierre Marion","Denis Efremov","Jayson Lynch","Kaiqu Liang","Aleksandar Mikov","Andrew Gritsevskiy","Julien Guillod","Gözdenur Demir","Dakotah Martinez","Ben Pageler","Kevin Zhou","Saeed Soori","Ori Press","Henry Tang","Paolo Rissone","Sean R. Green","Lina Brüssel","Moon Twayana","Aymeric Dieuleveut","Joseph Marvin Imperial","Ameya Prabhu","Jinzhou Yang","Nick Crispino","Arun Rao","Dimitri Zvonkine","Gabriel Loiseau","Mikhail Kalinin","Marco Lukas","Ciprian Manolescu","Nate Stambaugh","Subrata Mishra","Tad Hogg","Carlo Bosio","Brian P Coppola","Julian Salazar","Jaehyeok Jin","Rafael Sayous","Stefan Ivanov","Philippe Schwaller","Shaipranesh Senthilkuma","Andres M Bran","Andres Algaba","Kelsey Van den Houte","Lynn Van Der Sypt","Brecht Verbeken","David Noever","Alexei Kopylov","Benjamin Myklebust","Bikun Li","Lisa Schut","Evgenii Zheltonozhskii","Qiaochu Yuan","Derek Lim","Richard Stanley","Tong Yang","John Maar","Julian Wykowski","Martí Oller","Anmol Sahu","Cesare Giulio Ardito","Yuzheng Hu","Ariel Ghislain Kemogne Kamdoum","Alvin Jin","Tobias Garcia Vilchis","Yuexuan Zu","Martin Lackner","James Koppel","Gongbo Sun","Daniil S. Antonenko","Steffi Chern","Bingchen Zhao","Pierrot Arsene","Joseph M Cavanagh","Daofeng Li","Jiawei Shen","Donato Crisostomi","Wenjin Zhang","Ali Dehghan","Sergey Ivanov","David Perrella","Nurdin Kaparov","Allen Zang","Ilia Sucholutsky","Arina Kharlamova","Daniil Orel","Vladislav Poritski","Shalev Ben-David","Zachary Berger","Parker Whitfill","Michael Foster","Daniel Munro","Linh Ho","Shankar Sivarajan","Dan Bar Hava","Aleksey Kuchkin","David Holmes","Alexandra Rodriguez-Romero","Frank Sommerhage","Anji Zhang","Richard Moat","Keith Schneider","Zakayo Kazibwe","Don Clarke","Dae Hyun Kim","Felipe Meneguitti Dias","Sara Fish","Veit Elser","Tobias Kreiman","Victor Efren Guadarrama Vilchis","Immo Klose","Ujjwala Anantheswaran","Adam Zweiger","Kaivalya Rawal","Jeffery Li","Jeremy Nguyen","Nicolas Daans","Haline Heidinger","Maksim Radionov","Václav Rozhoň","Vincent Ginis","Christian Stump","Niv Cohen","Rafał Poświata","Josef Tkadlec","Alan Goldfarb","Chenguang Wang","Piotr Padlewski","Stanislaw Barzowski","Kyle Montgomery","Ryan Stendall","Jamie Tucker-Foltz","Jack Stade","T. Ryan Rogers","Tom Goertzen","Declan Grabb","Abhishek Shukla","Alan Givré","John Arnold Ambay","Archan Sen","Muhammad Fayez Aziz","Mark H Inlow","Hao He","Ling Zhang","Younesse Kaddar","Ivar Ängquist","Yanxu Chen","Harrison K Wang","Kalyan Ramakrishnan","Elliott Thornley","Antonio Terpin","Hailey Schoelkopf","Eric Zheng","Avishy Carmi","Ethan D. L. Brown","Kelin Zhu","Max Bartolo","Richard Wheeler","Martin Stehberger","Peter Bradshaw","JP Heimonen","Kaustubh Sridhar","Ido Akov","Jennifer Sandlin","Yury Makarychev","Joanna Tam","Hieu Hoang","David M. Cunningham","Vladimir Goryachev","Demosthenes Patramanis","Michael Krause","Andrew Redenti","David Aldous","Jesyin Lai","Shannon Coleman","Jiangnan Xu","Sangwon Lee","Ilias Magoulas","Sandy Zhao","Ning Tang","Michael K. Cohen","Orr Paradise","Jan Hendrik Kirchner","Maksym Ovchynnikov","Jason O. Matos","Adithya Shenoy","Michael Wang","Yuzhou Nie","Anna Sztyber-Betley","Paolo Faraboschi","Robin Riblet","Jonathan Crozier","Shiv Halasyamani","Shreyas Verma","Prashant Joshi","Eli Meril","Ziqiao Ma","Jérémy Andréoletti","Raghav Singhal","Jacob Platnick","Volodymyr Nevirkovets","Luke Basler","Alexander Ivanov","Seri Khoury","Nils Gustafsson","Marco Piccardo","Hamid Mostaghimi","Qijia Chen","Virendra Singh","Tran Quoc Khánh","Paul Rosu","Hannah Szlyk","Zachary Brown","Himanshu Narayan","Aline Menezes","Jonathan Roberts","William Alley","Kunyang Sun","Arkil Patel","Max Lamparth","Anka Reuel","Linwei Xin","Hanmeng Xu","Jacob Loader","Freddie Martin","Zixuan Wang","Andrea Achilleos","Thomas Preu","Tomek Korbak","Ida Bosio","Fereshteh Kazemi","Ziye Chen","Biró Bálint","Eve J. Y. Lo","Jiaqi Wang","Maria Inês S. Nunes","Jeremiah Milbauer","M Saiful Bari","Zihao Wang","Behzad Ansarinejad","Yewen Sun","Stephane Durand","Hossam Elgnainy","Guillaume Douville","Daniel Tordera","George Balabanian","Hew Wolff","Lynna Kvistad","Hsiaoyun Milliron","Ahmad Sakor","Murat Eron","Andrew Favre D. O.","Shailesh Shah","Xiaoxiang Zhou","Firuz Kamalov","Sherwin Abdoli","Tim Santens","Shaul Barkan","Allison Tee","Robin Zhang","Alessandro Tomasiello","G. Bruno De Luca","Shi-Zhuo Looi","Vinh-Kha Le","Noam Kolt","Jiayi Pan","Emma Rodman","Jacob Drori","Carl J Fossum","Niklas Muennighoff","Milind Jagota","Ronak Pradeep","Honglu Fan","Jonathan Eicher","Michael Chen","Kushal Thaman","William Merrill","Moritz Firsching","Carter Harris","Stefan Ciobâcă","Jason Gross","Rohan Pandey","Ilya Gusev","Adam Jones","Shashank Agnihotri","Pavel Zhelnov","Mohammadreza Mofayezi","Alexander Piperski","David K. Zhang","Kostiantyn Dobarskyi","Roman Leventov","Ignat Soroko","Joshua Duersch","Vage Taamazyan","Andrew Ho","Wenjie Ma","William Held","Ruicheng Xian","Armel Randy Zebaze","Mohanad Mohamed","Julian Noah Leser","Michelle X Yuan","Laila Yacar","Johannes Lengler","Katarzyna Olszewska","Claudio Di Fratta","Edson Oliveira","Joseph W. Jackson","Andy Zou","Muthu Chidambaram","Timothy Manik","Hector Haffenden","Dashiell Stander","Ali Dasouqi","Alexander Shen","Bita Golshani","David Stap","Egor Kretov","Mikalai Uzhou","Alina Borisovna Zhidkovskaya","Nick Winter","Miguel Orbegozo Rodriguez","Robert Lauff","Dustin Wehr","Colin Tang","Zaki Hossain","Shaun Phillips","Fortuna Samuele","Fredrik Ekström","Angela Hammon","Oam Patel","Faraz Farhidi","George Medley","Forough Mohammadzadeh","Madellene Peñaflor","Haile Kassahun","Alena Friedrich","Rayner Hernandez Perez","Daniel Pyda","Taom Sakal","Omkar Dhamane","Ali Khajegili Mirabadi","Eric Hallman","Kenchi Okutsu","Mike Battaglia","Mohammad Maghsoudimehrabani","Alon Amit","Dave Hulbert","Roberto Pereira","Simon Weber"," Handoko","Anton Peristyy","Stephen Malina","Mustafa Mehkary","Rami Aly","Frank Reidegeld","Anna-Katharina Dick","Cary Friday","Mukhwinder Singh","Hassan Shapourian","Wanyoung Kim","Mariana Costa","Hubeyb Gurdogan","Harsh Kumar","Chiara Ceconello","Chao Zhuang","Haon Park","Micah Carroll","Andrew R. Tawfeek","Stefan Steinerberger","Daattavya Aggarwal","Michael Kirchhof","Linjie Dai","Evan Kim","Johan Ferret","Jainam Shah","Yuzhou Wang","Minghao Yan","Krzysztof Burdzy","Lixin Zhang","Antonio Franca","Diana T. Pham","Kang Yong Loh","Joshua Robinson","Abram Jackson","Paolo Giordano","Philipp Petersen","Adrian Cosma","Jesus Colino","Colin White","Jacob Votava","Vladimir Vinnikov","Ethan Delaney","Petr Spelda","Vit Stritecky","Syed M. Shahid","Jean-Christophe Mourrat","Lavr Vetoshkin","Koen Sponselee","Renas Bacho","Zheng-Xin Yong","Florencia de la Rosa","Nathan Cho","Xiuyu Li","Guillaume Malod","Orion Weller","Guglielmo Albani","Leon Lang","Julien Laurendeau","Dmitry Kazakov","Fatimah Adesanya","Julien Portier","Lawrence Hollom","Victor Souza","Yuchen Anna Zhou","Julien Degorre","Yiğit Yalın","Gbenga Daniel Obikoya"," Rai","Filippo Bigi","M. C. Boscá","Oleg Shumar","Kaniuar Bacho","Gabriel Recchia","Mara Popescu","Nikita Shulga","Ngefor Mildred Tanwie","Thomas C. H. Lux","Ben Rank","Colin Ni","Matthew Brooks","Alesia Yakimchyk"," Huanxu"," Liu","Stefano Cavalleri","Olle Häggström","Emil Verkama","Joshua Newbould","Hans Gundlach","Leonor Brito-Santana","Brian Amaro","Vivek Vajipey","Rynaa Grover","Ting Wang","Yosi Kratish","Wen-Ding Li","Sivakanth Gopi","Andrea Caciolai","Christian Schroeder de Witt","Pablo Hernández-Cámara","Emanuele Rodolà","Jules Robins","Dominic Williamson","Vincent Cheng","Brad Raynor","Hao Qi","Ben Segev","Jingxuan Fan","Sarah Martinson","Erik Y. Wang","Kaylie Hausknecht","Michael P. Brenner","Mao Mao","Christoph Demian","Peyman Kassani","Xinyu Zhang","David Avagian","Eshawn Jessica Scipio","Alon Ragoler","Justin Tan","Blake Sims","Rebeka Plecnik","Aaron Kirtland","Omer Faruk Bodur","D. P. Shinde","Yan Carlos Leyva Labrador","Zahra Adoul","Mohamed Zekry","Ali Karakoc","Tania C. B. Santos","Samir Shamseldeen","Loukmane Karim","Anna Liakhovitskaia","Nate Resman","Nicholas Farina","Juan Carlos Gonzalez","Gabe Maayan","Earth Anderson","Rodrigo De Oliveira Pena","Elizabeth Kelley","Hodjat Mariji","Rasoul Pouriamanesh","Wentao Wu","Ross Finocchio","Ismail Alarab","Joshua Cole","Danyelle Ferreira","Bryan Johnson","Mohammad Safdari","Liangti Dai","Siriphan Arthornthurasuk","Isaac C. McAlister","Alejandro José Moyano","Alexey Pronin","Jing Fan","Angel Ramirez-Trinidad","Yana Malysheva","Daphiny Pottmaier","Omid Taheri","Stanley Stepanic","Samuel Perry","Luke Askew","Raúl Adrián Huerta Rodríguez","Ali M. R. Minissi","Ricardo Lorena","Krishnamurthy Iyer","Arshad Anil Fasiludeen","Ronald Clark","Josh Ducey","Matheus Piza","Maja Somrak","Eric Vergo","Juehang Qin","Benjámin Borbás","Eric Chu","Jack Lindsey","Antoine Jallon","I. M. J. McInnis","Evan Chen","Avi Semler","Luk Gloor","Tej Shah","Marc Carauleanu","Pascal Lauer","Tran Đuc Huy","Hossein Shahrtash","Emilien Duc","Lukas Lewark","Assaf Brown","Samuel Albanie","Brian Weber","Warren S. Vaz","Pierre Clavier","Yiyang Fan","Gabriel Poesia Reis e Silva"," Long"," Lian","Marcus Abramovitch","Xi Jiang","Sandra Mendoza","Murat Islam","Juan Gonzalez","Vasilios Mavroudis","Justin Xu","Pawan Kumar","Laxman Prasad Goswami","Daniel Bugas","Nasser Heydari","Ferenc Jeanplong","Thorben Jansen","Antonella Pinto","Archimedes Apronti","Abdallah Galal","Ng Ze-An","Ankit Singh","Tong Jiang","Joan of Arc Xavier","Kanu Priya Agarwal","Mohammed Berkani","Gang Zhang","Zhehang Du","Benedito Alves de Oliveira Junior","Dmitry Malishev","Nicolas Remy","Taylor D. Hartman","Tim Tarver","Stephen Mensah","Gautier Abou Loume","Wiktor Morak","Farzad Habibi","Sarah Hoback","Will Cai","Javier Gimenez","Roselynn Grace Montecillo","Jakub Łucki","Russell Campbell","Asankhaya Sharma","Khalida Meer","Shreen Gul","Daniel Espinosa Gonzalez","Xavier Alapont","Alex Hoover","Gunjan Chhablani","Freddie Vargus","Arunim Agarwal","Yibo Jiang","Deepakkumar Patil","David Outevsky","Kevin Joseph Scaria","Rajat Maheshwari","Abdelkader Dendane","Priti Shukla","Ashley Cartwright","Sergei Bogdanov","Niels Mündler","Sören Möller","Luca Arnaboldi","Kunvar Thaman","Muhammad Rehan Siddiqi","Prajvi Saxena","Himanshu Gupta","Tony Fruhauff","Glen Sherman","Mátyás Vincze","Siranut Usawasutsakorn","Dylan Ler","Anil Radhakrishnan","Innocent Enyekwe","Sk Md Salauddin","Jiang Muzhen","Aleksandr Maksapetyan","Vivien Rossbach","Chris Harjadi","Mohsen Bahaloohoreh","Claire Sparrow","Jasdeep Sidhu","Sam Ali","Song Bian","John Lai","Eric Singer","Justine Leon Uro","Greg Bateman","Mohamed Sayed","Ahmed Menshawy","Darling Duclosel","Dario Bezzi","Yashaswini Jain","Ashley Aaron","Murat Tiryakioglu","Sheeshram Siddh","Keith Krenek","Imad Ali Shah","Jun Jin","Scott Creighton","Denis Peskoff","Zienab EL-Wasif","Ragavendran P V","Michael Richmond","Joseph McGowan","Tejal Patwardhan","Hao-Yu Sun","Ting Sun","Nikola Zubić","Samuele Sala","Stephen Ebert","Jean Kaddour","Manuel Schottdorf","Dianzhuo Wang","Gerol Petruzella","Alex Meiburg","Tilen Medved","Ali ElSheikh","S Ashwin Hebbar","Lorenzo Vaquero","Xianjun Yang","Jason Poulos","Vilém Zouhar","Sergey Bogdanik","Mingfang Zhang","Jorge Sanz-Ros","David Anugraha","Yinwei Dai","Anh N. Nhu","Xue Wang","Ali Anil Demircali","Zhibai Jia","Yuyin Zhou","Juncheng Wu","Mike He","Nitin Chandok","Aarush Sinha","Gaoxiang Luo","Long Le","Mickaël Noyé","Ioannis Pantidis","Tianbo Qi","Soham Sachin Purohit","Letitia Parcalabescu","Thai-Hoa Nguyen","Genta Indra Winata","Edoardo M. Ponti","Hanchen Li","Kaustubh Dhole","Jongee Park","Dario Abbondanza","Yuanli Wang","Anupam Nayak","Diogo M. Caetano","Antonio A. W. L. Wong","Maria del Rio-Chanona","Dániel Kondor","Pieter Francois","Ed Chalstrey","Jakob Zsambok","Dan Hoyer","Jenny Reddish","Jakob Hauser","Francisco-Javier Rodrigo-Ginés","Suchandra Datta","Maxwell Shepherd","Thom Kamphuis","Qizheng Zhang","Hyunjun Kim","Ruiji Sun","Jianzhu Yao","Franck Dernoncourt","Satyapriya Krishna","Sina Rismanchian","Bonan Pu","Francesco Pinto","Yingheng Wang","Kumar Shridhar","Kalon J. Overholt","Glib Briia","Hieu Nguyen"," David","Soler Bartomeu","Tony CY Pang","Adam Wecker","Yifan Xiong","Fanfei Li","Lukas S. Huber","Joshua Jaeger","Romano De Maddalena","Xing Han Lù","Yuhui Zhang","Claas Beger","Patrick Tser Jern Kon","Sean Li","Vivek Sanker","Ming Yin","Yihao Liang","Xinlu Zhang","Ankit Agrawal","Li S. Yifei","Zechen Zhang","Mu Cai","Yasin Sonmez","Costin Cozianu","Changhao Li","Alex Slen","Shoubin Yu","Hyun Kyu Park","Gabriele Sarti","Marcin Briański","Alessandro Stolfo","Truong An Nguyen","Mike Zhang","Yotam Perlitz","Jose Hernandez-Orallo","Runjia Li","Amin Shabani","Felix Juefei-Xu","Shikhar Dhingra","Orr Zohar","My Chiffon Nguyen","Alexander Pondaven","Abdurrahim Yilmaz","Xuandong Zhao","Chuanyang Jin","Muyan Jiang","Stefan Todoran","Xinyao Han","Jules Kreuer","Brian Rabern","Anna Plassart","Martino Maggetti","Luther Yap","Robert Geirhos","Jonathon Kean","Dingsu Wang","Sina Mollaei","Chenkai Sun","Yifan Yin","Shiqi Wang","Rui Li","Yaowen Chang","Anjiang Wei","Alice Bizeul","Xiaohan Wang","Alexandre Oliveira Arrais","Kushin Mukherjee","Jorge Chamorro-Padial","Jiachen Liu","Xingyu Qu","Junyi Guan","Adam Bouyamourn","Shuyu Wu","Martyna Plomecka","Junda Chen","Mengze Tang","Jiaqi Deng","Shreyas Subramanian","Haocheng Xi","Haoxuan Chen","Weizhi Zhang","Yinuo Ren","Haoqin Tu","Sejong Kim","Yushun Chen","Sara Vera Marjanović","Junwoo Ha","Grzegorz Luczyna","Jeff J. Ma","Zewen Shen","Dawn Song","Cedegao E. Zhang","Zhun Wang","Gaël Gendron","Yunze Xiao","Leo Smucker","Erica Weng","Kwok Hao Lee","Zhe Ye","Stefano Ermon","Ignacio D. Lopez-Miguel","Theo Knights","Anthony Gitter","Namkyu Park","Boyi Wei","Hongzheng Chen","Kunal Pai","Ahmed Elkhanany","Han Lin","Philipp D. Siedler","Jichao Fang","Ritwik Mishra","Károly Zsolnai-Fehér","Xilin Jiang","Shadab Khan","Jun Yuan","Rishab Kumar Jain","Xi Lin","Mike Peterson","Zhe Wang","Aditya Malusare","Maosen Tang","Isha Gupta","Ivan Fosin","Timothy Kang","Barbara Dworakowska","Kazuki Matsumoto","Guangyao Zheng","Gerben Sewuster","Jorge Pretel Villanueva","Ivan Rannev","Igor Chernyavsky","Jiale Chen","Deepayan Banik","Ben Racz","Wenchao Dong","Jianxin Wang","Laila Bashmal","Duarte V. Gonçalves","Wei Hu","Kaushik Bar","Ondrej Bohdal","Atharv Singh Patlan","Shehzaad Dhuliawala","Caroline Geirhos","Julien Wist","Yuval Kansal","Bingsen Chen","Kutay Tire","Atak Talay Yücel","Brandon Christof","Veerupaksh Singla","Zijian Song","Sanxing Chen","Jiaxin Ge","Kaustubh Ponkshe","Isaac Park","Tianneng Shi","Martin Q. Ma","Joshua Mak","Sherwin Lai","Antoine Moulin","Zhuo Cheng","Zhanda Zhu","Ziyi Zhang","Vaidehi Patil","Ketan Jha","Qiutong Men","Jiaxuan Wu","Tianchi Zhang","Bruno Hebling Vieira","Alham Fikri Aji","Jae-Won Chung","Mohammed Mahfoud","Ha Thi Hoang","Marc Sperzel","Wei Hao","Kristof Meding","Sihan Xu","Vassilis Kostakos","Davide Manini","Yueying Liu","Christopher Toukmaji","Jay Paek","Eunmi Yu","Arif Engin Demircali","Zhiyi Sun","Ivan Dewerpe","Hongsen Qin","Roman Pflugfelder","James Bailey","Johnathan Morris","Ville Heilala","Sybille Rosset","Zishun Yu","Peter E. Chen","Woongyeong Yeo","Eeshaan Jain","Ryan Yang","Sreekar Chigurupati","Julia Chernyavsky","Sai Prajwal Reddy","Subhashini Venugopalan","Hunar Batra","Core Francisco Park","Hieu Tran","Guilherme Maximiano","Genghan Zhang","Yizhuo Liang","Hu Shiyu","Rongwu Xu","Rui Pan","Siddharth Suresh","Ziqi Liu","Samaksh Gulati","Songyang Zhang","Peter Turchin","Christopher W. Bartlett","Christopher R. Scotese","Phuong M. Cao","Aakaash Nattanmai","Gordon McKellips","Anish Cheraku","Asim Suhail","Ethan Luo","Marvin Deng","Jason Luo","Ashley Zhang","Kavin Jindel","Jay Paek","Kasper Halevy","Allen Baranov","Michael Liu","Advaith Avadhanam","David Zhang","Vincent Cheng","Brad Ma","Evan Fu","Liam Do","Joshua Lass","Hubert Yang","Surya Sunkari","Vishruth Bharath","Violet Ai","James Leung","Rishit Agrawal","Alan Zhou","Kevin Chen","Tejas Kalpathi","Ziqi Xu","Gavin Wang","Tyler Xiao","Erik Maung","Sam Lee","Ryan Yang","Roy Yue","Ben Zhao","Julia Yoon","Sunny Sun","Aryan Singh","Ethan Luo","Clark Peng","Tyler Osbey","Taozhi Wang","Daryl Echeazu","Hubert Yang","Timothy Wu","Spandan Patel","Vidhi Kulkarni","Vijaykaarti Sundarapandiyan","Ashley Zhang","Andrew Le","Zafir Nasim","Srikar Yalam","Ritesh Kasamsetty","Soham Samal","Hubert Yang","David Sun","Nihar Shah","Abhijeet Saha","Alex Zhang","Leon Nguyen","Laasya Nagumalli","Kaixin Wang","Alan Zhou","Aidan Wu","Jason Luo","Anwith Telluri","Summer Yue","Alexandr Wang","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2501.14249v6.pdf","comment":"29 pages, 6 figures"},{"id":"http://arxiv.org/abs/2504.08231v1","updated":"2025-04-11T03:30:26Z","published":"2025-04-11T03:30:26Z","title":"Out of Style: RAG's Fragility to Linguistic Variation","summary":"  Despite the impressive performance of Retrieval-augmented Generation (RAG)\nsystems across various NLP benchmarks, their robustness in handling real-world\nuser-LLM interaction queries remains largely underexplored. This presents a\ncritical gap for practical deployment, where user queries exhibit greater\nlinguistic variations and can trigger cascading errors across interdependent\nRAG components. In this work, we systematically analyze how varying four\nlinguistic dimensions (formality, readability, politeness, and grammatical\ncorrectness) impact RAG performance. We evaluate two retrieval models and nine\nLLMs, ranging from 3 to 72 billion parameters, across four information-seeking\nQuestion Answering (QA) datasets. Our results reveal that linguistic\nreformulations significantly impact both retrieval and generation stages,\nleading to a relative performance drop of up to 40.41% in Recall@5 scores for\nless formal queries and 38.86% in answer match scores for queries containing\ngrammatical errors. Notably, RAG systems exhibit greater sensitivity to such\nvariations compared to LLM-only generations, highlighting their vulnerability\nto error propagation due to linguistic shifts. These findings highlight the\nneed for improved robustness techniques to enhance reliability in diverse user\ninteractions.\n","authors":["Tianyu Cao","Neel Bhandari","Akhila Yerukola","Akari Asai","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2504.08231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07158v2","updated":"2025-04-11T02:47:17Z","published":"2025-04-09T11:24:32Z","title":"Holistic Capability Preservation: Towards Compact Yet Comprehensive\n  Reasoning Models","summary":"  This technical report presents Ring-Lite-Distill, a lightweight reasoning\nmodel derived from our open-source Mixture-of-Experts (MoE) Large Language\nModels (LLMs) Ling-Lite. This study demonstrates that through meticulous\nhigh-quality data curation and ingenious training paradigms, the compact MoE\nmodel Ling-Lite can be further trained to achieve exceptional reasoning\ncapabilities, while maintaining its parameter-efficient architecture with only\n2.75 billion activated parameters, establishing an efficient lightweight\nreasoning architecture. In particular, in constructing this model, we have not\nmerely focused on enhancing advanced reasoning capabilities, exemplified by\nhigh-difficulty mathematical problem solving, but rather aimed to develop a\nreasoning model with more comprehensive competency coverage. Our approach\nensures coverage across reasoning tasks of varying difficulty levels while\npreserving generic capabilities, such as instruction following, tool use, and\nknowledge retention. We show that, Ring-Lite-Distill's reasoning ability\nreaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general\ncapabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The\nmodels are accessible at https://huggingface.co/inclusionAI\n","authors":[" Ling Team","Caizhi Tang","Chilin Fu","Chunwei Wu","Jia Guo","Jianwen Wang","Jingyu Hu","Liang Jiang","Meng Li","Peng Jiao","Pingping Liu","Shaomian Zheng","Shiwei Liang","Shuaicheng Li","Yalin Zhang","Yingting Wu","Yongkang Liu","Zhenyu Huang"],"pdf_url":"https://arxiv.org/pdf/2504.07158v2.pdf","comment":"Based on the further discussion of the working group, the current\n  version is deemed unsuitable for release. We are currently undertaking\n  further work that is expected to involve significant revisions, but this\n  process will require some additional time. We plan to proceed with the\n  release once these updates have been fully implemented"},{"id":"http://arxiv.org/abs/2504.08213v1","updated":"2025-04-11T02:38:06Z","published":"2025-04-11T02:38:06Z","title":"Big Meaning: Qualitative Analysis on Large Bodies of Data Using AI","summary":"  This study introduces a framework that leverages AI-generated descriptive\ncodes to indicate a text's fecundity--the density of unique human-generated\ncodes--in thematic analysis. Rather than replacing human interpretation,\nAI-generated codes guide the selection of texts likely to yield richer\nqualitative insights. Using a dataset of 2,530 Malaysian news articles on\nrefugee attitudes, we compare AI-selected documents to randomly chosen ones by\nhaving three human coders independently derive codes. The results demonstrate\nthat AI-selected texts exhibit approximately twice the fecundity. Our findings\nsupport the use of AI-generated codes as an effective proxy for identifying\ndocuments with a high potential for meaning-making in thematic analysis.\n","authors":["Samuel Flanders","Melati Nungsari","Mark Cheong Wing Loong"],"pdf_url":"https://arxiv.org/pdf/2504.08213v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2504.07408"},{"id":"http://arxiv.org/abs/2504.06514v2","updated":"2025-04-11T02:36:28Z","published":"2025-04-09T01:25:27Z","title":"Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?","summary":"  We find that the response length of reasoning LLMs, whether trained by\nreinforcement learning or supervised learning, drastically increases for\nill-posed questions with missing premises (MiP), ending up with redundant and\nineffective thinking. This newly introduced scenario exacerbates the general\noverthinking issue to a large extent, which we name as the MiP-Overthinking.\nSuch failures are against the ``test-time scaling law'' but have been widely\nobserved on multiple datasets we curated with MiP, indicating the harm of cheap\noverthinking and a lack of critical thinking. Surprisingly, LLMs not\nspecifically trained for reasoning exhibit much better performance on the MiP\nscenario, producing much shorter responses that quickly identify ill-posed\nqueries. This implies a critical flaw of the current training recipe for\nreasoning LLMs, which does not encourage efficient thinking adequately, leading\nto the abuse of thinking patterns. To further investigate the reasons behind\nsuch failures, we conduct fine-grained analyses of the reasoning length,\noverthinking patterns, and location of critical thinking on different types of\nLLMs. Moreover, our extended ablation study reveals that the overthinking is\ncontagious through the distillation of reasoning models' responses. These\nresults improve the understanding of overthinking and shed novel insights into\nmitigating the problem.\n","authors":["Chenrui Fan","Ming Li","Lichao Sun","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.06514v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08211v1","updated":"2025-04-11T02:34:39Z","published":"2025-04-11T02:34:39Z","title":"LLM for Comparative Narrative Analysis","summary":"  In this paper, we conducted a Multi-Perspective Comparative Narrative\nAnalysis (CNA) on three prominent LLMs: GPT-3.5, PaLM2, and Llama2. We applied\nidentical prompts and evaluated their outputs on specific tasks, ensuring an\nequitable and unbiased comparison between various LLMs. Our study revealed that\nthe three LLMs generated divergent responses to the same prompt, indicating\nnotable discrepancies in their ability to comprehend and analyze the given\ntask. Human evaluation was used as the gold standard, evaluating four\nperspectives to analyze differences in LLM performance.\n","authors":["Leo Kampen","Carlos Rabat Villarreal","Louis Yu","Santu Karmaker","Dongji Feng"],"pdf_url":"https://arxiv.org/pdf/2504.08211v1.pdf","comment":"5 pages, 4 figures, Appendix included"},{"id":"http://arxiv.org/abs/2504.08202v1","updated":"2025-04-11T02:06:58Z","published":"2025-04-11T02:06:58Z","title":"Harnessing the Unseen: The Hidden Influence of Intrinsic Knowledge in\n  Long-Context Language Models","summary":"  Recent advances in long-context models (LCMs), designed to handle extremely\nlong input contexts, primarily focus on utilizing external contextual\ninformation, often leaving the influence of large language models' intrinsic\nknowledge underexplored. In this work, we investigate how this intrinsic\nknowledge affects content generation and demonstrate that its impact becomes\nincreasingly pronounced as context length extends. Furthermore, we show that\nthe model's ability to utilize intrinsic knowledge, which we call intrinsic\nretrieval ability, does not improve simultaneously with its ability to leverage\ncontextual knowledge through extrinsic retrieval ability. Moreover, better\nextrinsic retrieval can interfere with the model's ability to use its own\nknowledge effectively, limiting its full potential. To bridge this gap, we\ndesign a simple yet effective Hybrid Needle-in-a-Haystack test that evaluates\nmodels based on their capabilities across both retrieval abilities, rather than\nsolely emphasizing extrinsic retrieval ability. Our experimental results reveal\nthat Qwen-2.5 models significantly outperform Llama-3.1 models, demonstrating\nsuperior intrinsic retrieval ability. Moreover, even the more powerful\nLlama-3.1-70B-Instruct model fails to exhibit better performance under LCM\nconditions, highlighting the importance of evaluating models from a\ndual-retrieval perspective.\n","authors":["Yu Fu","Haz Sameen Shahgir","Hui Liu","Xianfeng Tang","Qi He","Yue Dong"],"pdf_url":"https://arxiv.org/pdf/2504.08202v1.pdf","comment":"21 pages,11figures"},{"id":"http://arxiv.org/abs/2504.08192v1","updated":"2025-04-11T01:24:03Z","published":"2025-04-11T01:24:03Z","title":"SAEs $\\textit{Can}$ Improve Unlearning: Dynamic Sparse Autoencoder\n  Guardrails for Precision Unlearning in LLMs","summary":"  Machine unlearning is a promising approach to improve LLM safety by removing\nunwanted knowledge from the model. However, prevailing gradient-based\nunlearning methods suffer from issues such as high computational costs,\nhyperparameter instability, poor sequential unlearning capability,\nvulnerability to relearning attacks, low data efficiency, and lack of\ninterpretability. While Sparse Autoencoders are well-suited to improve these\naspects by enabling targeted activation-based unlearning, prior approaches\nunderperform gradient-based methods. This work demonstrates that, contrary to\nthese earlier findings, SAEs can significantly improve unlearning when employed\ndynamically. We introduce $\\textbf{Dynamic DAE Guardrails}$ (DSG), a novel\nmethod for precision unlearning that leverages principled feature selection and\na dynamic classifier. Our experiments show DSG substantially outperforms\nleading unlearning methods, achieving superior forget-utility trade-offs. DSG\naddresses key drawbacks of gradient-based approaches for unlearning -- offering\nenhanced computational efficiency and stability, robust performance in\nsequential unlearning, stronger resistance to relearning attacks, better data\nefficiency including zero-shot settings, and more interpretable unlearning.\n","authors":["Aashiq Muhamed","Jacopo Bonato","Mona Diab","Virginia Smith"],"pdf_url":"https://arxiv.org/pdf/2504.08192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01869v2","updated":"2025-04-11T00:37:11Z","published":"2025-02-25T21:50:46Z","title":"From Small to Large Language Models: Revisiting the Federalist Papers","summary":"  For a long time, the authorship of the Federalist Papers had been a subject\nof inquiry and debate, not only by linguists and historians but also by\nstatisticians. In what was arguably the first Bayesian case study, Mosteller\nand Wallace (1963) provided the first statistical evidence for attributing all\ndisputed papers to Madison. Our paper revisits this historical dataset but from\na lens of modern language models, both small and large. We review some of the\nmore popular Large Language Model (LLM) tools and examine them from a\nstatistical point of view in the context of text classification. We investigate\nwhether, without any attempt to fine-tune, the general embedding constructs can\nbe useful for stylometry and attribution. We explain differences between\nvarious word/phrase embeddings and discuss how to aggregate them in a document.\nContrary to our expectations, we exemplify that dimension expansion with word\nembeddings may not always be beneficial for attribution relative to dimension\nreduction with topic embeddings. Our experiments demonstrate that default LLM\nembeddings (even after manual fine-tuning) may not consistently improve\nauthorship attribution accuracy. Instead, Bayesian analysis with topic\nembeddings trained on ``function words\" yields superior out-of-sample\nclassification performance. This suggests that traditional (small) statistical\nlanguage models, with their interpretability and solid theoretical foundation,\ncan offer significant advantages in authorship attribution tasks. The code used\nin this analysis is available at github.com/sowonjeong/slm-to-llm\n","authors":["So Won Jeong","Veronika Ročková"],"pdf_url":"https://arxiv.org/pdf/2503.01869v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2504.08736v1","updated":"2025-04-11T17:59:58Z","published":"2025-04-11T17:59:58Z","title":"GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for\n  Autoregressive Image Generation","summary":"  In autoregressive (AR) image generation, visual tokenizers compress images\ninto compact discrete latent tokens, enabling efficient training of downstream\nautoregressive models for visual generation via next-token prediction. While\nscaling visual tokenizers improves image reconstruction quality, it often\ndegrades downstream generation quality -- a challenge not adequately addressed\nin existing literature. To address this, we introduce GigaTok, the first\napproach to simultaneously improve image reconstruction, generation, and\nrepresentation learning when scaling visual tokenizers. We identify the growing\ncomplexity of latent space as the key factor behind the reconstruction vs.\ngeneration dilemma. To mitigate this, we propose semantic regularization, which\naligns tokenizer features with semantically consistent features from a\npre-trained visual encoder. This constraint prevents excessive latent space\ncomplexity during scaling, yielding consistent improvements in both\nreconstruction and downstream autoregressive generation. Building on semantic\nregularization, we explore three key practices for scaling tokenizers:(1) using\n1D tokenizers for better scalability, (2) prioritizing decoder scaling when\nexpanding both encoder and decoder, and (3) employing entropy loss to stabilize\ntraining for billion-scale tokenizers. By scaling to $\\bf{3 \\space billion}$\nparameters, GigaTok achieves state-of-the-art performance in reconstruction,\ndownstream AR generation, and downstream AR representation quality.\n","authors":["Tianwei Xiong","Jun Hao Liew","Zilong Huang","Jiashi Feng","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2504.08736v1.pdf","comment":"project page: https://silentview.github.io/GigaTok"},{"id":"http://arxiv.org/abs/2504.08729v1","updated":"2025-04-11T17:56:09Z","published":"2025-04-11T17:56:09Z","title":"Steering CLIP's vision transformer with sparse autoencoders","summary":"  While vision models are highly capable, their internal mechanisms remain\npoorly understood -- a challenge which sparse autoencoders (SAEs) have helped\naddress in language, but which remains underexplored in vision. We address this\ngap by training SAEs on CLIP's vision transformer and uncover key differences\nbetween vision and language processing, including distinct sparsity patterns\nfor SAEs trained across layers and token types. We then provide the first\nsystematic analysis on the steerability of CLIP's vision transformer by\nintroducing metrics to quantify how precisely SAE features can be steered to\naffect the model's output. We find that 10-15\\% of neurons and features are\nsteerable, with SAEs providing thousands more steerable features than the base\nmodel. Through targeted suppression of SAE features, we then demonstrate\nimproved performance on three vision disentanglement tasks (CelebA, Waterbirds,\nand typographic attacks), finding optimal disentanglement in middle model\nlayers, and achieving state-of-the-art performance on defense against\ntypographic attacks.\n","authors":["Sonia Joseph","Praneet Suresh","Ethan Goldfarb","Lorenz Hufe","Yossi Gandelsman","Robert Graham","Danilo Bzdok","Wojciech Samek","Blake Aaron Richards"],"pdf_url":"https://arxiv.org/pdf/2504.08729v1.pdf","comment":"8 pages, 7 figures. Accepted to the CVPR 2025 Workshop on Mechanistic\n  Interpretability for Vision (MIV)"},{"id":"http://arxiv.org/abs/2504.08727v1","updated":"2025-04-11T17:55:45Z","published":"2025-04-11T17:55:45Z","title":"Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images","summary":"  We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles.\n","authors":["Boyang Deng","Songyou Peng","Kyle Genova","Gordon Wetzstein","Noah Snavely","Leonidas Guibas","Thomas Funkhouser"],"pdf_url":"https://arxiv.org/pdf/2504.08727v1.pdf","comment":"Project page: https://boyangdeng.com/visual-chronicles; second and\n  third listed authors have equal contributions"},{"id":"http://arxiv.org/abs/2504.08718v1","updated":"2025-04-11T17:30:46Z","published":"2025-04-11T17:30:46Z","title":"EMO-X: Efficient Multi-Person Pose and Shape Estimation in One-Stage","summary":"  Expressive Human Pose and Shape Estimation (EHPS) aims to jointly estimate\nhuman pose, hand gesture, and facial expression from monocular images. Existing\nmethods predominantly rely on Transformer-based architectures, which suffer\nfrom quadratic complexity in self-attention, leading to substantial\ncomputational overhead, especially in multi-person scenarios. Recently, Mamba\nhas emerged as a promising alternative to Transformers due to its efficient\nglobal modeling capability. However, it remains limited in capturing\nfine-grained local dependencies, which are essential for precise EHPS. To\naddress these issues, we propose EMO-X, the Efficient Multi-person One-stage\nmodel for multi-person EHPS. Specifically, we explore a Scan-based Global-Local\nDecoder (SGLD) that integrates global context with skeleton-aware local\nfeatures to iteratively enhance human tokens. Our EMO-X leverages the superior\nglobal modeling capability of Mamba and designs a local bidirectional scan\nmechanism for skeleton-aware local refinement. Comprehensive experiments\ndemonstrate that EMO-X strikes an excellent balance between efficiency and\naccuracy. Notably, it achieves a significant reduction in computational\ncomplexity, requiring 69.8% less inference time compared to state-of-the-art\n(SOTA) methods, while outperforming most of them in accuracy.\n","authors":["Haohang Jian","Jinlu Zhang","Junyi Wu","Zhigang Tu"],"pdf_url":"https://arxiv.org/pdf/2504.08718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08714v1","updated":"2025-04-11T17:24:58Z","published":"2025-04-11T17:24:58Z","title":"Generating Fine Details of Entity Interactions","summary":"  Images not only depict objects but also encapsulate rich interactions between\nthem. However, generating faithful and high-fidelity images involving multiple\nentities interacting with each other, is a long-standing challenge. While\npre-trained text-to-image models are trained on large-scale datasets to follow\ndiverse text instructions, they struggle to generate accurate interactions,\nlikely due to the scarcity of training data for uncommon object interactions.\nThis paper introduces InterActing, an interaction-focused dataset with 1000\nfine-grained prompts covering three key scenarios: (1) functional and\naction-based interactions, (2) compositional spatial relationships, and (3)\nmulti-subject interactions. To address interaction generation challenges, we\npropose a decomposition-augmented refinement procedure. Our approach,\nDetailScribe, built on Stable Diffusion 3.5, leverages LLMs to decompose\ninteractions into finer-grained concepts, uses a VLM to critique generated\nimages, and applies targeted interventions within the diffusion process in\nrefinement. Automatic and human evaluations show significantly improved image\nquality, demonstrating the potential of enhanced inference strategies. Our\ndataset and code are available at https://concepts-ai.com/p/detailscribe/ to\nfacilitate future exploration of interaction-rich image generation.\n","authors":["Xinyi Gu","Jiayuan Mao"],"pdf_url":"https://arxiv.org/pdf/2504.08714v1.pdf","comment":"Project Page: https://concepts-ai.com/p/detailscribe/"},{"id":"http://arxiv.org/abs/2504.08710v1","updated":"2025-04-11T17:20:26Z","published":"2025-04-11T17:20:26Z","title":"Hypergraph Vision Transformers: Images are More than Nodes, More than\n  Edges","summary":"  Recent advancements in computer vision have highlighted the scalability of\nVision Transformers (ViTs) across various tasks, yet challenges remain in\nbalancing adaptability, computational efficiency, and the ability to model\nhigher-order relationships. Vision Graph Neural Networks (ViGs) offer an\nalternative by leveraging graph-based methodologies but are hindered by the\ncomputational bottlenecks of clustering algorithms used for edge generation. To\naddress these issues, we propose the Hypergraph Vision Transformer (HgVT),\nwhich incorporates a hierarchical bipartite hypergraph structure into the\nvision transformer framework to capture higher-order semantic relationships\nwhile maintaining computational efficiency. HgVT leverages population and\ndiversity regularization for dynamic hypergraph construction without\nclustering, and expert edge pooling to enhance semantic extraction and\nfacilitate graph-based image retrieval. Empirical results demonstrate that HgVT\nachieves strong performance on image classification and retrieval, positioning\nit as an efficient framework for semantic-based vision tasks.\n","authors":["Joshua Fixelle"],"pdf_url":"https://arxiv.org/pdf/2504.08710v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2504.08685v1","updated":"2025-04-11T16:46:20Z","published":"2025-04-11T16:46:20Z","title":"Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model","summary":"  This technical report presents a cost-efficient strategy for training a video\ngeneration foundation model. We present a mid-sized research model with\napproximately 7 billion parameters (7B) called Seaweed-7B trained from scratch\nusing 665,000 H100 GPU hours. Despite being trained with moderate computational\nresources, Seaweed-7B demonstrates highly competitive performance compared to\ncontemporary video generation models of much larger size. Design choices are\nespecially crucial in a resource-constrained setting. This technical report\nhighlights the key design decisions that enhance the performance of the\nmedium-sized diffusion model. Empirically, we make two observations: (1)\nSeaweed-7B achieves performance comparable to, or even surpasses, larger models\ntrained on substantially greater GPU resources, and (2) our model, which\nexhibits strong generalization ability, can be effectively adapted across a\nwide range of downstream applications either by lightweight fine-tuning or\ncontinue training. See the project page at https://seaweed.video/\n","authors":["Team Seawead","Ceyuan Yang","Zhijie Lin","Yang Zhao","Shanchuan Lin","Zhibei Ma","Haoyuan Guo","Hao Chen","Lu Qi","Sen Wang","Feng Cheng","Feilong Zuo Xuejiao Zeng","Ziyan Yang","Fangyuan Kong","Zhiwu Qing","Fei Xiao","Meng Wei","Tuyen Hoang","Siyu Zhang","Peihao Zhu","Qi Zhao","Jiangqiao Yan","Liangke Gui","Sheng Bi","Jiashi Li","Yuxi Ren","Rui Wang","Huixia Li","Xuefeng Xiao","Shu Liu","Feng Ling","Heng Zhang","Houmin Wei","Huafeng Kuang","Jerry Duncan","Junda Zhang","Junru Zheng","Li Sun","Manlin Zhang","Renfei Sun","Xiaobin Zhuang","Xiaojie Li","Xin Xia","Xuyan Chi","Yanghua Peng","Yuping Wang","Yuxuan Wang","Zhongkai Zhao","Zhuo Chen","Zuquan Song","Zhenheng Yang","Jiashi Feng","Jianchao Yang","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2504.08685v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2412.16739v2","updated":"2025-04-11T16:41:57Z","published":"2024-12-21T19:01:57Z","title":"UNEM: UNrolled Generalized EM for Transductive Few-Shot Learning","summary":"  Transductive few-shot learning has recently triggered wide attention in\ncomputer vision. Yet, current methods introduce key hyper-parameters, which\ncontrol the prediction statistics of the test batches, such as the level of\nclass balance, affecting performances significantly. Such hyper-parameters are\nempirically grid-searched over validation data, and their configurations may\nvary substantially with the target dataset and pre-training model, making such\nempirical searches both sub-optimal and computationally intractable. In this\nwork, we advocate and introduce the unrolling paradigm, also referred to as\n\"learning to optimize\", in the context of few-shot learning, thereby learning\nefficiently and effectively a set of optimized hyper-parameters. Specifically,\nwe unroll a generalization of the ubiquitous Expectation-Maximization (EM)\noptimizer into a neural network architecture, mapping each of its iterates to a\nlayer and learning a set of key hyper-parameters over validation data. Our\nunrolling approach covers various statistical feature distributions and\npre-training paradigms, including recent foundational vision-language models\nand standard vision-only classifiers. We report comprehensive experiments,\nwhich cover a breadth of fine-grained downstream image classification tasks,\nshowing significant gains brought by the proposed unrolled EM algorithm over\niterative variants. The achieved improvements reach up to 10% and 7.5% on\nvision-only and vision-language benchmarks, respectively.\n","authors":["Long Zhou","Fereshteh Shakeri","Aymen Sadraoui","Mounir Kaaniche","Jean-Christophe Pesquet","Ismail Ben Ayed"],"pdf_url":"https://arxiv.org/pdf/2412.16739v2.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2411.03976v2","updated":"2025-04-11T16:33:11Z","published":"2024-11-06T15:13:31Z","title":"HRDecoder: High-Resolution Decoder Network for Fundus Image Lesion\n  Segmentation","summary":"  High resolution is crucial for precise segmentation in fundus images, yet\nhandling high-resolution inputs incurs considerable GPU memory costs, with\ndiminishing performance gains as overhead increases. To address this issue\nwhile tackling the challenge of segmenting tiny objects, recent studies have\nexplored local-global fusion methods. These methods preserve fine details using\nlocal regions and capture long-range context information from downscaled global\nimages. However, the necessity of multiple forward passes inevitably incurs\nsignificant computational overhead, adversely affecting inference speed. In\nthis paper, we propose HRDecoder, a simple High-Resolution Decoder network for\nfundus lesion segmentation. It integrates a high-resolution representation\nlearning module to capture fine-grained local features and a high-resolution\nfusion module to fuse multi-scale predictions. Our method effectively improves\nthe overall segmentation accuracy of fundus lesions while consuming reasonable\nmemory and computational overhead, and maintaining satisfying inference speed.\nExperimental results on the IDRiD and DDR datasets demonstrate the\neffectiveness of our method. Code is available at\nhttps://github.com/CVIU-CSU/HRDecoder.\n","authors":["Ziyuan Ding","Yixiong Liang","Shichao Kan","Qing Liu"],"pdf_url":"https://arxiv.org/pdf/2411.03976v2.pdf","comment":"11 pages, 3 figures, accepted by MICCAI 2024, the revised version"},{"id":"http://arxiv.org/abs/2504.08675v1","updated":"2025-04-11T16:29:54Z","published":"2025-04-11T16:29:54Z","title":"X2BR: High-Fidelity 3D Bone Reconstruction from a Planar X-Ray Image\n  with Hybrid Neural Implicit Methods","summary":"  Accurate 3D bone reconstruction from a single planar X-ray remains a\nchallenge due to anatomical complexity and limited input data. We propose X2BR,\na hybrid neural implicit framework that combines continuous volumetric\nreconstruction with template-guided non-rigid registration. The core network,\nX2B, employs a ConvNeXt-based encoder to extract spatial features from X-rays\nand predict high-fidelity 3D bone occupancy fields without relying on\nstatistical shape models. To further refine anatomical accuracy, X2BR\nintegrates a patient-specific template mesh, constructed using YOLOv9-based\ndetection and the SKEL biomechanical skeleton model. The coarse reconstruction\nis aligned to the template using geodesic-based coherent point drift, enabling\nanatomically consistent 3D bone volumes. Experimental results on a clinical\ndataset show that X2B achieves the highest numerical accuracy, with an IoU of\n0.952 and Chamfer-L1 distance of 0.005, outperforming recent baselines\nincluding X2V and D2IM-Net. Building on this, X2BR incorporates anatomical\npriors via YOLOv9-based bone detection and biomechanical template alignment,\nleading to reconstructions that, while slightly lower in IoU (0.875), offer\nsuperior anatomical realism, especially in rib curvature and vertebral\nalignment. This numerical accuracy vs. visual consistency trade-off between X2B\nand X2BR highlights the value of hybrid frameworks for clinically relevant 3D\nreconstructions.\n","authors":["Gokce Guven","H. Fatih Ugurdag","Hasan F. Ates"],"pdf_url":"https://arxiv.org/pdf/2504.08675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20964v2","updated":"2025-04-11T16:02:25Z","published":"2025-02-28T11:25:38Z","title":"Fine-Grained Retrieval-Augmented Generation for Visual Question\n  Answering","summary":"  Visual Question Answering (VQA) focuses on providing answers to natural\nlanguage questions by utilizing information from images. Although cutting-edge\nmultimodal large language models (MLLMs) such as GPT-4o achieve strong\nperformance on VQA tasks, they frequently fall short in accessing\ndomain-specific or the latest knowledge. To mitigate this issue,\nretrieval-augmented generation (RAG) leveraging external knowledge bases (KBs),\nreferred to as KB-VQA, emerges as a promising approach. Nevertheless,\nconventional unimodal retrieval techniques, which translate images into textual\ndescriptions, often result in the loss of critical visual details. This study\npresents fine-grained knowledge units, which merge textual snippets with entity\nimages stored in vector databases. Furthermore, we introduce a knowledge unit\nretrieval-augmented generation framework (KU-RAG) that integrates fine-grained\nretrieval with MLLMs. The proposed KU-RAG framework ensures precise retrieval\nof relevant knowledge and enhances reasoning capabilities through a knowledge\ncorrection chain. Experimental findings demonstrate that our approach\nsignificantly boosts the performance of leading KB-VQA methods, achieving an\naverage improvement of approximately 3% and up to 11% in the best case.\n","authors":["Zhengxuan Zhang","Yin Wu","Yuyu Luo","Nan Tang"],"pdf_url":"https://arxiv.org/pdf/2502.20964v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08654v1","updated":"2025-04-11T15:58:31Z","published":"2025-04-11T15:58:31Z","title":"The Invisible EgoHand: 3D Hand Forecasting through EgoBody Pose\n  Estimation","summary":"  Forecasting hand motion and pose from an egocentric perspective is essential\nfor understanding human intention. However, existing methods focus solely on\npredicting positions without considering articulation, and only when the hands\nare visible in the field of view. This limitation overlooks the fact that\napproximate hand positions can still be inferred even when they are outside the\ncamera's view. In this paper, we propose a method to forecast the 3D\ntrajectories and poses of both hands from an egocentric video, both in and out\nof the field of view. We propose a diffusion-based transformer architecture for\nEgocentric Hand Forecasting, EgoH4, which takes as input the observation\nsequence and camera poses, then predicts future 3D motion and poses for both\nhands of the camera wearer. We leverage full-body pose information, allowing\nother joints to provide constraints on hand motion. We denoise the hand and\nbody joints along with a visibility predictor for hand joints and a 3D-to-2D\nreprojection loss that minimizes the error when hands are in-view. We evaluate\nEgoH4 on the Ego-Exo4D dataset, combining subsets with body and hand\nannotations. We train on 156K sequences and evaluate on 34K sequences,\nrespectively. EgoH4 improves the performance by 3.4cm and 5.1cm over the\nbaseline in terms of ADE for hand trajectory forecasting and MPJPE for hand\npose forecasting. Project page: https://masashi-hatano.github.io/EgoH4/\n","authors":["Masashi Hatano","Zhifan Zhu","Hideo Saito","Dima Damen"],"pdf_url":"https://arxiv.org/pdf/2504.08654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05636v2","updated":"2025-04-11T15:53:20Z","published":"2025-04-08T03:29:40Z","title":"A Multi-Modal AI System for Screening Mammography: Integrating 2D and 3D\n  Imaging to Improve Breast Cancer Detection in a Prospective Clinical Study","summary":"  Although digital breast tomosynthesis (DBT) improves diagnostic performance\nover full-field digital mammography (FFDM), false-positive recalls remain a\nconcern in breast cancer screening. We developed a multi-modal artificial\nintelligence system integrating FFDM, synthetic mammography, and DBT to provide\nbreast-level predictions and bounding-box localizations of suspicious findings.\nOur AI system, trained on approximately 500,000 mammography exams, achieved\n0.945 AUROC on an internal test set. It demonstrated capacity to reduce recalls\nby 31.7% and radiologist workload by 43.8% while maintaining 100% sensitivity,\nunderscoring its potential to improve clinical workflows. External validation\nconfirmed strong generalizability, reducing the gap to a perfect AUROC by\n35.31%-69.14% relative to strong baselines. In prospective deployment across 18\nsites, the system reduced recall rates for low-risk cases. An improved version,\ntrained on over 750,000 exams with additional labels, further reduced the gap\nby 18.86%-56.62% across large external datasets. Overall, these results\nunderscore the importance of utilizing all available imaging modalities,\ndemonstrate the potential for clinical impact, and indicate feasibility of\nfurther reduction of the test error with increased training set when using\nlarge-capacity neural networks.\n","authors":["Jungkyu Park","Jan Witowski","Yanqi Xu","Hari Trivedi","Judy Gichoya","Beatrice Brown-Mulry","Malte Westerhoff","Linda Moy","Laura Heacock","Alana Lewin","Krzysztof J. Geras"],"pdf_url":"https://arxiv.org/pdf/2504.05636v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08646v1","updated":"2025-04-11T15:45:23Z","published":"2025-04-11T15:45:23Z","title":"MBE-ARI: A Multimodal Dataset Mapping Bi-directional Engagement in\n  Animal-Robot Interaction","summary":"  Animal-robot interaction (ARI) remains an unexplored challenge in robotics,\nas robots struggle to interpret the complex, multimodal communication cues of\nanimals, such as body language, movement, and vocalizations. Unlike human-robot\ninteraction, which benefits from established datasets and frameworks,\nanimal-robot interaction lacks the foundational resources needed to facilitate\nmeaningful bidirectional communication. To bridge this gap, we present the\nMBE-ARI (Multimodal Bidirectional Engagement in Animal-Robot Interaction), a\nnovel multimodal dataset that captures detailed interactions between a legged\nrobot and cows. The dataset includes synchronized RGB-D streams from multiple\nviewpoints, annotated with body pose and activity labels across interaction\nphases, offering an unprecedented level of detail for ARI research.\nAdditionally, we introduce a full-body pose estimation model tailored for\nquadruped animals, capable of tracking 39 keypoints with a mean average\nprecision (mAP) of 92.7%, outperforming existing benchmarks in animal pose\nestimation. The MBE-ARI dataset and our pose estimation framework lay a robust\nfoundation for advancing research in animal-robot interaction, providing\nessential tools for developing perception, reasoning, and interaction\nframeworks needed for effective collaboration between robots and animals. The\ndataset and resources are publicly available at\nhttps://github.com/RISELabPurdue/MBE-ARI/, inviting further exploration and\ndevelopment in this critical area.\n","authors":["Ian Noronha","Advait Prasad Jawaji","Juan Camilo Soto","Jiajun An","Yan Gu","Upinder Kaur"],"pdf_url":"https://arxiv.org/pdf/2504.08646v1.pdf","comment":"Accepted to ICRA 2025"},{"id":"http://arxiv.org/abs/2504.08645v1","updated":"2025-04-11T15:45:17Z","published":"2025-04-11T15:45:17Z","title":"Title block detection and information extraction for enhanced building\n  drawings search","summary":"  The architecture, engineering, and construction (AEC) industry still heavily\nrelies on information stored in drawings for building construction,\nmaintenance, compliance and error checks. However, information extraction (IE)\nfrom building drawings is often time-consuming and costly, especially when\ndealing with historical buildings. Drawing search can be simplified by\nleveraging the information stored in the title block portion of the drawing,\nwhich can be seen as drawing metadata. However, title block IE can be complex\nespecially when dealing with historical drawings which do not follow existing\nstandards for uniformity. This work performs a comparison of existing methods\nfor this kind of IE task, and then proposes a novel title block detection and\nIE pipeline which outperforms existing methods, in particular when dealing with\ncomplex, noisy historical drawings. The pipeline is obtained by combining a\nlightweight Convolutional Neural Network and GPT-4o, the proposed inference\npipeline detects building engineering title blocks with high accuracy, and then\nextract structured drawing metadata from the title blocks, which can be used\nfor drawing search, filtering and grouping. The work demonstrates high accuracy\nand efficiency in IE for both vector (CAD) and hand-drawn (historical)\ndrawings. A user interface (UI) that leverages the extracted metadata for\ndrawing search is established and deployed on real projects, which demonstrates\nsignificant time savings. Additionally, an extensible domain-expert-annotated\ndataset for title block detection is developed, via an efficient AEC-friendly\nannotation workflow that lays the foundation for future work.\n","authors":["Alessio Lombardi","Li Duan","Ahmed Elnagar","Ahmed Zaalouk","Khalid Ismail","Edlira Vakaj"],"pdf_url":"https://arxiv.org/pdf/2504.08645v1.pdf","comment":"8 pages, 8 figures, 1 table. Accepted for publication in the 2025\n  European Conference on Computing in Construction (EC3,\n  https://ec-3.org/conference2025/)"},{"id":"http://arxiv.org/abs/2504.08641v1","updated":"2025-04-11T15:41:43Z","published":"2025-04-11T15:41:43Z","title":"Training-free Guidance in Text-to-Video Generation via Multimodal\n  Planning and Structured Noise Initialization","summary":"  Recent advancements in text-to-video (T2V) diffusion models have\nsignificantly enhanced the visual quality of the generated videos. However,\neven recent T2V models find it challenging to follow text descriptions\naccurately, especially when the prompt requires accurate control of spatial\nlayouts or object trajectories. A recent line of research uses layout guidance\nfor T2V models that require fine-tuning or iterative manipulation of the\nattention map during inference time. This significantly increases the memory\nrequirement, making it difficult to adopt a large T2V model as a backbone. To\naddress this, we introduce Video-MSG, a training-free Guidance method for T2V\ngeneration based on Multimodal planning and Structured noise initialization.\nVideo-MSG consists of three steps, where in the first two steps, Video-MSG\ncreates Video Sketch, a fine-grained spatio-temporal plan for the final video,\nspecifying background, foreground, and object trajectories, in the form of\ndraft video frames. In the last step, Video-MSG guides a downstream T2V\ndiffusion model with Video Sketch through noise inversion and denoising.\nNotably, Video-MSG does not need fine-tuning or attention manipulation with\nadditional memory during inference time, making it easier to adopt large T2V\nmodels. Video-MSG demonstrates its effectiveness in enhancing text alignment\nwith multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V\ngeneration benchmarks (T2VCompBench and VBench). We provide comprehensive\nablation studies about noise inversion ratio, different background generators,\nbackground object detection, and foreground object segmentation.\n","authors":["Jialu Li","Shoubin Yu","Han Lin","Jaemin Cho","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2504.08641v1.pdf","comment":"Website: https://video-msg.github.io; The first three authors\n  contributed equally"},{"id":"http://arxiv.org/abs/2504.08635v1","updated":"2025-04-11T15:37:46Z","published":"2025-04-11T15:37:46Z","title":"Latent Diffusion Autoencoders: Toward Efficient and Meaningful\n  Unsupervised Representation Learning in Medical Imaging","summary":"  This study presents Latent Diffusion Autoencoder (LDAE), a novel\nencoder-decoder diffusion-based framework for efficient and meaningful\nunsupervised learning in medical imaging, focusing on Alzheimer disease (AD)\nusing brain MR from the ADNI database as a case study. Unlike conventional\ndiffusion autoencoders operating in image space, LDAE applies the diffusion\nprocess in a compressed latent representation, improving computational\nefficiency and making 3D medical imaging representation learning tractable. To\nvalidate the proposed approach, we explore two key hypotheses: (i) LDAE\neffectively captures meaningful semantic representations on 3D brain MR\nassociated with AD and ageing, and (ii) LDAE achieves high-quality image\ngeneration and reconstruction while being computationally efficient.\nExperimental results support both hypotheses: (i) linear-probe evaluations\ndemonstrate promising diagnostic performance for AD (ROC-AUC: 90%, ACC: 84%)\nand age prediction (MAE: 4.1 years, RMSE: 5.2 years); (ii) the learned semantic\nrepresentations enable attribute manipulation, yielding anatomically plausible\nmodifications; (iii) semantic interpolation experiments show strong\nreconstruction of missing scans, with SSIM of 0.969 (MSE: 0.0019) for a 6-month\ngap. Even for longer gaps (24 months), the model maintains robust performance\n(SSIM > 0.93, MSE < 0.004), indicating an ability to capture temporal\nprogression trends; (iv) compared to conventional diffusion autoencoders, LDAE\nsignificantly increases inference throughput (20x faster) while also enhancing\nreconstruction quality. These findings position LDAE as a promising framework\nfor scalable medical imaging applications, with the potential to serve as a\nfoundation model for medical image analysis. Code available at\nhttps://github.com/GabrieleLozupone/LDAE\n","authors":["Gabriele Lozupone","Alessandro Bria","Francesco Fontanella","Frederick J. A. Meijer","Claudio De Stefano","Henkjan Huisman"],"pdf_url":"https://arxiv.org/pdf/2504.08635v1.pdf","comment":"15 pages, 9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2504.08626v1","updated":"2025-04-11T15:27:29Z","published":"2025-04-11T15:27:29Z","title":"Task-conditioned Ensemble of Expert Models for Continuous Learning","summary":"  One of the major challenges in machine learning is maintaining the accuracy\nof the deployed model (e.g., a classifier) in a non-stationary environment. The\nnon-stationary environment results in distribution shifts and, consequently, a\ndegradation in accuracy. Continuous learning of the deployed model with new\ndata could be one remedy. However, the question arises as to how we should\nupdate the model with new training data so that it retains its accuracy on the\nold data while adapting to the new data. In this work, we propose a\ntask-conditioned ensemble of models to maintain the performance of the existing\nmodel. The method involves an ensemble of expert models based on task\nmembership information. The in-domain models-based on the local outlier concept\n(different from the expert models) provide task membership information\ndynamically at run-time to each probe sample. To evaluate the proposed method,\nwe experiment with three setups: the first represents distribution shift\nbetween tasks (LivDet-Iris-2017), the second represents distribution shift both\nbetween and within tasks (LivDet-Iris-2020), and the third represents disjoint\ndistribution between tasks (Split MNIST). The experiments highlight the\nbenefits of the proposed method. The source code is available at\nhttps://github.com/iPRoBe-lab/Continuous_Learning_FE_DM.\n","authors":["Renu Sharma","Debasmita Pal","Arun Ross"],"pdf_url":"https://arxiv.org/pdf/2504.08626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08620v1","updated":"2025-04-11T15:25:36Z","published":"2025-04-11T15:25:36Z","title":"Efficient Mixture of Geographical Species for On Device Wildlife\n  Monitoring","summary":"  Efficient on-device models have become attractive for near-sensor insight\ngeneration, of particular interest to the ecological conservation community.\nFor this reason, deep learning researchers are proposing more approaches to\ndevelop lower compute models. However, since vision transformers are very new\nto the edge use case, there are still unexplored approaches, most notably\nconditional execution of subnetworks based on input data. In this work, we\nexplore the training of a single species detector which uses conditional\ncomputation to bias structured sub networks in a geographically-aware manner.\nWe propose a method for pruning the expert model per location and demonstrate\nconditional computation performance on two geographically distributed datasets:\niNaturalist and iWildcam.\n","authors":["Emmanuel Azuh Mensah","Joban Mand","Yueheng Ou","Min Jang","Kurtis Heimerl"],"pdf_url":"https://arxiv.org/pdf/2504.08620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08616v1","updated":"2025-04-11T15:21:12Z","published":"2025-04-11T15:21:12Z","title":"Preserving Privacy Without Compromising Accuracy: Machine Unlearning for\n  Handwritten Text Recognition","summary":"  Handwritten Text Recognition (HTR) is essential for document analysis and\ndigitization. However, handwritten data often contains user-identifiable\ninformation, such as unique handwriting styles and personal lexicon choices,\nwhich can compromise privacy and erode trust in AI services. Legislation like\nthe ``right to be forgotten'' underscores the necessity for methods that can\nexpunge sensitive information from trained models. Machine unlearning addresses\nthis by selectively removing specific data from models without necessitating\ncomplete retraining. Yet, it frequently encounters a privacy-accuracy tradeoff,\nwhere safeguarding privacy leads to diminished model performance. In this\npaper, we introduce a novel two-stage unlearning strategy for a multi-head\ntransformer-based HTR model, integrating pruning and random labeling. Our\nproposed method utilizes a writer classification head both as an indicator and\na trigger for unlearning, while maintaining the efficacy of the recognition\nhead. To our knowledge, this represents the first comprehensive exploration of\nmachine unlearning within HTR tasks. We further employ Membership Inference\nAttacks (MIA) to evaluate the effectiveness of unlearning user-identifiable\ninformation. Extensive experiments demonstrate that our approach effectively\npreserves privacy while maintaining model accuracy, paving the way for new\nresearch directions in the document analysis community. Our code will be\npublicly available upon acceptance.\n","authors":["Lei Kang","Xuanshuo Fu","Lluis Gomez","Alicia Fornés","Ernest Valveny","Dimosthenis Karatzas"],"pdf_url":"https://arxiv.org/pdf/2504.08616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08613v1","updated":"2025-04-11T15:20:08Z","published":"2025-04-11T15:20:08Z","title":"Enhancing knowledge retention for continual learning with\n  domain-specific adapters and features gating","summary":"  Continual learning empowers models to learn from a continuous stream of data\nwhile preserving previously acquired knowledge, effectively addressing the\nchallenge of catastrophic forgetting. In this study, we propose a new approach\nthat integrates adapters within the self-attention mechanisms of Vision\nTransformers to enhance knowledge retention when sequentially adding datasets\nfrom different domains. Unlike previous methods that continue learning with\nonly one dataset, our approach introduces domain-specific output heads and\nfeature gating, allowing the model to maintain high accuracy on previously\nlearned tasks while incorporating only the essential information from multiple\ndomains. The proposed method is compared to prominent parameter-efficient\nfine-tuning methods in the current state of the art. The results provide\nevidence that our method effectively alleviates the limitations of previous\nworks. Furthermore, we conduct a comparative analysis using three datasets,\nCIFAR-100, Flowers102, and DTD, each representing a distinct domain, to\ninvestigate the impact of task order on model performance. Our findings\nunderscore the critical role of dataset sequencing in shaping learning\noutcomes, demonstrating that strategic ordering can significantly improve the\nmodel's ability to adapt to evolving data distributions over time while\npreserving the integrity of previously learned knowledge.\n","authors":["Mohamed Abbas Hedjazi","Oussama Hadjerci","Adel Hafiane"],"pdf_url":"https://arxiv.org/pdf/2504.08613v1.pdf","comment":"Submitted to Applied Intelligence (Springer), under review since\n  November 26, 2024"},{"id":"http://arxiv.org/abs/2504.08603v1","updated":"2025-04-11T15:12:05Z","published":"2025-04-11T15:12:05Z","title":"FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot\n  Exploration in Any Environment","summary":"  Geometrically accurate and semantically expressive map representations have\nproven invaluable to facilitate robust and safe mobile robot navigation and\ntask planning. Nevertheless, real-time, open-vocabulary semantic understanding\nof large-scale unknown environments is still an open problem. In this paper we\npresent FindAnything, an open-world mapping and exploration framework that\nincorporates vision-language information into dense volumetric submaps. Thanks\nto the use of vision-language features, FindAnything bridges the gap between\npure geometric and open-vocabulary semantic information for a higher level of\nunderstanding while allowing to explore any environment without the help of any\nexternal source of ground-truth pose information. We represent the environment\nas a series of volumetric occupancy submaps, resulting in a robust and accurate\nmap representation that deforms upon pose updates when the underlying SLAM\nsystem corrects its drift, allowing for a locally consistent representation\nbetween submaps. Pixel-wise vision-language features are aggregated from\nefficient SAM (eSAM)-generated segments, which are in turn integrated into\nobject-centric volumetric submaps, providing a mapping from open-vocabulary\nqueries to 3D geometry that is scalable also in terms of memory usage. The\nopen-vocabulary map representation of FindAnything achieves state-of-the-art\nsemantic accuracy in closed-set evaluations on the Replica dataset. This level\nof scene understanding allows a robot to explore environments based on objects\nor areas of interest selected via natural language queries. Our system is the\nfirst of its kind to be deployed on resource-constrained devices, such as MAVs,\nleveraging vision-language information for real-world robotic tasks.\n","authors":["Sebastián Barbas Laina","Simon Boche","Sotiris Papatheodorou","Simon Schaefer","Jaehyung Jung","Stefan Leutenegger"],"pdf_url":"https://arxiv.org/pdf/2504.08603v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.08602v1","updated":"2025-04-11T15:10:41Z","published":"2025-04-11T15:10:41Z","title":"On Background Bias of Post-Hoc Concept Embeddings in Computer Vision\n  DNNs","summary":"  The thriving research field of concept-based explainable artificial\nintelligence (C-XAI) investigates how human-interpretable semantic concepts\nembed in the latent spaces of deep neural networks (DNNs). Post-hoc approaches\ntherein use a set of examples to specify a concept, and determine its\nembeddings in DNN latent space using data driven techniques. This proved useful\nto uncover biases between different target (foreground or concept) classes.\nHowever, given that the background is mostly uncontrolled during training, an\nimportant question has been left unattended so far: Are/to what extent are\nstate-of-the-art, data-driven post-hoc C-XAI approaches themselves prone to\nbiases with respect to their backgrounds? E.g., wild animals mostly occur\nagainst vegetation backgrounds, and they seldom appear on roads. Even simple\nand robust C-XAI methods might abuse this shortcut for enhanced performance. A\ndangerous performance degradation of the concept-corner cases of animals on the\nroad could thus remain undiscovered. This work validates and thoroughly\nconfirms that established Net2Vec-based concept segmentation techniques\nfrequently capture background biases, including alarming ones, such as\nunderperformance on road scenes. For the analysis, we compare 3 established\ntechniques from the domain of background randomization on >50 concepts from 2\ndatasets, and 7 diverse DNN architectures. Our results indicate that even\nlow-cost setups can provide both valuable insight and improved background\nrobustness.\n","authors":["Gesina Schwalbe","Georgii Mikriukov","Edgar Heinert","Stavros Gerolymatos","Mert Keser","Alois Knoll","Matthias Rottmann","Annika Mütze"],"pdf_url":"https://arxiv.org/pdf/2504.08602v1.pdf","comment":"camera-ready version for 3rd World Conference on eXplainable\n  Artificial Intelligence; 5 figures, 6 tables; code available at:\n  https://github.com/gesina/bg_randomized_loce"},{"id":"http://arxiv.org/abs/2504.08593v1","updated":"2025-04-11T14:52:59Z","published":"2025-04-11T14:52:59Z","title":"Hands-On: Segmenting Individual Signs from Continuous Sequences","summary":"  This work tackles the challenge of continuous sign language segmentation, a\nkey task with huge implications for sign language translation and data\nannotation. We propose a transformer-based architecture that models the\ntemporal dynamics of signing and frames segmentation as a sequence labeling\nproblem using the Begin-In-Out (BIO) tagging scheme. Our method leverages the\nHaMeR hand features, and is complemented with 3D Angles. Extensive experiments\nshow that our model achieves state-of-the-art results on the DGS Corpus, while\nour features surpass prior benchmarks on BSLCorpus.\n","authors":["Low Jian He","Harry Walsh","Ozge Mercanoglu Sincan","Richard Bowden"],"pdf_url":"https://arxiv.org/pdf/2504.08593v1.pdf","comment":"Accepted in the 19th IEEE International Conference on Automatic Face\n  and Gesture Recognition"},{"id":"http://arxiv.org/abs/2504.06432v2","updated":"2025-04-11T14:50:12Z","published":"2025-04-08T21:05:29Z","title":"D-Feat Occlusions: Diffusion Features for Robustness to Partial Visual\n  Occlusions in Object Recognition","summary":"  Applications of diffusion models for visual tasks have been quite noteworthy.\nThis paper targets making classification models more robust to occlusions for\nthe task of object recognition by proposing a pipeline that utilizes a frozen\ndiffusion model. Diffusion features have demonstrated success in image\ngeneration and image completion while understanding image context. Occlusion\ncan be posed as an image completion problem by deeming the pixels of the\noccluder to be `missing.' We hypothesize that such features can help\nhallucinate object visual features behind occluding objects, and hence we\npropose using them to enable models to become more occlusion robust. We design\nexperiments to include input-based augmentations as well as feature-based\naugmentations. Input-based augmentations involve finetuning on images where the\noccluder pixels are inpainted, and feature-based augmentations involve\naugmenting classification features with intermediate diffusion features. We\ndemonstrate that our proposed use of diffusion-based features results in models\nthat are more robust to partial object occlusions for both Transformers and\nConvNets on ImageNet with simulated occlusions. We also propose a dataset that\nencompasses real-world occlusions and demonstrate that our method is more\nrobust to partial object occlusions.\n","authors":["Rupayan Mallick","Sibo Dong","Nataniel Ruiz","Sarah Adel Bargal"],"pdf_url":"https://arxiv.org/pdf/2504.06432v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08591v1","updated":"2025-04-11T14:49:52Z","published":"2025-04-11T14:49:52Z","title":"ZipIR: Latent Pyramid Diffusion Transformer for High-Resolution Image\n  Restoration","summary":"  Recent progress in generative models has significantly improved image\nrestoration capabilities, particularly through powerful diffusion models that\noffer remarkable recovery of semantic details and local fidelity. However,\ndeploying these models at ultra-high resolutions faces a critical trade-off\nbetween quality and efficiency due to the computational demands of long-range\nattention mechanisms. To address this, we introduce ZipIR, a novel framework\nthat enhances efficiency, scalability, and long-range modeling for high-res\nimage restoration. ZipIR employs a highly compressed latent representation that\ncompresses image 32x, effectively reducing the number of spatial tokens, and\nenabling the use of high-capacity models like the Diffusion Transformer (DiT).\nToward this goal, we propose a Latent Pyramid VAE (LP-VAE) design that\nstructures the latent space into sub-bands to ease diffusion training. Trained\non full images up to 2K resolution, ZipIR surpasses existing diffusion-based\nmethods, offering unmatched speed and quality in restoring high-resolution\nimages from severely degraded inputs.\n","authors":["Yongsheng Yu","Haitian Zheng","Zhifei Zhang","Jianming Zhang","Yuqian Zhou","Connelly Barnes","Yuchen Liu","Wei Xiong","Zhe Lin","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2504.08591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08588v1","updated":"2025-04-11T14:46:36Z","published":"2025-04-11T14:46:36Z","title":"Hardware, Algorithms, and Applications of the Neuromorphic Vision\n  Sensor: a Review","summary":"  Neuromorphic, or event, cameras represent a transformation in the classical\napproach to visual sensing encodes detected instantaneous per-pixel\nillumination changes into an asynchronous stream of event packets. Their\nnovelty compared to standard cameras lies in the transition from capturing full\npicture frames at fixed time intervals to a sparse data format which, with its\ndistinctive qualities, offers potential improvements in various applications.\nHowever, these advantages come at the cost of reinventing algorithmic\nprocedures or adapting them to effectively process the new data format.\n  In this survey, we systematically examine neuromorphic vision along three\nmain dimensions. First, we highlight the technological evolution and\ndistinctive hardware features of neuromorphic cameras from their inception to\nrecent models. Second, we review image processing algorithms developed\nexplicitly for event-based data, covering key works on feature detection,\ntracking, and optical flow -which form the basis for analyzing image elements\nand transformations -as well as depth and pose estimation or object\nrecognition, which interpret more complex scene structures and components.\nThese techniques, drawn from classical computer vision and modern data-driven\napproaches, are examined to illustrate the breadth of applications for\nevent-based cameras. Third, we present practical application case studies\ndemonstrating how event cameras have been successfully used across various\nindustries and scenarios. Finally, we analyze the challenges limiting\nwidespread adoption, identify significant research gaps compared to standard\nimaging techniques, and outline promising future directions and opportunities\nthat neuromorphic vision offers.\n","authors":["Claudio Cimarelli","Jose Andres Millan-Romera","Holger Voos","Jose Luis Sanchez-Lopez"],"pdf_url":"https://arxiv.org/pdf/2504.08588v1.pdf","comment":"26 pages total, 26 without references, two images and five tables.\n  Submitted to IEEE Sensors"},{"id":"http://arxiv.org/abs/2504.08584v1","updated":"2025-04-11T14:38:09Z","published":"2025-04-11T14:38:09Z","title":"Boosting multi-demographic federated learning for chest x-ray analysis\n  using general-purpose self-supervised representations","summary":"  Reliable artificial intelligence (AI) models for medical image analysis often\ndepend on large and diverse labeled datasets. Federated learning (FL) offers a\ndecentralized and privacy-preserving approach to training but struggles in\nhighly non-independent and identically distributed (non-IID) settings, where\ninstitutions with more representative data may experience degraded performance.\nMoreover, existing large-scale FL studies have been limited to adult datasets,\nneglecting the unique challenges posed by pediatric data, which introduces\nadditional non-IID variability. To address these limitations, we analyzed\nn=398,523 adult chest radiographs from diverse institutions across multiple\ncountries and n=9,125 pediatric images, leveraging transfer learning from\ngeneral-purpose self-supervised image representations to classify pneumonia and\ncases with no abnormality. Using state-of-the-art vision transformers, we found\nthat FL improved performance only for smaller adult datasets (P<0.001) but\ndegraded performance for larger datasets (P<0.064) and pediatric cases\n(P=0.242). However, equipping FL with self-supervised weights significantly\nenhanced outcomes across pediatric cases (P=0.031) and most adult datasets\n(P<0.008), except the largest dataset (P=0.052). These findings underscore the\npotential of easily deployable general-purpose self-supervised image\nrepresentations to address non-IID challenges in clinical FL applications and\nhighlight their promise for enhancing patient outcomes and advancing pediatric\nhealthcare, where data scarcity and variability remain persistent obstacles.\n","authors":["Mahshad Lotfinia","Arash Tayebiarasteh","Samaneh Samiei","Mehdi Joodaki","Soroosh Tayebi Arasteh"],"pdf_url":"https://arxiv.org/pdf/2504.08584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08581v1","updated":"2025-04-11T14:33:27Z","published":"2025-04-11T14:33:27Z","title":"FMLGS: Fast Multilevel Language Embedded Gaussians for Part-level\n  Interactive Agents","summary":"  The semantically interactive radiance field has long been a promising\nbackbone for 3D real-world applications, such as embodied AI to achieve scene\nunderstanding and manipulation. However, multi-granularity interaction remains\na challenging task due to the ambiguity of language and degraded quality when\nit comes to queries upon object components. In this work, we present FMLGS, an\napproach that supports part-level open-vocabulary query within 3D Gaussian\nSplatting (3DGS). We propose an efficient pipeline for building and querying\nconsistent object- and part-level semantics based on Segment Anything Model 2\n(SAM2). We designed a semantic deviation strategy to solve the problem of\nlanguage ambiguity among object parts, which interpolates the semantic features\nof fine-grained targets for enriched information. Once trained, we can query\nboth objects and their describable parts using natural language. Comparisons\nwith other state-of-the-art methods prove that our method can not only better\nlocate specified part-level targets, but also achieve first-place performance\nconcerning both speed and accuracy, where FMLGS is 98 x faster than LERF, 4 x\nfaster than LangSplat and 2.5 x faster than LEGaussians. Meanwhile, we further\nintegrate FMLGS as a virtual agent that can interactively navigate through 3D\nscenes, locate targets, and respond to user demands through a chat interface,\nwhich demonstrates the potential of our work to be further expanded and applied\nin the future.\n","authors":["Xin Tan","Yuzhou Ji","He Zhu","Yuan Xie"],"pdf_url":"https://arxiv.org/pdf/2504.08581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08578v1","updated":"2025-04-11T14:30:42Z","published":"2025-04-11T14:30:42Z","title":"Knowledge Distillation for Multimodal Egocentric Action Recognition\n  Robust to Missing Modalities","summary":"  Action recognition is an essential task in egocentric vision due to its wide\nrange of applications across many fields. While deep learning methods have been\nproposed to address this task, most rely on a single modality, typically video.\nHowever, including additional modalities may improve the robustness of the\napproaches to common issues in egocentric videos, such as blurriness and\nocclusions. Recent efforts in multimodal egocentric action recognition often\nassume the availability of all modalities, leading to failures or performance\ndrops when any modality is missing. To address this, we introduce an efficient\nmultimodal knowledge distillation approach for egocentric action recognition\nthat is robust to missing modalities (KARMMA) while still benefiting when\nmultiple modalities are available. Our method focuses on resource-efficient\ndevelopment by leveraging pre-trained models as unimodal feature extractors in\nour teacher model, which distills knowledge into a much smaller and faster\nstudent model. Experiments on the Epic-Kitchens and Something-Something\ndatasets demonstrate that our student model effectively handles missing\nmodalities while reducing its accuracy drop in this scenario.\n","authors":["Maria Santos-Villafranca","Dustin Carrión-Ojeda","Alejandro Perez-Yus","Jesus Bermudez-Cameo","Jose J. Guerrero","Simone Schaub-Meyer"],"pdf_url":"https://arxiv.org/pdf/2504.08578v1.pdf","comment":"Project Page: https://visinf.github.io/KARMMA"},{"id":"http://arxiv.org/abs/2310.08948v2","updated":"2025-04-11T14:25:43Z","published":"2023-10-13T08:35:02Z","title":"Federated Class-Incremental Learning with Prompting","summary":"  As Web technology continues to develop, it has become increasingly common to\nuse data stored on different clients. At the same time, federated learning has\nreceived widespread attention due to its ability to protect data privacy when\nlet models learn from data which is distributed across various clients.\nHowever, most existing works assume that the client's data are fixed. In\nreal-world scenarios, such an assumption is most likely not true as data may be\ncontinuously generated and new classes may also appear. To this end, we focus\non the practical and challenging federated class-incremental learning (FCIL)\nproblem. For FCIL, the local and global models may suffer from catastrophic\nforgetting on old classes caused by the arrival of new classes and the data\ndistributions of clients are non-independent and identically distributed\n(non-iid).\n  In this paper, we propose a novel method called Federated Class-Incremental\nLearning with PrompTing (FCILPT). Given the privacy and limited memory, FCILPT\ndoes not use a rehearsal-based buffer to keep exemplars of old data. We choose\nto use prompts to ease the catastrophic forgetting of the old classes.\nSpecifically, we encode the task-relevant and task-irrelevant knowledge into\nprompts, preserving the old and new knowledge of the local clients and solving\nthe problem of catastrophic forgetting. We first sort the task information in\nthe prompt pool in the local clients to align the task information on different\nclients before global aggregation. It ensures that the same task's knowledge\nare fully integrated, solving the problem of non-iid caused by the lack of\nclasses among different clients in the same incremental task. Experiments on\nCIFAR-100, Mini-ImageNet, and Tiny-ImageNet demonstrate that FCILPT achieves\nsignificant accuracy improvements over the state-of-the-art methods.\n","authors":["Xin Luo","Fang-Yi Liang","Jiale Liu","Yu-Wei Zhan","Zhen-Duo Chen","Xin-Shun Xu"],"pdf_url":"https://arxiv.org/pdf/2310.08948v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08568v1","updated":"2025-04-11T14:24:30Z","published":"2025-04-11T14:24:30Z","title":"Banana Ripeness Level Classification using a Simple CNN Model Trained\n  with Real and Synthetic Datasets","summary":"  The level of ripeness is essential in determining the quality of bananas. To\ncorrectly estimate banana maturity, the metrics of international marketing\nstandards need to be considered. However, the process of assessing the maturity\nof bananas at an industrial level is still carried out using manual methods.\nThe use of CNN models is an attractive tool to solve the problem, but there is\na limitation regarding the availability of sufficient data to train these\nmodels reliably. On the other hand, in the state-of-the-art, existing CNN\nmodels and the available data have reported that the accuracy results are\nacceptable in identifying banana maturity. For this reason, this work presents\nthe generation of a robust dataset that combines real and synthetic data for\ndifferent levels of banana ripeness. In addition, it proposes a simple CNN\narchitecture, which is trained with synthetic data and using the transfer\nlearning technique, the model is improved to classify real data, managing to\ndetermine the level of maturity of the banana. The proposed CNN model is\nevaluated with several architectures, then hyper-parameter configurations are\nvaried, and optimizers are used. The results show that the proposed CNN model\nreaches a high accuracy of 0.917 and a fast execution time.\n","authors":["Luis Chuquimarca","Boris Vintimilla","Sergio Velastin"],"pdf_url":"https://arxiv.org/pdf/2504.08568v1.pdf","comment":"9 pages, 7 figures, conference"},{"id":"http://arxiv.org/abs/2406.05821v3","updated":"2025-04-11T14:21:24Z","published":"2024-06-09T15:14:26Z","title":"F-LMM: Grounding Frozen Large Multimodal Models","summary":"  Endowing Large Multimodal Models (LMMs) with visual grounding capability can\nsignificantly enhance AIs' understanding of the visual world and their\ninteraction with humans. However, existing methods typically fine-tune the\nparameters of LMMs to learn additional segmentation tokens and overfit\ngrounding and segmentation datasets. Such a design would inevitably cause a\ncatastrophic diminution in the indispensable conversational capability of\ngeneral AI assistants. In this paper, we comprehensively evaluate\nstate-of-the-art grounding LMMs across a suite of multimodal question-answering\nbenchmarks, observing drastic performance drops that indicate vanishing general\nknowledge comprehension and weakened instruction following ability. To address\nthis issue, we present F-LMM -- grounding frozen off-the-shelf LMMs in human-AI\nconversations -- a straightforward yet effective design based on the fact that\nword-pixel correspondences conducive to visual grounding inherently exist in\nthe attention mechanism of well-trained LMMs. Using only a few trainable CNN\nlayers, we can translate word-pixel attention weights to mask logits, which a\nSAM-based mask refiner can further optimise. Our F-LMM neither learns special\nsegmentation tokens nor utilises high-quality grounded instruction-tuning data,\nbut achieves competitive performance on referring expression segmentation and\npanoptic narrative grounding benchmarks while completely preserving LMMs'\noriginal conversational ability. Additionally, with instruction-following\nability preserved and grounding ability obtained, F-LMM can be directly applied\nto complex tasks like reasoning segmentation, grounded conversation generation\nand visual chain-of-thought reasoning. Our code can be found at\nhttps://github.com/wusize/F-LMM.\n","authors":["Size Wu","Sheng Jin","Wenwei Zhang","Lumin Xu","Wentao Liu","Wei Li","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2406.05821v3.pdf","comment":"Project Page: https://github.com/wusize/F-LMM"},{"id":"http://arxiv.org/abs/2504.08551v1","updated":"2025-04-11T14:02:11Z","published":"2025-04-11T14:02:11Z","title":"Shadow Erosion and Nighttime Adaptability for Camera-Based Automated\n  Driving Applications","summary":"  Enhancement of images from RGB cameras is of particular interest due to its\nwide range of ever-increasing applications such as medical imaging, satellite\nimaging, automated driving, etc. In autonomous driving, various techniques are\nused to enhance image quality under challenging lighting conditions. These\ninclude artificial augmentation to improve visibility in poor nighttime\nconditions, illumination-invariant imaging to reduce the impact of lighting\nvariations, and shadow mitigation to ensure consistent image clarity in bright\ndaylight. This paper proposes a pipeline for Shadow Erosion and Nighttime\nAdaptability in images for automated driving applications while preserving\ncolor and texture details. The Shadow Erosion and Nighttime Adaptability\npipeline is compared to the widely used CLAHE technique and evaluated based on\nillumination uniformity and visual perception quality metrics. The results also\ndemonstrate a significant improvement over CLAHE, enhancing a YOLO-based\ndrivable area segmentation algorithm.\n","authors":["Mohamed Sabry","Gregory Schroeder","Joshua Varughese","Cristina Olaverri-Monreal"],"pdf_url":"https://arxiv.org/pdf/2504.08551v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2504.08550v1","updated":"2025-04-11T14:01:49Z","published":"2025-04-11T14:01:49Z","title":"Proxy-Anchor and EVT-Driven Continual Learning Method for Generalized\n  Category Discovery","summary":"  Continual generalized category discovery has been introduced and studied in\nthe literature as a method that aims to continuously discover and learn novel\ncategories in incoming data batches while avoiding catastrophic forgetting of\npreviously learned categories. A key component in addressing this challenge is\nthe model's ability to separate novel samples, where Extreme Value Theory (EVT)\nhas been effectively employed. In this work, we propose a novel method that\nintegrates EVT with proxy anchors to define boundaries around proxies using a\nprobability of inclusion function, enabling the rejection of unknown samples.\nAdditionally, we introduce a novel EVT-based loss function to enhance the\nlearned representation, achieving superior performance compared to other\ndeep-metric learning methods in similar settings. Using the derived probability\nfunctions, novel samples are effectively separated from previously known\ncategories. However, category discovery within these novel samples can\nsometimes overestimate the number of new categories. To mitigate this issue, we\npropose a novel EVT-based approach to reduce the model size and discard\nredundant proxies. We also incorporate experience replay and knowledge\ndistillation mechanisms during the continual learning stage to prevent\ncatastrophic forgetting. Experimental results demonstrate that our proposed\napproach outperforms state-of-the-art methods in continual generalized category\ndiscovery scenarios.\n","authors":["Alireza Fathalizadeh","Roozbeh Razavi-Far"],"pdf_url":"https://arxiv.org/pdf/2504.08550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08548v1","updated":"2025-04-11T14:00:46Z","published":"2025-04-11T14:00:46Z","title":"COP-GEN-Beta: Unified Generative Modelling of COPernicus Imagery\n  Thumbnails","summary":"  In remote sensing, multi-modal data from various sensors capturing the same\nscene offers rich opportunities, but learning a unified representation across\nthese modalities remains a significant challenge. Traditional methods have\noften been limited to single or dual-modality approaches. In this paper, we\nintroduce COP-GEN-Beta, a generative diffusion model trained on optical, radar,\nand elevation data from the Major TOM dataset. What sets COP-GEN-Beta apart is\nits ability to map any subset of modalities to any other, enabling zero-shot\nmodality translation after training. This is achieved through a sequence-based\ndiffusion transformer, where each modality is controlled by its own timestep\nembedding. We extensively evaluate COP-GEN-Beta on thumbnail images from the\nMajor TOM dataset, demonstrating its effectiveness in generating high-quality\nsamples. Qualitative and quantitative evaluations validate the model's\nperformance, highlighting its potential as a powerful pre-trained model for\nfuture remote sensing tasks.\n","authors":["Miguel Espinosa","Valerio Marsocci","Yuru Jia","Elliot J. Crowley","Mikolaj Czerkawski"],"pdf_url":"https://arxiv.org/pdf/2504.08548v1.pdf","comment":"Accepted at CVPR 2025 Workshop MORSE"},{"id":"http://arxiv.org/abs/2409.02664v4","updated":"2025-04-11T13:57:48Z","published":"2024-09-04T12:46:30Z","title":"Standing on the Shoulders of Giants: Reprogramming Visual-Language Model\n  for General Deepfake Detection","summary":"  The proliferation of deepfake faces poses huge potential negative impacts on\nour daily lives. Despite substantial advancements in deepfake detection over\nthese years, the generalizability of existing methods against forgeries from\nunseen datasets or created by emerging generative models remains constrained.\nIn this paper, inspired by the zero-shot advantages of Vision-Language Models\n(VLMs), we propose a novel approach that repurposes a well-trained VLM for\ngeneral deepfake detection. Motivated by the model reprogramming paradigm that\nmanipulates the model prediction via input perturbations, our method can\nreprogram a pre-trained VLM model (e.g., CLIP) solely based on manipulating its\ninput without tuning the inner parameters. First, learnable visual\nperturbations are used to refine feature extraction for deepfake detection.\nThen, we exploit information of face embedding to create sample-level\nadaptative text prompts, improving the performance. Extensive experiments on\nseveral popular benchmark datasets demonstrate that (1) the cross-dataset and\ncross-manipulation performances of deepfake detection can be significantly and\nconsistently improved (e.g., over 88\\% AUC in cross-dataset setting from FF++\nto WildDeepfake); (2) the superior performances are achieved with fewer\ntrainable parameters, making it a promising approach for real-world\napplications.\n","authors":["Kaiqing Lin","Yuzhen Lin","Weixiang Li","Taiping Yao","Bin Li"],"pdf_url":"https://arxiv.org/pdf/2409.02664v4.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2504.08542v1","updated":"2025-04-11T13:55:48Z","published":"2025-04-11T13:55:48Z","title":"Discriminator-Free Direct Preference Optimization for Video Diffusion","summary":"  Direct Preference Optimization (DPO), which aligns models with human\npreferences through win/lose data pairs, has achieved remarkable success in\nlanguage and image generation. However, applying DPO to video diffusion models\nfaces critical challenges: (1) Data inefficiency. Generating thousands of\nvideos per DPO iteration incurs prohibitive costs; (2) Evaluation uncertainty.\nHuman annotations suffer from subjective bias, and automated discriminators\nfail to detect subtle temporal artifacts like flickering or motion incoherence.\nTo address these, we propose a discriminator-free video DPO framework that: (1)\nUses original real videos as win cases and their edited versions (e.g.,\nreversed, shuffled, or noise-corrupted clips) as lose cases; (2) Trains video\ndiffusion models to distinguish and avoid artifacts introduced by editing. This\napproach eliminates the need for costly synthetic video comparisons, provides\nunambiguous quality signals, and enables unlimited training data expansion\nthrough simple editing operations. We theoretically prove the framework's\neffectiveness even when real videos and model-generated videos follow different\ndistributions. Experiments on CogVideoX demonstrate the efficiency of the\nproposed method.\n","authors":["Haoran Cheng","Qide Dong","Liang Peng","Zhizhou Sha","Weiguo Feng","Jinghui Xie","Zhao Song","Shilei Wen","Xiaofei He","Boxi Wu"],"pdf_url":"https://arxiv.org/pdf/2504.08542v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2412.14167 by other authors"},{"id":"http://arxiv.org/abs/2504.08541v1","updated":"2025-04-11T13:54:19Z","published":"2025-04-11T13:54:19Z","title":"Digital Twin Catalog: A Large-Scale Photorealistic 3D Object Digital\n  Twin Dataset","summary":"  We introduce Digital Twin Catalog (DTC), a new large-scale photorealistic 3D\nobject digital twin dataset. A digital twin of a 3D object is a highly\ndetailed, virtually indistinguishable representation of a physical object,\naccurately capturing its shape, appearance, physical properties, and other\nattributes. Recent advances in neural-based 3D reconstruction and inverse\nrendering have significantly improved the quality of 3D object reconstruction.\nDespite these advancements, there remains a lack of a large-scale, digital twin\nquality real-world dataset and benchmark that can quantitatively assess and\ncompare the performance of different reconstruction methods, as well as improve\nreconstruction quality through training or fine-tuning. Moreover, to\ndemocratize 3D digital twin creation, it is essential to integrate creation\ntechniques with next-generation egocentric computing platforms, such as AR\nglasses. Currently, there is no dataset available to evaluate 3D object\nreconstruction using egocentric captured images. To address these gaps, the DTC\ndataset features 2,000 scanned digital twin-quality 3D objects, along with\nimage sequences captured under different lighting conditions using DSLR cameras\nand egocentric AR glasses. This dataset establishes the first comprehensive\nreal-world evaluation benchmark for 3D digital twin creation tasks, offering a\nrobust foundation for comparing and improving existing reconstruction methods.\nThe DTC dataset is already released at\nhttps://www.projectaria.com/datasets/dtc/ and we will also make the baseline\nevaluations open-source.\n","authors":["Zhao Dong","Ka Chen","Zhaoyang Lv","Hong-Xing Yu","Yunzhi Zhang","Cheng Zhang","Yufeng Zhu","Stephen Tian","Zhengqin Li","Geordie Moffatt","Sean Christofferson","James Fort","Xiaqing Pan","Mingfei Yan","Jiajun Wu","Carl Yuheng Ren","Richard Newcombe"],"pdf_url":"https://arxiv.org/pdf/2504.08541v1.pdf","comment":"accepted to CVPR 2025 highlights"},{"id":"http://arxiv.org/abs/2504.08540v1","updated":"2025-04-11T13:54:04Z","published":"2025-04-11T13:54:04Z","title":"Datasets for Lane Detection in Autonomous Driving: A Comprehensive\n  Review","summary":"  Accurate lane detection is essential for automated driving, enabling safe and\nreliable vehicle navigation in a variety of road scenarios. Numerous datasets\nhave been introduced to support the development and evaluation of lane\ndetection algorithms, each differing in terms of the amount of data, sensor\ntypes, annotation granularity, environmental conditions, and scenario\ndiversity. This paper provides a comprehensive review of over 30 publicly\navailable lane detection datasets, systematically analysing their\ncharacteristics, advantages and limitations. We classify these datasets based\non key factors such as sensor resolution, annotation types and diversity of\nroad and weather conditions. By identifying existing challenges and research\ngaps, we highlight opportunities for future dataset improvements that can\nfurther drive innovation in robust lane detection. This survey serves as a\nresource for researchers seeking appropriate datasets for lane detection, and\ncontributes to the broader goal of advancing autonomous driving.\n","authors":["Jörg Gamerdinger","Sven Teufel","Oliver Bringmann"],"pdf_url":"https://arxiv.org/pdf/2504.08540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08531v1","updated":"2025-04-11T13:41:17Z","published":"2025-04-11T13:41:17Z","title":"Embodied Image Captioning: Self-supervised Learning Agents for Spatially\n  Coherent Image Descriptions","summary":"  We present a self-supervised method to improve an agent's abilities in\ndescribing arbitrary objects while actively exploring a generic environment.\nThis is a challenging problem, as current models struggle to obtain coherent\nimage captions due to different camera viewpoints and clutter. We propose a\nthree-phase framework to fine-tune existing captioning models that enhances\ncaption accuracy and consistency across views via a consensus mechanism. First,\nan agent explores the environment, collecting noisy image-caption pairs. Then,\na consistent pseudo-caption for each object instance is distilled via consensus\nusing a large language model. Finally, these pseudo-captions are used to\nfine-tune an off-the-shelf captioning model, with the addition of contrastive\nlearning. We analyse the performance of the combination of captioning models,\nexploration policies, pseudo-labeling methods, and fine-tuning strategies, on\nour manually labeled test set. Results show that a policy can be trained to\nmine samples with higher disagreement compared to classical baselines. Our\npseudo-captioning method, in combination with all policies, has a higher\nsemantic similarity compared to other existing methods, and fine-tuning\nimproves caption accuracy and consistency by a significant margin. Code and\ntest set annotations available at\nhttps://hsp-iit.github.io/embodied-captioning/\n","authors":["Tommaso Galliena","Tommaso Apicella","Stefano Rosa","Pietro Morerio","Alessio Del Bue","Lorenzo Natale"],"pdf_url":"https://arxiv.org/pdf/2504.08531v1.pdf","comment":"11 pages, 8 figures, 5 tables, code and test set annotations\n  available at https://hsp-iit.github.io/embodied-captioning/"},{"id":"http://arxiv.org/abs/2504.06553v3","updated":"2025-04-11T12:57:13Z","published":"2025-04-09T03:22:52Z","title":"ASHiTA: Automatic Scene-grounded HIerarchical Task Analysis","summary":"  While recent work in scene reconstruction and understanding has made strides\nin grounding natural language to physical 3D environments, it is still\nchallenging to ground abstract, high-level instructions to a 3D scene.\nHigh-level instructions might not explicitly invoke semantic elements in the\nscene, and even the process of breaking a high-level task into a set of more\nconcrete subtasks, a process called hierarchical task analysis, is\nenvironment-dependent. In this work, we propose ASHiTA, the first framework\nthat generates a task hierarchy grounded to a 3D scene graph by breaking down\nhigh-level tasks into grounded subtasks. ASHiTA alternates LLM-assisted\nhierarchical task analysis, to generate the task breakdown, with task-driven 3D\nscene graph construction to generate a suitable representation of the\nenvironment. Our experiments show that ASHiTA performs significantly better\nthan LLM baselines in breaking down high-level tasks into environment-dependent\nsubtasks and is additionally able to achieve grounding performance comparable\nto state-of-the-art methods.\n","authors":["Yun Chang","Leonor Fermoselle","Duy Ta","Bernadette Bucher","Luca Carlone","Jiuguang Wang"],"pdf_url":"https://arxiv.org/pdf/2504.06553v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10882v2","updated":"2025-04-11T12:47:12Z","published":"2022-11-20T06:31:53Z","title":"Multi-head Ensemble of Smoothed Classifiers for Certified Robustness","summary":"  Randomized Smoothing (RS) is a promising technique for certified robustness,\nand recently in RS the ensemble of multiple Deep Neural Networks (DNNs) has\nshown state-of-the-art performances due to its variance reduction effect over\nGaussian noises. However, such an ensemble brings heavy computation burdens in\nboth training and certification, and yet under-exploits individual DNNs and\ntheir mutual effects, as the communication between these classifiers is\ncommonly ignored in optimization. In this work, we consider a novel\nensemble-based training way for a single DNN with multiple augmented heads,\nnamed as SmOothed Multi-head Ensemble (SOME). In SOME, similar to the pursuit\nof variance reduction via ensemble, an ensemble of multiple heads imposed with\na cosine constraint inside a single DNN is employed with much cheaper training\nand certification computation overloads in RS. In such network structure, an\nassociated training strategy is designed by introducing a circular\ncommunication flow among those augmented heads. That is, each head teaches its\nneighbor with the self-paced learning strategy using smoothed losses, which are\nspecifically designed in relation to certified robustness. The deployed\nmulti-head structure and the circular-teaching scheme in SOME jointly\ncontribute to the diversities among multiple heads and benefit their ensemble,\nleading to a competitively stronger certifiably-robust RS-based defense than\nensembling multiple DNNs (effectiveness) at the cost of much less computational\nexpenses (efficiency), verified by extensive experiments and discussions.\n","authors":["Kun Fang","Qinghua Tao","Yingwen Wu","Tao Li","Xiaolin Huang","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2211.10882v2.pdf","comment":"Accepted by Neural Networks"},{"id":"http://arxiv.org/abs/2407.15317v2","updated":"2025-04-11T12:42:25Z","published":"2024-07-22T01:04:16Z","title":"Open-CD: A Comprehensive Toolbox for Change Detection","summary":"  We present Open-CD, a change detection toolbox that contains a rich set of\nchange detection methods as well as related components and modules. The toolbox\nstarted from a series of open source general vision task tools, including\nOpenMMLab Toolkits, PyTorch Image Models, etc. It gradually evolves into a\nunified platform that covers many popular change detection methods and\ncontemporary modules. It not only includes training and inference codes, but\nalso provides some useful scripts for data analysis. We believe this toolbox is\nby far the most complete change detection toolbox. In this report, we introduce\nthe various features, supported methods and applications of Open-CD. In\naddition, we also conduct a benchmarking study on different methods and\ncomponents. We wish that the toolbox and benchmark could serve the growing\nresearch community by providing a flexible toolkit to reimplement existing\nmethods and develop their own new change detectors. Code and models are\navailable at https://github.com/likyoo/open-cd. Pioneeringly, this report also\nincludes brief descriptions of the algorithms supported in Open-CD, mainly\ncontributed by their authors. We sincerely encourage researchers in this field\nto participate in this project and work together to create a more open\ncommunity. This toolkit and report will be kept updated.\n","authors":["Kaiyu Li","Jiawei Jiang","Andrea Codegoni","Chengxi Han","Yupeng Deng","Keyan Chen","Zhuo Zheng","Hao Chen","Ziyuan Liu","Yuantao Gu","Zhengxia Zou","Zhenwei Shi","Sheng Fang","Deyu Meng","Zhi Wang","Xiangyong Cao"],"pdf_url":"https://arxiv.org/pdf/2407.15317v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2411.17459v3","updated":"2025-04-11T12:31:07Z","published":"2024-11-26T14:23:53Z","title":"WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model","summary":"  Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.\n","authors":["Zongjian Li","Bin Lin","Yang Ye","Liuhan Chen","Xinhua Cheng","Shenghai Yuan","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2411.17459v3.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.08481v1","updated":"2025-04-11T12:15:22Z","published":"2025-04-11T12:15:22Z","title":"A Hybrid Fully Convolutional CNN-Transformer Model for Inherently\n  Interpretable Medical Image Classification","summary":"  In many medical imaging tasks, convolutional neural networks (CNNs)\nefficiently extract local features hierarchically. More recently, vision\ntransformers (ViTs) have gained popularity, using self-attention mechanisms to\ncapture global dependencies, but lacking the inherent spatial localization of\nconvolutions. Therefore, hybrid models combining CNNs and ViTs have been\ndeveloped to combine the strengths of both architectures. However, such hybrid\nCNN-ViT models are difficult to interpret, which hinders their application in\nmedical imaging. In this work, we introduce an interpretable-by-design hybrid\nfully convolutional CNN-Transformer architecture for medical image\nclassification. Unlike widely used post-hoc saliency methods for ViTs, our\napproach generates faithful and localized evidence maps that directly reflect\nthe model's decision process. We evaluated our method on two medical image\nclassification tasks using color fundus images. Our model not only achieves\nstate-of-the-art predictive performance compared to both black-box and\ninterpretable models but also provides class-specific sparse evidence maps in a\nsingle forward pass. The code is available at:\nhttps://anonymous.4open.science/r/Expl-CNN-Transformer/.\n","authors":["Kerol Djoumessi","Samuel Ofosu Mensah","Philipp Berens"],"pdf_url":"https://arxiv.org/pdf/2504.08481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21443v2","updated":"2025-04-11T12:13:02Z","published":"2024-10-28T18:40:06Z","title":"TACO: Adversarial Camouflage Optimization on Trucks to Fool Object\n  Detectors","summary":"  Adversarial attacks threaten the reliability of machine learning models in\ncritical applications like autonomous vehicles and defense systems. As object\ndetectors become more robust with models like YOLOv8, developing effective\nadversarial methodologies is increasingly challenging. We present Truck\nAdversarial Camouflage Optimization (TACO), a novel framework that generates\nadversarial camouflage patterns on 3D vehicle models to deceive\nstate-of-the-art object detectors. Adopting Unreal Engine 5, TACO integrates\ndifferentiable rendering with a Photorealistic Rendering Network to optimize\nadversarial textures targeted at YOLOv8. To ensure the generated textures are\nboth effective in deceiving detectors and visually plausible, we introduce the\nConvolutional Smooth Loss function, a generalized smooth loss function.\nExperimental evaluations demonstrate that TACO significantly degrades YOLOv8's\ndetection performance, achieving an AP@0.5 of 0.0099 on unseen test data.\nFurthermore, these adversarial patterns exhibit strong transferability to other\nobject detection models such as Faster R-CNN and earlier YOLO versions.\n","authors":["Adonisz Dimitriu","Tamás Michaletzky","Viktor Remeli"],"pdf_url":"https://arxiv.org/pdf/2410.21443v2.pdf","comment":"This version matches the final published version in Big Data and\n  Cognitive Computing (MDPI). Please cite the journal version when referencing\n  this work (doi: https://doi.org/10.3390/bdcc9030072)"},{"id":"http://arxiv.org/abs/2502.14314v3","updated":"2025-04-11T12:06:13Z","published":"2025-02-20T06:57:58Z","title":"ODverse33: Is the New YOLO Version Always Better? A Multi Domain\n  benchmark from YOLO v5 to v11","summary":"  You Look Only Once (YOLO) models have been widely used for building real-time\nobject detectors across various domains. With the increasing frequency of new\nYOLO versions being released, key questions arise. Are the newer versions\nalways better than their previous versions? What are the core innovations in\neach YOLO version and how do these changes translate into real-world\nperformance gains? In this paper, we summarize the key innovations from YOLOv1\nto YOLOv11, introduce a comprehensive benchmark called ODverse33, which\nincludes 33 datasets spanning 11 diverse domains (Autonomous driving,\nAgricultural, Underwater, Medical, Videogame, Industrial, Aerial, Wildlife,\nRetail, Microscopic, and Security), and explore the practical impact of model\nimprovements in real-world, multi-domain applications through extensive\nexperimental results. We hope this study can provide some guidance to the\nextensive users of object detection models and give some references for future\nreal-time object detector development.\n","authors":["Tianyou Jiang","Yang Zhong"],"pdf_url":"https://arxiv.org/pdf/2502.14314v3.pdf","comment":"20 pages, 4 figures, 7 tables"},{"id":"http://arxiv.org/abs/2409.16938v2","updated":"2025-04-11T12:04:56Z","published":"2024-09-25T13:52:50Z","title":"Generative Object Insertion in Gaussian Splatting with a Multi-View\n  Diffusion Model","summary":"  Generating and inserting new objects into 3D content is a compelling approach\nfor achieving versatile scene recreation. Existing methods, which rely on SDS\noptimization or single-view inpainting, often struggle to produce high-quality\nresults. To address this, we propose a novel method for object insertion in 3D\ncontent represented by Gaussian Splatting. Our approach introduces a multi-view\ndiffusion model, dubbed MVInpainter, which is built upon a pre-trained stable\nvideo diffusion model to facilitate view-consistent object inpainting. Within\nMVInpainter, we incorporate a ControlNet-based conditional injection module to\nenable controlled and more predictable multi-view generation. After generating\nthe multi-view inpainted results, we further propose a mask-aware 3D\nreconstruction technique to refine Gaussian Splatting reconstruction from these\nsparse inpainted views. By leveraging these fabricate techniques, our approach\nyields diverse results, ensures view-consistent and harmonious insertions, and\nproduces better object quality. Extensive experiments demonstrate that our\napproach outperforms existing methods.\n","authors":["Hongliang Zhong","Can Wang","Jingbo Zhang","Jing Liao"],"pdf_url":"https://arxiv.org/pdf/2409.16938v2.pdf","comment":"Accepted by Visual Informatics. Project Page:\n  https://github.com/JiuTongBro/MultiView_Inpaint"},{"id":"http://arxiv.org/abs/2504.08473v1","updated":"2025-04-11T12:04:49Z","published":"2025-04-11T12:04:49Z","title":"Cut-and-Splat: Leveraging Gaussian Splatting for Synthetic Data\n  Generation","summary":"  Generating synthetic images is a useful method for cheaply obtaining labeled\ndata for training computer vision models. However, obtaining accurate 3D models\nof relevant objects is necessary, and the resulting images often have a gap in\nrealism due to challenges in simulating lighting effects and camera artifacts.\nWe propose using the novel view synthesis method called Gaussian Splatting to\naddress these challenges. We have developed a synthetic data pipeline for\ngenerating high-quality context-aware instance segmentation training data for\nspecific objects. This process is fully automated, requiring only a video of\nthe target object. We train a Gaussian Splatting model of the target object and\nautomatically extract the object from the video. Leveraging Gaussian Splatting,\nwe then render the object on a random background image, and monocular depth\nestimation is employed to place the object in a believable pose. We introduce a\nnovel dataset to validate our approach and show superior performance over other\ndata generation approaches, such as Cut-and-Paste and Diffusion model-based\ngeneration.\n","authors":["Bram Vanherle","Brent Zoomers","Jeroen Put","Frank Van Reeth","Nick Michiels"],"pdf_url":"https://arxiv.org/pdf/2504.08473v1.pdf","comment":"Accepted at the International Conference on Robotics, Computer Vision\n  and Intelligent Systems 2025 (ROBOVIS)"},{"id":"http://arxiv.org/abs/2504.08452v1","updated":"2025-04-11T11:28:00Z","published":"2025-04-11T11:28:00Z","title":"Road Grip Uncertainty Estimation Through Surface State Segmentation","summary":"  Slippery road conditions pose significant challenges for autonomous driving.\nBeyond predicting road grip, it is crucial to estimate its uncertainty reliably\nto ensure safe vehicle control. In this work, we benchmark several uncertainty\nprediction methods to assess their effectiveness for grip uncertainty\nestimation. Additionally, we propose a novel approach that leverages road\nsurface state segmentation to predict grip uncertainty. Our method estimates a\npixel-wise grip probability distribution based on inferred road surface\nconditions. Experimental results indicate that the proposed approach enhances\nthe robustness of grip uncertainty prediction.\n","authors":["Jyri Maanpää","Julius Pesonen","Iaroslav Melekhov","Heikki Hyyti","Juha Hyyppä"],"pdf_url":"https://arxiv.org/pdf/2504.08452v1.pdf","comment":"15 pages, 5 figures (supplementary material 2 pages, 1 figure).\n  Anonymized version submitted to Scandinavian Conference on Image Analysis\n  (SCIA) 2025"},{"id":"http://arxiv.org/abs/2504.08451v1","updated":"2025-04-11T11:27:29Z","published":"2025-04-11T11:27:29Z","title":"Muon-Accelerated Attention Distillation for Real-Time Edge Synthesis via\n  Optimized Latent Diffusion","summary":"  Recent advances in visual synthesis have leveraged diffusion models and\nattention mechanisms to achieve high-fidelity artistic style transfer and\nphotorealistic text-to-image generation. However, real-time deployment on edge\ndevices remains challenging due to computational and memory constraints. We\npropose Muon-AD, a co-designed framework that integrates the Muon optimizer\nwith attention distillation for real-time edge synthesis. By eliminating\ngradient conflicts through orthogonal parameter updates and dynamic pruning,\nMuon-AD achieves 3.2 times faster convergence compared to Stable\nDiffusion-TensorRT, while maintaining synthesis quality (15% lower FID, 4%\nhigher SSIM). Our framework reduces peak memory to 7GB on Jetson Orin and\nenables 24FPS real-time generation through mixed-precision quantization and\ncurriculum learning. Extensive experiments on COCO-Stuff and ImageNet-Texture\ndemonstrate Muon-AD's Pareto-optimal efficiency-quality trade-offs. Here, we\nshow a 65% reduction in communication overhead during distributed training and\nreal-time 10s/image generation on edge GPUs. These advancements pave the way\nfor democratizing high-quality visual synthesis in resource-constrained\nenvironments.\n","authors":["Weiye Chen","Qingen Zhu","Qian Long"],"pdf_url":"https://arxiv.org/pdf/2504.08451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08449v1","updated":"2025-04-11T11:18:57Z","published":"2025-04-11T11:18:57Z","title":"Ego4o: Egocentric Human Motion Capture and Understanding from\n  Multi-Modal Input","summary":"  This work focuses on tracking and understanding human motion using consumer\nwearable devices, such as VR/AR headsets, smart glasses, cellphones, and\nsmartwatches. These devices provide diverse, multi-modal sensor inputs,\nincluding egocentric images, and 1-3 sparse IMU sensors in varied combinations.\nMotion descriptions can also accompany these signals. The diverse input\nmodalities and their intermittent availability pose challenges for consistent\nmotion capture and understanding. In this work, we present Ego4o (o for omni),\na new framework for simultaneous human motion capture and understanding from\nmulti-modal egocentric inputs. This method maintains performance with partial\ninputs while achieving better results when multiple modalities are combined.\nFirst, the IMU sensor inputs, the optional egocentric image, and text\ndescription of human motion are encoded into the latent space of a motion\nVQ-VAE. Next, the latent vectors are sent to the VQ-VAE decoder and optimized\nto track human motion. When motion descriptions are unavailable, the latent\nvectors can be input into a multi-modal LLM to generate human motion\ndescriptions, which can further enhance motion capture accuracy. Quantitative\nand qualitative evaluations demonstrate the effectiveness of our method in\npredicting accurate human motion and high-quality motion descriptions.\n","authors":["Jian Wang","Rishabh Dabral","Diogo Luvizon","Zhe Cao","Lingjie Liu","Thabo Beeler","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2504.08449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14695v3","updated":"2025-04-11T11:15:41Z","published":"2024-11-22T03:05:06Z","title":"Anti-Forgetting Adaptation for Unsupervised Person Re-identification","summary":"  Regular unsupervised domain adaptive person re-identification (ReID) focuses\non adapting a model from a source domain to a fixed target domain. However, an\nadapted ReID model can hardly retain previously-acquired knowledge and\ngeneralize to unseen data. In this paper, we propose a Dual-level Joint\nAdaptation and Anti-forgetting (DJAA) framework, which incrementally adapts a\nmodel to new domains without forgetting source domain and each adapted target\ndomain. We explore the possibility of using prototype and instance-level\nconsistency to mitigate the forgetting during the adaptation. Specifically, we\nstore a small number of representative image samples and corresponding cluster\nprototypes in a memory buffer, which is updated at each adaptation step. With\nthe buffered images and prototypes, we regularize the image-to-image similarity\nand image-to-prototype similarity to rehearse old knowledge. After the\nmulti-step adaptation, the model is tested on all seen domains and several\nunseen domains to validate the generalization ability of our method. Extensive\nexperiments demonstrate that our proposed method significantly improves the\nanti-forgetting, generalization and backward-compatible ability of an\nunsupervised person ReID model.\n","authors":["Hao Chen","Francois Bremond","Nicu Sebe","Shiliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14695v3.pdf","comment":"Accepted to TPAMI"},{"id":"http://arxiv.org/abs/2504.08441v1","updated":"2025-04-11T11:06:12Z","published":"2025-04-11T11:06:12Z","title":"SARFormer -- An Acquisition Parameter Aware Vision Transformer for\n  Synthetic Aperture Radar Data","summary":"  This manuscript introduces SARFormer, a modified Vision Transformer (ViT)\narchitecture designed for processing one or multiple synthetic aperture radar\n(SAR) images. Given the complex image geometry of SAR data, we propose an\nacquisition parameter encoding module that significantly guides the learning\nprocess, especially in the case of multiple images, leading to improved\nperformance on downstream tasks. We further explore self-supervised\npre-training, conduct experiments with limited labeled data, and benchmark our\ncontribution and adaptations thoroughly in ablation experiments against a\nbaseline, where the model is tested on tasks such as height reconstruction and\nsegmentation. Our approach achieves up to 17% improvement in terms of RMSE over\nbaseline models\n","authors":["Jonathan Prexl","Michael Recla","Michael Schmitt"],"pdf_url":"https://arxiv.org/pdf/2504.08441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08431v1","updated":"2025-04-11T10:44:30Z","published":"2025-04-11T10:44:30Z","title":"The Composite Visual-Laser Navigation Method Applied in Indoor Poultry\n  Farming Environments","summary":"  Indoor poultry farms require inspection robots to maintain precise\nenvironmental control, which is crucial for preventing the rapid spread of\ndisease and large-scale bird mortality. However, the complex conditions within\nthese facilities, characterized by areas of intense illumination and water\naccumulation, pose significant challenges. Traditional navigation methods that\nrely on a single sensor often perform poorly in such environments, resulting in\nissues like laser drift and inaccuracies in visual navigation line extraction.\nTo overcome these limitations, we propose a novel composite navigation method\nthat integrates both laser and vision technologies. This approach dynamically\ncomputes a fused yaw angle based on the real-time reliability of each sensor\nmodality, thereby eliminating the need for physical navigation lines.\nExperimental validation in actual poultry house environments demonstrates that\nour method not only resolves the inherent drawbacks of single-sensor systems,\nbut also significantly enhances navigation precision and operational\nefficiency. As such, it presents a promising solution for improving the\nperformance of inspection robots in complex indoor poultry farming settings.\n","authors":["Jiafan Lu","Dongcheng Hu","Yitian Ye","Anqi Liu","Zixian Zhang","Xin Peng"],"pdf_url":"https://arxiv.org/pdf/2504.08431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08422v1","updated":"2025-04-11T10:28:29Z","published":"2025-04-11T10:28:29Z","title":"CMIP-CIL: A Cross-Modal Benchmark for Image-Point Class Incremental\n  Learning","summary":"  Image-point class incremental learning helps the 3D-points-vision robots\ncontinually learn category knowledge from 2D images, improving their perceptual\ncapability in dynamic environments. However, some incremental learning methods\naddress unimodal forgetting but fail in cross-modal cases, while others handle\nmodal differences within training/testing datasets but assume no modal gaps\nbetween them. We first explore this cross-modal task, proposing a benchmark\nCMIP-CIL and relieving the cross-modal catastrophic forgetting problem. It\nemploys masked point clouds and rendered multi-view images within a contrastive\nlearning framework in pre-training, empowering the vision model with the\ngeneralizations of image-point correspondence. In the incremental stage, by\nfreezing the backbone and promoting object representations close to their\nrespective prototypes, the model effectively retains and generalizes knowledge\nacross previously seen categories while continuing to learn new ones. We\nconduct comprehensive experiments on the benchmark datasets. Experiments prove\nthat our method achieves state-of-the-art results, outperforming the baseline\nmethods by a large margin.\n","authors":["Chao Qi","Jianqin Yin","Ren Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.08422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08421v1","updated":"2025-04-11T10:27:07Z","published":"2025-04-11T10:27:07Z","title":"Poisson multi-Bernoulli mixture filter for trajectory measurements","summary":"  This paper presents a Poisson multi-Bernoulli mixture (PMBM) filter for\nmulti-target filtering based on sensor measurements that are sets of\ntrajectories in the last two-time step window. The proposed filter, the\ntrajectory measurement PMBM (TM-PMBM) filter, propagates a PMBM density on the\nset of target states. In prediction, the filter obtains the PMBM density on the\nset of trajectories over the last two time steps. This density is then updated\nwith the set of trajectory measurements. After the update step, the PMBM\nposterior on the set of two-step trajectories is marginalised to obtain a PMBM\ndensity on the set of target states. The filter provides a closed-form solution\nfor multi-target filtering based on sets of trajectory measurements, estimating\nthe set of target states at the end of each time window. Additionally, the\npaper proposes computationally lighter alternatives to the TM-PMBM filter by\nderiving a Poisson multi-Bernoulli (PMB) density through Kullback-Leibler\ndivergence minimisation in an augmented space with auxiliary variables. The\nperformance of the proposed filters are evaluated in a simulation study.\n","authors":["Marco Fontana","Ángel F. García-Fernández","Simon Maskell"],"pdf_url":"https://arxiv.org/pdf/2504.08421v1.pdf","comment":"16 pages, 7 figures, journal paper"},{"id":"http://arxiv.org/abs/2504.08419v1","updated":"2025-04-11T10:23:55Z","published":"2025-04-11T10:23:55Z","title":"GeoTexBuild: 3D Building Model Generation from Map Footprints","summary":"  We introduce GeoTexBuild, a modular generative framework for creating 3D\nbuilding models from map footprints. The proposed framework employs a\nthree-stage process comprising height map generation, geometry reconstruction,\nand appearance stylization, culminating in building models with intricate\ngeometry and appearance attributes. By integrating customized ControlNet and\nText2Mesh models, we explore effective methods for controlling both geometric\nand visual attributes during the generation process. By this, we eliminate the\nproblem of structural variations behind a single facade photo of the existing\n3D generation techniques. Experimental results at each stage validate the\ncapability of GeoTexBuild to generate detailed and accurate building models\nfrom footprints derived from site planning or map designs. Our framework\nsignificantly reduces manual labor in modeling buildings and can offer\ninspiration for designers.\n","authors":["Ruizhe Wang","Junyan Yang","Qiao Wang"],"pdf_url":"https://arxiv.org/pdf/2504.08419v1.pdf","comment":"16 pages(excluding references), 10 figures"},{"id":"http://arxiv.org/abs/2504.08414v1","updated":"2025-04-11T10:19:29Z","published":"2025-04-11T10:19:29Z","title":"Adversarial Examples in Environment Perception for Automated Driving\n  (Review)","summary":"  The renaissance of deep learning has led to the massive development of\nautomated driving. However, deep neural networks are vulnerable to adversarial\nexamples. The perturbations of adversarial examples are imperceptible to human\neyes but can lead to the false predictions of neural networks. It poses a huge\nrisk to artificial intelligence (AI) applications for automated driving. This\nsurvey systematically reviews the development of adversarial robustness\nresearch over the past decade, including the attack and defense methods and\ntheir applications in automated driving. The growth of automated driving pushes\nforward the realization of trustworthy AI applications. This review lists\nsignificant references in the research history of adversarial examples.\n","authors":["Jun Yan","Huilin Yin"],"pdf_url":"https://arxiv.org/pdf/2504.08414v1.pdf","comment":"One chapter of upcoming Springer book: Recent Advances in Autonomous\n  Vehicle Technology, 2025"},{"id":"http://arxiv.org/abs/2504.08412v1","updated":"2025-04-11T10:18:35Z","published":"2025-04-11T10:18:35Z","title":"Boosting the Class-Incremental Learning in 3D Point Clouds via\n  Zero-Collection-Cost Basic Shape Pre-Training","summary":"  Existing class-incremental learning methods in 3D point clouds rely on\nexemplars (samples of former classes) to resist the catastrophic forgetting of\nmodels, and exemplar-free settings will greatly degrade the performance. For\nexemplar-free incremental learning, the pre-trained model methods have achieved\nstate-of-the-art results in 2D domains. However, these methods cannot be\nmigrated to the 3D domains due to the limited pre-training datasets and\ninsufficient focus on fine-grained geometric details. This paper breaks through\nthese limitations, proposing a basic shape dataset with zero collection cost\nfor model pre-training. It helps a model obtain extensive knowledge of 3D\ngeometries. Based on this, we propose a framework embedded with 3D geometry\nknowledge for incremental learning in point clouds, compatible with\nexemplar-free (-based) settings. In the incremental stage, the geometry\nknowledge is extended to represent objects in point clouds. The class prototype\nis calculated by regularizing the data representation with the same category\nand is kept adjusting in the learning process. It helps the model remember the\nshape features of different categories. Experiments show that our method\noutperforms other baseline methods by a large margin on various benchmark\ndatasets, considering both exemplar-free (-based) settings.\n","authors":["Chao Qi","Jianqin Yin","Meng Chen","Yingchun Niu","Yuan Sun"],"pdf_url":"https://arxiv.org/pdf/2504.08412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08411v1","updated":"2025-04-11T10:18:13Z","published":"2025-04-11T10:18:13Z","title":"A Knowledge-guided Adversarial Defense for Resisting Malicious Visual\n  Manipulation","summary":"  Malicious applications of visual manipulation have raised serious threats to\nthe security and reputation of users in many fields. To alleviate these issues,\nadversarial noise-based defenses have been enthusiastically studied in recent\nyears. However, ``data-only\" methods tend to distort fake samples in the\nlow-level feature space rather than the high-level semantic space, leading to\nlimitations in resisting malicious manipulation. Frontier research has shown\nthat integrating knowledge in deep learning can produce reliable and\ngeneralizable solutions. Inspired by these, we propose a knowledge-guided\nadversarial defense (KGAD) to actively force malicious manipulation models to\noutput semantically confusing samples. Specifically, in the process of\ngenerating adversarial noise, we focus on constructing significant semantic\nconfusions at the domain-specific knowledge level, and exploit a metric closely\nrelated to visual perception to replace the general pixel-wise metrics. The\ngenerated adversarial noise can actively interfere with the malicious\nmanipulation model by triggering knowledge-guided and perception-related\ndisruptions in the fake samples. To validate the effectiveness of the proposed\nmethod, we conduct qualitative and quantitative experiments on human perception\nand visual quality assessment. The results on two different tasks both show\nthat our defense provides better protection compared to state-of-the-art\nmethods and achieves great generalizability.\n","authors":["Dawei Zhou","Suzhi Gang","Decheng Liu","Tongliang Liu","Nannan Wang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2504.08411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08410v1","updated":"2025-04-11T10:16:55Z","published":"2025-04-11T10:16:55Z","title":"PMNI: Pose-free Multi-view Normal Integration for Reflective and\n  Textureless Surface Reconstruction","summary":"  Reflective and textureless surfaces remain a challenge in multi-view 3D\nreconstruction.Both camera pose calibration and shape reconstruction often fail\ndue to insufficient or unreliable cross-view visual features. To address these\nissues, we present PMNI (Pose-free Multi-view Normal Integration), a neural\nsurface reconstruction method that incorporates rich geometric information by\nleveraging surface normal maps instead of RGB images. By enforcing geometric\nconstraints from surface normals and multi-view shape consistency within a\nneural signed distance function (SDF) optimization framework, PMNI\nsimultaneously recovers accurate camera poses and high-fidelity surface\ngeometry. Experimental results on synthetic and real-world datasets show that\nour method achieves state-of-the-art performance in the reconstruction of\nreflective surfaces, even without reliable initial camera poses.\n","authors":["Mingzhi Pei","Xu Cao","Xiangyi Wang","Heng Guo","Zhanyu Ma"],"pdf_url":"https://arxiv.org/pdf/2504.08410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10727v3","updated":"2025-04-11T10:01:13Z","published":"2024-01-19T14:44:37Z","title":"MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning","summary":"  Recently, the astonishing performance of large language models (LLMs) in\nnatural language comprehension and generation tasks triggered lots of\nexploration of using them as central controllers to build agent systems.\nMultiple studies focus on bridging the LLMs to external tools to extend the\napplication scenarios. However, the current LLMs' ability to perceive tool use\nis limited to a single text query, which may result in ambiguity in\nunderstanding the users' real intentions. LLMs are expected to eliminate that\nby perceiving the information in the visual- or auditory-grounded instructions.\nTherefore, in this paper, we propose MLLM-Tool, a system incorporating\nopen-source LLMs and multi-modal encoders so that the learned LLMs can be\nconscious of multi-modal input instruction and then select the function-matched\ntool correctly. To facilitate the evaluation of the model's capability, we\ncollect a dataset featuring multi-modal input tools from HuggingFace. Another\nessential feature of our dataset is that it also contains multiple potential\nchoices for the same instruction due to the existence of identical functions\nand synonymous functions, which provides more potential solutions for the same\nquery. The experiments reveal that our MLLM-Tool is capable of recommending\nappropriate tools for multi-modal instructions. Codes and data are available at\nhttps://github.com/MLLM-Tool/MLLM-Tool.\n","authors":["Chenyu Wang","Weixin Luo","Sixun Dong","Xiaohua Xuan","Zhengxin Li","Lin Ma","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2401.10727v3.pdf","comment":"WACV 2025"},{"id":"http://arxiv.org/abs/2504.04893v2","updated":"2025-04-11T09:50:50Z","published":"2025-04-07T10:01:38Z","title":"SCAM: A Real-World Typographic Robustness Evaluation for Multimodal\n  Foundation Models","summary":"  Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper under https://huggingface.co/datasets/BLISS-e-V/SCAM, along with\nthe code for evaluations at https://github.com/Bliss-e-V/SCAM.\n","authors":["Justus Westerhoff","Erblina Purelku","Jakob Hackstein","Leo Pinetzki","Lorenz Hufe"],"pdf_url":"https://arxiv.org/pdf/2504.04893v2.pdf","comment":"Submitted to CVPR 2025 Workshop EVAL-FoMo-2"},{"id":"http://arxiv.org/abs/2504.08389v1","updated":"2025-04-11T09:42:46Z","published":"2025-04-11T09:42:46Z","title":"Light-YOLOv8-Flame: A Lightweight High-Performance Flame Detection\n  Algorithm","summary":"  Fire detection algorithms, particularly those based on computer vision,\nencounter significant challenges such as high computational costs and delayed\nresponse times, which hinder their application in real-time systems. To address\nthese limitations, this paper introduces Light-YOLOv8-Flame, a lightweight\nflame detection algorithm specifically designed for fast and efficient\nreal-time deployment. The proposed model enhances the YOLOv8 architecture\nthrough the substitution of the original C2f module with the FasterNet Block\nmodule. This new block combines Partial Convolution (PConv) and Convolution\n(Conv) layers, reducing both computational complexity and model size. A dataset\ncomprising 7,431 images, representing both flame and non-flame scenarios, was\ncollected and augmented for training purposes. Experimental findings indicate\nthat the modified YOLOv8 model achieves a 0.78% gain in mean average precision\n(mAP) and a 2.05% boost in recall, while reducing the parameter count by\n25.34%, with only a marginal decrease in precision by 0.82%. These findings\nhighlight that Light-YOLOv8-Flame offers enhanced detection performance and\nspeed, making it well-suited for real-time fire detection on\nresource-constrained devices.\n","authors":["Jiawei Lan","Zhibiao Wang","Haoyang Yu","Ye Tao","Wenhua Cui"],"pdf_url":"https://arxiv.org/pdf/2504.08389v1.pdf","comment":"12 pages, 19 figures, 6 tables. Submitted to Engineering Letters"},{"id":"http://arxiv.org/abs/2504.08388v1","updated":"2025-04-11T09:41:04Z","published":"2025-04-11T09:41:04Z","title":"MineWorld: a Real-Time and Open-Source Interactive World Model on\n  Minecraft","summary":"  World modeling is a crucial task for enabling intelligent agents to\neffectively interact with humans and operate in dynamic environments. In this\nwork, we propose MineWorld, a real-time interactive world model on Minecraft,\nan open-ended sandbox game which has been utilized as a common testbed for\nworld modeling. MineWorld is driven by a visual-action autoregressive\nTransformer, which takes paired game scenes and corresponding actions as input,\nand generates consequent new scenes following the actions. Specifically, by\ntransforming visual game scenes and actions into discrete token ids with an\nimage tokenizer and an action tokenizer correspondingly, we consist the model\ninput with the concatenation of the two kinds of ids interleaved. The model is\nthen trained with next token prediction to learn rich representations of game\nstates as well as the conditions between states and actions simultaneously. In\ninference, we develop a novel parallel decoding algorithm that predicts the\nspatial redundant tokens in each frame at the same time, letting models in\ndifferent scales generate $4$ to $7$ frames per second and enabling real-time\ninteractions with game players. In evaluation, we propose new metrics to assess\nnot only visual quality but also the action following capacity when generating\nnew scenes, which is crucial for a world model. Our comprehensive evaluation\nshows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion\nbased world models significantly. The code and model have been released.\n","authors":["Junliang Guo","Yang Ye","Tianyu He","Haoyu Wu","Yushu Jiang","Tim Pearce","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2504.08388v1.pdf","comment":"Technical report. Project page https://aka.ms/mineworld"},{"id":"http://arxiv.org/abs/2504.08384v1","updated":"2025-04-11T09:36:46Z","published":"2025-04-11T09:36:46Z","title":"Towards Efficient and Robust Moment Retrieval System: A Unified\n  Framework for Multi-Granularity Models and Temporal Reranking","summary":"  Long-form video understanding presents significant challenges for interactive\nretrieval systems, as conventional methods struggle to process extensive video\ncontent efficiently. Existing approaches often rely on single models,\ninefficient storage, unstable temporal search, and context-agnostic reranking,\nlimiting their effectiveness. This paper presents a novel framework to enhance\ninteractive video retrieval through four key innovations: (1) an ensemble\nsearch strategy that integrates coarse-grained (CLIP) and fine-grained (BEIT3)\nmodels to improve retrieval accuracy, (2) a storage optimization technique that\nreduces redundancy by selecting representative keyframes via TransNetV2 and\ndeduplication, (3) a temporal search mechanism that localizes video segments\nusing dual queries for start and end points, and (4) a temporal reranking\napproach that leverages neighboring frame context to stabilize rankings.\nEvaluated on known-item search and question-answering tasks, our framework\ndemonstrates substantial improvements in retrieval precision, efficiency, and\nuser interpretability, offering a robust solution for real-world interactive\nvideo retrieval applications.\n","authors":["Huu-Loc Tran","Tinh-Anh Nguyen-Nhu","Huu-Phong Phan-Nguyen","Tien-Huy Nguyen","Nhat-Minh Nguyen-Dich","Anh Dao","Huy-Duc Do","Quan Nguyen","Hoang M. Le","Quang-Vinh Dinh"],"pdf_url":"https://arxiv.org/pdf/2504.08384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14367v3","updated":"2025-04-11T09:24:26Z","published":"2024-07-19T14:53:18Z","title":"Thinking Racial Bias in Fair Forgery Detection: Models, Datasets and\n  Evaluations","summary":"  Due to the successful development of deep image generation technology,\nforgery detection plays a more important role in social and economic security.\nRacial bias has not been explored thoroughly in the deep forgery detection\nfield. In the paper, we first contribute a dedicated dataset called the Fair\nForgery Detection (FairFD) dataset, where we prove the racial bias of public\nstate-of-the-art (SOTA) methods. Different from existing forgery detection\ndatasets, the self-constructed FairFD dataset contains a balanced racial ratio\nand diverse forgery generation images with the largest-scale subjects.\nAdditionally, we identify the problems with naive fairness metrics when\nbenchmarking forgery detection models. To comprehensively evaluate fairness, we\ndesign novel metrics including Approach Averaged Metric and Utility Regularized\nMetric, which can avoid deceptive results. We also present an effective and\nrobust post-processing technique, Bias Pruning with Fair Activations (BPFA),\nwhich improves fairness without requiring retraining or weight updates.\nExtensive experiments conducted with 12 representative forgery detection models\ndemonstrate the value of the proposed dataset and the reasonability of the\ndesigned fairness metrics. By applying the BPFA to the existing fairest\ndetector, we achieve a new SOTA. Furthermore, we conduct more in-depth analyses\nto offer more insights to inspire researchers in the community.\n","authors":["Decheng Liu","Zongqi Wang","Chunlei Peng","Nannan Wang","Ruimin Hu","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2407.14367v3.pdf","comment":"The paper is accepted in AAAI 2025"},{"id":"http://arxiv.org/abs/2401.12452v4","updated":"2025-04-11T09:09:40Z","published":"2024-01-23T02:41:06Z","title":"Self-supervised Learning of LiDAR 3D Point Clouds via 2D-3D Neural\n  Calibration","summary":"  This paper introduces a novel self-supervised learning framework for\nenhancing 3D perception in autonomous driving scenes. Specifically, our\napproach, namely NCLR, focuses on 2D-3D neural calibration, a novel pretext\ntask that estimates the rigid pose aligning camera and LiDAR coordinate\nsystems. First, we propose the learnable transformation alignment to bridge the\ndomain gap between image and point cloud data, converting features into a\nunified representation space for effective comparison and matching. Second, we\nidentify the overlapping area between the image and point cloud with the fused\nfeatures. Third, we establish dense 2D-3D correspondences to estimate the rigid\npose. The framework not only learns fine-grained matching from points to pixels\nbut also achieves alignment of the image and point cloud at a holistic level,\nunderstanding their relative pose. We demonstrate the efficacy of NCLR by\napplying the pre-trained backbone to downstream tasks, such as LiDAR-based 3D\nsemantic segmentation, object detection, and panoptic segmentation.\nComprehensive experiments on various datasets illustrate the superiority of\nNCLR over existing self-supervised methods. The results confirm that joint\nlearning from different modalities significantly enhances the network's\nunderstanding abilities and effectiveness of learned representation. The code\nis publicly available at https://github.com/Eaphan/NCLR.\n","authors":["Yifan Zhang","Siyu Ren","Junhui Hou","Jinjian Wu","Yixuan Yuan","Guangming Shi"],"pdf_url":"https://arxiv.org/pdf/2401.12452v4.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2504.08368v1","updated":"2025-04-11T09:07:05Z","published":"2025-04-11T09:07:05Z","title":"FocalLens: Instruction Tuning Enables Zero-Shot Conditional Image\n  Representations","summary":"  Visual understanding is inherently contextual -- what we focus on in an image\ndepends on the task at hand. For instance, given an image of a person holding a\nbouquet of flowers, we may focus on either the person such as their clothing,\nor the type of flowers, depending on the context of interest. Yet, most\nexisting image encoding paradigms represent an image as a fixed, generic\nfeature vector, overlooking the potential needs of prioritizing varying visual\ninformation for different downstream use cases. In this work, we introduce\nFocalLens, a conditional visual encoding method that produces different\nrepresentations for the same image based on the context of interest, expressed\nflexibly through natural language. We leverage vision instruction tuning data\nand contrastively finetune a pretrained vision encoder to take natural language\ninstructions as additional inputs for producing conditional image\nrepresentations. Extensive experiments validate that conditional image\nrepresentation from FocalLens better pronounce the visual features of interest\ncompared to generic features produced by standard vision encoders like CLIP. In\naddition, we show FocalLens further leads to performance improvements on a\nrange of downstream tasks including image-image retrieval, image\nclassification, and image-text retrieval, with an average gain of 5 and 10\npoints on the challenging SugarCrepe and MMVP-VLM benchmarks, respectively.\n","authors":["Cheng-Yu Hsieh","Pavan Kumar Anasosalu Vasu","Fartash Faghri","Raviteja Vemulapalli","Chun-Liang Li","Ranjay Krishna","Oncel Tuzel","Hadi Pouransari"],"pdf_url":"https://arxiv.org/pdf/2504.08368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08366v1","updated":"2025-04-11T09:01:09Z","published":"2025-04-11T09:01:09Z","title":"In-2-4D: Inbetweening from Two Single-View Images to 4D Generation","summary":"  We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion)\ninbetweening from a minimalistic input setting: two single-view images\ncapturing an object in two distinct motion states. Given two images\nrepresenting the start and end states of an object in motion, our goal is to\ngenerate and reconstruct the motion in 4D. We utilize a video interpolation\nmodel to predict the motion, but large frame-to-frame motions can lead to\nambiguous interpretations. To overcome this, we employ a hierarchical approach\nto identify keyframes that are visually close to the input states and show\nsignificant motion, then generate smooth fragments between them. For each\nfragment, we construct the 3D representation of the keyframe using Gaussian\nSplatting. The temporal frames within the fragment guide the motion, enabling\ntheir transformation into dynamic Gaussians through a deformation field. To\nimprove temporal consistency and refine 3D motion, we expand the self-attention\nof multi-view diffusion across timesteps and apply rigid transformation\nregularization. Finally, we merge the independently generated 3D motion\nsegments by interpolating boundary deformation fields and optimizing them to\nalign with the guiding video, ensuring smooth and flicker-free transitions.\nThrough extensive qualitative and quantitiave experiments as well as a user\nstudy, we show the effectiveness of our method and its components. The project\npage is available at https://in-2-4d.github.io/\n","authors":["Sauradip Nag","Daniel Cohen-Or","Hao Zhang","Ali Mahdavi-Amiri"],"pdf_url":"https://arxiv.org/pdf/2504.08366v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2504.08361v1","updated":"2025-04-11T08:51:23Z","published":"2025-04-11T08:51:23Z","title":"SN-LiDAR: Semantic Neural Fields for Novel Space-time View LiDAR\n  Synthesis","summary":"  Recent research has begun exploring novel view synthesis (NVS) for LiDAR\npoint clouds, aiming to generate realistic LiDAR scans from unseen viewpoints.\nHowever, most existing approaches do not reconstruct semantic labels, which are\ncrucial for many downstream applications such as autonomous driving and robotic\nperception. Unlike images, which benefit from powerful segmentation models,\nLiDAR point clouds lack such large-scale pre-trained models, making semantic\nannotation time-consuming and labor-intensive. To address this challenge, we\npropose SN-LiDAR, a method that jointly performs accurate semantic\nsegmentation, high-quality geometric reconstruction, and realistic LiDAR\nsynthesis. Specifically, we employ a coarse-to-fine planar-grid feature\nrepresentation to extract global features from multi-frame point clouds and\nleverage a CNN-based encoder to extract local semantic features from the\ncurrent frame point cloud. Extensive experiments on SemanticKITTI and KITTI-360\ndemonstrate the superiority of SN-LiDAR in both semantic and geometric\nreconstruction, effectively handling dynamic objects and large-scale scenes.\nCodes will be available on https://github.com/dtc111111/SN-Lidar.\n","authors":["Yi Chen","Tianchen Deng","Wentao Zhao","Xiaoning Wang","Wenqian Xi","Weidong Chen","Jingchuan Wang"],"pdf_url":"https://arxiv.org/pdf/2504.08361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08358v1","updated":"2025-04-11T08:46:49Z","published":"2025-04-11T08:46:49Z","title":"LMM4LMM: Benchmarking and Evaluating Large-multimodal Image Generation\n  with LMMs","summary":"  Recent breakthroughs in large multimodal models (LMMs) have significantly\nadvanced both text-to-image (T2I) generation and image-to-text (I2T)\ninterpretation. However, many generated images still suffer from issues related\nto perceptual quality and text-image alignment. Given the high cost and\ninefficiency of manual evaluation, an automatic metric that aligns with human\npreferences is desirable. To this end, we present EvalMi-50K, a comprehensive\ndataset and benchmark for evaluating large-multimodal image generation, which\nfeatures (i) comprehensive tasks, encompassing 2,100 extensive prompts across\n20 fine-grained task dimensions, and (ii) large-scale human-preference\nannotations, including 100K mean-opinion scores (MOSs) and 50K\nquestion-answering (QA) pairs annotated on 50,400 images generated from 24 T2I\nmodels. Based on EvalMi-50K, we propose LMM4LMM, an LMM-based metric for\nevaluating large multimodal T2I generation from multiple dimensions including\nperception, text-image correspondence, and task-specific accuracy. Extensive\nexperimental results show that LMM4LMM achieves state-of-the-art performance on\nEvalMi-50K, and exhibits strong generalization ability on other AI-generated\nimage evaluation benchmark datasets, manifesting the generality of both the\nEvalMi-50K dataset and LMM4LMM metric. Both EvalMi-50K and LMM4LMM will be\nreleased at https://github.com/IntMeGroup/LMM4LMM.\n","authors":["Jiarui Wang","Huiyu Duan","Yu Zhao","Juntong Wang","Guangtao Zhai","Xiongkuo Min"],"pdf_url":"https://arxiv.org/pdf/2504.08358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08353v1","updated":"2025-04-11T08:39:18Z","published":"2025-04-11T08:39:18Z","title":"Single View Garment Reconstruction Using Diffusion Mapping Via Pattern\n  Coordinates","summary":"  Reconstructing 3D clothed humans from images is fundamental to applications\nlike virtual try-on, avatar creation, and mixed reality. While recent advances\nhave enhanced human body recovery, accurate reconstruction of garment geometry\n-- especially for loose-fitting clothing -- remains an open challenge. We\npresent a novel method for high-fidelity 3D garment reconstruction from single\nimages that bridges 2D and 3D representations. Our approach combines Implicit\nSewing Patterns (ISP) with a generative diffusion model to learn rich garment\nshape priors in a 2D UV space. A key innovation is our mapping model that\nestablishes correspondences between 2D image pixels, UV pattern coordinates,\nand 3D geometry, enabling joint optimization of both 3D garment meshes and the\ncorresponding 2D patterns by aligning learned priors with image observations.\nDespite training exclusively on synthetically simulated cloth data, our method\ngeneralizes effectively to real-world images, outperforming existing approaches\non both tight- and loose-fitting garments. The reconstructed garments maintain\nphysical plausibility while capturing fine geometric details, enabling\ndownstream applications including garment retargeting and texture manipulation.\n","authors":["Ren Li","Cong Cao","Corentin Dumery","Yingxuan You","Hao Li","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2504.08353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08348v1","updated":"2025-04-11T08:28:41Z","published":"2025-04-11T08:28:41Z","title":"Geometric Consistency Refinement for Single Image Novel View Synthesis\n  via Test-Time Adaptation of Diffusion Models","summary":"  Diffusion models for single image novel view synthesis (NVS) can generate\nhighly realistic and plausible images, but they are limited in the geometric\nconsistency to the given relative poses. The generated images often show\nsignificant errors with respect to the epipolar constraints that should be\nfulfilled, as given by the target pose. In this paper we address this issue by\nproposing a methodology to improve the geometric correctness of images\ngenerated by a diffusion model for single image NVS. We formulate a loss\nfunction based on image matching and epipolar constraints, and optimize the\nstarting noise in a diffusion sampling process such that the generated image\nshould both be a realistic image and fulfill geometric constraints derived from\nthe given target pose. Our method does not require training data or fine-tuning\nof the diffusion models, and we show that we can apply it to multiple\nstate-of-the-art models for single image NVS. The method is evaluated on the\nMegaScenes dataset and we show that geometric consistency is improved compared\nto the baseline models while retaining the quality of the generated images.\n","authors":["Josef Bengtson","David Nilsson","Fredrik Kahl"],"pdf_url":"https://arxiv.org/pdf/2504.08348v1.pdf","comment":"Accepted to CVPR 2025 EDGE Workshop. Project page:\n  https://gc-ref.github.io/"},{"id":"http://arxiv.org/abs/2504.08344v1","updated":"2025-04-11T08:19:18Z","published":"2025-04-11T08:19:18Z","title":"EasyGenNet: An Efficient Framework for Audio-Driven Gesture Video\n  Generation Based on Diffusion Model","summary":"  Audio-driven cospeech video generation typically involves two stages:\nspeech-to-gesture and gesture-to-video. While significant advances have been\nmade in speech-to-gesture generation, synthesizing natural expressions and\ngestures remains challenging in gesture-to-video systems. In order to improve\nthe generation effect, previous works adopted complex input and training\nstrategies and required a large amount of data sets for pre-training, which\nbrought inconvenience to practical applications. We propose a simple one-stage\ntraining method and a temporal inference method based on a diffusion model to\nsynthesize realistic and continuous gesture videos without the need for\nadditional training of temporal modules.The entire model makes use of existing\npre-trained weights, and only a few thousand frames of data are needed for each\ncharacter at a time to complete fine-tuning. Built upon the video generator, we\nintroduce a new audio-to-video pipeline to synthesize co-speech videos, using\n2D human skeleton as the intermediate motion representation. Our experiments\nshow that our method outperforms existing GAN-based and diffusion-based\nmethods.\n","authors":["Renda Li","Xiaohua Qi","Qiang Ling","Jun Yu","Ziyi Chen","Peng Chang","Mei HanJing Xiao"],"pdf_url":"https://arxiv.org/pdf/2504.08344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10148v4","updated":"2025-04-11T08:02:02Z","published":"2025-03-13T08:20:54Z","title":"3D Student Splatting and Scooping","summary":"  Recently, 3D Gaussian Splatting (3DGS) provides a new framework for novel\nview synthesis, and has spiked a new wave of research in neural rendering and\nrelated applications. As 3DGS is becoming a foundational component of many\nmodels, any improvement on 3DGS itself can bring huge benefits. To this end, we\naim to improve the fundamental paradigm and formulation of 3DGS. We argue that\nas an unnormalized mixture model, it needs to be neither Gaussians nor\nsplatting. We subsequently propose a new mixture model consisting of flexible\nStudent's t distributions, with both positive (splatting) and negative\n(scooping) densities. We name our model Student Splatting and Scooping, or SSS.\nWhen providing better expressivity, SSS also poses new challenges in learning.\nTherefore, we also propose a new principled sampling approach for optimization.\nThrough exhaustive evaluation and comparison, across multiple datasets,\nsettings, and metrics, we demonstrate that SSS outperforms existing methods in\nterms of quality and parameter efficiency, e.g. achieving matching or better\nquality with similar numbers of components, and obtaining comparable results\nwhile reducing the component number by as much as 82%.\n","authors":["Jialin Zhu","Jiangbei Yue","Feixiang He","He Wang"],"pdf_url":"https://arxiv.org/pdf/2503.10148v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09752v2","updated":"2025-04-11T07:31:49Z","published":"2024-08-19T07:24:36Z","title":"A Unified Framework for Iris Anti-Spoofing: Introducing Iris\n  Anti-Spoofing Cross-Domain-Testing Protocol and Masked-MoE Method","summary":"  Iris recognition is widely used in high-security scenarios due to its\nstability and distinctiveness. However, iris images captured by different\ndevices exhibit certain and device-related consistent differences, which has a\ngreater impact on the classification algorithm for anti-spoofing. The iris of\nvarious races would also affect the classification, causing the risk of\nidentity theft. So it is necessary to improve the cross-domain capabilities of\nthe iris anti-spoofing (IAS) methods to enable it more robust in facing\ndifferent races and devices. However, there is no existing protocol that is\ncomprehensively available. To address this gap, we propose an Iris\nAnti-Spoofing Cross-Domain-Testing (IAS-CDT) Protocol, which involves 10\ndatasets, belonging to 7 databases, published by 4 institutions, and collected\nwith 6 different devices. It contains three sub-protocols hierarchically, aimed\nat evaluating average performance, cross-racial generalization, and\ncross-device generalization of IAS models. Moreover, to address the\ncross-device generalization challenge brought by the IAS-CDT Protocol, we\nemploy multiple model parameter sets to learn from the multiple sub-datasets.\nSpecifically, we utilize the Mixture of Experts (MoE) to fit complex data\ndistributions using multiple sub-neural networks. To further enhance the\ngeneralization capabilities, we propose a novel method Masked-MoE (MMoE), which\nrandomly masks a portion of tokens for some experts and requires their outputs\nto be similar to the unmasked experts, which can effectively mitigate the\noverfitting issue of MoE. For the evaluation, we selected ResNet50, VIT-B/16,\nCLIP, and FLIP as representative models and benchmarked them under the proposed\nIAS-CDT Protocol.\n","authors":["Hang Zou","Chenxi Du","Ajian Liu","Yuan Zhang","Jing Liu","Mingchuan Yang","Jun Wan","Hui Zhang","Zhenan Sun"],"pdf_url":"https://arxiv.org/pdf/2408.09752v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14985v2","updated":"2025-04-11T07:26:47Z","published":"2023-05-24T10:19:57Z","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via\n  Large Language Models","summary":"  The field of vision-and-language (VL) understanding has made unprecedented\nprogress with end-to-end large pre-trained VL models (VLMs). However, they\nstill fall short in zero-shot reasoning tasks that require multi-step\ninferencing. To achieve this goal, previous works resort to a\ndivide-and-conquer pipeline. In this paper, we argue that previous efforts have\nseveral inherent shortcomings: 1) They rely on domain-specific sub-question\ndecomposing models. 2) They force models to predict the final answer even if\nthe sub-questions or sub-answers provide insufficient information. We address\nthese limitations via IdealGPT, a framework that iteratively decomposes VL\nreasoning using large language models (LLMs). Specifically, IdealGPT utilizes\nan LLM to generate sub-questions, a VLM to provide corresponding sub-answers,\nand another LLM to reason to achieve the final answer. These three modules\nperform the divide-and-conquer procedure iteratively until the model is\nconfident about the final answer to the main question. We evaluate IdealGPT on\nmultiple challenging VL reasoning tasks under a zero-shot setting. In\nparticular, our IdealGPT outperforms the best existing GPT-4-like models by an\nabsolute 10% on VCR and 15% on SNLI-VE. Code is available at\nhttps://github.com/Hxyou/IdealGPT\n","authors":["Haoxuan You","Zhecan Wang","Rui Sun","Long Chen","Gengyu Wang","Hammad A. Ayyubi","Kai-Wei Chang","Shih-Fu Chang"],"pdf_url":"https://arxiv.org/pdf/2305.14985v2.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.08307v1","updated":"2025-04-11T07:18:42Z","published":"2025-04-11T07:18:42Z","title":"DSM: Building A Diverse Semantic Map for 3D Visual Grounding","summary":"  In recent years, with the growing research and application of multimodal\nlarge language models (VLMs) in robotics, there has been an increasing trend of\nutilizing VLMs for robotic scene understanding tasks. Existing approaches that\nuse VLMs for 3D Visual Grounding tasks often focus on obtaining scene\ninformation through geometric and visual information, overlooking the\nextraction of diverse semantic information from the scene and the understanding\nof rich implicit semantic attributes, such as appearance, physics, and\naffordance. The 3D scene graph, which combines geometry and language, is an\nideal representation method for environmental perception and is an effective\ncarrier for language models in 3D Visual Grounding tasks. To address these\nissues, we propose a diverse semantic map construction method specifically\ndesigned for robotic agents performing 3D Visual Grounding tasks. This method\nleverages VLMs to capture the latent semantic attributes and relations of\nobjects within the scene and creates a Diverse Semantic Map (DSM) through a\ngeometry sliding-window map construction strategy. We enhance the understanding\nof grounding information based on DSM and introduce a novel approach named\nDSM-Grounding. Experimental results show that our method outperforms current\napproaches in tasks like semantic segmentation and 3D Visual Grounding,\nparticularly excelling in overall metrics compared to the state-of-the-art. In\naddition, we have deployed this method on robots to validate its effectiveness\nin navigation and grasping tasks.\n","authors":["Qinghongbing Xie","Zijian Liang","Long Zeng"],"pdf_url":"https://arxiv.org/pdf/2504.08307v1.pdf","comment":"8 pages, 6 figures, submitted to IROS, Project Page:\n  https://binicey.github.io/DSM"},{"id":"http://arxiv.org/abs/2504.08306v1","updated":"2025-04-11T07:15:32Z","published":"2025-04-11T07:15:32Z","title":"STSeg-Complex Video Object Segmentation: The 1st Solution for 4th PVUW\n  MOSE Challenge","summary":"  Segmentation of video objects in complex scenarios is highly challenging, and\nthe MOSE dataset has significantly contributed to the development of this\nfield. This technical report details the STSeg solution proposed by the\n\"imaplus\" team.By finetuning SAM2 and the unsupervised model TMO on the MOSE\ndataset, the STSeg solution demonstrates remarkable advantages in handling\ncomplex object motions and long-video sequences. In the inference phase, an\nAdaptive Pseudo-labels Guided Model Refinement Pipeline is adopted to\nintelligently select appropriate models for processing each video. Through\nfinetuning the models and employing the Adaptive Pseudo-labels Guided Model\nRefinement Pipeline in the inference phase, the STSeg solution achieved a J&F\nscore of 87.26% on the test set of the 2025 4th PVUW Challenge MOSE Track,\nsecuring the 1st place and advancing the technology for video object\nsegmentation in complex scenarios.\n","authors":["Kehuan Song","Xinglin Xie","Kexin Zhang","Licheng Jiao","Lingling Li","Shuyuan Yang"],"pdf_url":"https://arxiv.org/pdf/2504.08306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04447v2","updated":"2025-04-11T07:10:02Z","published":"2024-12-05T18:57:23Z","title":"EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning\n  in Real-World Scenarios","summary":"  The advent of Multimodal Large Language Models, leveraging the power of Large\nLanguage Models, has recently demonstrated superior multimodal understanding\nand reasoning abilities, heralding a new era for artificial general\nintelligence. However, achieving AGI necessitates more than just comprehension\nand reasoning. A crucial capability required is effective planning in diverse\nscenarios, which involves making reasonable decisions based on complex\nenvironments to solve real-world problems. Despite its importance, the planning\nabilities of current MLLMs in varied scenarios remain underexplored. In this\npaper, we introduce EgoPlan-Bench2, a rigorous and comprehensive benchmark\ndesigned to assess the planning capabilities of MLLMs across a wide range of\nreal-world scenarios. EgoPlan-Bench2 encompasses everyday tasks spanning 4\nmajor domains and 24 detailed scenarios, closely aligned with human daily life.\nEgoPlan-Bench2 is constructed through a semi-automatic process utilizing\negocentric videos, complemented by manual verification. Grounded in a\nfirst-person perspective, it mirrors the way humans approach problem-solving in\neveryday life. We evaluate 21 competitive MLLMs and provide an in-depth\nanalysis of their limitations, revealing that they face significant challenges\nin real-world planning. To further improve the planning proficiency of current\nMLLMs, we propose a training-free approach using multimodal Chain-of-Thought\n(CoT) prompting through investigating the effectiveness of various multimodal\nprompts in complex planning. Our approach enhances the performance of GPT-4V by\n10.24 on EgoPlan-Bench2 without additional training. Our work not only sheds\nlight on the current limitations of MLLMs in planning, but also provides\ninsights for future enhancements in this critical area. We have made data and\ncode available at https://qiulu66.github.io/egoplanbench2/.\n","authors":["Lu Qiu","Yi Chen","Yuying Ge","Yixiao Ge","Ying Shan","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2412.04447v2.pdf","comment":"Code & data are available at:\n  https://qiulu66.github.io/egoplanbench2/"},{"id":"http://arxiv.org/abs/2504.07792v2","updated":"2025-04-11T06:59:07Z","published":"2025-04-10T14:27:25Z","title":"Breaking the Barriers: Video Vision Transformers for Word-Level Sign\n  Language Recognition","summary":"  Sign language is a fundamental means of communication for the deaf and\nhard-of-hearing (DHH) community, enabling nuanced expression through gestures,\nfacial expressions, and body movements. Despite its critical role in\nfacilitating interaction within the DHH population, significant barriers\npersist due to the limited fluency in sign language among the hearing\npopulation. Overcoming this communication gap through automatic sign language\nrecognition (SLR) remains a challenge, particularly at a dynamic word-level,\nwhere temporal and spatial dependencies must be effectively recognized. While\nConvolutional Neural Networks (CNNs) have shown potential in SLR, they are\ncomputationally intensive and have difficulties in capturing global temporal\ndependencies between video sequences. To address these limitations, we propose\na Video Vision Transformer (ViViT) model for word-level American Sign Language\n(ASL) recognition. Transformer models make use of self-attention mechanisms to\neffectively capture global relationships across spatial and temporal\ndimensions, which makes them suitable for complex gesture recognition tasks.\nThe VideoMAE model achieves a Top-1 accuracy of 75.58% on the WLASL100 dataset,\nhighlighting its strong performance compared to traditional CNNs with 65.89%.\nOur study demonstrates that transformer-based architectures have great\npotential to advance SLR, overcome communication barriers and promote the\ninclusion of DHH individuals.\n","authors":["Alexander Brettmann","Jakob Grävinghoff","Marlene Rüschoff","Marie Westhues"],"pdf_url":"https://arxiv.org/pdf/2504.07792v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10193v2","updated":"2025-04-11T06:56:20Z","published":"2024-11-15T13:47:33Z","title":"DiMoDif: Discourse Modality-information Differentiation for Audio-visual\n  Deepfake Detection and Localization","summary":"  Deepfake technology has rapidly advanced and poses significant threats to\ninformation integrity and trust in online multimedia. While significant\nprogress has been made in detecting deepfakes, the simultaneous manipulation of\naudio and visual modalities, sometimes at small parts or in subtle ways,\npresents highly challenging detection scenarios. To address these challenges,\nwe present DiMoDif, an audio-visual deepfake detection framework that leverages\nthe inter-modality differences in machine perception of speech, based on the\nassumption that in real samples -- in contrast to deepfakes -- visual and audio\nsignals coincide in terms of information. DiMoDif leverages features from deep\nnetworks that specialize in visual and audio speech recognition to spot\nframe-level cross-modal incongruities, and in that way to temporally localize\nthe deepfake forgery. To this end, we devise a hierarchical cross-modal fusion\nnetwork, integrating adaptive temporal alignment modules and a learned\ndiscrepancy mapping layer to explicitly model the subtle differences between\nvisual and audio representations. Then, the detection model is optimized\nthrough a composite loss function accounting for frame-level detections and\nfake intervals localization. DiMoDif outperforms the state-of-the-art on the\nDeepfake Detection task by 30.5 AUC on the highly challenging AV-Deepfake1M,\nwhile it performs exceptionally on FakeAVCeleb and LAV-DF. On the Temporal\nForgery Localization task, it outperforms the state-of-the-art by 47.88 AP@0.75\non AV-Deepfake1M, and performs on-par on LAV-DF. Code available at\nhttps://github.com/mever-team/dimodif.\n","authors":["Christos Koutlis","Symeon Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2411.10193v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08296v1","updated":"2025-04-11T06:54:29Z","published":"2025-04-11T06:54:29Z","title":"Generative AI for Film Creation: A Survey of Recent Advances","summary":"  Generative AI (GenAI) is transforming filmmaking, equipping artists with\ntools like text-to-image and image-to-video diffusion, neural radiance fields,\navatar generation, and 3D synthesis. This paper examines the adoption of these\ntechnologies in filmmaking, analyzing workflows from recent AI-driven films to\nunderstand how GenAI contributes to character creation, aesthetic styling, and\nnarration. We explore key strategies for maintaining character consistency,\nachieving stylistic coherence, and ensuring motion continuity. Additionally, we\nhighlight emerging trends such as the growing use of 3D generation and the\nintegration of real footage with AI-generated elements.\n  Beyond technical advancements, we examine how GenAI is enabling new artistic\nexpressions, from generating hard-to-shoot footage to dreamlike diffusion-based\nmorphing effects, abstract visuals, and unworldly objects. We also gather\nartists' feedback on challenges and desired improvements, including\nconsistency, controllability, fine-grained editing, and motion refinement. Our\nstudy provides insights into the evolving intersection of AI and filmmaking,\noffering a roadmap for researchers and artists navigating this rapidly\nexpanding field.\n","authors":["Ruihan Zhang","Borou Yu","Jiajian Min","Yetong Xin","Zheng Wei","Juncheng Nemo Shi","Mingzhen Huang","Xianghao Kong","Nix Liu Xin","Shanshan Jiang","Praagya Bahuguna","Mark Chan","Khushi Hora","Lijian Yang","Yongqi Liang","Runhe Bian","Yunlei Liu","Isabela Campillo Valencia","Patricia Morales Tredinick","Ilia Kozlov","Sijia Jiang","Peiwen Huang","Na Chen","Xuanxuan Liu","Anyi Rao"],"pdf_url":"https://arxiv.org/pdf/2504.08296v1.pdf","comment":"Accepted at CVPR 2025 CVEU workshop: AI for Creative Visual Content\n  Generation Editing and Understanding"},{"id":"http://arxiv.org/abs/2408.11278v2","updated":"2025-04-11T06:53:12Z","published":"2024-08-21T02:01:15Z","title":"The Key of Parameter Skew in Federated Learning","summary":"  Federated Learning (FL) has emerged as an excellent solution for performing\ndeep learning on different data owners without exchanging raw data. However,\nstatistical heterogeneity in FL presents a key challenge, leading to a\nphenomenon of skewness in local model parameter distributions that researchers\nhave largely overlooked. In this work, we propose the concept of parameter skew\nto describe the phenomenon that can substantially affect the accuracy of global\nmodel parameter estimation. Additionally, we introduce FedSA, an aggregation\nstrategy to obtain a high-quality global model, to address the implication from\nparameter skew. Specifically, we categorize parameters into high-dispersion and\nlow-dispersion groups based on the coefficient of variation. For\nhigh-dispersion parameters, Micro-Classes (MIC) and Macro-Classes (MAC)\nrepresent the dispersion at the micro and macro levels, respectively, forming\nthe foundation of FedSA. To evaluate the effectiveness of FedSA, we conduct\nextensive experiments with different FL algorithms on three computer vision\ndatasets. FedSA outperforms eight state-of-the-art baselines by about 4.7% in\ntest accuracy.\n","authors":["Junfeng Liao","Sifan Wang","Ye Yuan","Riquan Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.11278v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17161v2","updated":"2025-04-11T06:50:39Z","published":"2024-11-26T07:05:05Z","title":"Enhancing Lane Segment Perception and Topology Reasoning with\n  Crowdsourcing Trajectory Priors","summary":"  In autonomous driving, recent advances in lane segment perception provide\nautonomous vehicles with a comprehensive understanding of driving scenarios.\nMoreover, incorporating prior information input into such perception model\nrepresents an effective approach to ensure the robustness and accuracy.\nHowever, utilizing diverse sources of prior information still faces three key\nchallenges: the acquisition of high-quality prior information, alignment\nbetween prior and online perception, efficient integration. To address these\nissues, we investigate prior augmentation from a novel perspective of\ntrajectory priors. In this paper, we initially extract crowdsourcing trajectory\ndata from Argoverse2 motion forecasting dataset and encode trajectory data into\nrasterized heatmap and vectorized instance tokens, then we incorporate such\nprior information into the online mapping model through different ways.\nBesides, with the purpose of mitigating the misalignment between prior and\nonline perception, we design a confidence-based fusion module that takes\nalignment into account during the fusion process. We conduct extensive\nexperiments on OpenLane-V2 dataset. The results indicate that our method's\nperformance significantly outperforms the current state-of-the-art methods.\nCode is released is at https://github.com/wowlza/TrajTopo\n","authors":["Peijin Jia","Ziang Luo","Tuopu Wen","Mengmeng Yang","Kun Jiang","Le Cui","Diange Yang"],"pdf_url":"https://arxiv.org/pdf/2411.17161v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2504.08291v1","updated":"2025-04-11T06:49:33Z","published":"2025-04-11T06:49:33Z","title":"DreamFuse: Adaptive Image Fusion with Diffusion Transformer","summary":"  Image fusion seeks to seamlessly integrate foreground objects with background\nscenes, producing realistic and harmonious fused images. Unlike existing\nmethods that directly insert objects into the background, adaptive and\ninteractive fusion remains a challenging yet appealing task. It requires the\nforeground to adjust or interact with the background context, enabling more\ncoherent integration. To address this, we propose an iterative\nhuman-in-the-loop data generation pipeline, which leverages limited initial\ndata with diverse textual prompts to generate fusion datasets across various\nscenarios and interactions, including placement, holding, wearing, and style\ntransfer. Building on this, we introduce DreamFuse, a novel approach based on\nthe Diffusion Transformer (DiT) model, to generate consistent and harmonious\nfused images with both foreground and background information. DreamFuse employs\na Positional Affine mechanism to inject the size and position of the foreground\ninto the background, enabling effective foreground-background interaction\nthrough shared attention. Furthermore, we apply Localized Direct Preference\nOptimization guided by human feedback to refine DreamFuse, enhancing background\nconsistency and foreground harmony. DreamFuse achieves harmonious fusion while\ngeneralizing to text-driven attribute editing of the fused results.\nExperimental results demonstrate that our method outperforms state-of-the-art\napproaches across multiple metrics.\n","authors":["Junjia Huang","Pengxiang Yan","Jiyang Liu","Jie Wu","Zhao Wang","Yitong Wang","Liang Lin","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2504.08291v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2504.07951v2","updated":"2025-04-11T06:35:42Z","published":"2025-04-10T17:57:28Z","title":"Scaling Laws for Native Multimodal Models","summary":"  Building general-purpose models that can effectively perceive the world\nthrough multimodal signals has been a long-standing goal. Current approaches\ninvolve integrating separately pre-trained components, such as connecting\nvision encoders to LLMs and continuing multimodal training. While such\napproaches exhibit remarkable sample efficiency, it remains an open question\nwhether such late-fusion architectures are inherently superior. In this work,\nwe revisit the architectural design of native multimodal models (NMMs)--those\ntrained from the ground up on all modalities--and conduct an extensive scaling\nlaws study, spanning 457 trained models with different architectures and\ntraining mixtures. Our investigation reveals no inherent advantage to\nlate-fusion architectures over early-fusion ones, which do not rely on image\nencoders. On the contrary, early-fusion exhibits stronger performance at lower\nparameter counts, is more efficient to train, and is easier to deploy.\nMotivated by the strong performance of the early-fusion architectures, we show\nthat incorporating Mixture of Experts (MoEs) allows for models that learn\nmodality-specific weights, significantly enhancing performance.\n","authors":["Mustafa Shukor","Enrico Fini","Victor Guilherme Turrisi da Costa","Matthieu Cord","Joshua Susskind","Alaaeldin El-Nouby"],"pdf_url":"https://arxiv.org/pdf/2504.07951v2.pdf","comment":"31 pages, 26 figures, 13 tables"},{"id":"http://arxiv.org/abs/2504.08280v1","updated":"2025-04-11T06:25:11Z","published":"2025-04-11T06:25:11Z","title":"PNE-SGAN: Probabilistic NDT-Enhanced Semantic Graph Attention Network\n  for LiDAR Loop Closure Detection","summary":"  LiDAR loop closure detection (LCD) is crucial for consistent Simultaneous\nLocalization and Mapping (SLAM) but faces challenges in robustness and\naccuracy. Existing methods, including semantic graph approaches, often suffer\nfrom coarse geometric representations and lack temporal robustness against\nnoise, dynamics, and viewpoint changes. We introduce PNE-SGAN, a Probabilistic\nNDT-Enhanced Semantic Graph Attention Network, to overcome these limitations.\nPNE-SGAN enhances semantic graphs by using Normal Distributions Transform (NDT)\ncovariance matrices as rich, discriminative geometric node features, processed\nvia a Graph Attention Network (GAT). Crucially, it integrates graph similarity\nscores into a probabilistic temporal filtering framework (modeled as an\nHMM/Bayes filter), incorporating uncertain odometry for motion modeling and\nutilizing forward-backward smoothing to effectively handle ambiguities.\nEvaluations on challenging KITTI sequences (00 and 08) demonstrate\nstate-of-the-art performance, achieving Average Precision of 96.2\\% and 95.1\\%,\nrespectively. PNE-SGAN significantly outperforms existing methods, particularly\nin difficult bidirectional loop scenarios where others falter. By synergizing\ndetailed NDT geometry with principled probabilistic temporal reasoning,\nPNE-SGAN offers a highly accurate and robust solution for LiDAR LCD, enhancing\nSLAM reliability in complex, large-scale environments.\n","authors":["Xiong Li","Shulei Liu","Xingning Chen","Yisong Wu","Dong Zhu"],"pdf_url":"https://arxiv.org/pdf/2504.08280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08272v1","updated":"2025-04-11T06:00:06Z","published":"2025-04-11T06:00:06Z","title":"Palmprint De-Identification Using Diffusion Model for High-Quality and\n  Diverse Synthesis","summary":"  Palmprint recognition techniques have advanced significantly in recent years,\nenabling reliable recognition even when palmprints are captured in uncontrolled\nor challenging environments. However, this strength also introduces new risks,\nas publicly available palmprint images can be misused by adversaries for\nmalicious activities. Despite this growing concern, research on methods to\nobscure or anonymize palmprints remains largely unexplored. Thus, it is\nessential to develop a palmprint de-identification technique capable of\nremoving identity-revealing features while retaining the image's utility and\npreserving non-sensitive information. In this paper, we propose a training-free\nframework that utilizes pre-trained diffusion models to generate diverse,\nhigh-quality palmprint images that conceal identity features for\nde-identification purposes. To ensure greater stability and controllability in\nthe synthesis process, we incorporate a semantic-guided embedding fusion\nalongside a prior interpolation mechanism. We further propose the\nde-identification ratio, a novel metric for intuitive de-identification\nassessment. Extensive experiments across multiple palmprint datasets and\nrecognition methods demonstrate that our method effectively conceals\nidentity-related traits with significant diversity across de-identified\nsamples. The de-identified samples preserve high visual fidelity and maintain\nexcellent usability, achieving a balance between de-identification and\nretaining non-identity information.\n","authors":["Licheng Yan","Bob Zhang","Andrew Beng Jin Teoh","Lu Leng","Shuyi Li","Yuqi Wang","Ziyuan Yang"],"pdf_url":"https://arxiv.org/pdf/2504.08272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15344v3","updated":"2025-04-11T05:53:17Z","published":"2024-09-10T07:04:48Z","title":"Video-Driven Graph Network-Based Simulators","summary":"  Lifelike visualizations in design, cinematography, and gaming rely on precise\nphysics simulations, typically requiring extensive computational resources and\ndetailed physical input. This paper presents a method that can infer a system's\nphysical properties from a short video, eliminating the need for explicit\nparameter input, provided it is close to the training condition. The learned\nrepresentation is then used within a Graph Network-based Simulator to emulate\nthe trajectories of physical systems. We demonstrate that the video-derived\nencodings effectively capture the physical properties of the system and\nshowcase a linear dependence between some of the encodings and the system's\nmotion.\n","authors":["Franciszek Szewczyk","Gilles Louppe","Matthia Sabatelli"],"pdf_url":"https://arxiv.org/pdf/2409.15344v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08269v1","updated":"2025-04-11T05:51:44Z","published":"2025-04-11T05:51:44Z","title":"VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop\n  Question Answering","summary":"  The increasing availability of multimodal data across text, tables, and\nimages presents new challenges for developing models capable of complex\ncross-modal reasoning. Existing methods for Multimodal Multi-hop Question\nAnswering (MMQA) often suffer from limited reasoning capabilities, reliance on\nmodality conversion, and inadequate alignment between visual and textual\nrepresentations. To address these limitations, this paper introduces\nVision-Language Multimodal Transformer (VLMT), a unified architecture that\nintegrates a transformer-based vision encoder with a sequence-to-sequence\nlanguage model. VLMT employs a direct token-level injection mechanism to fuse\nvisual and textual inputs within a shared embedding space, eliminating the need\nfor intermediate projection layers. To enhance cross-modal alignment and\nreasoning, a three-stage pretraining strategy is proposed to progressively\nalign vision-language representations and improve the model's capacity for\nmultimodal understanding. Based on the pretrained backbone, two task-specific\nmodules are instantiated to form a two-stage MMQA framework: a multimodal\nreranker that predicts document relevance scores and utilizes a relative\nthreshold with top-k strategy for context retrieval, and a multimodal question\nanswering model that generates contextually grounded answers based on the\nretrieved evidence. Comprehensive experiments on two benchmark datasets\ndemonstrate the effectiveness of the proposed approach. On MultimodalQA\nvalidation set, VLMT-Large achieves 76.5% Exact Match and 80.1% F1,\noutperforming the previous state-of-the-art by +9.1% in Exact Match and +8.8%\nin F1. On WebQA, it attains a QA score of 47.6, surpassing prior models such as\nPERQA by +3.2. These results highlight VLMT's strong capabilities in multimodal\nreasoning and its potential to advance real-world information retrieval and\nquestion answering systems.\n","authors":["Qi Zhi Lim","Chin Poo Lee","Kian Ming Lim","Kalaiarasi Sonai Muthu Anbananthen"],"pdf_url":"https://arxiv.org/pdf/2504.08269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12165v2","updated":"2025-04-11T05:47:28Z","published":"2025-03-15T15:08:48Z","title":"VTON 360: High-Fidelity Virtual Try-On from Any Viewing Direction","summary":"  Virtual Try-On (VTON) is a transformative technology in e-commerce and\nfashion design, enabling realistic digital visualization of clothing on\nindividuals. In this work, we propose VTON 360, a novel 3D VTON method that\naddresses the open challenge of achieving high-fidelity VTON that supports\nany-view rendering. Specifically, we leverage the equivalence between a 3D\nmodel and its rendered multi-view 2D images, and reformulate 3D VTON as an\nextension of 2D VTON that ensures 3D consistent results across multiple views.\nTo achieve this, we extend 2D VTON models to include multi-view garments and\nclothing-agnostic human body images as input, and propose several novel\ntechniques to enhance them, including: i) a pseudo-3D pose representation using\nnormal maps derived from the SMPL-X 3D human model, ii) a multi-view spatial\nattention mechanism that models the correlations between features from\ndifferent viewing angles, and iii) a multi-view CLIP embedding that enhances\nthe garment CLIP features used in 2D VTON with camera information. Extensive\nexperiments on large-scale real datasets and clothing images from e-commerce\nplatforms demonstrate the effectiveness of our approach. Project page:\nhttps://scnuhealthy.github.io/VTON360.\n","authors":["Zijian He","Yuwei Ning","Yipeng Qin","Guangrun Wang","Sibei Yang","Liang Lin","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2503.12165v2.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2504.06121v2","updated":"2025-04-11T05:25:05Z","published":"2025-04-08T15:13:01Z","title":"A Robust Real-Time Lane Detection Method with Fog-Enhanced Feature\n  Fusion for Foggy Conditions","summary":"  Lane detection is a critical component of Advanced Driver Assistance Systems\n(ADAS). Existing lane detection algorithms generally perform well under\nfavorable weather conditions. However, their performance degrades significantly\nin adverse conditions, such as fog, which increases the risk of traffic\naccidents. This challenge is compounded by the lack of specialized datasets and\nmethods designed for foggy environments. To address this, we introduce the\nFoggyLane dataset, captured in real-world foggy scenarios, and synthesize two\nadditional datasets, FoggyCULane and FoggyTusimple, from existing popular lane\ndetection datasets. Furthermore, we propose a robust Fog-Enhanced Network for\nlane detection, incorporating a Global Feature Fusion Module (GFFM) to capture\nglobal relationships in foggy images, a Kernel Feature Fusion Module (KFFM) to\nmodel the structural and positional relationships of lane instances, and a\nLow-level Edge Enhanced Module (LEEM) to address missing edge details in foggy\nconditions. Comprehensive experiments demonstrate that our method achieves\nstate-of-the-art performance, with F1-scores of 95.04 on FoggyLane, 79.85 on\nFoggyCULane, and 96.95 on FoggyTusimple. Additionally, with TensorRT\nacceleration, the method reaches a processing speed of 38.4 FPS on the NVIDIA\nJetson AGX Orin, confirming its real-time capabilities and robustness in foggy\nenvironments.\n","authors":["Ronghui Zhang","Yuhang Ma","Tengfei Li","Ziyu Lin","Yueying Wu","Junzhou Chen","Lin Zhang","Jia Hu","Tony Z. Qiu","Konghui Guo"],"pdf_url":"https://arxiv.org/pdf/2504.06121v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13983v3","updated":"2025-04-11T05:22:55Z","published":"2025-03-18T07:40:36Z","title":"SpaceVLLM: Endowing Multimodal Large Language Model with Spatio-Temporal\n  Video Grounding Capability","summary":"  Multimodal large language models (MLLMs) have made remarkable progress in\neither temporal or spatial localization. However, they struggle to perform\nspatio-temporal video grounding. This limitation stems from two major\nchallenges. Firstly, it is difficult to extract accurate spatio-temporal\ninformation of each frame in the video. Secondly, the substantial number of\nvisual tokens makes it challenging to precisely map visual tokens of each frame\nto their corresponding spatial coordinates. To address these issues, we\nintroduce SpaceVLLM, a MLLM endowed with spatio-temporal video grounding\ncapability. Specifically, we adopt a set of interleaved Spatio-Temporal Aware\nQueries to capture temporal perception and dynamic spatial information.\nMoreover, we propose a Query-Guided Space Decoder to establish a corresponding\nconnection between the queries and spatial coordinates. Additionally, due to\nthe lack of spatio-temporal datasets, we construct the Unified Spatio-Temporal\nGrounding (Uni-STG) dataset, comprising 480K instances across three tasks. This\ndataset fully exploits the potential of MLLM to simultaneously facilitate\nlocalization in both temporal and spatial dimensions. Extensive experiments\ndemonstrate that SpaceVLLM achieves the state-of-the-art performance across 11\nbenchmarks covering temporal, spatial, spatio-temporal and video understanding\ntasks, highlighting the effectiveness of our approach. Our code, datasets and\nmodel will be released at https://github.com/Jayce1kk/SpaceVLLM.\n","authors":["Jiankang Wang","Zhihan Zhang","Zhihang Liu","Yang Li","Jiannan Ge","Hongtao Xie","Yongdong Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.13983v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08259v1","updated":"2025-04-11T05:11:17Z","published":"2025-04-11T05:11:17Z","title":"CoProSketch: Controllable and Progressive Sketch Generation with\n  Diffusion Model","summary":"  Sketches serve as fundamental blueprints in artistic creation because sketch\nediting is easier and more intuitive than pixel-level RGB image editing for\npainting artists, yet sketch generation remains unexplored despite advancements\nin generative models. We propose a novel framework CoProSketch, providing\nprominent controllability and details for sketch generation with diffusion\nmodels. A straightforward method is fine-tuning a pretrained image generation\ndiffusion model with binarized sketch images. However, we find that the\ndiffusion models fail to generate clear binary images, which makes the produced\nsketches chaotic. We thus propose to represent the sketches by unsigned\ndistance field (UDF), which is continuous and can be easily decoded to sketches\nthrough a lightweight network. With CoProSketch, users generate a rough sketch\nfrom a bounding box and a text prompt. The rough sketch can be manually edited\nand fed back into the model for iterative refinement and will be decoded to a\ndetailed sketch as the final result. Additionally, we curate the first\nlarge-scale text-sketch paired dataset as the training data. Experiments\ndemonstrate superior semantic consistency and controllability over baselines,\noffering a practical solution for integrating user feedback into generative\nworkflows.\n","authors":["Ruohao Zhan","Yijin Li","Yisheng He","Shuo Chen","Yichen Shen","Xinyu Chen","Zilong Dong","Zhaoyang Huang","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.08259v1.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.00124v2","updated":"2025-04-11T04:50:35Z","published":"2024-11-28T11:19:04Z","title":"Auto-Encoded Supervision for Perceptual Image Super-Resolution","summary":"  This work tackles the fidelity objective in the perceptual\nsuper-resolution~(SR). Specifically, we address the shortcomings of pixel-level\n$L_\\text{p}$ loss ($\\mathcal{L}_\\text{pix}$) in the GAN-based SR framework.\nSince $L_\\text{pix}$ is known to have a trade-off relationship against\nperceptual quality, prior methods often multiply a small scale factor or\nutilize low-pass filters. However, this work shows that these circumventions\nfail to address the fundamental factor that induces blurring. Accordingly, we\nfocus on two points: 1) precisely discriminating the subcomponent of\n$L_\\text{pix}$ that contributes to blurring, and 2) only guiding based on the\nfactor that is free from this trade-off relationship. We show that they can be\nachieved in a surprisingly simple manner, with an Auto-Encoder (AE) pretrained\nwith $L_\\text{pix}$. Accordingly, we propose the Auto-Encoded Supervision for\nOptimal Penalization loss ($L_\\text{AESOP}$), a novel loss function that\nmeasures distance in the AE space, instead of the raw pixel space. Note that\nthe AE space indicates the space after the decoder, not the bottleneck. By\nsimply substituting $L_\\text{pix}$ with $L_\\text{AESOP}$, we can provide\neffective reconstruction guidance without compromising perceptual quality.\nDesigned for simplicity, our method enables easy integration into existing SR\nframeworks. Experimental results verify that AESOP can lead to favorable\nresults in the perceptual SR task.\n","authors":["MinKyu Lee","Sangeek Hyun","Woojin Jun","Jae-Pil Heo"],"pdf_url":"https://arxiv.org/pdf/2412.00124v2.pdf","comment":"Codes are available at\n  https://github.com/2minkyulee/AESOP-Auto-Encoded-Supervision-for-Perceptual-Image-Super-Resolution"},{"id":"http://arxiv.org/abs/2504.08253v1","updated":"2025-04-11T04:34:18Z","published":"2025-04-11T04:34:18Z","title":"Knowledge Distillation for Underwater Feature Extraction and Matching\n  via GAN-synthesized Images","summary":"  Autonomous Underwater Vehicles (AUVs) play a crucial role in underwater\nexploration. Vision-based methods offer cost-effective solutions for\nlocalization and mapping in the absence of conventional sensors like GPS and\nLIDAR. However, underwater environments present significant challenges for\nfeature extraction and matching due to image blurring and noise caused by\nattenuation, scattering, and the interference of \\textit{marine snow}. In this\npaper, we aim to improve the robustness of the feature extraction and matching\nin the turbid underwater environment using the cross-modal knowledge\ndistillation method that transfers the in-air feature extraction models to\nunderwater settings using synthetic underwater images as the medium. We first\npropose a novel adaptive GAN-synthesis method to estimate water parameters and\nunderwater noise distribution, to generate environment-specific synthetic\nunderwater images. We then introduce a general knowledge distillation framework\ncompatible with different teacher models. The evaluation of GAN-based synthesis\nhighlights the significance of the new components, i.e. GAN-synthesized noise\nand forward scattering, in the proposed model. Additionally, the downstream\napplication of feature extraction and matching (VSLAM) on real underwater\nsequences validates the effectiveness of the transferred model.\n","authors":["Jinghe Yang","Mingming Gong","Ye Pu"],"pdf_url":"https://arxiv.org/pdf/2504.08253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08252v1","updated":"2025-04-11T04:33:56Z","published":"2025-04-11T04:33:56Z","title":"Stereophotoclinometry Revisited","summary":"  Image-based surface reconstruction and characterization is crucial for\nmissions to small celestial bodies, as it informs mission planning, navigation,\nand scientific analysis. However, current state-of-the-practice methods, such\nas stereophotoclinometry (SPC), rely heavily on human-in-the-loop verification\nand high-fidelity a priori information. This paper proposes\nPhotoclinometry-from-Motion (PhoMo), a novel framework that incorporates\nphotoclinometry techniques into a keypoint-based structure-from-motion (SfM)\nsystem to estimate the surface normal and albedo at detected landmarks to\nimprove autonomous surface and shape characterization of small celestial bodies\nfrom in-situ imagery. In contrast to SPC, we forego the expensive maplet\nestimation step and instead use dense keypoint measurements and correspondences\nfrom an autonomous keypoint detection and matching method based on deep\nlearning. Moreover, we develop a factor graph-based approach allowing for\nsimultaneous optimization of the spacecraft's pose, landmark positions,\nSun-relative direction, and surface normals and albedos via fusion of Sun\nvector measurements and image keypoint measurements. The proposed framework is\nvalidated on real imagery taken by the Dawn mission to the asteroid 4 Vesta and\nthe minor planet 1 Ceres and compared against an SPC reconstruction, where we\ndemonstrate superior rendering performance compared to an SPC solution and\nprecise alignment to a stereophotogrammetry (SPG) solution without relying on\nany a priori camera pose and topography information or humans-in-the-loop.\n","authors":["Travis Driver","Andrew Vaughan","Yang Cheng","Adnan Ansar","John Christian","Panagiotis Tsiotras"],"pdf_url":"https://arxiv.org/pdf/2504.08252v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2312.06865"},{"id":"http://arxiv.org/abs/2501.11858v2","updated":"2025-04-11T04:26:42Z","published":"2025-01-21T03:22:10Z","title":"EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents","summary":"  Multimodal Large Language Models (MLLMs) have shown significant advancements,\nproviding a promising future for embodied agents. Existing benchmarks for\nevaluating MLLMs primarily utilize static images or videos, limiting\nassessments to non-interactive scenarios. Meanwhile, existing embodied AI\nbenchmarks are task-specific and not diverse enough, which do not adequately\nevaluate the embodied capabilities of MLLMs. To address this, we propose\nEmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs\nwith embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied\n3D scenes, each of which is rigorously selected and annotated. It covers a\nbroad spectrum of existing embodied AI tasks with significantly enhanced\ndiversity, all within a unified simulation and evaluation framework tailored\nfor MLLMs. The tasks are organized into five categories: navigation, object\ninteraction, social interaction, attribute question answering, and spatial\nquestion answering to assess different capabilities of the agents. We evaluated\nthe state-of-the-art MLLMs on EmbodiedEval and found that they have a\nsignificant shortfall compared to human level on embodied tasks. Our analysis\ndemonstrates the limitations of existing MLLMs in embodied capabilities,\nproviding insights for their future development. We open-source all evaluation\ndata and simulation framework at https://github.com/thunlp/EmbodiedEval.\n","authors":["Zhili Cheng","Yuge Tu","Ran Li","Shiqi Dai","Jinyi Hu","Shengding Hu","Jiahao Li","Yang Shi","Tianyu Yu","Weize Chen","Lei Shi","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2501.11858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01508v2","updated":"2025-04-11T04:10:10Z","published":"2023-12-03T21:16:37Z","title":"CityGen: Infinite and Controllable City Layout Generation","summary":"  The recent surge in interest in city layout generation underscores its\nsignificance in urban planning and smart city development. The task involves\nprocedurally or automatically generating spatial arrangements for urban\nelements such as roads, buildings, water, and vegetation. Previous methods,\nwhether procedural modeling or deep learning-based approaches like VAEs and\nGANs, rely on complex priors, expert guidance, or initial layouts, and often\nlack diversity and interactivity. In this paper, we present CityGen, an\nend-to-end framework for infinite, diverse, and controllable city layout\ngeneration. Our framework introduces an infinite expansion module to extend\nlocal layouts to city-scale layouts and a multi-scale refinement module to\nupsample and refine them. We also designed a user-friendly control scheme,\nallowing users to guide generation through simple sketching. Additionally, we\nconvert the 2D layout to 3D by synthesizing a height field, facilitating\ndownstream applications. Extensive experiments demonstrate CityGen's\nstate-of-the-art performance across various metrics, making it suitable for a\nwide range of downstream applications.\n","authors":["Jie Deng","Wenhao Chai","Jianshu Guo","Qixuan Huang","Junsheng Huang","Wenhao Hu","Shengyu Hao","Jenq-Neng Hwang","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2312.01508v2.pdf","comment":"Accepted to CVPR 2025 USM3D Workshop"},{"id":"http://arxiv.org/abs/2312.00092v3","updated":"2025-04-11T04:04:43Z","published":"2023-11-30T11:01:37Z","title":"Mixture of Gaussian-distributed Prototypes with Generative Modelling for\n  Interpretable and Trustworthy Image Recognition","summary":"  Prototypical-part methods, e.g., ProtoPNet, enhance interpretability in image\nrecognition by linking predictions to training prototypes, thereby offering\nintuitive insights into their decision-making. Existing methods, which rely on\na point-based learning of prototypes, typically face two critical issues: 1)\nthe learned prototypes have limited representation power and are not suitable\nto detect Out-of-Distribution (OoD) inputs, reducing their decision\ntrustworthiness; and 2) the necessary projection of the learned prototypes back\ninto the space of training images causes a drastic degradation in the\npredictive performance. Furthermore, current prototype learning adopts an\naggressive approach that considers only the most active object parts during\ntraining, while overlooking sub-salient object regions which still hold crucial\nclassification information. In this paper, we present a new generative paradigm\nto learn prototype distributions, termed as Mixture of Gaussian-distributed\nPrototypes (MGProto). The distribution of prototypes from MGProto enables both\ninterpretable image classification and trustworthy recognition of OoD inputs.\nThe optimisation of MGProto naturally projects the learned prototype\ndistributions back into the training image space, thereby addressing the\nperformance degradation caused by prototype projection. Additionally, we\ndevelop a novel and effective prototype mining strategy that considers not only\nthe most active but also sub-salient object parts. To promote model\ncompactness, we further propose to prune MGProto by removing prototypes with\nlow importance priors. Experiments on CUB-200-2011, Stanford Cars, Stanford\nDogs, and Oxford-IIIT Pets datasets show that MGProto achieves state-of-the-art\nimage recognition and OoD detection performances, while providing encouraging\ninterpretability results.\n","authors":["Chong Wang","Yuanhong Chen","Fengbei Liu","Yuyuan Liu","Davis James McCarthy","Helen Frazer","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2312.00092v3.pdf","comment":"IEEE TPAMI"},{"id":"http://arxiv.org/abs/2311.16445v5","updated":"2025-04-11T04:01:29Z","published":"2023-11-28T03:00:59Z","title":"CLAP: Isolating Content from Style through Contrastive Learning with\n  Augmented Prompts","summary":"  Contrastive vision-language models, such as CLIP, have garnered considerable\nattention for various downstream tasks, mainly due to the remarkable\ngeneralization ability of the learned features. However, the features they\nlearn often blend content and style information, which somewhat limits their\ngeneralization capabilities under distribution shifts. To address this\nlimitation, we adopt a causal generative perspective for multimodal data and\npropose contrastive learning with data augmentation to disentangle content\nfeatures from the original representations. To achieve this, we begin by\nexploring image augmentation techniques and develop a method to seamlessly\nintegrate them into pre-trained CLIP-like models to extract pure content\nfeatures. Taking a step further, and recognizing the inherent semantic richness\nand logical structure of text data, we explore the use of text augmentation to\nisolate latent content from style features. This enables CLIP-like models'\nencoders to concentrate on latent content information, refining the\nrepresentations learned by pre-trained CLIP-like models. Our extensive\nexperiments across diverse datasets demonstrate significant improvements in\nzero-shot and few-shot classification tasks, alongside enhanced robustness to\nvarious perturbations. These results underscore the effectiveness of our\nproposed methods in refining vision-language representations and advancing the\nstate of the art in multimodal learning.\n","authors":["Yichao Cai","Yuhang Liu","Zhen Zhang","Javen Qinfeng Shi"],"pdf_url":"https://arxiv.org/pdf/2311.16445v5.pdf","comment":"Accepted as a conference paper at ECCV 2024"},{"id":"http://arxiv.org/abs/2503.01248v3","updated":"2025-04-11T03:23:52Z","published":"2025-03-03T07:23:56Z","title":"Comprehensive Evaluation of OCT-based Automated Segmentation of Retinal\n  Layer, Fluid and Hyper-Reflective Foci: Impact on Diabetic Retinopathy\n  Severity Assessment","summary":"  Diabetic retinopathy (DR) is a leading cause of vision loss, requiring early\nand accurate assessment to prevent irreversible damage. Spectral Domain Optical\nCoherence Tomography (SD-OCT) enables high-resolution retinal imaging, but\nautomated segmentation performance varies, especially in cases with complex\nfluid and hyperreflective foci (HRF) patterns. This study proposes an\nactive-learning-based deep learning pipeline for automated segmentation of\nretinal layers, fluid, and HRF, using four state-of-the-art models: U-Net,\nSegFormer, SwinUNETR, and VM-UNet, trained on expert-annotated SD-OCT volumes.\nSegmentation accuracy was evaluated with five-fold cross-validation, and\nretinal thickness was quantified using a K-nearest neighbors algorithm and\nvisualized with Early Treatment Diabetic Retinopathy Study (ETDRS) maps.\nSwinUNETR achieved the highest overall accuracy (DSC = 0.7719; NSD = 0.8149),\nwhile VM-UNet excelled in specific layers. Structural differences were observed\nbetween non-proliferative and proliferative DR, with layer-specific thickening\ncorrelating with visual acuity impairment. The proposed framework enables\nrobust, clinically relevant DR assessment while reducing the need for manual\nannotation, supporting improved disease monitoring and treatment planning.\n","authors":["S. Chen","D. Ma","M. Raviselvan","S. Sundaramoorthy","K. Popuri","M. J. Ju","M. V. Sarunic","D. Ratra","M. F. Beg"],"pdf_url":"https://arxiv.org/pdf/2503.01248v3.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.10137v2","updated":"2025-04-11T03:17:52Z","published":"2024-12-13T13:38:41Z","title":"Constraint-Aware Zero-Shot Vision-Language Navigation in Continuous\n  Environments","summary":"  We address the task of Vision-Language Navigation in Continuous Environments\n(VLN-CE) under the zero-shot setting. Zero-shot VLN-CE is particularly\nchallenging due to the absence of expert demonstrations for training and\nminimal environment structural prior to guide navigation. To confront these\nchallenges, we propose a Constraint-Aware Navigator (CA-Nav), which reframes\nzero-shot VLN-CE as a sequential, constraint-aware sub-instruction completion\nprocess. CA-Nav continuously translates sub-instructions into navigation plans\nusing two core modules: the Constraint-Aware Sub-instruction Manager (CSM) and\nthe Constraint-Aware Value Mapper (CVM). CSM defines the completion criteria\nfor decomposed sub-instructions as constraints and tracks navigation progress\nby switching sub-instructions in a constraint-aware manner. CVM, guided by\nCSM's constraints, generates a value map on the fly and refines it using\nsuperpixel clustering to improve navigation stability. CA-Nav achieves the\nstate-of-the-art performance on two VLN-CE benchmarks, surpassing the previous\nbest method by 12 percent and 13 percent in Success Rate on the validation\nunseen splits of R2R-CE and RxR-CE, respectively. Moreover, CA-Nav demonstrates\nits effectiveness in real-world robot deployments across various indoor scenes\nand instructions.\n","authors":["Kehan Chen","Dong An","Yan Huang","Rongtao Xu","Yifei Su","Yonggen Ling","Ian Reid","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.10137v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07378v2","updated":"2025-04-11T03:08:12Z","published":"2025-04-10T01:36:06Z","title":"BRepFormer: Transformer-Based B-rep Geometric Feature Recognition","summary":"  Recognizing geometric features on B-rep models is a cornerstone technique for\nmultimedia content-based retrieval and has been widely applied in intelligent\nmanufacturing. However, previous research often merely focused on Machining\nFeature Recognition (MFR), falling short in effectively capturing the intricate\ntopological and geometric characteristics of complex geometry features. In this\npaper, we propose BRepFormer, a novel transformer-based model to recognize both\nmachining feature and complex CAD models' features. BRepFormer encodes and\nfuses the geometric and topological features of the models. Afterwards,\nBRepFormer utilizes a transformer architecture for feature propagation and a\nrecognition head to identify geometry features. During each iteration of the\ntransformer, we incorporate a bias that combines edge features and topology\nfeatures to reinforce geometric constraints on each face. In addition, we also\nproposed a dataset named Complex B-rep Feature Dataset (CBF), comprising 20,000\nB-rep models. By covering more complex B-rep models, it is better aligned with\nindustrial applications. The experimental results demonstrate that BRepFormer\nachieves state-of-the-art accuracy on the MFInstSeg, MFTRCAD, and our CBF\ndatasets.\n","authors":["Yongkang Dai","Xiaoshui Huang","Yunpeng Bai","Hao Guo","Hongping Gan","Ling Yang","Yilei Shi"],"pdf_url":"https://arxiv.org/pdf/2504.07378v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08222v1","updated":"2025-04-11T03:05:35Z","published":"2025-04-11T03:05:35Z","title":"F$^3$Set: Towards Analyzing Fast, Frequent, and Fine-grained Events from\n  Videos","summary":"  Analyzing Fast, Frequent, and Fine-grained (F$^3$) events presents a\nsignificant challenge in video analytics and multi-modal LLMs. Current methods\nstruggle to identify events that satisfy all the F$^3$ criteria with high\naccuracy due to challenges such as motion blur and subtle visual discrepancies.\nTo advance research in video understanding, we introduce F$^3$Set, a benchmark\nthat consists of video datasets for precise F$^3$ event detection. Datasets in\nF$^3$Set are characterized by their extensive scale and comprehensive detail,\nusually encompassing over 1,000 event types with precise timestamps and\nsupporting multi-level granularity. Currently, F$^3$Set contains several sports\ndatasets, and this framework may be extended to other applications as well. We\nevaluated popular temporal action understanding methods on F$^3$Set, revealing\nsubstantial challenges for existing techniques. Additionally, we propose a new\nmethod, F$^3$ED, for F$^3$ event detections, achieving superior performance.\nThe dataset, model, and benchmark code are available at\nhttps://github.com/F3Set/F3Set.\n","authors":["Zhaoyu Liu","Kan Jiang","Murong Ma","Zhe Hou","Yun Lin","Jin Song Dong"],"pdf_url":"https://arxiv.org/pdf/2504.08222v1.pdf","comment":"The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)"},{"id":"http://arxiv.org/abs/2503.13947v2","updated":"2025-04-11T03:03:26Z","published":"2025-03-18T06:27:57Z","title":"Conformal Prediction and MLLM aided Uncertainty Quantification in Scene\n  Graph Generation","summary":"  Scene Graph Generation (SGG) aims to represent visual scenes by identifying\nobjects and their pairwise relationships, providing a structured understanding\nof image content. However, inherent challenges like long-tailed class\ndistributions and prediction variability necessitate uncertainty quantification\nin SGG for its practical viability. In this paper, we introduce a novel\nConformal Prediction (CP) based framework, adaptive to any existing SGG method,\nfor quantifying their predictive uncertainty by constructing well-calibrated\nprediction sets over their generated scene graphs. These scene graph prediction\nsets are designed to achieve statistically rigorous coverage guarantees.\nAdditionally, to ensure these prediction sets contain the most practically\ninterpretable scene graphs, we design an effective MLLM-based post-processing\nstrategy for selecting the most visually and semantically plausible scene\ngraphs within these prediction sets. We show that our proposed approach can\nproduce diverse possible scene graphs from an image, assess the reliability of\nSGG methods, and improve overall SGG performance.\n","authors":["Sayak Nag","Udita Ghosh","Calvin-Khang Ta","Sarosij Bose","Jiachen Li","Amit K Roy Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2503.13947v2.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2504.08219v1","updated":"2025-04-11T02:59:06Z","published":"2025-04-11T02:59:06Z","title":"VL-UR: Vision-Language-guided Universal Restoration of Images Degraded\n  by Adverse Weather Conditions","summary":"  Image restoration is critical for improving the quality of degraded images,\nwhich is vital for applications like autonomous driving, security surveillance,\nand digital content enhancement. However, existing methods are often tailored\nto specific degradation scenarios, limiting their adaptability to the diverse\nand complex challenges in real-world environments. Moreover, real-world\ndegradations are typically non-uniform, highlighting the need for adaptive and\nintelligent solutions. To address these issues, we propose a novel\nvision-language-guided universal restoration (VL-UR) framework. VL-UR leverages\na zero-shot contrastive language-image pre-training (CLIP) model to enhance\nimage restoration by integrating visual and semantic information. A scene\nclassifier is introduced to adapt CLIP, generating high-quality language\nembeddings aligned with degraded images while predicting degraded types for\ncomplex scenarios. Extensive experiments across eleven diverse degradation\nsettings demonstrate VL-UR's state-of-the-art performance, robustness, and\nadaptability. This positions VL-UR as a transformative solution for modern\nimage restoration challenges in dynamic, real-world environments.\n","authors":["Ziyan Liu","Yuxu Lu","Huashan Yu","Dong yang"],"pdf_url":"https://arxiv.org/pdf/2504.08219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16958v4","updated":"2025-04-11T02:56:30Z","published":"2024-03-25T17:17:45Z","title":"TwinLiteNetPlus: A Real-Time Multi-Task Segmentation Model for\n  Autonomous Driving","summary":"  Semantic segmentation is crucial for autonomous driving, particularly for the\ntasks of Drivable Area and Lane Segmentation, ensuring safety and navigation.\nTo address the high computational costs of current state-of-the-art (SOTA)\nmodels, this paper introduces TwinLiteNetPlus, a model capable of balancing\nefficiency and accuracy. TwinLiteNetPlus incorporates standard and depth-wise\nseparable dilated convolutions, reducing complexity while maintaining high\naccuracy. It is available in four configurations, from the robust 1.94\nmillion-parameter TwinLiteNetPlus_{Large} to the ultra-lightweight\n34K-parameter TwinLiteNetPlus_{Nano}. Notably, TwinLiteNetPlus_{Large} attains\na 92.9% mIoU (mean Intersection over Union) for Drivable Area Segmentation and\na 34.2% IoU (Intersection over Union) for Lane Segmentation. These results\nachieve remarkable performance, surpassing current state-of-the-art models\nwhile only requiring 11 times fewer Floating Point Operations (FLOPs) for\ncomputation. Rigorously evaluated on various embedded devices, TwinLiteNetPlus\ndemonstrates promising latency and power efficiency, underscoring its potential\nfor real-world autonomous vehicle applications. The code is available on\nhttps://github.com/chequanghuy/TwinLiteNetPlus.\n","authors":["Quang-Huy Che","Duc-Tri Le","Minh-Quan Pham","Vinh-Tiep Nguyen","Duc-Khai Lam"],"pdf_url":"https://arxiv.org/pdf/2403.16958v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01142v2","updated":"2025-04-11T02:48:36Z","published":"2025-01-02T08:54:01Z","title":"Adaptive Hardness-driven Augmentation and Alignment Strategies for\n  Multi-Source Domain Adaptations","summary":"  Multi-source Domain Adaptation (MDA) aims to transfer knowledge from multiple\nlabeled source domains to an unlabeled target domain. Nevertheless, traditional\nmethods primarily focus on achieving inter-domain alignment through\nsample-level constraints, such as Maximum Mean Discrepancy (MMD), neglecting\nthree pivotal aspects: 1) the potential of data augmentation, 2) the\nsignificance of intra-domain alignment, and 3) the design of cluster-level\nconstraints. In this paper, we introduce a novel hardness-driven strategy for\nMDA tasks, named \"A3MDA\" , which collectively considers these three aspects\nthrough Adaptive hardness quantification and utilization in both data\nAugmentation and domain Alignment.To achieve this, \"A3MDA\" progressively\nproposes three Adaptive Hardness Measurements (AHM), i.e., Basic, Smooth, and\nComparative AHMs, each incorporating distinct mechanisms for diverse scenarios.\nSpecifically, Basic AHM aims to gauge the instantaneous hardness for each\nsource/target sample. Then, hardness values measured by Smooth AHM will\nadaptively adjust the intensity level of strong data augmentation to maintain\ncompatibility with the model's generalization capacity.In contrast, Comparative\nAHM is designed to facilitate cluster-level constraints. By leveraging hardness\nvalues as sample-specific weights, the traditional MMD is enhanced into a\nweighted-clustered variant, strengthening the robustness and precision of\ninter-domain alignment. As for the often-neglected intra-domain alignment, we\nadaptively construct a pseudo-contrastive matrix by selecting harder samples\nbased on the hardness rankings, enhancing the quality of pseudo-labels, and\nshaping a well-clustered target feature space. Experiments on multiple MDA\nbenchmarks show that \" A3MDA \" outperforms other methods.\n","authors":["Yang Yuxiang","Zeng Xinyi","Zeng Pinxian","Zu Chen","Yan Binyu","Zhou Jiliu","Wang Yan"],"pdf_url":"https://arxiv.org/pdf/2501.01142v2.pdf","comment":"15 pages, 12 figures. Under review"},{"id":"http://arxiv.org/abs/2410.16995v2","updated":"2025-04-11T02:45:54Z","published":"2024-10-22T13:17:20Z","title":"E-3DGS: Gaussian Splatting with Exposure and Motion Events","summary":"  Achieving 3D reconstruction from images captured under optimal conditions has\nbeen extensively studied in the vision and imaging fields. However, in\nreal-world scenarios, challenges such as motion blur and insufficient\nillumination often limit the performance of standard frame-based cameras in\ndelivering high-quality images. To address these limitations, we incorporate a\ntransmittance adjustment device at the hardware level, enabling event cameras\nto capture both motion and exposure events for diverse 3D reconstruction\nscenarios. Motion events (triggered by camera or object movement) are collected\nin fast-motion scenarios when the device is inactive, while exposure events\n(generated through controlled camera exposure) are captured during slower\nmotion to reconstruct grayscale images for high-quality training and\noptimization of event-based 3D Gaussian Splatting (3DGS). Our framework\nsupports three modes: High-Quality Reconstruction using exposure events, Fast\nReconstruction relying on motion events, and Balanced Hybrid optimizing with\ninitial exposure events followed by high-speed motion events. On the EventNeRF\ndataset, we demonstrate that exposure events significantly improve fine detail\nreconstruction compared to motion events and outperform frame-based cameras\nunder challenging conditions such as low illumination and overexposure.\nFurthermore, we introduce EME-3D, a real-world 3D dataset with exposure events,\nmotion events, camera calibration parameters, and sparse point clouds. Our\nmethod achieves faster and higher-quality reconstruction than event-based NeRF\nand is more cost-effective than methods combining event and RGB data. E-3DGS\nsets a new benchmark for event-based 3D reconstruction with robust performance\nin challenging conditions and lower hardware demands. The source code and\ndataset will be available at https://github.com/MasterHow/E-3DGS.\n","authors":["Xiaoting Yin","Hao Shi","Yuhan Bao","Zhenshan Bing","Yiyi Liao","Kailun Yang","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16995v2.pdf","comment":"Accepted to Applied Optics (AO). The source code and dataset will be\n  available at https://github.com/MasterHow/E-3DGS"},{"id":"http://arxiv.org/abs/2504.08212v1","updated":"2025-04-11T02:35:19Z","published":"2025-04-11T02:35:19Z","title":"RealCam-Vid: High-resolution Video Dataset with Dynamic Scenes and\n  Metric-scale Camera Movements","summary":"  Recent advances in camera-controllable video generation have been constrained\nby the reliance on static-scene datasets with relative-scale camera\nannotations, such as RealEstate10K. While these datasets enable basic viewpoint\ncontrol, they fail to capture dynamic scene interactions and lack metric-scale\ngeometric consistency-critical for synthesizing realistic object motions and\nprecise camera trajectories in complex environments. To bridge this gap, we\nintroduce the first fully open-source, high-resolution dynamic-scene dataset\nwith metric-scale camera annotations in https://github.com/ZGCTroy/RealCam-Vid.\n","authors":["Guangcong Zheng","Teng Li","Xianpan Zhou","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2504.08212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08205v1","updated":"2025-04-11T02:13:24Z","published":"2025-04-11T02:13:24Z","title":"EO-VLM: VLM-Guided Energy Overload Attacks on Vision Models","summary":"  Vision models are increasingly deployed in critical applications such as\nautonomous driving and CCTV monitoring, yet they remain susceptible to\nresource-consuming attacks. In this paper, we introduce a novel\nenergy-overloading attack that leverages vision language model (VLM) prompts to\ngenerate adversarial images targeting vision models. These images, though\nimperceptible to the human eye, significantly increase GPU energy consumption\nacross various vision models, threatening the availability of these systems.\nOur framework, EO-VLM (Energy Overload via VLM), is model-agnostic, meaning it\nis not limited by the architecture or type of the target vision model. By\nexploiting the lack of safety filters in VLMs like DALL-E 3, we create\nadversarial noise images without requiring prior knowledge or internal\nstructure of the target vision models. Our experiments demonstrate up to a 50%\nincrease in energy consumption, revealing a critical vulnerability in current\nvision models.\n","authors":["Minjae Seo","Myoungsung You","Junhee Lee","Jaehan Kim","Hwanjo Heo","Jintae Oh","Jinwoo Kim"],"pdf_url":"https://arxiv.org/pdf/2504.08205v1.pdf","comment":"Presented as a poster at ACSAC 2024"},{"id":"http://arxiv.org/abs/2504.07836v2","updated":"2025-04-11T01:47:14Z","published":"2025-04-10T15:13:00Z","title":"AerialVG: A Challenging Benchmark for Aerial Visual Grounding by\n  Exploring Positional Relations","summary":"  Visual grounding (VG) aims to localize target objects in an image based on\nnatural language descriptions. In this paper, we propose AerialVG, a new task\nfocusing on visual grounding from aerial views. Compared to traditional VG,\nAerialVG poses new challenges, \\emph{e.g.}, appearance-based grounding is\ninsufficient to distinguish among multiple visually similar objects, and\npositional relations should be emphasized. Besides, existing VG models struggle\nwhen applied to aerial imagery, where high-resolution images cause significant\ndifficulties. To address these challenges, we introduce the first AerialVG\ndataset, consisting of 5K real-world aerial images, 50K manually annotated\ndescriptions, and 103K objects. Particularly, each annotation in AerialVG\ndataset contains multiple target objects annotated with relative spatial\nrelations, requiring models to perform comprehensive spatial reasoning.\nFurthermore, we propose an innovative model especially for the AerialVG task,\nwhere a Hierarchical Cross-Attention is devised to focus on target regions, and\na Relation-Aware Grounding module is designed to infer positional relations.\nExperimental results validate the effectiveness of our dataset and method,\nhighlighting the importance of spatial reasoning in aerial visual grounding.\nThe code and dataset will be released.\n","authors":["Junli Liu","Qizhi Chen","Zhigang Wang","Yiwen Tang","Yiting Zhang","Chi Yan","Dong Wang","Xuelong Li","Bin Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.07836v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.14174v2","updated":"2025-04-11T01:26:58Z","published":"2024-03-21T06:53:40Z","title":"Unified Static and Dynamic Network: Efficient Temporal Filtering for\n  Video Grounding","summary":"  Inspired by the activity-silent and persistent activity mechanisms in human\nvisual perception biology, we design a Unified Static and Dynamic Network\n(UniSDNet), to learn the semantic association between the video and text/audio\nqueries in a cross-modal environment for efficient video grounding. For static\nmodeling, we devise a novel residual structure (ResMLP) to boost the global\ncomprehensive interaction between the video segments and queries, achieving\nmore effective semantic enhancement/supplement. For dynamic modeling, we\neffectively exploit three characteristics of the persistent activity mechanism\nin our network design for a better video context comprehension. Specifically,\nwe construct a diffusely connected video clip graph on the basis of 2D sparse\ntemporal masking to reflect the \"short-term effect\" relationship. We\ninnovatively consider the temporal distance and relevance as the joint\n\"auxiliary evidence clues\" and design a multi-kernel Temporal Gaussian Filter\nto expand the context clue into high-dimensional space, simulating the \"complex\nvisual perception\", and then conduct element level filtering convolution\noperations on neighbour clip nodes in message passing stage for finally\ngenerating and ranking the candidate proposals. Our UniSDNet is applicable to\nboth Natural Language Video Grounding (NLVG) and Spoken Language Video\nGrounding (SLVG) tasks. Our UniSDNet achieves SOTA performance on three widely\nused datasets for NLVG, as well as three datasets for SLVG, e.g., reporting new\nrecords at 38.88% R@1,IoU@0.7 on ActivityNet Captions and 40.26% R@1,IoU@0.5 on\nTACoS. To facilitate this field, we collect two new datasets (Charades-STA\nSpeech and TACoS Speech) for SLVG task. Meanwhile, the inference speed of our\nUniSDNet is 1.56$\\times$ faster than the strong multi-query benchmark. Code is\navailable at: https://github.com/xian-sh/UniSDNet.\n","authors":["Jingjing Hu","Dan Guo","Kun Li","Zhan Si","Xun Yang","Xiaojun Chang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14174v2.pdf","comment":"Accepted to IEEE TPAMI 2025"},{"id":"http://arxiv.org/abs/2504.08186v1","updated":"2025-04-11T01:06:36Z","published":"2025-04-11T01:06:36Z","title":"Comparative Analysis of Different Methods for Classifying Polychromatic\n  Sketches","summary":"  Image classification is a significant challenge in computer vision,\nparticularly in domains humans are not accustomed to. As machine learning and\nartificial intelligence become more prominent, it is crucial these algorithms\ndevelop a sense of sight that is on par with or exceeds human ability. For this\nreason, we have collected, cleaned, and parsed a large dataset of hand-drawn\ndoodles and compared multiple machine learning solutions to classify these\nimages into 170 distinct categories. The best model we found achieved a Top-1\naccuracy of 47.5%, significantly surpassing human performance on the dataset,\nwhich stands at 41%.\n","authors":["Fahd Baba","Devon Mack"],"pdf_url":"https://arxiv.org/pdf/2504.08186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08181v1","updated":"2025-04-11T00:41:25Z","published":"2025-04-11T00:41:25Z","title":"TokenMotion: Decoupled Motion Control via Token Disentanglement for\n  Human-centric Video Generation","summary":"  Human-centric motion control in video generation remains a critical\nchallenge, particularly when jointly controlling camera movements and human\nposes in scenarios like the iconic Grammy Glambot moment. While recent video\ndiffusion models have made significant progress, existing approaches struggle\nwith limited motion representations and inadequate integration of camera and\nhuman motion controls. In this work, we present TokenMotion, the first\nDiT-based video diffusion framework that enables fine-grained control over\ncamera motion, human motion, and their joint interaction. We represent camera\ntrajectories and human poses as spatio-temporal tokens to enable local control\ngranularity. Our approach introduces a unified modeling framework utilizing a\ndecouple-and-fuse strategy, bridged by a human-aware dynamic mask that\neffectively handles the spatially-and-temporally varying nature of combined\nmotion signals. Through extensive experiments, we demonstrate TokenMotion's\neffectiveness across both text-to-video and image-to-video paradigms,\nconsistently outperforming current state-of-the-art methods in human-centric\nmotion control tasks. Our work represents a significant advancement in\ncontrollable video generation, with particular relevance for creative\nproduction applications.\n","authors":["Ruineng Li","Daitao Xing","Huiming Sun","Yuanzhou Ha","Jinglin Shen","Chiuman Ho"],"pdf_url":"https://arxiv.org/pdf/2504.08181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08177v1","updated":"2025-04-11T00:14:28Z","published":"2025-04-11T00:14:28Z","title":"SynthFM: Training Modality-agnostic Foundation Models for Medical Image\n  Segmentation without Real Medical Data","summary":"  Foundation models like the Segment Anything Model (SAM) excel in zero-shot\nsegmentation for natural images but struggle with medical image segmentation\ndue to differences in texture, contrast, and noise. Annotating medical images\nis costly and requires domain expertise, limiting large-scale annotated data\navailability. To address this, we propose SynthFM, a synthetic data generation\nframework that mimics the complexities of medical images, enabling foundation\nmodels to adapt without real medical data. Using SAM's pretrained encoder and\ntraining the decoder from scratch on SynthFM's dataset, we evaluated our method\non 11 anatomical structures across 9 datasets (CT, MRI, and Ultrasound).\nSynthFM outperformed zero-shot baselines like SAM and MedSAM, achieving\nsuperior results under different prompt settings and on out-of-distribution\ndatasets.\n","authors":["Sourya Sengupta","Satrajit Chakrabarty","Keerthi Sravan Ravi","Gopal Avinash","Ravi Soni"],"pdf_url":"https://arxiv.org/pdf/2504.08177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08175v1","updated":"2025-04-11T00:08:14Z","published":"2025-04-11T00:08:14Z","title":"Multi-person Physics-based Pose Estimation for Combat Sports","summary":"  We propose a novel framework for accurate 3D human pose estimation in combat\nsports using sparse multi-camera setups. Our method integrates robust\nmulti-view 2D pose tracking via a transformer-based top-down approach,\nemploying epipolar geometry constraints and long-term video object segmentation\nfor consistent identity tracking across views. Initial 3D poses are obtained\nthrough weighted triangulation and spline smoothing, followed by kinematic\noptimization to refine pose accuracy. We further enhance pose realism and\nrobustness by introducing a multi-person physics-based trajectory optimization\nstep, effectively addressing challenges such as rapid motions, occlusions, and\nclose interactions. Experimental results on diverse datasets, including a new\nbenchmark of elite boxing footage, demonstrate state-of-the-art performance.\nAdditionally, we release comprehensive annotated video datasets to advance\nfuture research in multi-person pose estimation for combat sports.\n","authors":["Hossein Feiz","David Labbé","Thomas Romeas","Jocelyn Faubert","Sheldon Andrews"],"pdf_url":"https://arxiv.org/pdf/2504.08175v1.pdf","comment":null}]},"2025-04-14T00:00:00Z":{"Statistical Machine Learning (Theory)":[{"id":"http://arxiv.org/abs/2504.10428v1","updated":"2025-04-14T17:19:29Z","published":"2025-04-14T17:19:29Z","title":"Learning with Positive and Imperfect Unlabeled Data","summary":"  We study the problem of learning binary classifiers from positive and\nunlabeled data when the unlabeled data distribution is shifted, which we call\nPositive and Imperfect Unlabeled (PIU) Learning. In the absence of covariate\nshifts, i.e., with perfect unlabeled data, Denis (1998) reduced this problem to\nlearning under Massart noise; however, that reduction fails under even slight\nshifts.\n  Our main results on PIU learning are the characterizations of the sample\ncomplexity of PIU learning and a computationally and sample-efficient algorithm\nachieving a misclassification error $\\varepsilon$. We further show that our\nresults lead to new algorithms for several related problems.\n  1. Learning from smooth distributions: We give algorithms that learn\ninteresting concept classes from only positive samples under smooth feature\ndistributions, bypassing known existing impossibility results and contributing\nto recent advances in smoothened learning (Haghtalab et al, J.ACM'24)\n(Chandrasekaran et al., COLT'24).\n  2. Learning with a list of unlabeled distributions: We design new algorithms\nthat apply to a broad class of concept classes under the assumption that we are\ngiven a list of unlabeled distributions, one of which--unknown to the\nlearner--is $O(1)$-close to the true feature distribution.\n  3. Estimation in the presence of unknown truncation: We give the first\npolynomial sample and time algorithm for estimating the parameters of an\nexponential family distribution from samples truncated to an unknown set\napproximable by polynomials in $L_1$-norm. This improves the algorithm by Lee\net al. (FOCS'24) that requires approximation in $L_2$-norm.\n  4. Detecting truncation: We present new algorithms for detecting whether\ngiven samples have been truncated (or not) for a broad class of non-product\ndistributions, including non-product distributions, improving the algorithm by\nDe et al. (STOC'24).\n","authors":["Jane H. Lee","Anay Mehrotra","Manolis Zampetakis"],"pdf_url":"https://arxiv.org/pdf/2504.10428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09525v2","updated":"2025-04-14T16:41:56Z","published":"2025-02-13T17:37:42Z","title":"Robust Learning of Multi-index Models via Iterative Subspace\n  Approximation","summary":"  We study the task of learning Multi-Index Models (MIMs) with label noise\nunder the Gaussian distribution. A $K$-MIM is any function $f$ that only\ndepends on a $K$-dimensional subspace. We focus on well-behaved MIMs with\nfinite ranges that satisfy certain regularity properties. Our main contribution\nis a general robust learner that is qualitatively optimal in the Statistical\nQuery (SQ) model. Our algorithm iteratively constructs better approximations to\nthe defining subspace by computing low-degree moments conditional on the\nprojection to the subspace computed thus far, and adding directions with\nrelatively large empirical moments. This procedure efficiently finds a subspace\n$V$ so that $f(\\mathbf{x})$ is close to a function of the projection of\n$\\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional\nmoments do not help, we prove an SQ lower bound suggesting that no efficient\nlearner exists. As applications, we provide faster robust learners for the\nfollowing concept classes:\n  * {\\bf Multiclass Linear Classifiers} We give a constant-factor approximate\nagnostic learner with sample complexity $N = O(d)\n2^{\\mathrm{poly}(K/\\epsilon)}$ and computational complexity $\\mathrm{poly}(N\n,d)$. This is the first constant-factor agnostic learner for this class whose\ncomplexity is a fixed-degree polynomial in $d$.\n  * {\\bf Intersections of Halfspaces} We give an approximate agnostic learner\nfor this class achieving 0-1 error $K \\tilde{O}(\\mathrm{OPT}) + \\epsilon$ with\nsample complexity $N=O(d^2) 2^{\\mathrm{poly}(K/\\epsilon)}$ and computational\ncomplexity $\\mathrm{poly}(N ,d)$. This is the first agnostic learner for this\nclass with near-linear error dependence and complexity a fixed-degree\npolynomial in $d$.\n  Furthermore, we show that in the presence of random classification noise, the\ncomplexity of our algorithm scales polynomially with $1/\\epsilon$.\n","authors":["Ilias Diakonikolas","Giannis Iakovidis","Daniel M. Kane","Nikos Zarifis"],"pdf_url":"https://arxiv.org/pdf/2502.09525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00104v2","updated":"2025-04-14T16:36:07Z","published":"2024-05-31T18:00:12Z","title":"Scalable Bayesian Learning with posteriors","summary":"  Although theoretically compelling, Bayesian learning with modern machine\nlearning models is computationally challenging since it requires approximating\na high dimensional posterior distribution. In this work, we (i) introduce\nposteriors, an easily extensible PyTorch library hosting general-purpose\nimplementations making Bayesian learning accessible and scalable to large data\nand parameter regimes; (ii) present a tempered framing of stochastic gradient\nMarkov chain Monte Carlo, as implemented in posteriors, that transitions\nseamlessly into optimization and unveils a minor modification to deep ensembles\nto ensure they are asymptotically unbiased for the Bayesian posterior, and\n(iii) demonstrate and compare the utility of Bayesian approximations through\nexperiments including an investigation into the cold posterior effect and\napplications with large language models.\n","authors":["Samuel Duffield","Kaelan Donatella","Johnathan Chiu","Phoebe Klett","Daniel Simpson"],"pdf_url":"https://arxiv.org/pdf/2406.00104v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10373v1","updated":"2025-04-14T16:20:55Z","published":"2025-04-14T16:20:55Z","title":"DUE: A Deep Learning Framework and Library for Modeling Unknown\n  Equations","summary":"  Equations, particularly differential equations, are fundamental for\nunderstanding natural phenomena and predicting complex dynamics across various\nscientific and engineering disciplines. However, the governing equations for\nmany complex systems remain unknown due to intricate underlying mechanisms.\nRecent advancements in machine learning and data science offer a new paradigm\nfor modeling unknown equations from measurement or simulation data. This\nparadigm shift, known as data-driven discovery or modeling, stands at the\nforefront of AI for science, with significant progress made in recent years. In\nthis paper, we introduce a systematic framework for data-driven modeling of\nunknown equations using deep learning. This versatile framework is capable of\nlearning unknown ODEs, PDEs, DAEs, IDEs, SDEs, reduced or partially observed\nsystems, and non-autonomous differential equations. Based on this framework, we\nhave developed Deep Unknown Equations (DUE), an open-source software package\ndesigned to facilitate the data-driven modeling of unknown equations using\nmodern deep learning techniques. DUE serves as an educational tool for\nclassroom instruction, enabling students and newcomers to gain hands-on\nexperience with differential equations, data-driven modeling, and contemporary\ndeep learning approaches such as FNN, ResNet, generalized ResNet, operator\nsemigroup networks (OSG-Net), and Transformers. Additionally, DUE is a\nversatile and accessible toolkit for researchers across various scientific and\nengineering fields. It is applicable not only for learning unknown equations\nfrom data but also for surrogate modeling of known, yet complex, equations that\nare costly to solve using traditional numerical methods. We provide detailed\ndescriptions of DUE and demonstrate its capabilities through diverse examples,\nwhich serve as templates that can be easily adapted for other applications.\n","authors":["Junfeng Chen","Kailiang Wu","Dongbin Xiu"],"pdf_url":"https://arxiv.org/pdf/2504.10373v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2409.06560v2","updated":"2025-04-14T15:44:36Z","published":"2024-09-10T14:43:03Z","title":"A Primer on Variational Inference for Physics-Informed Deep Generative\n  Modelling","summary":"  Variational inference (VI) is a computationally efficient and scalable\nmethodology for approximate Bayesian inference. It strikes a balance between\naccuracy of uncertainty quantification and practical tractability. It excels at\ngenerative modelling and inversion tasks due to its built-in Bayesian\nregularisation and flexibility, essential qualities for physics related\nproblems. For such problems, the underlying physical model determines the\ndependence between variables of interest, which in turn will require a tailored\nderivation for the central VI learning objective. Furthermore, in many physical\ninference applications this structure has rich meaning and is essential for\naccurately capturing the dynamics of interest. In this paper, we provide an\naccessible and thorough technical introduction to VI for forward and inverse\nproblems, guiding the reader through standard derivations of the VI framework\nand how it can best be realized through deep learning. We then review and unify\nrecent literature exemplifying the flexibility allowed by VI. This paper is\ndesigned for a general scientific audience looking to solve physics-based\nproblems with an emphasis on uncertainty quantification\n","authors":["Alex Glyn-Davies","Arnaud Vadeboncoeur","O. Deniz Akyildiz","Ieva Kazlauskaite","Mark Girolami"],"pdf_url":"https://arxiv.org/pdf/2409.06560v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10283v1","updated":"2025-04-14T14:51:45Z","published":"2025-04-14T14:51:45Z","title":"$α$-Flow: A Unified Framework for Continuous-State Discrete Flow\n  Matching Models","summary":"  Recent efforts have extended the flow-matching framework to discrete\ngenerative modeling. One strand of models directly works with the continuous\nprobabilities instead of discrete tokens, which we colloquially refer to as\nContinuous-State Discrete Flow Matching (CS-DFM). Existing CS-DFM models differ\nsignificantly in their representations and geometric assumptions. This work\npresents a unified framework for CS-DFM models, under which the existing\nvariants can be understood as operating on different $\\alpha$-representations\nof probabilities. Building upon the theory of information geometry, we\nintroduce $\\alpha$-Flow, a family of CS-DFM models that adheres to the\ncanonical $\\alpha$-geometry of the statistical manifold, and demonstrate its\noptimality in minimizing the generalized kinetic energy. Theoretically, we show\nthat the flow matching loss for $\\alpha$-flow establishes a unified variational\nbound for the discrete negative log-likelihood. We comprehensively evaluate\ndifferent instantiations of $\\alpha$-flow on various discrete generation\ndomains to demonstrate their effectiveness in discrete generative modeling,\nincluding intermediate values whose geometries have never been explored before.\n$\\alpha$-flow significantly outperforms its discrete-state counterpart in image\nand protein sequence generation and better captures the entropy in language\nmodeling.\n","authors":["Chaoran Cheng","Jiahan Li","Jiajun Fan","Ge Liu"],"pdf_url":"https://arxiv.org/pdf/2504.10283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10248v1","updated":"2025-04-14T14:11:00Z","published":"2025-04-14T14:11:00Z","title":"Adaptive Sensor Steering Strategy Using Deep Reinforcement Learning for\n  Dynamic Data Acquisition in Digital Twins","summary":"  This paper introduces a sensor steering methodology based on deep\nreinforcement learning to enhance the predictive accuracy and decision support\ncapabilities of digital twins by optimising the data acquisition process.\nTraditional sensor placement techniques are often constrained by one-off\noptimisation strategies, which limit their applicability for online\napplications requiring continuous informative data assimilation. The proposed\napproach addresses this limitation by offering an adaptive framework for sensor\nplacement within the digital twin paradigm. The sensor placement problem is\nformulated as a Markov decision process, enabling the training and deployment\nof an agent capable of dynamically repositioning sensors in response to the\nevolving conditions of the physical structure as represented by the digital\ntwin. This ensures that the digital twin maintains a highly representative and\nreliable connection to its physical counterpart. The proposed framework is\nvalidated through a series of comprehensive case studies involving a cantilever\nplate structure subjected to diverse conditions, including healthy and damaged\nconditions. The results demonstrate the capability of the deep reinforcement\nlearning agent to adaptively reposition sensors improving the quality of data\nacquisition and hence enhancing the overall accuracy of digital twins.\n","authors":["Collins O. Ogbodo","Timothy J. Rogers","Mattia Dal Borgo","David J. Wagg"],"pdf_url":"https://arxiv.org/pdf/2504.10248v1.pdf","comment":"18 pages, 14 figures"},{"id":"http://arxiv.org/abs/2504.08178v2","updated":"2025-04-14T13:32:35Z","published":"2025-04-11T00:20:37Z","title":"A Piecewise Lyapunov Analysis of Sub-quadratic SGD: Applications to\n  Robust and Quantile Regression","summary":"  Motivated by robust and quantile regression problems, we investigate the\nstochastic gradient descent (SGD) algorithm for minimizing an objective\nfunction $f$ that is locally strongly convex with a sub--quadratic tail. This\nsetting covers many widely used online statistical methods. We introduce a\nnovel piecewise Lyapunov function that enables us to handle functions $f$ with\nonly first-order differentiability, which includes a wide range of popular loss\nfunctions such as Huber loss. Leveraging our proposed Lyapunov function, we\nderive finite-time moment bounds under general diminishing stepsizes, as well\nas constant stepsizes. We further establish the weak convergence, central limit\ntheorem and bias characterization under constant stepsize, providing the first\ngeometrical convergence result for sub--quadratic SGD. Our results have wide\napplications, especially in online statistical methods. In particular, we\ndiscuss two applications of our results. 1) Online robust regression: We\nconsider a corrupted linear model with sub--exponential covariates and\nheavy--tailed noise. Our analysis provides convergence rates comparable to\nthose for corrupted models with Gaussian covariates and noise. 2) Online\nquantile regression: Importantly, our results relax the common assumption in\nprior work that the conditional density is continuous and provide a more\nfine-grained analysis for the moment bounds.\n","authors":["Yixuan Zhang"," Dongyan"," Huo","Yudong Chen","Qiaomin Xie"],"pdf_url":"https://arxiv.org/pdf/2504.08178v2.pdf","comment":"ACM SIGMETRICS 2025. 40 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.00664v2","updated":"2025-04-14T13:25:03Z","published":"2024-12-01T03:57:21Z","title":"Improving Decoupled Posterior Sampling for Inverse Problems using Data\n  Consistency Constraint","summary":"  Diffusion models have shown strong performances in solving inverse problems\nthrough posterior sampling while they suffer from errors during earlier steps.\nTo mitigate this issue, several Decoupled Posterior Sampling methods have been\nrecently proposed. However, the reverse process in these methods ignores\nmeasurement information, leading to errors that impede effective optimization\nin subsequent steps. To solve this problem, we propose Guided Decoupled\nPosterior Sampling (GDPS) by integrating a data consistency constraint in the\nreverse process. The constraint performs a smoother transition within the\noptimization process, facilitating a more effective convergence toward the\ntarget distribution. Furthermore, we extend our method to latent diffusion\nmodels and Tweedie's formula, demonstrating its scalability. We evaluate GDPS\non the FFHQ and ImageNet datasets across various linear and nonlinear tasks\nunder both standard and challenging conditions. Experimental results\ndemonstrate that GDPS achieves state-of-the-art performance, improving accuracy\nover existing methods.\n","authors":["Zhi Qi","Shihong Yuan","Yulin Yuan","Linling Kuang","Yoshiyuki Kabashima","Xiangming Meng"],"pdf_url":"https://arxiv.org/pdf/2412.00664v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.06115v6","updated":"2025-04-14T12:27:12Z","published":"2022-08-12T04:43:26Z","title":"A Nonparametric Approach with Marginals for Modeling Consumer Choice","summary":"  Given data on the choices made by consumers for different offer sets, a key\nchallenge is to develop parsimonious models that describe and predict consumer\nchoice behavior while being amenable to prescriptive tasks such as pricing and\nassortment optimization. The marginal distribution model (MDM) is one such\nmodel, which requires only the specification of marginal distributions of the\nrandom utilities. This paper aims to establish necessary and sufficient\nconditions for given choice data to be consistent with the MDM hypothesis,\ninspired by the usefulness of similar characterizations for the random utility\nmodel (RUM). This endeavor leads to an exact characterization of the set of\nchoice probabilities that the MDM can represent. Verifying the consistency of\nchoice data with this characterization is equivalent to solving a\npolynomial-sized linear program. Since the analogous verification task for RUM\nis computationally intractable and neither of these models subsumes the other,\nMDM is helpful in striking a balance between tractability and representational\npower. The characterization is then used with robust optimization for making\ndata-driven sales and revenue predictions for new unseen assortments. When the\nchoice data lacks consistency with the MDM hypothesis, finding the best-fitting\nMDM choice probabilities reduces to solving a mixed integer convex program.\nNumerical results using real world data and synthetic data demonstrate that MDM\nexhibits competitive representational power and prediction performance compared\nto RUM and parametric models while being significantly faster in computation\nthan RUM.\n","authors":["Yanqiu Ruan","Xiaobo Li","Karthyek Murthy","Karthik Natarajan"],"pdf_url":"https://arxiv.org/pdf/2208.06115v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10171v1","updated":"2025-04-14T12:25:11Z","published":"2025-04-14T12:25:11Z","title":"Kullback-Leibler excess risk bounds for exponential weighted aggregation\n  in Generalized linear models","summary":"  Aggregation methods have emerged as a powerful and flexible framework in\nstatistical learning, providing unified solutions across diverse problems such\nas regression, classification, and density estimation. In the context of\ngeneralized linear models (GLMs), where responses follow exponential family\ndistributions, aggregation offers an attractive alternative to classical\nparametric modeling. This paper investigates the problem of sparse aggregation\nin GLMs, aiming to approximate the true parameter vector by a sparse linear\ncombination of predictors. We prove that an exponential weighted aggregation\nscheme yields a sharp oracle inequality for the Kullback-Leibler risk with\nleading constant equal to one, while also attaining the minimax-optimal rate of\naggregation. These results are further enhanced by establishing\nhigh-probability bounds on the excess risk.\n","authors":["The Tien Mai"],"pdf_url":"https://arxiv.org/pdf/2504.10171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10169v1","updated":"2025-04-14T12:24:17Z","published":"2025-04-14T12:24:17Z","title":"Challenges in interpretability of additive models","summary":"  We review generalized additive models as a type of ``transparent'' model that\nhas recently seen renewed interest in the deep learning community as neural\nadditive models. We highlight multiple types of nonidentifiability in this\nmodel class and discuss challenges in interpretability, arguing for restraint\nwhen claiming ``interpretability'' or ``suitability for safety-critical\napplications'' of such models.\n","authors":["Xinyu Zhang","Julien Martinelli","ST John"],"pdf_url":"https://arxiv.org/pdf/2504.10169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19460v2","updated":"2025-04-14T12:22:06Z","published":"2025-02-26T10:28:44Z","title":"Practical Evaluation of Copula-based Survival Metrics: Beyond the\n  Independent Censoring Assumption","summary":"  Conventional survival metrics, such as Harrell's concordance index and the\nBrier Score, rely on the independent censoring assumption for valid inference\nin the presence of right-censored data. However, when instances are censored\nfor reasons related to the event of interest, this assumption no longer holds,\nas this kind of dependent censoring biases the marginal survival estimates of\npopular nonparametric estimators. In this paper, we propose three copula-based\nmetrics to evaluate survival models in the presence of dependent censoring, and\ndesign a framework to create realistic, semi-synthetic datasets with dependent\ncensoring to facilitate the evaluation of the metrics. Our empirical analyses\nin synthetic and semi-synthetic datasets show that our metrics can give error\nestimates that are closer to the true error, mainly in terms of prediction\naccuracy.\n","authors":["Christian Marius Lillelund","Shi-ang Qi","Russell Greiner"],"pdf_url":"https://arxiv.org/pdf/2502.19460v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10139v1","updated":"2025-04-14T11:53:29Z","published":"2025-04-14T11:53:29Z","title":"Conditional Distribution Compression via the Kernel Conditional Mean\n  Embedding","summary":"  Existing distribution compression methods, like Kernel Herding (KH), were\noriginally developed for unlabelled data. However, no existing approach\ndirectly compresses the conditional distribution of labelled data. To address\nthis gap, we first introduce the Average Maximum Conditional Mean Discrepancy\n(AMCMD), a natural metric for comparing conditional distributions. We then\nderive a consistent estimator for the AMCMD and establish its rate of\nconvergence. Next, we make a key observation: in the context of distribution\ncompression, the cost of constructing a compressed set targeting the AMCMD can\nbe reduced from $\\mathcal{O}(n^3)$ to $\\mathcal{O}(n)$. Building on this, we\nextend the idea of KH to develop Average Conditional Kernel Herding (ACKH), a\nlinear-time greedy algorithm that constructs a compressed set targeting the\nAMCMD. To better understand the advantages of directly compressing the\nconditional distribution rather than doing so via the joint distribution, we\nintroduce Joint Kernel Herding (JKH), a straightforward adaptation of KH\ndesigned to compress the joint distribution of labelled data. While herding\nmethods provide a simple and interpretable selection process, they rely on a\ngreedy heuristic. To explore alternative optimisation strategies, we propose\nJoint Kernel Inducing Points (JKIP) and Average Conditional Kernel Inducing\nPoints (ACKIP), which jointly optimise the compressed set while maintaining\nlinear complexity. Experiments show that directly preserving conditional\ndistributions with ACKIP outperforms both joint distribution compression (via\nJKH and JKIP) and the greedy selection used in ACKH. Moreover, we see that JKIP\nconsistently outperforms JKH.\n","authors":["Dominic Broadbent","Nick Whiteley","Robert Allison","Tom Lovett"],"pdf_url":"https://arxiv.org/pdf/2504.10139v1.pdf","comment":"68 pages, 28 figures"},{"id":"http://arxiv.org/abs/2405.08825v4","updated":"2025-04-14T11:44:36Z","published":"2024-05-12T17:57:25Z","title":"Thermodynamic limit in learning period three","summary":"  A continuous one-dimensional map with period three includes all periods. This\nraises the following question: Can we obtain any types of periodic orbits\nsolely by learning three data points? In this paper, we report the answer to be\nyes. Considering a random neural network in its thermodynamic limit, we first\nshow that almost all learned periods are unstable, and each network has its own\ncharacteristic attractors (which can even be untrained ones). The latently\nacquired dynamics, which are unstable within the trained network, serve as a\nfoundation for the diversity of characteristic attractors and may even lead to\nthe emergence of attractors of all periods after learning. When the neural\nnetwork interpolation is quadratic, a universal post-learning bifurcation\nscenario appears, which is consistent with a topological conjugacy between the\ntrained network and the classical logistic map. In addition to universality, we\nexplore specific properties of certain networks, including the singular\nbehavior of the scale of weight at the infinite limit, the finite-size effects,\nand the symmetry in learning period three.\n","authors":["Yuichiro Terasaki","Kohei Nakajima"],"pdf_url":"https://arxiv.org/pdf/2405.08825v4.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2411.02253v3","updated":"2025-04-14T11:36:12Z","published":"2024-11-04T16:43:16Z","title":"Towards safe Bayesian optimization with Wiener kernel regression","summary":"  Bayesian Optimization (BO) is a data-driven strategy for\nminimizing/maximizing black-box functions based on probabilistic surrogate\nmodels. In the presence of safety constraints, the performance of BO crucially\nrelies on tight probabilistic error bounds related to the uncertainty\nsurrounding the surrogate model. For the case of Gaussian Process surrogates\nand Gaussian measurement noise, we present a novel error bound based on the\nrecently proposed Wiener kernel regression. We prove that under rather mild\nassumptions, the proposed error bound is tighter than bounds previously\ndocumented in the literature, leading to enlarged safety regions. We draw upon\na numerical example to demonstrate the efficacy of the proposed error bound in\nsafe BO.\n","authors":["Oleksii Molodchyk","Johannes Teutsch","Timm Faulwasser"],"pdf_url":"https://arxiv.org/pdf/2411.02253v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10076v1","updated":"2025-04-14T10:21:08Z","published":"2025-04-14T10:21:08Z","title":"Towards Scalable Bayesian Optimization via Gradient-Informed Bayesian\n  Neural Networks","summary":"  Bayesian optimization (BO) is a widely used method for data-driven\noptimization that generally relies on zeroth-order data of objective function\nto construct probabilistic surrogate models. These surrogates guide the\nexploration-exploitation process toward finding global optimum. While Gaussian\nprocesses (GPs) are commonly employed as surrogates of the unknown objective\nfunction, recent studies have highlighted the potential of Bayesian neural\nnetworks (BNNs) as scalable and flexible alternatives. Moreover, incorporating\ngradient observations into GPs, when available, has been shown to improve BO\nperformance. However, the use of gradients within BNN surrogates remains\nunexplored. By leveraging automatic differentiation, gradient information can\nbe seamlessly integrated into BNN training, resulting in more informative\nsurrogates for BO. We propose a gradient-informed loss function for BNN\ntraining, effectively augmenting function observations with local gradient\ninformation. The effectiveness of this approach is demonstrated on well-known\nbenchmarks in terms of improved BNN predictions and faster BO convergence as\nthe number of decision variables increases.\n","authors":["Georgios Makrygiorgos","Joshua Hang Sai Ip","Ali Mesbah"],"pdf_url":"https://arxiv.org/pdf/2504.10076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10005v1","updated":"2025-04-14T09:08:40Z","published":"2025-04-14T09:08:40Z","title":"Session-based Recommender Systems: User Interest as a Stochastic Process\n  in the Latent Space","summary":"  This paper jointly addresses the problem of data uncertainty, popularity\nbias, and exposure bias in session-based recommender systems. We study the\nsymptoms of this bias both in item embeddings and in recommendations. We\npropose treating user interest as a stochastic process in the latent space and\nproviding a model-agnostic implementation of this mathematical concept. The\nproposed stochastic component consists of elements: debiasing item embeddings\nwith regularization for embedding uniformity, modeling dense user interest from\nsession prefixes, and introducing fake targets in the data to simulate extended\nexposure. We conducted computational experiments on two popular benchmark\ndatasets, Diginetica and YooChoose 1/64, as well as several modifications of\nthe YooChoose dataset with different ratios of popular items. The results show\nthat the proposed approach allows us to mitigate the challenges mentioned.\n","authors":["Klaudia Balcer","Piotr Lipinski"],"pdf_url":"https://arxiv.org/pdf/2504.10005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09951v1","updated":"2025-04-14T07:26:34Z","published":"2025-04-14T07:26:34Z","title":"Towards Weaker Variance Assumptions for Stochastic Optimization","summary":"  We revisit a classical assumption for analyzing stochastic gradient\nalgorithms where the squared norm of the stochastic subgradient (or the\nvariance for smooth problems) is allowed to grow as fast as the squared norm of\nthe optimization variable. We contextualize this assumption in view of its\ninception in the 1960s, its seemingly independent appearance in the recent\nliterature, its relationship to weakest-known variance assumptions for\nanalyzing stochastic gradient algorithms, and its relevance in deterministic\nproblems for non-Lipschitz nonsmooth convex optimization. We build on and\nextend a connection recently made between this assumption and the Halpern\niteration. For convex nonsmooth, and potentially stochastic, optimization, we\nanalyze horizon-free, anytime algorithms with last-iterate rates. For problems\nbeyond simple constrained optimization, such as convex problems with functional\nconstraints or regularized convex-concave min-max problems, we obtain rates for\noptimality measures that do not require boundedness of the feasible set.\n","authors":["Ahmet Alacaoglu","Yura Malitsky","Stephen J. Wright"],"pdf_url":"https://arxiv.org/pdf/2504.09951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.12664v3","updated":"2025-04-14T05:41:01Z","published":"2021-11-24T17:51:29Z","title":"MIO : Mutual Information Optimization using Self-Supervised Binary\n  Contrastive Learning","summary":"  Self-supervised contrastive learning frameworks have progressed rapidly over\nthe last few years. In this paper, we propose a novel loss function for\ncontrastive learning. We model our pre-training task as a binary classification\nproblem to induce an implicit contrastive effect. We further improve the\nn\\\"aive loss function after removing the effect of the positive-positive\nrepulsion and incorporating the upper bound of the negative pair repulsion.\nUnlike existing methods, the proposed loss function optimizes the mutual\ninformation in positive and negative pairs. We also present a closed-form\nexpression for the parameter gradient flow and compare the behaviour of\nself-supervised contrastive frameworks using Hessian eigenspectrum to\nanalytically study their convergence. The proposed method outperforms SOTA\nself-supervised contrastive frameworks on benchmark datasets such as CIFAR-10,\nCIFAR-100, STL-10, and Tiny-ImageNet. After 200 pretraining epochs with\nResNet-18 as the backbone, the proposed model achieves an accuracy of 86.36%,\n58.18%, 80.50%, and 30.87% on the CIFAR-10, CIFAR-100, STL-10, and\nTiny-ImageNet datasets, respectively, and surpasses the SOTA contrastive\nbaseline by 1.93%, 3.57%, 4.85%, and 0.33%, respectively. The proposed\nframework also achieves a state-of-the-art accuracy of 78.4% (200 epochs) and\n65.22% (100 epochs) Top-1 Linear Evaluation accuracy on ImageNet100 and\nImageNet1K datasets, respectively.\n","authors":["Siladittya Manna","Umapada Pal","Saumik Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2111.12664v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01432v2","updated":"2025-04-14T05:09:11Z","published":"2024-10-02T11:33:13Z","title":"Adaptive teachers for amortized samplers","summary":"  Amortized inference is the task of training a parametric model, such as a\nneural network, to approximate a distribution with a given unnormalized density\nwhere exact sampling is intractable. When sampling is implemented as a\nsequential decision-making process, reinforcement learning (RL) methods, such\nas generative flow networks, can be used to train the sampling policy.\nOff-policy RL training facilitates the discovery of diverse, high-reward\ncandidates, but existing methods still face challenges in efficient\nexploration. We propose to use an adaptive training distribution (the \\teacher)\nto guide the training of the primary amortized sampler (the \\student). The\n\\teacher, an auxiliary behavior model, is trained to sample high-loss regions\nof the \\student and can generalize across unexplored modes, thereby enhancing\nmode coverage by providing an efficient training curriculum. We validate the\neffectiveness of this approach in a synthetic environment designed to present\nan exploration challenge, two diffusion-based sampling tasks, and four\nbiochemical discovery tasks demonstrating its ability to improve sample\nefficiency and mode coverage. Source code is available at\nhttps://github.com/alstn12088/adaptive-teacher.\n","authors":["Minsu Kim","Sanghyeok Choi","Taeyoung Yun","Emmanuel Bengio","Leo Feng","Jarrid Rector-Brooks","Sungsoo Ahn","Jinkyoo Park","Nikolay Malkin","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2410.01432v2.pdf","comment":"ICLR 2025, 27 pages, 12 figures"},{"id":"http://arxiv.org/abs/2504.09873v1","updated":"2025-04-14T04:42:00Z","published":"2025-04-14T04:42:00Z","title":"Truncated Matrix Completion - An Empirical Study","summary":"  Low-rank Matrix Completion (LRMC) describes the problem where we wish to\nrecover missing entries of partially observed low-rank matrix. Most existing\nmatrix completion work deals with sampling procedures that are independent of\nthe underlying data values. While this assumption allows the derivation of nice\ntheoretical guarantees, it seldom holds in real-world applications. In this\npaper, we consider various settings where the sampling mask is dependent on the\nunderlying data values, motivated by applications in sensing, sequential\ndecision-making, and recommender systems. Through a series of experiments, we\nstudy and compare the performance of various LRMC algorithms that were\noriginally successful for data-independent sampling patterns.\n","authors":["Rishhabh Naik","Nisarg Trivedi","Davoud Ataee Tarzanagh","Laura Balzano"],"pdf_url":"https://arxiv.org/pdf/2504.09873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03974v2","updated":"2025-04-14T04:16:25Z","published":"2024-10-04T23:27:33Z","title":"Robust Barycenter Estimation using Semi-Unbalanced Neural Optimal\n  Transport","summary":"  Aggregating data from multiple sources can be formalized as an Optimal\nTransport (OT) barycenter problem, which seeks to compute the average of\nprobability distributions with respect to OT discrepancies. However, in\nreal-world scenarios, the presence of outliers and noise in the data measures\ncan significantly hinder the performance of traditional statistical methods for\nestimating OT barycenters. To address this issue, we propose a novel scalable\napproach for estimating the robust continuous barycenter, leveraging the dual\nformulation of the (semi-)unbalanced OT problem. To the best of our knowledge,\nthis paper is the first attempt to develop an algorithm for robust barycenters\nunder the continuous distribution setup. Our method is framed as a min-max\noptimization problem and is adaptable to general cost functions. We rigorously\nestablish the theoretical underpinnings of the proposed method and demonstrate\nits robustness to outliers and class imbalance through a number of illustrative\nexperiments. Our source code is publicly available at\nhttps://github.com/milenagazdieva/U-NOTBarycenters.\n","authors":["Milena Gazdieva","Jaemoo Choi","Alexander Kolesov","Jaewoong Choi","Petr Mokrov","Alexander Korotin"],"pdf_url":"https://arxiv.org/pdf/2410.03974v2.pdf","comment":"30 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.01320v5","updated":"2025-04-14T03:36:15Z","published":"2024-06-03T13:38:18Z","title":"Convergence of the denoising diffusion probabilistic models for general\n  noise schedules","summary":"  This work presents a theoretical analysis of the original formulation of\ndenoising diffusion probabilistic models (DDPMs), introduced by Ho, Jain, and\nAbbeel in Advances in Neural Information Processing Systems, 33 (2020), pp.\n6840-6851. An explicit upper bound is derived for the total variation distance\nbetween the distribution of the discrete-time DDPM sampling algorithm and a\ntarget data distribution, under general noise schedule parameters. The analysis\nassumes certain technical conditions on the data distribution and a linear\ngrowth condition on the noise estimation function. The sampling sequence\nemerges as an exponential integrator-type approximation of a reverse-time\nstochastic differential equation (SDE) over a finite time interval.\nSchr\\\"odinger's problem provides a tool for estimating the distributional error\nin reverse time, which connects the reverse-time error with its forward-time\ncounterpart. The score function in DDPMs appears as an adapted solution of a\nforward-backward SDE, providing a foundation for analyzing the\ntime-discretization error associated with the reverse-time SDE.\n","authors":["Yumiharu Nakano"],"pdf_url":"https://arxiv.org/pdf/2406.01320v5.pdf","comment":"Substantial changes have been made in this version"},{"id":"http://arxiv.org/abs/2409.19777v2","updated":"2025-04-14T03:00:35Z","published":"2024-09-29T20:56:54Z","title":"Automatic debiasing of neural networks via moment-constrained learning","summary":"  Causal and nonparametric estimands in economics and biostatistics can often\nbe viewed as the mean of a linear functional applied to an unknown outcome\nregression function. Naively learning the regression function and taking a\nsample mean of the target functional results in biased estimators, and a rich\ndebiasing literature has developed where one additionally learns the so-called\nRiesz representer (RR) of the target estimand (targeted learning, double ML,\nautomatic debiasing etc.). Learning the RR via its derived functional form can\nbe challenging, e.g. due to extreme inverse probability weights or the need to\nlearn conditional density functions. Such challenges have motivated recent\nadvances in automatic debiasing (AD), where the RR is learned directly via\nminimization of a bespoke loss. We propose moment-constrained learning as a new\nRR learning approach that addresses some shortcomings in AD, constraining the\npredicted moments and improving the robustness of RR estimates to optimization\nhyperparamters. Though our approach is not tied to a particular class of\nlearner, we illustrate it using neural networks, and evaluate on the problems\nof average treatment/derivative effect estimation using semi-synthetic data.\nOur numerical experiments show improved performance versus state of the art\nbenchmarks.\n","authors":["Christian L. Hines","Oliver J. Hines"],"pdf_url":"https://arxiv.org/pdf/2409.19777v2.pdf","comment":"Code repository and license available at\n  https://github.com/crimbs/madnet"},{"id":"http://arxiv.org/abs/2504.09831v1","updated":"2025-04-14T02:57:51Z","published":"2025-04-14T02:57:51Z","title":"Offline Dynamic Inventory and Pricing Strategy: Addressing Censored and\n  Dependent Demand","summary":"  In this paper, we study the offline sequential feature-based pricing and\ninventory control problem where the current demand depends on the past demand\nlevels and any demand exceeding the available inventory is lost. Our goal is to\nleverage the offline dataset, consisting of past prices, ordering quantities,\ninventory levels, covariates, and censored sales levels, to estimate the\noptimal pricing and inventory control policy that maximizes long-term profit.\nWhile the underlying dynamic without censoring can be modeled by Markov\ndecision process (MDP), the primary obstacle arises from the observed process\nwhere demand censoring is present, resulting in missing profit information, the\nfailure of the Markov property, and a non-stationary optimal policy. To\novercome these challenges, we first approximate the optimal policy by solving a\nhigh-order MDP characterized by the number of consecutive censoring instances,\nwhich ultimately boils down to solving a specialized Bellman equation tailored\nfor this problem. Inspired by offline reinforcement learning and survival\nanalysis, we propose two novel data-driven algorithms to solving these Bellman\nequations and, thus, estimate the optimal policy. Furthermore, we establish\nfinite sample regret bounds to validate the effectiveness of these algorithms.\nFinally, we conduct numerical experiments to demonstrate the efficacy of our\nalgorithms in estimating the optimal policy. To the best of our knowledge, this\nis the first data-driven approach to learning optimal pricing and inventory\ncontrol policies in a sequential decision-making environment characterized by\ncensored and dependent demand. The implementations of the proposed algorithms\nare available at https://github.com/gundemkorel/Inventory_Pricing_Control\n","authors":["Korel Gundem","Zhengling Qi"],"pdf_url":"https://arxiv.org/pdf/2504.09831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05722v3","updated":"2025-04-14T21:23:21Z","published":"2025-02-08T23:35:37Z","title":"Explainable and Class-Revealing Signal Feature Extraction via Scattering\n  Transform and Constrained Zeroth-Order Optimization","summary":"  We propose a new method to extract discriminant and explainable features from\na particular machine learning model, i.e., a combination of the scattering\ntransform and the multiclass logistic regression. Although this model is\nwell-known for its ability to learn various signal classes with high\nclassification rate, it remains elusive to understand why it can generate such\nsuccessful classification, mainly due to the nonlinearity of the scattering\ntransform. In order to uncover the meaning of the scattering transform\ncoefficients selected by the multiclass logistic regression (with the Lasso\npenalty), we adopt zeroth-order optimization algorithms to search an input\npattern that maximizes the class probability of a class of interest given the\nlearned model. In order to do so, it turns out that imposing sparsity and\nsmoothness of input patterns is important. We demonstrate the effectiveness of\nour proposed method using a couple of synthetic time-series classification\nproblems.\n","authors":["Naoki Saito","David Weber"],"pdf_url":"https://arxiv.org/pdf/2502.05722v3.pdf","comment":"5 pages; 6 figures; to appear in Proceedings of 2025 IEEE Workshop on\n  Statistical Signal Processing"},{"id":"http://arxiv.org/abs/2504.10667v1","updated":"2025-04-14T19:40:39Z","published":"2025-04-14T19:40:39Z","title":"Beyond Coordinates: Meta-Equivariance in Statistical Inference","summary":"  Optimal statistical decisions should transcend the language used to describe\nthem. Yet, how do we guarantee that the choice of coordinates - the\nparameterisation of an optimisation problem - does not subtly dictate the\nsolution? This paper reveals a fundamental geometric invariance principle. We\nfirst analyse the optimal combination of two asymptotically normal estimators\nunder a strictly convex trace-AMSE risk. While methods for finding optimal\nweights are known, we prove that the resulting optimal estimator is invariant\nunder direct affine reparameterisations of the weighting scheme. This\nexemplifies a broader principle we term meta-equivariance: the unique minimiser\nof any strictly convex, differentiable scalar objective over a matrix space\ntransforms covariantly under any invertible affine reparameterisation of that\nspace. Distinct from classical statistical equivariance tied to data\nsymmetries, meta-equivariance arises from the immutable geometry of convex\noptimisation itself. It guarantees that optimality, in these settings, is not\nan artefact of representation but an intrinsic, coordinate-free truth.\n","authors":["William Cook"],"pdf_url":"https://arxiv.org/pdf/2504.10667v1.pdf","comment":"21 pages. Includes numerical simulations and visualisations.\n  Developed independently using a live co-discovery process combining human\n  intuition and computational validation. The principle of meta-equivariance\n  generalises convex invariance under affine reparameterisation, with\n  implications for statistical decision theory and information geometry"},{"id":"http://arxiv.org/abs/2412.10575v2","updated":"2025-04-14T19:40:08Z","published":"2024-12-13T21:36:12Z","title":"Who's the (Multi-)Fairest of Them All: Rethinking Interpolation-Based\n  Data Augmentation Through the Lens of Multicalibration","summary":"  Data augmentation methods, especially SoTA interpolation-based methods such\nas Fair Mixup, have been widely shown to increase model fairness. However, this\nfairness is evaluated on metrics that do not capture model uncertainty and on\ndatasets with only one, relatively large, minority group. As a remedy,\nmulticalibration has been introduced to measure fairness while accommodating\nuncertainty and accounting for multiple minority groups. However, existing\nmethods of improving multicalibration involve reducing initial training data to\ncreate a holdout set for post-processing, which is not ideal when minority\ntraining data is already sparse. This paper uses multicalibration to more\nrigorously examine data augmentation for classification fairness. We\nstress-test four versions of Fair Mixup on two structured data classification\nproblems with up to 81 marginalized groups, evaluating multicalibration\nviolations and balanced accuracy. We find that on nearly every experiment, Fair\nMixup \\textit{worsens} baseline performance and fairness, but the simple\nvanilla Mixup \\textit{outperforms} both Fair Mixup and the baseline, especially\nwhen calibrating on small groups. \\textit{Combining} vanilla Mixup with\nmulticalibration post-processing, which enforces multicalibration through\npost-processing on a holdout set, further increases fairness.\n","authors":["Karina Halevy","Karly Hou","Charumathi Badrinath"],"pdf_url":"https://arxiv.org/pdf/2412.10575v2.pdf","comment":"Expanded version of AAAI 2025 main track paper. 8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2504.10653v1","updated":"2025-04-14T19:10:22Z","published":"2025-04-14T19:10:22Z","title":"On the Contractivity of Stochastic Interpolation Flow","summary":"  We investigate stochastic interpolation, a recently introduced framework for\nhigh dimensional sampling which bears many similarities to diffusion modeling.\nStochastic interpolation generates a data sample by first randomly initializing\na particle drawn from a simple base distribution, then simulating deterministic\nor stochastic dynamics such that in finite time the particle's distribution\nconverges to the target. We show that for a Gaussian base distribution and a\nstrongly log-concave target distribution, the stochastic interpolation flow map\nis Lipschitz with a sharp constant which matches that of Caffarelli's theorem\nfor optimal transport maps. We are further able to construct Lipschitz\ntransport maps between non-Gaussian distributions, generalizing some recent\nconstructions in the literature on transport methods for establishing\nfunctional inequalities. We discuss the practical implications of our theorem\nfor the sampling and estimation problems required by stochastic interpolation.\n","authors":["Max Daniels"],"pdf_url":"https://arxiv.org/pdf/2504.10653v1.pdf","comment":"Proof of concept. I would be excited to chat about extensions!"},{"id":"http://arxiv.org/abs/2409.10096v2","updated":"2025-04-14T18:57:13Z","published":"2024-09-16T08:54:59Z","title":"Robust Reinforcement Learning with Dynamic Distortion Risk Measures","summary":"  In a reinforcement learning (RL) setting, the agent's optimal strategy\nheavily depends on her risk preferences and the underlying model dynamics of\nthe training environment. These two aspects influence the agent's ability to\nmake well-informed and time-consistent decisions when facing testing\nenvironments. In this work, we devise a framework to solve robust risk-aware RL\nproblems where we simultaneously account for environmental uncertainty and risk\nwith a class of dynamic robust distortion risk measures. Robustness is\nintroduced by considering all models within a Wasserstein ball around a\nreference model. We estimate such dynamic robust risk measures using neural\nnetworks by making use of strictly consistent scoring functions, derive policy\ngradient formulae using the quantile representation of distortion risk\nmeasures, and construct an actor-critic algorithm to solve this class of robust\nrisk-aware RL problems. We demonstrate the performance of our algorithm on a\nportfolio allocation example.\n","authors":["Anthony Coache","Sebastian Jaimungal"],"pdf_url":"https://arxiv.org/pdf/2409.10096v2.pdf","comment":"27 pages, 3 figures"},{"id":"http://arxiv.org/abs/2504.10620v1","updated":"2025-04-14T18:20:47Z","published":"2025-04-14T18:20:47Z","title":"SPreV","summary":"  SPREV, short for hyperSphere Reduced to two-dimensional Regular Polygon for\nVisualisation, is a novel dimensionality reduction technique developed to\naddress the challenges of reducing dimensions and visualizing labeled datasets\nthat exhibit a unique combination of three characteristics: small class size,\nhigh dimensionality, and low sample size. SPREV is designed not only to uncover\nbut also to visually represent hidden patterns within such datasets. Its\ndistinctive integration of geometric principles, adapted for discrete\ncomputational environments, makes it an indispensable tool in the modern data\nscience toolkit, enabling users to identify trends, extract insights, and\nnavigate complex data efficiently and effectively.\n","authors":["Srivathsan Amruth"],"pdf_url":"https://arxiv.org/pdf/2504.10620v1.pdf","comment":"45 Pages, 7 Figures, 3 Tables, 9 Algorithms, Opensource"},{"id":"http://arxiv.org/abs/2504.10612v1","updated":"2025-04-14T18:10:58Z","published":"2025-04-14T18:10:58Z","title":"Energy Matching: Unifying Flow Matching and Energy-Based Models for\n  Generative Modeling","summary":"  Generative models often map noise to data by matching flows or scores, but\nthese approaches become cumbersome for incorporating partial observations or\nadditional priors. Inspired by recent advances in Wasserstein gradient flows,\nwe propose Energy Matching, a framework that unifies flow-based approaches with\nthe flexibility of energy-based models (EBMs). Far from the data manifold,\nsamples move along curl-free, optimal transport paths from noise to data. As\nthey approach the data manifold, an entropic energy term guides the system into\na Boltzmann equilibrium distribution, explicitly capturing the underlying\nlikelihood structure of the data. We parameterize this dynamic with a single\ntime-independent scalar field, which serves as both a powerful generator and a\nflexible prior for effective regularization of inverse problems. Our method\nsubstantially outperforms existing EBMs on CIFAR-10 generation (FID 3.97\ncompared to 8.61), while retaining the simulation-free training of\ntransport-based approaches away from the data manifold. Additionally, we\nexploit the flexibility of our method and introduce an interaction energy for\ndiverse mode exploration. Our approach focuses on learning a static scalar\npotential energy -- without time conditioning, auxiliary generators, or\nadditional networks -- marking a significant departure from recent EBM methods.\nWe believe this simplified framework significantly advances EBM capabilities\nand paves the way for their broader adoption in generative modeling across\ndiverse domains.\n","authors":["Michal Balcerak","Tamaz Amiranashvili","Suprosanna Shit","Antonio Terpin","Sebastian Kaltenbach","Petros Koumoutsakos","Bjoern Menze"],"pdf_url":"https://arxiv.org/pdf/2504.10612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10598v1","updated":"2025-04-14T18:00:23Z","published":"2025-04-14T18:00:23Z","title":"Beyond Worst-Case Online Classification: VC-Based Regret Bounds for\n  Relaxed Benchmarks","summary":"  We revisit online binary classification by shifting the focus from competing\nwith the best-in-class binary loss to competing against relaxed benchmarks that\ncapture smoothed notions of optimality. Instead of measuring regret relative to\nthe exact minimal binary error -- a standard approach that leads to worst-case\nbounds tied to the Littlestone dimension -- we consider comparing with\npredictors that are robust to small input perturbations, perform well under\nGaussian smoothing, or maintain a prescribed output margin. Previous examples\nof this were primarily limited to the hinge loss. Our algorithms achieve regret\nguarantees that depend only on the VC dimension and the complexity of the\ninstance space (e.g., metric entropy), and notably, they incur only an\n$O(\\log(1/\\gamma))$ dependence on the generalized margin $\\gamma$. This stands\nin contrast to most existing regret bounds, which typically exhibit a\npolynomial dependence on $1/\\gamma$. We complement this with matching lower\nbounds. Our analysis connects recent ideas from adversarial robustness and\nsmoothed online learning.\n","authors":["Omar Montasser","Abhishek Shetty","Nikita Zhivotovskiy"],"pdf_url":"https://arxiv.org/pdf/2504.10598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09309v2","updated":"2025-04-14T17:28:22Z","published":"2025-03-12T12:02:02Z","title":"Steering No-Regret Agents in MFGs under Model Uncertainty","summary":"  Incentive design is a popular framework for guiding agents' learning dynamics\ntowards desired outcomes by providing additional payments beyond intrinsic\nrewards. However, most existing works focus on a finite, small set of agents or\nassume complete knowledge of the game, limiting their applicability to\nreal-world scenarios involving large populations and model uncertainty. To\naddress this gap, we study the design of steering rewards in Mean-Field Games\n(MFGs) with density-independent transitions, where both the transition dynamics\nand intrinsic reward functions are unknown. This setting presents non-trivial\nchallenges, as the mediator must incentivize the agents to explore for its\nmodel learning under uncertainty, while simultaneously steer them to converge\nto desired behaviors without incurring excessive incentive payments. Assuming\nagents exhibit no(-adaptive) regret behaviors, we contribute novel optimistic\nexploration algorithms. Theoretically, we establish sub-linear regret\nguarantees for the cumulative gaps between the agents' behaviors and the\ndesired ones. In terms of the steering cost, we demonstrate that our total\nincentive payments incur only sub-linear excess, competing with a baseline\nsteering strategy that stabilizes the target policy as an equilibrium. Our work\npresents an effective framework for steering agents behaviors in\nlarge-population systems under uncertainty.\n","authors":["Leo Widmer","Jiawei Huang","Niao He"],"pdf_url":"https://arxiv.org/pdf/2503.09309v2.pdf","comment":"AISTATS 2025; 34 Pages"}],"Optimization":[{"id":"http://arxiv.org/abs/2504.10469v1","updated":"2025-04-14T17:53:34Z","published":"2025-04-14T17:53:34Z","title":"Bounds as blueprints: towards optimal and accelerated photonic inverse\n  design","summary":"  Our ability to structure materials at the nanoscale has, and continues to,\nenable key advances in optical control. In pursuit of optimal photonic designs,\nsubstantial progress has been made on two complementary fronts: bottom-up\nstructural optimizations (inverse design) discover complex high-performing\nstructures but offer no guarantees of optimality; top-down field optimizations\n(convex relaxations) reveal fundamental performance limits but offer no\nguarantees that structures meeting the limits exist. We bridge the gap between\nthese two parallel paradigms by introducing a ``verlan'' initialization method\nthat exploits the encoded local and global wave information in duality-based\nconvex relaxations to guide inverse design towards better-performing\nstructures. We illustrate this technique via the challenging problem of Purcell\nenhancement, maximizing the power extracted from a small emitter in the\nvicinity of a photonic structure, where ill-conditioning and the presence of\ncompeting local maxima lead to sub-optimal designs for adjoint optimization.\nStructures discovered by our verlan method outperform standard (random)\ninitializations by close to an order of magnitude and approach fundamental\nperformance limits within a factor of two, highlighting the possibility of\naccessing significant untapped performance improvements.\n","authors":["Pengning Chao","Alessio Amaolo","Sean Molesky","Alejandro W. Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2504.10469v1.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2302.05816v3","updated":"2025-04-14T17:34:46Z","published":"2023-02-11T23:30:50Z","title":"A Policy Gradient Framework for Stochastic Optimal Control Problems with\n  Global Convergence Guarantee","summary":"  We consider policy gradient methods for stochastic optimal control problem in\ncontinuous time. In particular, we analyze the gradient flow for the control,\nviewed as a continuous time limit of the policy gradient method. We prove the\nglobal convergence of the gradient flow and establish a convergence rate under\nsome regularity assumptions. The main novelty in the analysis is the notion of\nlocal optimal control function, which is introduced to characterize the local\noptimality of the iterate.\n","authors":["Mo Zhou","Jianfeng Lu"],"pdf_url":"https://arxiv.org/pdf/2302.05816v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13506v2","updated":"2025-04-14T17:22:19Z","published":"2024-05-22T10:06:13Z","title":"Large Deviations in Safety-Critical Systems with Probabilistic Initial\n  Conditions","summary":"  We often rely on probabilistic measures--e.g. event probability or expected\ntime--to characterize systems safety. However, determining these quantities for\nextremely low-probability events is generally challenging, as standard safety\nmethods usually struggle due to conservativeness, high-dimension scalability,\ntractability or numerical limitations. We address these issues by leveraging\nrigorous approximations grounded in the principles of Large Deviations theory.\nBy assuming deterministic initial conditions, Large Deviations identifies a\nsingle dominant path as the most significant contributor to the rare-event\nprobability: the instanton. We extend this result to incorporate stochastic\nuncertainty in the initial states, which is a common assumption in many\napplications. To that end, we determine an expression for the probability\ndensity of the initial states, conditioned on the instanton--the most dominant\npath hitting the unsafe region--being observed. This expression gives access to\nthe most probable initial conditions, as well as the most probable hitting time\nand path deviations, leading to an unsafe rare event. We demonstrate its\neffectiveness by solving a high-dimensional and non-linear problem: a space\ncollision.\n","authors":["Aitor R. Gomez","Manuela L. Bujorianu","Rafal Wisniewski"],"pdf_url":"https://arxiv.org/pdf/2405.13506v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14902v2","updated":"2025-04-14T17:09:26Z","published":"2023-09-26T13:02:57Z","title":"Magnetic Bernstein inequalities and spectral inequality on thick sets\n  for the Landau operator","summary":"  We prove a spectral inequality for the Landau operator. This means that for\nall $f$ in the spectral subspace corresponding to energies up to $E$, the\n$L^2$-integral over suitable $S \\subset \\mathbb{R}^2$ can be lower bounded by\nan explicit constant times the $L^2$-norm of $f$ itself. We identify the class\nof all measurable sets $S \\subset \\mathbb{R}^2$ for which such an inequality\ncan hold, namely so-called thick or relatively dense sets, and deduce an\nasymptotically optimal expression for the constant in terms of the energy, the\nmagnetic field strength and in terms of parameters determining the thick set\n$S$. Our proofs rely on so-called magnetic Bernstein inequalities. As a\nconsequence, we obtain the first proof of null-controllability for the magnetic\nheat equation (with sharp bound on the control cost), and can relax assumptions\nin existing proofs of Anderson localization in the continuum alloy-type model.\n","authors":["Paul Pfeiffer","Matthias Täufer"],"pdf_url":"https://arxiv.org/pdf/2309.14902v2.pdf","comment":"27 pages, minor corrections with respect to the previous version"},{"id":"http://arxiv.org/abs/2504.10389v1","updated":"2025-04-14T16:35:20Z","published":"2025-04-14T16:35:20Z","title":"Diversity-Fair Online Selection","summary":"  Online selection problems frequently arise in applications such as\ncrowdsourcing and employee recruitment. Existing research typically focuses on\ncandidates with a single attribute. However, crowdsourcing tasks often require\ncontributions from individuals across various demographics. Further motivated\nby the dynamic nature of crowdsourcing and hiring, we study the diversity-fair\nonline selection problem, in which a recruiter must make real-time decisions to\nfoster workforce diversity across many dimensions. We propose two scenarios for\nthis problem. The fixed-capacity scenario, suited for short-term hiring for\ncrowdsourced workers, provides the recruiter with a fixed capacity to fill\ntemporary job vacancies. In contrast, in the unknown-capacity scenario,\nrecruiters optimize diversity across recruitment seasons with increasing\ncapacities, reflecting that the firm honors diversity consideration in a\nlong-term employee acquisition strategy. By modeling the diversity over $d$\ndimensions as a max-min fairness objective, we show that no policy can surpass\na competitive ratio of $O(1/d^{1/3})$ for either scenario, indicating that any\nachievable result inevitably decays by some polynomial factor in $d$. To this\nend, we develop bilevel hierarchical randomized policies that ensure compliance\nwith the capacity constraint. For the fixed-capacity scenario, leveraging\nmarginal information about the arriving population allows us to achieve a\ncompetitive ratio of $1/(4\\sqrt{d} \\lceil \\log_2 d \\rceil)$. For the\nunknown-capacity scenario, we establish a competitive ratio of\n$\\Omega(1/d^{3/4})$ under mild boundedness conditions. In both bilevel\nhierarchical policies, the higher level determines ex-ante selection\nprobabilities and then informs the lower level's randomized selection that\nensures no loss in efficiency. Both policies prioritize core diversity and then\nadjust for underrepresented dimensions.\n","authors":["Ming Hu","Yanzhi Li","Tongwen Wu"],"pdf_url":"https://arxiv.org/pdf/2504.10389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10360v1","updated":"2025-04-14T16:08:32Z","published":"2025-04-14T16:08:32Z","title":"Reactive power flow optimization in AC drive systems","summary":"  This paper explores a limit avoidance approach in the case of input\n(modulation) and output (current) constraints with the aim of enhancing system\navailability of AC drives. Drawing on the observation that, in a certain range\nof reactive power, there exists a trade-off between current and modulation\nmagnitude, we exploit this freedom and define a constrained optimization\nproblem. We propose two approaches, one in the form of an activation-function\nwhich drives the reactive power set-point towards safety, and an approach which\nuses online feedback optimization to set the reactive power dynamically. Both\nmethods compromise reactive power tracking accuracy for increased system\nrobustness. Through a high fidelity simulation, we compare the benefits of the\ntwo methods, highlighting their effectiveness in industrial applications.\n","authors":["Sanjay Chandrasekaran","Catalin Arghir","Pieder Joerg","Florian Doerfler","Silvia Mastellone"],"pdf_url":"https://arxiv.org/pdf/2504.10360v1.pdf","comment":"Submitted to the Conference on Decision and Control, 2025"},{"id":"http://arxiv.org/abs/2504.10336v1","updated":"2025-04-14T15:46:19Z","published":"2025-04-14T15:46:19Z","title":"Analysis of the complex gas pipeline exploitation process in various\n  operating modes","summary":"  The study aims to decrease gas loss and enhance system reliability during gas\npipeline accidents. A computational scheme has been developed that can enable\nthe elimination of gas leakage through the modeling and management of parallel\ngas pipeline systems. The dynamic state of processes for the supply of modern\nautomatic equipment to gas pipelines and the use of an efficient automated\ncontrol system have been extensively studied. The analytical determination of\nthe optimal transition time has been widely applied to ensure the most\nfavorable operating conditions for the system. Methods for calculating complex\ntransient processes in main gas pipelines, from a non-stationary regime to a\nstationary regime, have been developed, particularly at the moment of gas flow\ningress. A comparison of mathematical expressions for calculating transient\nprocesses in complex main gas pipelines has been conducted through theoretical\nsources.\n","authors":["Ilgar Aliyev"],"pdf_url":"https://arxiv.org/pdf/2504.10336v1.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.10315v1","updated":"2025-04-14T15:24:23Z","published":"2025-04-14T15:24:23Z","title":"An energy optimization method based on mixed-integer model and\n  variational quantum computing algorithm for faster IMPT","summary":"  Intensity-modulated proton therapy (IMPT) offers superior dose conformity\nwith reduced exposure to surrounding healthy tissues compared to conventional\nphoton therapy. Improving IMPT delivery efficiency reduces motion-related\nuncertainties, enhances plan robustness, and benefits breath-hold techniques by\nshortening treatment time. Among various factors, energy switching time plays a\ncritical role, making energy layer optimization (ELO) essential. This work\ndevelops an energy layer optimization method based on mixed integer model and\nvariational quantum computing algorithm to enhance the efficiency of IMPT. The\nenergy layer optimization problem is modeled as a mixed-integer program, where\ncontinuous variables optimize the dose distribution and binary variables\nindicate energy layer selection. To solve it, iterative convex relaxation\ndecouples the dose-volume constraints, followed by the alternating direction\nmethod of multipliers (ADMM) to separate mixed-variable optimization and the\nminimum monitor unit (MMU) constraint. The resulting beam intensity subproblem,\nsubject to MMU, either admits a closed-form solution or is efficiently solvable\nvia conjugate gradient. The binary subproblem is cast as a quadratic\nunconstrained binary optimization (QUBO) problem, solvable using variational\nquantum computing algorithms. With nearly the same plan quality, the proposed\nmethod noticeable reduces the number of the used energies. For example,\ncompared to conventional IMPT, QC can reduce the number of energy layers from\n61 to 35 in HN case, from 56 to 35 in lung case, and from 59 to 32 to abdomen\ncase. The reduced number of energies also results in fewer delivery time, e.g.,\nthe delivery time is reduced from 100.6, 232.0, 185.3 seconds to 90.7, 215.4,\n154.0 seconds, respectively.\n","authors":["Ya-Nan Zhu","Nimita Shinde","Bowen Lin","Hao Gao"],"pdf_url":"https://arxiv.org/pdf/2504.10315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20142v2","updated":"2025-04-14T15:16:11Z","published":"2025-03-26T01:16:42Z","title":"Local Linear Convergence of the Alternating Direction Method of\n  Multipliers for Semidefinite Programming under Strict Complementarity","summary":"  We investigate the local linear convergence properties of the Alternating\nDirection Method of Multipliers (ADMM) when applied to Semidefinite Programming\n(SDP). A longstanding belief suggests that ADMM is only capable of solving SDPs\nto moderate accuracy, primarily due to its sublinear worst-case complexity and\nempirical observations of slow convergence. We challenge this notion by\nintroducing a new sufficient condition for local linear convergence: as long as\nthe converged primal-dual optimal solutions satisfy strict complementarity,\nADMM attains local linear convergence, independent of nondegeneracy conditions.\nOur proof is based on a direct local linearization of the ADMM operator and a\nrefined error bound for the projection onto the positive semidefinite cone,\nimproving previous bounds and revealing the anisotropic nature of projection\nresiduals. Extensive numerical experiments confirm the significance of our\ntheoretical results, demonstrating that ADMM achieves local linear convergence\nand computes high-accuracy solutions in a variety of SDP instances, including\nthose cases where nondegeneracy fails. Furthermore, we identify where ADMM\nstruggles, linking these difficulties with near violations of strict\ncomplementarity-a phenomenon that parallels recent findings in linear\nprogramming.\n","authors":["Shucheng Kang","Xin Jiang","Heng Yang"],"pdf_url":"https://arxiv.org/pdf/2503.20142v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.23600v2","updated":"2025-04-14T15:12:29Z","published":"2025-03-30T21:48:11Z","title":"Online Convex Optimization and Integral Quadratic Constraints: A new\n  approach to regret analysis","summary":"  We propose a novel approach for analyzing dynamic regret of first-order\nconstrained online convex optimization algorithms for strongly convex and\nLipschitz-smooth objectives. Crucially, we provide a general analysis that is\napplicable to a wide range of first-order algorithms that can be expressed as\nan interconnection of a linear dynamical system in feedback with a first-order\noracle. By leveraging Integral Quadratic Constraints (IQCs), we derive a\nsemi-definite program which, when feasible, provides a regret guarantee for the\nonline algorithm. For this, the concept of variational IQCs is introduced as\nthe generalization of IQCs to time-varying monotone operators. Our bounds\ncapture the temporal rate of change of the problem in the form of the path\nlength of the time-varying minimizer and the objective function variation. In\ncontrast to standard results in OCO, our results do not require nerither the\nassumption of gradient boundedness, nor that of a bounded feasible set.\nNumerical analyses showcase the ability of the approach to capture the\ndependence of the regret on the function class condition number.\n","authors":["Fabian Jakob","Andrea Iannelli"],"pdf_url":"https://arxiv.org/pdf/2503.23600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10302v1","updated":"2025-04-14T15:11:12Z","published":"2025-04-14T15:11:12Z","title":"Nonnegativity of signomials with Newton simplex over convex sets","summary":"  We study a class of signomials whose positive support is the set of vertices\nof a simplex and which may have multiple negative support points in the\nsimplex. Various groups of authors have provided an exact characterization for\nthe global nonnegativity of a signomial in this class in terms of circuit\nsignomials and that characterization provides a tractable nonnegativity test.\nWe generalize this characterization to the constrained nonnegativity over a\nconvex set $X$. This provides a tractable $X$-nonnegativity test for the class\nin terms of relative entropy programming and in terms of the support function\nof $X$. Our proof methods rely on the convex cone of constrained SAGE\nsignomials (sums of arithmetic-geometric exponentials) and the duality theory\nof this cone.\n","authors":["Jonas Ellwanger","Thorsten Theobald","Timo de Wolff"],"pdf_url":"https://arxiv.org/pdf/2504.10302v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2503.11736v2","updated":"2025-04-14T14:57:10Z","published":"2025-03-14T14:02:12Z","title":"A Smooth Analytical Formulation of Collision Detection and Rigid Body\n  Dynamics With Contact","summary":"  Generating intelligent robot behavior in contact-rich settings is a research\nproblem where zeroth-order methods currently prevail. A major contributor to\nthe success of such methods is their robustness in the face of non-smooth and\ndiscontinuous optimization landscapes that are characteristic of contact\ninteractions, yet zeroth-order methods remain computationally inefficient. It\nis therefore desirable to develop methods for perception, planning and control\nin contact-rich settings that can achieve further efficiency by making use of\nfirst and second order information (i.e., gradients and Hessians). To\nfacilitate this, we present a joint formulation of collision detection and\ncontact modelling which, compared to existing differentiable simulation\napproaches, provides the following benefits: i) it results in forward and\ninverse dynamics that are entirely analytical (i.e. do not require solving\noptimization or root-finding problems with iterative methods) and smooth (i.e.\ntwice differentiable), ii) it supports arbitrary collision geometries without\nneeding a convex decomposition, and iii) its runtime is independent of the\nnumber of contacts. Through simulation experiments, we demonstrate the validity\nof the proposed formulation as a \"physics for inference\" that can facilitate\nfuture development of efficient methods to generate intelligent contact-rich\nbehavior.\n","authors":["Onur Beker","Nico Gürtler","Ji Shi","A. René Geist","Amirreza Razmjoo","Georg Martius","Sylvain Calinon"],"pdf_url":"https://arxiv.org/pdf/2503.11736v2.pdf","comment":"Added references to point-based implicit surface representations"},{"id":"http://arxiv.org/abs/2311.13094v3","updated":"2025-04-14T14:25:36Z","published":"2023-11-22T01:50:43Z","title":"Newton-CG methods for nonconvex unconstrained optimization with Hölder\n  continuous Hessian","summary":"  In this paper we consider a nonconvex unconstrained optimization problem\nminimizing a twice differentiable objective function with H\\\"older continuous\nHessian. Specifically, we first propose a Newton-conjugate gradient (Newton-CG)\nmethod for finding an approximate first- and second-order stationary point of\nthis problem, assuming the associated the H\\\"older parameters are explicitly\nknown. Then we develop a parameter-free Newton-CG method without requiring any\nprior knowledge of these parameters. To the best of our knowledge, this method\nis the first parameter-free second-order method achieving the best-known\niteration and operation complexity for finding an approximate first- and\nsecond-order stationary point of this problem. Finally, we present preliminary\nnumerical results to demonstrate the superior practical performance of our\nparameter-free Newton-CG method over a well-known regularized Newton method.\n","authors":["Chuan He","Heng Huang","Zhaosong Lu"],"pdf_url":"https://arxiv.org/pdf/2311.13094v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2301.03139"},{"id":"http://arxiv.org/abs/2504.03443v2","updated":"2025-04-14T14:12:37Z","published":"2025-04-04T13:36:23Z","title":"Probabilistic Reachable Set Estimation for Saturated Systems with\n  Unbounded Additive Disturbances","summary":"  In this paper, we present an analytical approach for the synthesis of\nellipsoidal probabilistic reachable sets of saturated systems subject to\nunbounded additive noise. Using convex optimization methods, we compute a\ncontraction factor of the saturated error dynamics that allows us to tightly\nbound its evolution and therefore construct accurate reachable sets. The\nproposed approach is applicable to independent, zero mean disturbances with a\nknown covariance. A numerical example illustrates the applicability and\neffectiveness of the proposed design.\n","authors":["Carlo Karam","Matteo Tacchi-Bénard","Mirko Fiacchini"],"pdf_url":"https://arxiv.org/pdf/2504.03443v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00810v3","updated":"2025-04-14T13:53:34Z","published":"2024-03-31T21:51:28Z","title":"Off-the-grid regularisation for Poisson inverse problems","summary":"  Off-the-grid regularisation has been extensively employed over the last\ndecade in the context of ill-posed inverse problems formulated in the\ncontinuous setting of the space of Radon measures $\\mathcal{M}(\\mathcal{X})$.\nThese approaches enjoy convexity and counteract the discretisation biases as\nwell the numerical instabilities typical of their discrete counterparts. In the\nframework of sparse reconstruction of discrete point measures (sum of weighted\nDiracs), a Total Variation regularisation norm in $\\mathcal{M}(\\mathcal{X})$ is\ntypically combined with an $L^2$ data term modelling additive Gaussian noise.\nTo asses the framework of off-the-grid regularisation in the presence of\nsignal-dependent Poisson noise, we consider in this work a variational model\ncoupling the Total Variation regularisation with a Kullback-Leibler data term\nunder a non-negativity constraint. Analytically, we study the optimality\nconditions of the composite functional and analyse its dual problem. Then, we\nconsider an homotopy strategy to select an optimal regularisation parameter and\nuse it within a Sliding Frank-Wolfe algorithm. Several numerical experiments on\nboth 1D/2D simulated and real 3D fluorescent microscopy data are reported.\n","authors":["Marta Lazzaretti","Claudio Estatico","Alejandro Melero","Luca Calatroni"],"pdf_url":"https://arxiv.org/pdf/2404.00810v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08178v2","updated":"2025-04-14T13:32:35Z","published":"2025-04-11T00:20:37Z","title":"A Piecewise Lyapunov Analysis of Sub-quadratic SGD: Applications to\n  Robust and Quantile Regression","summary":"  Motivated by robust and quantile regression problems, we investigate the\nstochastic gradient descent (SGD) algorithm for minimizing an objective\nfunction $f$ that is locally strongly convex with a sub--quadratic tail. This\nsetting covers many widely used online statistical methods. We introduce a\nnovel piecewise Lyapunov function that enables us to handle functions $f$ with\nonly first-order differentiability, which includes a wide range of popular loss\nfunctions such as Huber loss. Leveraging our proposed Lyapunov function, we\nderive finite-time moment bounds under general diminishing stepsizes, as well\nas constant stepsizes. We further establish the weak convergence, central limit\ntheorem and bias characterization under constant stepsize, providing the first\ngeometrical convergence result for sub--quadratic SGD. Our results have wide\napplications, especially in online statistical methods. In particular, we\ndiscuss two applications of our results. 1) Online robust regression: We\nconsider a corrupted linear model with sub--exponential covariates and\nheavy--tailed noise. Our analysis provides convergence rates comparable to\nthose for corrupted models with Gaussian covariates and noise. 2) Online\nquantile regression: Importantly, our results relax the common assumption in\nprior work that the conditional density is continuous and provide a more\nfine-grained analysis for the moment bounds.\n","authors":["Yixuan Zhang"," Dongyan"," Huo","Yudong Chen","Qiaomin Xie"],"pdf_url":"https://arxiv.org/pdf/2504.08178v2.pdf","comment":"ACM SIGMETRICS 2025. 40 pages, 12 figures"},{"id":"http://arxiv.org/abs/2503.12238v2","updated":"2025-04-14T12:51:59Z","published":"2025-03-15T19:23:54Z","title":"Transition Uncertainties in Constrained Markov Decision Models: A Robust\n  Optimization Approach","summary":"  We examine a constrained Markov decision process under uncertain transition\nprobabilities, with the uncertainty modeled as deviations from observed\ntransition probabilities. We construct the uncertainty set associated with the\ndeviations using polyhedral and second-order cone constraints and employ a\nrobust optimization framework. We demonstrate that each inner optimization\nproblem of the robust model can be equivalently transformed into a second-order\ncone programming problem. Using strong duality arguments, we show that the\nresulting robust problem can be equivalently reformulated into a second-order\ncone programming problem with bilinear constraints. In the numerical\nexperiments, we study a machine replacement problem and explore potential\nsources of uncertainty in the transition probabilities. We examine how the\noptimal values and solutions differ as we vary the feasible region of the\nuncertainty set, considering only polyhedral constraints and a combination of\npolyhedral and second-order cone constraints. Furthermore, we analyze the\nimpact of the number of states, the discount factor, and variations in the\nfeasible region of the uncertainty set on the optimal values.\n","authors":["V Varagapriya"],"pdf_url":"https://arxiv.org/pdf/2503.12238v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.06115v6","updated":"2025-04-14T12:27:12Z","published":"2022-08-12T04:43:26Z","title":"A Nonparametric Approach with Marginals for Modeling Consumer Choice","summary":"  Given data on the choices made by consumers for different offer sets, a key\nchallenge is to develop parsimonious models that describe and predict consumer\nchoice behavior while being amenable to prescriptive tasks such as pricing and\nassortment optimization. The marginal distribution model (MDM) is one such\nmodel, which requires only the specification of marginal distributions of the\nrandom utilities. This paper aims to establish necessary and sufficient\nconditions for given choice data to be consistent with the MDM hypothesis,\ninspired by the usefulness of similar characterizations for the random utility\nmodel (RUM). This endeavor leads to an exact characterization of the set of\nchoice probabilities that the MDM can represent. Verifying the consistency of\nchoice data with this characterization is equivalent to solving a\npolynomial-sized linear program. Since the analogous verification task for RUM\nis computationally intractable and neither of these models subsumes the other,\nMDM is helpful in striking a balance between tractability and representational\npower. The characterization is then used with robust optimization for making\ndata-driven sales and revenue predictions for new unseen assortments. When the\nchoice data lacks consistency with the MDM hypothesis, finding the best-fitting\nMDM choice probabilities reduces to solving a mixed integer convex program.\nNumerical results using real world data and synthetic data demonstrate that MDM\nexhibits competitive representational power and prediction performance compared\nto RUM and parametric models while being significantly faster in computation\nthan RUM.\n","authors":["Yanqiu Ruan","Xiaobo Li","Karthyek Murthy","Karthik Natarajan"],"pdf_url":"https://arxiv.org/pdf/2208.06115v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10132v1","updated":"2025-04-14T11:38:35Z","published":"2025-04-14T11:38:35Z","title":"Asymptotic Optimality of Projected Inventory Level Policies for Lost\n  Sales Inventory Systems with Large Leadtime and Penalty Cost","summary":"  We study the canonical periodic review lost sales inventory system with\npositive leadtime and independent and identically distributed (i.i.d.) demand\nunder the average cost criterion. We demonstrate that the relative value\nfunction under the constant order policy satisfies the Wiener-Hopf equation. We\nemploy ladder processes associated with a random walk featuring i.i.d.\nincrements, to obtain an explicit solution for the relative value function.\nThis solution can be expressed as a quadratic form and a term that grows\nsublinearly. Then we perform an approximate policy iteration step on the\nconstant order policy and bound the approximation errors as a function of the\ncost of losing a sale. This leads to our main result that projected inventory\nlevel policies are asymptotically optimal as the leadtime grows when the cost\nof losing a sale is sufficiently large and demand has a finite second moment.\nUnder these conditions, we also show that the optimal cost rate approaches\ninfinity, proportional to the square root of the cost of losing a sale.\n","authors":["Poulad Moradi","Joachim Arts","Melvin Drent"],"pdf_url":"https://arxiv.org/pdf/2504.10132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02253v3","updated":"2025-04-14T11:36:12Z","published":"2024-11-04T16:43:16Z","title":"Towards safe Bayesian optimization with Wiener kernel regression","summary":"  Bayesian Optimization (BO) is a data-driven strategy for\nminimizing/maximizing black-box functions based on probabilistic surrogate\nmodels. In the presence of safety constraints, the performance of BO crucially\nrelies on tight probabilistic error bounds related to the uncertainty\nsurrounding the surrogate model. For the case of Gaussian Process surrogates\nand Gaussian measurement noise, we present a novel error bound based on the\nrecently proposed Wiener kernel regression. We prove that under rather mild\nassumptions, the proposed error bound is tighter than bounds previously\ndocumented in the literature, leading to enlarged safety regions. We draw upon\na numerical example to demonstrate the efficacy of the proposed error bound in\nsafe BO.\n","authors":["Oleksii Molodchyk","Johannes Teutsch","Timm Faulwasser"],"pdf_url":"https://arxiv.org/pdf/2411.02253v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10118v1","updated":"2025-04-14T11:28:20Z","published":"2025-04-14T11:28:20Z","title":"Stochastic Multigrid Minimization for Ptychographic Phase Retrieval","summary":"  We propose a novel stochastic multigrid minimization method for ptychographic\nphase retrieval. In our formulation, the challenging nonconvex and ill-posed\ninverse problem is recast as the iterative minimization of a quadratic\nsurrogate model that majorizes the original objective function. Our general\nframework encompasses the Ptychographic Iterative Engine (PIE) family of\nalgorithms. By efficiently solving the surrogate problem using a multigrid\nmethod, our approach delivers significant improvements in both convergence\nspeed and reconstruction quality compared with conventional PIE techniques.\n","authors":["Borong Zhang","Qin Li","Zichao Wendy Di"],"pdf_url":"https://arxiv.org/pdf/2504.10118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11780v3","updated":"2025-04-14T10:10:39Z","published":"2025-02-17T13:15:02Z","title":"Robust Optimization of Rank-Dependent Models with Uncertain\n  Probabilities","summary":"  This paper studies distributionally robust optimization for a rich class of\nrisk measures with ambiguity sets defined by $\\phi$-divergences. The risk\nmeasures are allowed to be non-linear in probabilities, are represented by\nChoquet integrals possibly induced by a probability weighting function, and\nencompass many well-known examples. Optimization for this class of risk\nmeasures is challenging due to their rank-dependent nature. We show that for\nvarious shapes of probability weighting functions, including concave, convex\nand inverse $S$-shaped, the robust optimization problem can be reformulated\ninto a rank-independent problem. In the case of a concave probability weighting\nfunction, the problem can be reformulated further into a convex optimization\nproblem that admits explicit conic representability for a collection of\ncanonical examples. While the number of constraints in general scales\nexponentially with the dimension of the state space, we circumvent this\ndimensionality curse and develop two types of algorithms. They yield tight\nupper and lower bounds on the exact optimal value and are formally shown to\nconverge asymptotically. This is illustrated numerically in a robust newsvendor\nproblem and a robust portfolio choice problem.\n","authors":["Guanyu Jin","Roger J. A. Laeven","Dick den Hertog"],"pdf_url":"https://arxiv.org/pdf/2502.11780v3.pdf","comment":"72 pages"},{"id":"http://arxiv.org/abs/2504.09974v1","updated":"2025-04-14T08:26:06Z","published":"2025-04-14T08:26:06Z","title":"Towards Resilient Tracking in Autonomous Vehicles: A Distributionally\n  Robust Input and State Estimation Approach","summary":"  This paper proposes a novel framework for the distributionally robust input\nand state estimation (DRISE) for autonomous vehicles operating under model\nuncertainties and measurement outliers. The proposed framework improves the\ninput and state estimation (ISE) approach by integrating distributional\nrobustness, enhancing the estimator's resilience and robustness to adversarial\ninputs and unmodeled dynamics. Moment-based ambiguity sets capture\nprobabilistic uncertainties in both system dynamics and measurement noise,\noffering analytical tractability and efficiently handling uncertainties in mean\nand covariance. In particular, the proposed framework minimizes the worst-case\nestimation error, ensuring robustness against deviations from nominal\ndistributions. The effectiveness of the proposed approach is validated through\nsimulations conducted in the CARLA autonomous driving simulator, demonstrating\nimproved performance in state estimation accuracy and robustness in dynamic and\nuncertain environments.\n","authors":["Kasra Azizi","Kumar Anurag","Wenbin Wan"],"pdf_url":"https://arxiv.org/pdf/2504.09974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09959v1","updated":"2025-04-14T07:56:41Z","published":"2025-04-14T07:56:41Z","title":"Exact Parameter Identification in PET Pharmacokinetic Modeling:\n  Extension to the Reversible Two Tissue Compartment Model","summary":"  This paper addresses the problem of recovering tracer kinetic parameters from\nmulti-region measurement data in quantitative PET imaging using the reversible\ntwo tissue compartment model. Its main result is an extension of our previous\nwork on the irreversible two tissue compartment model. In analogy to our\nprevious work, we show that also in the (practically highly relevant)\nreversible case, most tracer kinetic parameters can be uniquely identified from\nstandard PET measurements (without additional full blood sample analysis that\nis usually performed in practice) and under reasonable assumptions. In\naddition, unique identifiability of all parameters is shown provided that\nadditional measurements from the (uncorrected) total arterial blood tracer\nconcentration (which can be obtained from standard PET measurements or from a\nsimple blood sample analysis) are available.\n","authors":["Martin Holler","Erion Morina","Georg Schramm"],"pdf_url":"https://arxiv.org/pdf/2504.09959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00471v3","updated":"2025-04-14T07:47:46Z","published":"2025-02-01T15:56:19Z","title":"Evolution of Society Caused by Collective and Individual Decisions","summary":"  Decision-making societies may vary in their level of cooperation and degree\nof conservatism, both of which influence their overall performance. Moreover,\nthese factors are not fixed -- they can change based on the decisions agents in\nthe society make in their interests. But can these changes lead to cyclical\npatterns in societal evolution? To explore this question, we use the ViSE\n(Voting in Stochastic Environment) model. In this framework, the level of\ncooperation can be measured by group size, while the degree of conservatism is\ndetermined by the voting threshold. Agents can adopt either individualistic or\ngroup-oriented strategies when voting on stochastically generated external\nproposals. For Gaussian proposal generators, the expected capital gain (ECG) --\na measure of agents' performance -- can be expressed in standard mathematical\nfunctions. Our findings show that in neutral environments, societal evolution\nwith open or democratic groups can follow cyclic patterns. We also find that\nhighly conservative societies or conservative societies with low levels of\ncooperation can evolve into liberal (less conservative than majoritarian)\nsocieties and that mafia groups never let their members go when they want to.\n","authors":["Pavel Chebotarev"],"pdf_url":"https://arxiv.org/pdf/2502.00471v3.pdf","comment":"15 pages, 9 figures, a converence paper. Accepted for Springer\n  Lecture Notes in Business Information Processing"},{"id":"http://arxiv.org/abs/2504.09951v1","updated":"2025-04-14T07:26:34Z","published":"2025-04-14T07:26:34Z","title":"Towards Weaker Variance Assumptions for Stochastic Optimization","summary":"  We revisit a classical assumption for analyzing stochastic gradient\nalgorithms where the squared norm of the stochastic subgradient (or the\nvariance for smooth problems) is allowed to grow as fast as the squared norm of\nthe optimization variable. We contextualize this assumption in view of its\ninception in the 1960s, its seemingly independent appearance in the recent\nliterature, its relationship to weakest-known variance assumptions for\nanalyzing stochastic gradient algorithms, and its relevance in deterministic\nproblems for non-Lipschitz nonsmooth convex optimization. We build on and\nextend a connection recently made between this assumption and the Halpern\niteration. For convex nonsmooth, and potentially stochastic, optimization, we\nanalyze horizon-free, anytime algorithms with last-iterate rates. For problems\nbeyond simple constrained optimization, such as convex problems with functional\nconstraints or regularized convex-concave min-max problems, we obtain rates for\noptimality measures that do not require boundedness of the feasible set.\n","authors":["Ahmet Alacaoglu","Yura Malitsky","Stephen J. Wright"],"pdf_url":"https://arxiv.org/pdf/2504.09951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09934v1","updated":"2025-04-14T06:54:00Z","published":"2025-04-14T06:54:00Z","title":"Tight Semidefinite Relaxations for Verifying Robustness of Neural\n  Networks","summary":"  For verifying the safety of neural networks (NNs), Fazlyab et al. (2019)\nintroduced a semidefinite programming (SDP) approach called DeepSDP. This\nformulation can be viewed as the dual of the SDP relaxation for a problem\nformulated as a quadratically constrained quadratic program (QCQP). While SDP\nrelaxations of QCQPs generally provide approximate solutions with some gaps,\nthis work focuses on tight SDP relaxations that provide exact solutions to the\nQCQP for single-layer NNs. Specifically, we analyze tightness conditions in\nthree cases: (i) NNs with a single neuron, (ii) single-layer NNs with an\nellipsoidal input set, and (iii) single-layer NNs with a rectangular input set.\nFor NNs with a single neuron, we propose a condition that ensures the SDP\nadmits a rank-1 solution to DeepSDP by transforming the QCQP into an equivalent\ntwo-stage problem leads to a solution collinear with a predetermined vector.\nFor single-layer NNs with an ellipsoidal input set, the collinearity of\nsolutions is proved via the Karush-Kuhn-Tucker condition in the two-stage\nproblem. In case of single-layer NNs with a rectangular input set, we\ndemonstrate that the tightness of DeepSDP can be reduced to the single-neuron\nNNs, case (i), if the weight matrix is a diagonal matrix.\n","authors":["Godai Azuma","Sunyoung Kim","Makoto Yamashita"],"pdf_url":"https://arxiv.org/pdf/2504.09934v1.pdf","comment":"27 pages, 2 figures"},{"id":"http://arxiv.org/abs/2504.09930v1","updated":"2025-04-14T06:44:13Z","published":"2025-04-14T06:44:13Z","title":"Multi-objective Bayesian Optimization With Mixed-categorical Design\n  Variables for Expensive-to-evaluate Aeronautical Applications","summary":"  This work aims at developing new methodologies to optimize computational\ncostly complex systems (e.g., aeronautical engineering systems). The proposed\nsurrogate-based method (often called Bayesian optimization) uses adaptive\nsampling to promote a trade-off between exploration and exploitation. Our\nin-house implementation, called SEGOMOE, handles a high number of design\nvariables (continuous, discrete or categorical) and nonlinearities by combining\nmixtures of experts for the objective and/or the constraints. Additionally, the\nmethod handles multi-objective optimization settings, as it allows the\nconstruction of accurate Pareto fronts with a minimal number of function\nevaluations. Different infill criteria have been implemented to handle multiple\nobjectives with or without constraints. The effectiveness of the proposed\nmethod was tested on practical aeronautical applications within the context of\nthe European Project AGILE 4.0 and demonstrated favorable results. A first\nexample concerns a retrofitting problem where a comparison between two\noptimizers have been made. A second example introduces hierarchical variables\nto deal with architecture system in order to design an aircraft family. The\nthird example increases drastically the number of categorical variables as it\ncombines aircraft design, supply chain and manufacturing process. In this\narticle, we show, on three different realistic problems, various aspects of our\noptimization codes thanks to the diversity of the treated aircraft problems.\n","authors":["Nathalie Bartoli","Thierry Lefebvre","Rémi Lafage","Paul Saves","Youssef Diouane","Joseph Morlier","Jasper Bussemaker","Giuseppa Donelli","Joao Marcos Gomes de Mello","Massimo Mandorino","Pierluigi Della Vecchia"],"pdf_url":"https://arxiv.org/pdf/2504.09930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09913v1","updated":"2025-04-14T06:22:14Z","published":"2025-04-14T06:22:14Z","title":"Optimal Non-Asymptotic Rates of Value Iteration for Average-Reward\n  Markov Decision Processes","summary":"  While there is an extensive body of research on the analysis of Value\nIteration (VI) for discounted cumulative-reward MDPs, prior work on analyzing\nVI for (undiscounted) average-reward MDPs has been limited, and most prior\nresults focus on asymptotic rates in terms of Bellman error. In this work, we\nconduct refined non-asymptotic analyses of average-reward MDPs, obtaining a\ncollection of convergence results that advance our understanding of the setup.\nAmong our new results, most notable are the $\\mathcal{O}(1/k)$-rates of\nAnchored Value Iteration on the Bellman error under the multichain setup and\nthe span-based complexity lower bound that matches the $\\mathcal{O}(1/k)$ upper\nbound up to a constant factor of $8$ in the weakly communicating and unichain\nsetups\n","authors":["Jonmin Lee","Ernest K. Ryu"],"pdf_url":"https://arxiv.org/pdf/2504.09913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14308v2","updated":"2025-04-14T04:50:48Z","published":"2023-03-25T00:38:11Z","title":"Polynomial Optimization Relaxations for Generalized Semi-Infinite\n  Programs","summary":"  This paper studies generalized semi-infinite programs (GSIPs) given by\npolynomials. We propose a hierarchy of polynomial optimization relaxations to\nsolve them. They are based on Lagrange multiplier expressions and polynomial\nextensions. Moment-SOS relaxations are applied to solve the polynomial\noptimization. The convergence of this hierarchy is shown under certain\nconditions. In particular, the classical semi-infinite programs (SIPs) can be\nsolved as a special case of GSIPs. We also study GSIPs that have convex\ninfinity constraints and show that they can be solved exactly by a single\npolynomial optimization relaxation. The computational efficiency is\ndemonstrated by extensive numerical results.\n","authors":["Xiaomeng Hu","Jiawang Nie"],"pdf_url":"https://arxiv.org/pdf/2303.14308v2.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2504.09836v1","updated":"2025-04-14T03:04:48Z","published":"2025-04-14T03:04:48Z","title":"Score Matching Diffusion Based Feedback Control and Planning of\n  Nonlinear Systems","summary":"  We propose a novel control-theoretic framework that leverages principles from\ngenerative modeling -- specifically, Denoising Diffusion Probabilistic Models\n(DDPMs) -- to stabilize control-affine systems with nonholonomic constraints.\nUnlike traditional stochastic approaches, which rely on noise-driven dynamics\nin both forward and reverse processes, our method crucially eliminates the need\nfor noise in the reverse phase, making it particularly relevant for control\napplications. We introduce two formulations: one where noise perturbs all state\ndimensions during the forward phase while the control system enforces time\nreversal deterministically, and another where noise is restricted to the\ncontrol channels, embedding system constraints directly into the forward\nprocess.\n  For controllable nonlinear drift-free systems, we prove that deterministic\nfeedback laws can exactly reverse the forward process, ensuring that the\nsystem's probability density evolves correctly without requiring artificial\ndiffusion in the reverse phase. Furthermore, for linear time-invariant systems,\nwe establish a time-reversal result under the second formulation. By\neliminating noise in the backward process, our approach provides a more\npractical alternative to machine learning-based denoising methods, which are\nunsuitable for control applications due to the presence of stochasticity. We\nvalidate our results through numerical simulations on benchmark systems,\nincluding a unicycle model in a domain with obstacles, a driftless\nfive-dimensional system, and a four-dimensional linear system, demonstrating\nthe potential for applying diffusion-inspired techniques in linear, nonlinear,\nand settings with state space constraints.\n","authors":["Karthik Elamvazhuthi","Darshan Gadginmath","Fabio Pasqualetti"],"pdf_url":"https://arxiv.org/pdf/2504.09836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10743v1","updated":"2025-04-14T22:30:23Z","published":"2025-04-14T22:30:23Z","title":"Robust Gittins for Stochastic Scheduling","summary":"  A common theme in stochastic optimization problems is that, theoretically,\nstochastic algorithms need to \"know\" relatively rich information about the\nunderlying distributions. This is at odds with most applications, where\ndistributions are rough predictions based on historical data. Thus, commonly,\nstochastic algorithms are making decisions using imperfect predicted\ndistributions, while trying to optimize over some unknown true distributions.\nWe consider the fundamental problem of scheduling stochastic jobs preemptively\non a single machine to minimize expected mean completion time in the setting\nwhere the scheduler is only given imperfect predicted job size distributions.\nIf the predicted distributions are perfect, then it is known that this problem\ncan be solved optimally by the Gittins index policy. The goal of our work is to\ndesign a scheduling policy that is robust in the sense that it produces nearly\noptimal schedules even if there are modest discrepancies between the predicted\ndistributions and the underlying real distributions. Our main contributions\nare:\n  (1) We show that the standard Gittins index policy is not robust in this\nsense. If the true distributions are perturbed by even an arbitrarily small\namount, then running the Gittins index policy using the perturbed distributions\ncan lead to an unbounded increase in mean completion time.\n  (2) We explain how to modify the Gittins index policy to make it robust, that\nis, to produce nearly optimal schedules, where the approximation depends on a\nnew measure of error between the true and predicted distributions that we\ndefine.\n  Looking forward, the approach we develop here can be applied more broadly to\nmany other stochastic optimization problems to better understand the impact of\nmispredictions, and lead to the development of new algorithms that are robust\nagainst such mispredictions.\n","authors":["Benjamin Moseley","Heather Newman","Kirk Pruhs","Rudy Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.10743v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2404.10987v2","updated":"2025-04-14T22:20:15Z","published":"2024-04-17T01:50:38Z","title":"A Relative Inexact Proximal Gradient Method with an Explicit Linesearch","summary":"  This paper presents and investigates an inexact proximal gradient method for\nsolving composite convex optimization problems characterized by an objective\nfunction composed of a sum of a full-domain differentiable convex function and\na non-differentiable convex function. We introduce an explicit line search\napplied specifically to the differentiable component of the objective function,\nrequiring only a relative inexact solution of the proximal subproblem per\niteration. We prove the convergence of the sequence generated by our scheme and\nestablish its iteration complexity, considering both the functional values and\na residual associated with first-order stationary solutions. Additionally, we\nprovide numerical experiments to illustrate the practical efficacy of our\nmethod.\n","authors":["Yunier Bello-Cruz","Max L. N. Gonçalves","Jefferson G. Melo","Cassandra Mohr"],"pdf_url":"https://arxiv.org/pdf/2404.10987v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19779v4","updated":"2025-04-14T21:39:09Z","published":"2024-12-27T18:19:27Z","title":"Extended Set Difference : Inverse Operation of Minkowski Summation","summary":"  This paper introduces the extended set difference, a generalization of the\nHukuhara and generalized Hukuhara differences, defined for compact convex sets\nin $\\mathbb{R}^d$. The proposed difference guarantees existence for any pair of\nsuch sets, offering a broader framework for set arithmetic. The difference may\nnot be necessarily unique, but we offer a bound on the variety of solutions.\nThe definition of the extended set difference is formulated through an\noptimization problem, which provides a constructive approach to its\ncomputation. The paper explores the properties of this new difference,\nincluding its stability under orthogonal transformations and its robustness to\nperturbations of the input sets. We propose a method to compute this difference\nthrough a formulated linear optimization problem.\n","authors":["Arie Beresteanu","Behrooz Moosavi Ramezanzadeh"],"pdf_url":"https://arxiv.org/pdf/2412.19779v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05722v3","updated":"2025-04-14T21:23:21Z","published":"2025-02-08T23:35:37Z","title":"Explainable and Class-Revealing Signal Feature Extraction via Scattering\n  Transform and Constrained Zeroth-Order Optimization","summary":"  We propose a new method to extract discriminant and explainable features from\na particular machine learning model, i.e., a combination of the scattering\ntransform and the multiclass logistic regression. Although this model is\nwell-known for its ability to learn various signal classes with high\nclassification rate, it remains elusive to understand why it can generate such\nsuccessful classification, mainly due to the nonlinearity of the scattering\ntransform. In order to uncover the meaning of the scattering transform\ncoefficients selected by the multiclass logistic regression (with the Lasso\npenalty), we adopt zeroth-order optimization algorithms to search an input\npattern that maximizes the class probability of a class of interest given the\nlearned model. In order to do so, it turns out that imposing sparsity and\nsmoothness of input patterns is important. We demonstrate the effectiveness of\nour proposed method using a couple of synthetic time-series classification\nproblems.\n","authors":["Naoki Saito","David Weber"],"pdf_url":"https://arxiv.org/pdf/2502.05722v3.pdf","comment":"5 pages; 6 figures; to appear in Proceedings of 2025 IEEE Workshop on\n  Statistical Signal Processing"},{"id":"http://arxiv.org/abs/2503.23520v2","updated":"2025-04-14T20:31:09Z","published":"2025-03-30T17:03:38Z","title":"Tractable Characterization of Discrete-Time Periodic Monotonicity\n  Preserving Systems","summary":"  This paper studies three classes of discrete-time linear time-invariant\nsystems, which differ by the set of periodic signals that they leave invariant.\nThe first class preserves the property of periodic monotonicity, i.e.,\nperiod-wise unimodality. The second class is invariant to signals with at most\ntwo sign changes per period, and the third class results from the second by\nadditionally requiring that periodic signals with zero sign-changes are mapped\nto the same kind. We provide tractable characterizations for each system class\nby the use and extension of total positivity theory and combination with its\ngeometric interpretations. In particular, central to our results is the\ncharacterization of sequentially convex contours.\n  Moreover, as many static non-linearities, e.g., ideal relay, saturation,\nsigmoid function, quantizer, etc. also preserve these signal sets, our\ninvariance characterizations also apply to the loop gain of Lur'e feedback\nsystems. Thus, potentially forming the base for new developments of\nsignal-based fixed-point theorems towards the prediction of self-sustained\noscillations. In particular, our examples provide first indications for how the\nproperty of periodic monotonicity preservation is valuable to the study of\nrelay feedback systems.\n","authors":["Christian Grussler"],"pdf_url":"https://arxiv.org/pdf/2503.23520v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10693v1","updated":"2025-04-14T20:30:17Z","published":"2025-04-14T20:30:17Z","title":"Load Balancing with Network Latencies via Distributed Gradient Descent","summary":"  Motivated by the growing demand for serving large language model inference\nrequests, we study distributed load balancing for global serving systems with\nnetwork latencies. We consider a fluid model in which continuous flows of\nrequests arrive at different frontends and need to be routed to distant\nbackends for processing whose processing rates are workload dependent. Network\nlatencies can lead to long travel times for requests and delayed feedback from\nbackends. The objective is to minimize the average latency of requests,\ncomposed of the network latency and the serving latency at the backends.\n  We introduce Distributed Gradient Descent Load Balancing (DGD-LB), a\nprobabilistic routing algorithm in which each frontend adjusts the routing\nprobabilities dynamically using gradient descent. Our algorithm is distributed:\nthere is no coordination between frontends, except by observing the delayed\nimpact other frontends have on shared backends. The algorithm uses an\napproximate gradient that measures the marginal impact of an additional request\nevaluated at a delayed system state. Equilibrium points of our algorithm\nminimize the centralized optimal average latencies, and we provide a novel\nlocal stability analysis showing that our algorithm converges to an optimal\nsolution when started sufficiently close to that point. Moreover, we present\nsufficient conditions on the step-size of gradient descent that guarantee\nconvergence in the presence of network latencies. Numerical experiments show\nthat our algorithm is globally stable and optimal, confirm our stability\nconditions are nearly tight, and demonstrate that DGD-LB can lead to\nsubstantial gains relative to other load balancers studied in the literature\nwhen network latencies are large.\n","authors":["Santiago R. Balseiro","Vahab S. Mirrokni","Bartek Wydrowski"],"pdf_url":"https://arxiv.org/pdf/2504.10693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14104v2","updated":"2025-04-14T20:11:46Z","published":"2025-02-19T21:17:22Z","title":"A two-stage search framework for constrained multi-gradient descent","summary":"  The multi-gradient descent algorithm (MGDA) finds a common descent direction\nthat can improve all objectives by identifying the minimum-norm point in the\nconvex hull of the objective gradients. This method has become a foundational\ntool in large-scale multi-objective optimization, particularly in multi-task\nlearning. However, MGDA may struggle with constrained problems, whether\nconstraints are incorporated into the gradient hull or handled via projection\nonto the feasible region. To address this limitation, we propose a two-stage\nsearch algorithm for constrained multi-objective optimization. The first stage\nformulates a min-max problem that minimizes the upper bound of directional\nderivatives under constraints, yielding a weakly Pareto stationary solution\nwith balanced progress across objectives. The second stage refines this\nsolution by minimizing the lower bound of directional derivatives to achieve\nfull Pareto stationarity. We evaluate the proposed method on three numerical\nexamples. In a simple case with a known analytical Pareto front, our algorithm\nconverges rapidly. In more complex real-world problems, it consistently\noutperforms the evolutionary baselines NSGA-II and NSGA-III.\n","authors":["Yuan-Zheng Lei","Yaobang Gong","Xianfeng Terry Yang"],"pdf_url":"https://arxiv.org/pdf/2502.14104v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.07871v2","updated":"2025-04-14T20:05:44Z","published":"2024-08-15T00:56:51Z","title":"Monotonicity in Quadratically Regularized Linear Programs","summary":"  In optimal transport, quadratic regularization is a sparse alternative to\nentropic regularization: the solution measure tends to have small support.\nComputational experience suggests that the support decreases monotonically to\nthe unregularized counterpart as the regularization parameter is relaxed. We\nfind it useful to investigate this monotonicity more abstractly for linear\nprograms over polytopes, regularized with the squared norm. Here, monotonicity\ncan be stated as an invariance property of the curve mapping the regularization\nparameter to the solution: once the curve enters a face of the polytope, does\nit remain in that face forever? We show that this invariance is equivalent to a\ngeometric property of the polytope, namely that each face contains the minimum\nnorm point of its affine hull. Returning to the optimal transport problem and\nits associated Birkhoff polytope, we verify this property for low dimensions,\nbut show that it fails for marginals with five or more point masses. As a\nconsequence, the conjectured monotonicity of the support fails in general, even\nif experiments suggest that monotonicity holds for many cost matrices.\nSeparately, we apply our geometric point of view to a problem of Erd\\H{o}s,\nnamely to characterize the doubly stochastic matrices whose maximal trace\nequals their squared norm.\n","authors":["Alberto González-Sanz","Marcel Nutz","Andrés Riveros Valdevenito"],"pdf_url":"https://arxiv.org/pdf/2408.07871v2.pdf","comment":"To appear in 'SIAM Journal on Optimization'"},{"id":"http://arxiv.org/abs/2504.10667v1","updated":"2025-04-14T19:40:39Z","published":"2025-04-14T19:40:39Z","title":"Beyond Coordinates: Meta-Equivariance in Statistical Inference","summary":"  Optimal statistical decisions should transcend the language used to describe\nthem. Yet, how do we guarantee that the choice of coordinates - the\nparameterisation of an optimisation problem - does not subtly dictate the\nsolution? This paper reveals a fundamental geometric invariance principle. We\nfirst analyse the optimal combination of two asymptotically normal estimators\nunder a strictly convex trace-AMSE risk. While methods for finding optimal\nweights are known, we prove that the resulting optimal estimator is invariant\nunder direct affine reparameterisations of the weighting scheme. This\nexemplifies a broader principle we term meta-equivariance: the unique minimiser\nof any strictly convex, differentiable scalar objective over a matrix space\ntransforms covariantly under any invertible affine reparameterisation of that\nspace. Distinct from classical statistical equivariance tied to data\nsymmetries, meta-equivariance arises from the immutable geometry of convex\noptimisation itself. It guarantees that optimality, in these settings, is not\nan artefact of representation but an intrinsic, coordinate-free truth.\n","authors":["William Cook"],"pdf_url":"https://arxiv.org/pdf/2504.10667v1.pdf","comment":"21 pages. Includes numerical simulations and visualisations.\n  Developed independently using a live co-discovery process combining human\n  intuition and computational validation. The principle of meta-equivariance\n  generalises convex invariance under affine reparameterisation, with\n  implications for statistical decision theory and information geometry"},{"id":"http://arxiv.org/abs/2504.10648v1","updated":"2025-04-14T19:01:12Z","published":"2025-04-14T19:01:12Z","title":"Modeling and solving an integrated periodic vehicle routing and\n  capacitated facility location problem in the context of solid waste\n  collection","summary":"  Few activities are as crucial in urban environments as waste management.\nMismanagement of waste can cause significant economic, social, and\nenvironmental damage. However, waste management is often a complex system to\nmanage and therefore where computational decision-support tools can play a\npivotal role in assisting managers to make faster and better decisions. In this\nsense, this article proposes, on the one hand, a unified optimization model to\naddress two common waste management system optimization problem: the\ndetermination of the capacity of waste bins in the collection network and the\ndesign and scheduling of collection routes. The integration of these two\nproblems is not usual in the literature since each of them separately is\nalready a major computational challenge. On the other hand, two improved exact\nformulations based on mathematical programming and a genetic algorithm (GA) are\nprovided to solve this proposed unified optimization model. It should be noted\nthat the GA considers a mixed chromosome representation of the solutions\ncombining binary and integer alleles, in order to solve realistic instances of\nthis complex problem. Also, different genetic operators have been tested to\nstudy which combination of them obtained better results in execution times on\nthe order of that of the exact solvers. The obtained results show that the\nproposed GA is able to match the results of exact solvers on small instances\nand, in addition, can obtain feasible solutions on large instances, where exact\nformulations are not applicable, in reasonable computation times.\n","authors":["Begoña González","Diego Rossit","Mariano Frutos","Máximo Méndez"],"pdf_url":"https://arxiv.org/pdf/2504.10648v1.pdf","comment":"Submitted to Annals of Operations Research"},{"id":"http://arxiv.org/abs/2504.11488v1","updated":"2025-04-14T17:38:04Z","published":"2025-04-14T17:38:04Z","title":"Technological foundations of management decision-making in the\n  reconstruction of complex gas pipeline system","summary":"  This monograph presents a comprehensive analysis of the technological\nfoundations of management decision-making in the reconstruction of complex gas\npipeline systems. The study addresses the challenges posed by the aging\ninfrastructure of gas supply networks and explores advanced strategies to\nimprove their reliability, efficiency, and automation. Particular attention is\ngiven to the reconstruction of pipelines with various configurations linear,\nlooped, and parallel systems under non-stationary gas flow conditions. The\nproposed models and methodologies offer solutions for optimizing operational\nparameters, improving emergency valve response, and ensuring uninterrupted gas\nsupply through advanced management systems and data-driven decision support\ntools. Emphasis is placed on the integration of modern technologies, system\ntheory, and feedback mechanisms in the design and operation of reconstructed\npipeline systems. This work is intended for engineers, system designers, and\nresearchers in the fields of gas supply, systems engineering, and energy\ninfrastructure.\n","authors":["Ilgar Giyas oglu Aliyev"],"pdf_url":"https://arxiv.org/pdf/2504.11488v1.pdf","comment":"209 pages, includes illustrations and references, Monograph"},{"id":"http://arxiv.org/abs/2101.04986v3","updated":"2025-04-14T14:26:35Z","published":"2021-01-13T10:33:17Z","title":"Weak Optimal Entropy Transport Problems","summary":"  In this paper, we introduce weak optimal entropy transport problems that\ncover both optimal entropy transport problems and weak optimal transport\nproblems introduced by Liero, Mielke, and Savar\\'{e} [27]; and Gozlan, Roberto,\nSamson and Tetali [20], respectively. Under some mild assumptions of entropy\nfunctionals, we establish a Kantorovich type duality for our weak optimal\nentropy transport problem. We also introduce martingale optimal entropy\ntransport problems, and express them in terms of duality, homogeneous marginal\nperspective functionals and homogeneous constraints.\n","authors":["Nhan-Phu Chung","Thanh-Son Trinh"],"pdf_url":"https://arxiv.org/pdf/2101.04986v3.pdf","comment":"36 pages. Minor changes, and one reference is added. We also added\n  Lemma 9 which was suggested by one of the two anonymous referees"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2504.10487v1","updated":"2025-04-14T17:59:59Z","published":"2025-04-14T17:59:59Z","title":"FLOSS: Free Lunch in Open-vocabulary Semantic Segmentation","summary":"  Recent Open-Vocabulary Semantic Segmentation (OVSS) models extend the CLIP\nmodel to segmentation while maintaining the use of multiple templates (e.g., a\nphoto of <class>, a sketch of a <class>, etc.) for constructing class-wise\naveraged text embeddings, acting as a classifier. In this paper, we challenge\nthis status quo and investigate the impact of templates for OVSS. Empirically,\nwe observe that for each class, there exist single-template classifiers\nsignificantly outperforming the conventional averaged classifier. We refer to\nthem as class-experts. Given access to unlabeled images and without any\ntraining involved, we estimate these experts by leveraging the class-wise\nprediction entropy of single-template classifiers, selecting as class-wise\nexperts those which yield the lowest entropy. All experts, each specializing in\na specific class, collaborate in a newly proposed fusion method to generate\nmore accurate OVSS predictions. Our plug-and-play method, coined FLOSS, is\northogonal and complementary to existing OVSS methods, offering a ''free\nlunch'' to systematically improve OVSS without labels and additional training.\nExtensive experiments demonstrate that FLOSS consistently boosts\nstate-of-the-art methods on various OVSS benchmarks. Moreover, the selected\nexpert templates can generalize well from one dataset to others sharing the\nsame semantic categories, yet exhibiting distribution shifts. Additionally, we\nobtain satisfactory improvements under a low-data regime, where only a few\nunlabeled images are available. Our code is available at\nhttps://github.com/yasserben/FLOSS .\n","authors":["Yasser Benigmim","Mohammad Fahes","Tuan-Hung Vu","Andrei Bursuc","Raoul de Charette"],"pdf_url":"https://arxiv.org/pdf/2504.10487v1.pdf","comment":"Project Page: https://yasserben.github.io/FLOSS/"},{"id":"http://arxiv.org/abs/2504.10483v1","updated":"2025-04-14T17:59:53Z","published":"2025-04-14T17:59:53Z","title":"REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion\n  Transformers","summary":"  In this paper we tackle a fundamental question: \"Can we train latent\ndiffusion models together with the variational auto-encoder (VAE) tokenizer in\nan end-to-end manner?\" Traditional deep-learning wisdom dictates that\nend-to-end training is often preferable when possible. However, for latent\ndiffusion transformers, it is observed that end-to-end training both VAE and\ndiffusion-model using standard diffusion-loss is ineffective, even causing a\ndegradation in final performance. We show that while diffusion loss is\nineffective, end-to-end training can be unlocked through the\nrepresentation-alignment (REPA) loss -- allowing both VAE and diffusion model\nto be jointly tuned during the training process. Despite its simplicity, the\nproposed training recipe (REPA-E) shows remarkable performance; speeding up\ndiffusion model training by over 17x and 45x over REPA and vanilla training\nrecipes, respectively. Interestingly, we observe that end-to-end tuning with\nREPA-E also improves the VAE itself; leading to improved latent space structure\nand downstream generation performance. In terms of final performance, our\napproach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and\nwithout classifier-free guidance on ImageNet 256 x 256. Code is available at\nhttps://end2end-diffusion.github.io.\n","authors":["Xingjian Leng","Jaskirat Singh","Yunzhong Hou","Zhenchang Xing","Saining Xie","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2504.10483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13423v2","updated":"2025-04-14T17:59:35Z","published":"2025-03-17T17:53:23Z","title":"SuperBPE: Space Travel for Language Models","summary":"  The assumption across nearly all language model (LM) tokenization schemes is\nthat tokens should be subwords, i.e., contained within word boundaries. While\nproviding a seemingly reasonable inductive bias, is this common practice\nlimiting the potential of modern LMs? Whitespace is not a reliable delimiter of\nmeaning, as evidenced by multi-word expressions (e.g., \"by the way\"),\ncrosslingual variation in the number of words needed to express a concept\n(e.g., \"spacesuit helmet\" in German is \"raumanzughelm\"), and languages that do\nnot use whitespace at all (e.g., Chinese). To explore the potential of\ntokenization beyond subwords, we introduce a \"superword\" tokenizer, SuperBPE,\nwhich incorporates a simple pretokenization curriculum into the byte-pair\nencoding (BPE) algorithm to first learn subwords, then superwords that bridge\nwhitespace. This brings dramatic improvements in encoding efficiency: when\nfixing the vocabulary size to 200k, SuperBPE encodes a fixed piece of text with\nup to 33% fewer tokens than BPE on average. In experiments, we pretrain 8B\ntransformer LMs from scratch while fixing the model size, vocabulary size, and\ntrain compute, varying *only* the algorithm for learning the vocabulary. Our\nmodel trained with SuperBPE achieves an average +4.0% absolute improvement over\nthe BPE baseline across 30 downstream tasks (including +8.2% on MMLU), while\nsimultaneously requiring 27% less compute at inference time. In analysis, we\nfind that SuperBPE results in segmentations of text that are more uniform in\nper-token difficulty. Qualitatively, this may be because SuperBPE tokens often\ncapture common multi-word expressions that function semantically as a single\nunit. SuperBPE is a straightforward, local modification to tokenization that\nimproves both encoding efficiency and downstream performance, yielding better\nlanguage models overall.\n","authors":["Alisa Liu","Jonathan Hayase","Valentin Hofmann","Sewoong Oh","Noah A. Smith","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2503.13423v2.pdf","comment":"updated related work"},{"id":"http://arxiv.org/abs/2504.10478v1","updated":"2025-04-14T17:59:07Z","published":"2025-04-14T17:59:07Z","title":"Weight Ensembling Improves Reasoning in Language Models","summary":"  We investigate a failure mode that arises during the training of reasoning\nmodels, where the diversity of generations begins to collapse, leading to\nsuboptimal test-time scaling. Notably, the Pass@1 rate reliably improves during\nsupervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a\nsimple intervention of interpolating the weights of the latest SFT checkpoint\nwith an early checkpoint, otherwise known as WiSE-FT, almost completely\nrecovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves\nbetter test-time scaling (Best@k, majority vote) and achieves superior results\nwith less data when tuned further by reinforcement learning. Finally, we find\nthat WiSE-FT provides complementary performance gains that cannot be achieved\nonly through diversity-inducing decoding strategies, like temperature scaling.\nWe formalize a bias-variance tradeoff of Pass@k with respect to the expectation\nand variance of Pass@1 over the test distribution. We find that WiSE-FT can\nreduce bias and variance simultaneously, while temperature scaling inherently\ntrades-off between bias and variance.\n","authors":["Xingyu Dang","Christina Baek","Kaiyue Wen","Zico Kolter","Aditi Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2504.10478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10453v1","updated":"2025-04-14T17:40:18Z","published":"2025-04-14T17:40:18Z","title":"Anchors no more: Using peculiar velocities to constrain $H_0$ and the\n  primordial Universe without calibrators","summary":"  We develop a novel approach to constrain the Hubble parameter $H_0$ and the\nprimordial power spectrum amplitude $A_\\mathrm{s}$ using supernovae type Ia\n(SNIa) data. By considering SNIa as tracers of the peculiar velocity field, we\ncan model their distance and their covariance as a function of cosmological\nparameters without the need of calibrators like Cepheids; this yields a new\nindependent probe of the large-scale structure based on SNIa data without\ndistance anchors. Crucially, we implement a differentiable pipeline in JAX,\nincluding efficient emulators and affine sampling, reducing inference time from\nyears to hours on a single GPU. We first validate our method on mock datasets,\ndemonstrating that we can constrain $H_0$ and $\\log 10^{10}A_\\mathrm{s}$ within\n$\\sim10\\%$ using $\\sim10^3$ SNIa. We then test our pipeline with SNIa from an\n$N$-body simulation, obtaining $7\\%$-level unbiased constraints on $H_0$ with a\nmoderate noise level. We finally apply our method to Pantheon+ data,\nconstraining $H_0$ at the $10\\%$ level without Cepheids when fixing\n$A_\\mathrm{s}$ to its $\\it{Planck}$ value. On the other hand, we obtain\n$15\\%$-level constraints on $\\log 10^{10}A_\\mathrm{s}$ in agreement with\n$\\it{Planck}$ when including Cepheids in the analysis. In light of upcoming\nobservations of low redshift SNIa from the Zwicky Transient Facility and the\nVera Rubin Legacy Survey of Space and Time, surveys for which our method will\ndevelop its full potential, we make our code publicly available.\n","authors":["Davide Piras","Francesco Sorrenti","Ruth Durrer","Martin Kunz"],"pdf_url":"https://arxiv.org/pdf/2504.10453v1.pdf","comment":"22 pages, 5 figures, comments welcome. Code available at\n  https://github.com/dpiras/veloce"},{"id":"http://arxiv.org/abs/2504.10449v1","updated":"2025-04-14T17:38:25Z","published":"2025-04-14T17:38:25Z","title":"M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models","summary":"  Effective reasoning is crucial to solving complex mathematical problems.\nRecent large language models (LLMs) have boosted performance by scaling\ntest-time computation through long chain-of-thought reasoning. However,\ntransformer-based models are inherently limited in extending context length due\nto their quadratic computational complexity and linear memory requirements. In\nthis paper, we introduce a novel hybrid linear RNN reasoning model, M1, built\non the Mamba architecture, which allows memory-efficient inference. Our\napproach leverages a distillation process from existing reasoning models and is\nfurther enhanced through RL training. Experimental results on the AIME and MATH\nbenchmarks show that M1 not only outperforms previous linear RNN models but\nalso matches the performance of state-of-the-art Deepseek R1 distilled\nreasoning models at a similar scale. We also compare our generation speed with\na highly performant general purpose inference engine, vLLM, and observe more\nthan a 3x speedup compared to a same size transformer. With throughput speedup,\nwe are able to achieve higher accuracy compared to DeepSeek R1 distilled\ntransformer reasoning models under a fixed generation time budget using\nself-consistency voting. Overall, we introduce a hybrid Mamba reasoning model\nand provide a more effective approach to scaling test-time generation using\nself-consistency or long chain of thought reasoning.\n","authors":["Junxiong Wang","Wen-Ding Li","Daniele Paliotta","Daniel Ritter","Alexander M. Rush","Tri Dao"],"pdf_url":"https://arxiv.org/pdf/2504.10449v1.pdf","comment":"Code is available https://github.com/jxiw/M1"},{"id":"http://arxiv.org/abs/2504.10445v1","updated":"2025-04-14T17:36:46Z","published":"2025-04-14T17:36:46Z","title":"RealWebAssist: A Benchmark for Long-Horizon Web Assistance with\n  Real-World Users","summary":"  To achieve successful assistance with long-horizon web-based tasks, AI agents\nmust be able to sequentially follow real-world user instructions over a long\nperiod. Unlike existing web-based agent benchmarks, sequential instruction\nfollowing in the real world poses significant challenges beyond performing a\nsingle, clearly defined task. For instance, real-world human instructions can\nbe ambiguous, require different levels of AI assistance, and may evolve over\ntime, reflecting changes in the user's mental state. To address this gap, we\nintroduce RealWebAssist, a novel benchmark designed to evaluate sequential\ninstruction-following in realistic scenarios involving long-horizon\ninteractions with the web, visual GUI grounding, and understanding ambiguous\nreal-world user instructions. RealWebAssist includes a dataset of sequential\ninstructions collected from real-world human users. Each user instructs a\nweb-based assistant to perform a series of tasks on multiple websites. A\nsuccessful agent must reason about the true intent behind each instruction,\nkeep track of the mental state of the user, understand user-specific routines,\nand ground the intended tasks to actions on the correct GUI elements. Our\nexperimental results show that state-of-the-art models struggle to understand\nand ground user instructions, posing critical challenges in following\nreal-world user instructions for long-horizon web assistance.\n","authors":["Suyu Ye","Haojun Shi","Darren Shih","Hyokun Yun","Tanya Roosta","Tianmin Shu"],"pdf_url":"https://arxiv.org/pdf/2504.10445v1.pdf","comment":"Project Website: https://scai.cs.jhu.edu/projects/RealWebAssist/\n  Code: https://github.com/SCAI-JHU/RealWebAssist"},{"id":"http://arxiv.org/abs/2302.05816v3","updated":"2025-04-14T17:34:46Z","published":"2023-02-11T23:30:50Z","title":"A Policy Gradient Framework for Stochastic Optimal Control Problems with\n  Global Convergence Guarantee","summary":"  We consider policy gradient methods for stochastic optimal control problem in\ncontinuous time. In particular, we analyze the gradient flow for the control,\nviewed as a continuous time limit of the policy gradient method. We prove the\nglobal convergence of the gradient flow and establish a convergence rate under\nsome regularity assumptions. The main novelty in the analysis is the notion of\nlocal optimal control function, which is introduced to characterize the local\noptimality of the iterate.\n","authors":["Mo Zhou","Jianfeng Lu"],"pdf_url":"https://arxiv.org/pdf/2302.05816v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10443v1","updated":"2025-04-14T17:34:06Z","published":"2025-04-14T17:34:06Z","title":"Multimodal Long Video Modeling Based on Temporal Dynamic Context","summary":"  Recent advances in Large Language Models (LLMs) have led to significant\nbreakthroughs in video understanding. However, existing models still struggle\nwith long video processing due to the context length constraint of LLMs and the\nvast amount of information within the video. Although some recent methods are\ndesigned for long video understanding, they often lose crucial information\nduring token compression and struggle with additional modality like audio. In\nthis work, we propose a dynamic long video encoding method utilizing the\ntemporal relationship between frames, named Temporal Dynamic Context (TDC).\nFirstly, we segment the video into semantically consistent scenes based on\ninter-frame similarities, then encode each frame into tokens using visual-audio\nencoders. Secondly, we propose a novel temporal context compressor to reduce\nthe number of tokens within each segment. Specifically, we employ a query-based\nTransformer to aggregate video, audio, and instruction text tokens into a\nlimited set of temporal context tokens. Finally, we feed the static frame\ntokens and the temporal context tokens into the LLM for video understanding.\nFurthermore, to handle extremely long videos, we propose a training-free\nchain-of-thought strategy that progressively extracts answers from multiple\nvideo segments. These intermediate answers serve as part of the reasoning\nprocess and contribute to the final answer. We conduct extensive experiments on\ngeneral video understanding and audio-video understanding benchmarks, where our\nmethod demonstrates strong performance. The code and models are available at\nhttps://github.com/Hoar012/TDC-Video.\n","authors":["Haoran Hao","Jiaming Han","Yiyuan Zhang","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2504.10443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07210v2","updated":"2025-04-14T17:25:41Z","published":"2025-04-09T18:37:24Z","title":"MESA: Text-Driven Terrain Generation Using Latent Diffusion and Global\n  Copernicus Data","summary":"  Terrain modeling has traditionally relied on procedural techniques, which\noften require extensive domain expertise and handcrafted rules. In this paper,\nwe present MESA - a novel data-centric alternative by training a diffusion\nmodel on global remote sensing data. This approach leverages large-scale\ngeospatial information to generate high-quality terrain samples from text\ndescriptions, showcasing a flexible and scalable solution for terrain\ngeneration. The model's capabilities are demonstrated through extensive\nexperiments, highlighting its ability to generate realistic and diverse terrain\nlandscapes. The dataset produced to support this work, the Major TOM Core-DEM\nextension dataset, is released openly as a comprehensive resource for global\nterrain data. The results suggest that data-driven models, trained on remote\nsensing data, can provide a powerful tool for realistic terrain modeling and\ngeneration.\n","authors":["Paul Borne--Pons","Mikolaj Czerkawski","Rosalie Martin","Romain Rouffet"],"pdf_url":"https://arxiv.org/pdf/2504.07210v2.pdf","comment":"Accepted at CVPR 2025 Workshop MORSE"},{"id":"http://arxiv.org/abs/2504.10428v1","updated":"2025-04-14T17:19:29Z","published":"2025-04-14T17:19:29Z","title":"Learning with Positive and Imperfect Unlabeled Data","summary":"  We study the problem of learning binary classifiers from positive and\nunlabeled data when the unlabeled data distribution is shifted, which we call\nPositive and Imperfect Unlabeled (PIU) Learning. In the absence of covariate\nshifts, i.e., with perfect unlabeled data, Denis (1998) reduced this problem to\nlearning under Massart noise; however, that reduction fails under even slight\nshifts.\n  Our main results on PIU learning are the characterizations of the sample\ncomplexity of PIU learning and a computationally and sample-efficient algorithm\nachieving a misclassification error $\\varepsilon$. We further show that our\nresults lead to new algorithms for several related problems.\n  1. Learning from smooth distributions: We give algorithms that learn\ninteresting concept classes from only positive samples under smooth feature\ndistributions, bypassing known existing impossibility results and contributing\nto recent advances in smoothened learning (Haghtalab et al, J.ACM'24)\n(Chandrasekaran et al., COLT'24).\n  2. Learning with a list of unlabeled distributions: We design new algorithms\nthat apply to a broad class of concept classes under the assumption that we are\ngiven a list of unlabeled distributions, one of which--unknown to the\nlearner--is $O(1)$-close to the true feature distribution.\n  3. Estimation in the presence of unknown truncation: We give the first\npolynomial sample and time algorithm for estimating the parameters of an\nexponential family distribution from samples truncated to an unknown set\napproximable by polynomials in $L_1$-norm. This improves the algorithm by Lee\net al. (FOCS'24) that requires approximation in $L_2$-norm.\n  4. Detecting truncation: We present new algorithms for detecting whether\ngiven samples have been truncated (or not) for a broad class of non-product\ndistributions, including non-product distributions, improving the algorithm by\nDe et al. (STOC'24).\n","authors":["Jane H. Lee","Anay Mehrotra","Manolis Zampetakis"],"pdf_url":"https://arxiv.org/pdf/2504.10428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10422v1","updated":"2025-04-14T17:09:05Z","published":"2025-04-14T17:09:05Z","title":"Foundation models for electronic health records: representation dynamics\n  and transferability","summary":"  Foundation models (FMs) trained on electronic health records (EHRs) have\nshown strong performance on a range of clinical prediction tasks. However,\nadapting these models to local health systems remains challenging due to\nlimited data availability and resource constraints. In this study, we\ninvestigated what these models learn and evaluated the transferability of an FM\ntrained on MIMIC-IV to an institutional EHR dataset at the University of\nChicago Medical Center. We assessed their ability to identify outlier patients\nand examined representation-space patient trajectories in relation to future\nclinical outcomes. We also evaluated the performance of supervised fine-tuned\nclassifiers on both source and target datasets. Our findings offer insights\ninto the adaptability of FMs across different healthcare systems, highlight\nconsiderations for their effective implementation, and provide an empirical\nanalysis of the underlying factors that contribute to their predictive\nperformance.\n","authors":["Michael C. Burkhart","Bashar Ramadan","Zewei Liao","Kaveri Chhikara","Juan C. Rojas","William F. Parker","Brett K. Beaulieu-Jones"],"pdf_url":"https://arxiv.org/pdf/2504.10422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07901v2","updated":"2025-04-14T17:02:26Z","published":"2025-03-10T22:43:07Z","title":"Intelligent Framework for Human-Robot Collaboration: Dynamic Ergonomics\n  and Adaptive Decision-Making","summary":"  The integration of collaborative robots into industrial environments has\nimproved productivity, but has also highlighted significant challenges related\nto operator safety and ergonomics. This paper proposes an innovative framework\nthat integrates advanced visual perception, continuous ergonomic monitoring,\nand adaptive Behaviour Tree decision-making to overcome the limitations of\ntraditional methods that typically operate as isolated components. Our approach\nsynthesizes deep learning models, advanced tracking algorithms, and dynamic\nergonomic assessments into a modular, scalable, and adaptive system.\nExperimental validation demonstrates the framework's superiority over existing\nsolutions across multiple dimensions: the visual perception module outperformed\nprevious detection models with 72.4% mAP@50:95; the system achieved high\naccuracy in recognizing operator intentions (92.5%); it promptly classified\nergonomic risks with minimal latency (0.57 seconds); and it dynamically managed\nrobotic interventions with exceptionally responsive decision-making\ncapabilities (0.07 seconds), representing a 56% improvement over benchmark\nsystems. This comprehensive solution provides a robust platform for enhancing\nhuman-robot collaboration in industrial environments by prioritizing ergonomic\nsafety, operational efficiency, and real-time adaptability.\n","authors":["Francesco Iodice","Elena De Momi","Arash Ajoudani"],"pdf_url":"https://arxiv.org/pdf/2503.07901v2.pdf","comment":"15 pagine, 8figure, 3 tabelle, formato conferenza IEEE"},{"id":"http://arxiv.org/abs/2504.10415v1","updated":"2025-04-14T17:00:13Z","published":"2025-04-14T17:00:13Z","title":"LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\n  Large Language Models","summary":"  Scientific equation discovery is a fundamental task in the history of\nscientific progress, enabling the derivation of laws governing natural\nphenomena. Recently, Large Language Models (LLMs) have gained interest for this\ntask due to their potential to leverage embedded scientific knowledge for\nhypothesis generation. However, evaluating the true discovery capabilities of\nthese methods remains challenging, as existing benchmarks often rely on common\nequations that are susceptible to memorization by LLMs, leading to inflated\nperformance metrics that do not reflect discovery. In this paper, we introduce\nLLM-SRBench, a comprehensive benchmark with 239 challenging problems across\nfour scientific domains specifically designed to evaluate LLM-based scientific\nequation discovery methods while preventing trivial memorization. Our benchmark\ncomprises two main categories: LSR-Transform, which transforms common physical\nmodels into less common mathematical representations to test reasoning beyond\nmemorized forms, and LSR-Synth, which introduces synthetic, discovery-driven\nproblems requiring data-driven reasoning. Through extensive evaluation of\nseveral state-of-the-art methods, using both open and closed LLMs, we find that\nthe best-performing system so far achieves only 31.5% symbolic accuracy. These\nfindings highlight the challenges of scientific equation discovery, positioning\nLLM-SRBench as a valuable resource for future research.\n","authors":["Parshin Shojaee","Ngoc-Hieu Nguyen","Kazem Meidani","Amir Barati Farimani","Khoa D Doan","Chandan K Reddy"],"pdf_url":"https://arxiv.org/pdf/2504.10415v1.pdf","comment":"Project page:\n  https://github.com/deep-symbolic-mathematics/llm-srbench , Benchmark page:\n  https://huggingface.co/datasets/nnheui/llm-srbench"},{"id":"http://arxiv.org/abs/2504.10412v1","updated":"2025-04-14T16:58:54Z","published":"2025-04-14T16:58:54Z","title":"AI-Driven Code Refactoring: Using Graph Neural Networks to Enhance\n  Software Maintainability","summary":"  This study explores Graph Neural Networks (GNNs) as a transformative tool for\ncode refactoring, using abstract syntax trees (ASTs) to boost software\nmaintainability. It analyzes a dataset of 2 million snippets from CodeSearchNet\nand a custom 75000-file GitHub Python corpus, comparing GNNs against rule-based\nSonarQube and decision trees. Metrics include cyclomatic complexity (target\nbelow 10), coupling (target below 5), and refactoring precision. GNNs achieve\n92% accuracy, reducing complexity by 35% and coupling by 33%, outperforming\nSonarQube (78%, 16%) and decision trees (85%, 25%). Preprocessing fixed 60% of\nsyntax errors. Bar graphs, tables, and AST visuals clarify results. This offers\na scalable AI-driven path to cleaner codebases, which is crucial for software\nengineering.\n","authors":["Gopichand Bandarupalli"],"pdf_url":"https://arxiv.org/pdf/2504.10412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14701v3","updated":"2025-04-14T16:58:34Z","published":"2024-02-22T16:56:44Z","title":"COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies\n  with Language Modeling","summary":"  The therapeutic working alliance is a critical predictor of psychotherapy\nsuccess. Traditionally, working alliance assessment relies on questionnaires\ncompleted by both therapists and patients. In this paper, we present COMPASS, a\nnovel framework to directly infer the therapeutic working alliance from the\nnatural language used in psychotherapy sessions. Our approach leverages\nadvanced large language models (LLMs) to analyze session transcripts and map\nthem to distributed representations. These representations capture the semantic\nsimilarities between the dialogues and psychometric instruments, such as the\nWorking Alliance Inventory. Analyzing a dataset of over 950 sessions spanning\ndiverse psychiatric conditions -- including anxiety (N=498), depression\n(N=377), schizophrenia (N=71), and suicidal tendencies (N=12) -- collected\nbetween 1970 and 2012, we demonstrate the effectiveness of our method in\nproviding fine-grained mapping of patient-therapist alignment trajectories,\noffering interpretable insights for clinical practice, and identifying emerging\npatterns related to the condition being treated. By employing various deep\nlearning-based topic modeling techniques in combination with prompting\ngenerative language models, we analyze the topical characteristics of different\npsychiatric conditions and how these topics evolve during each turn of the\nconversation. This integrated framework enhances the understanding of\ntherapeutic interactions, enables timely feedback for therapists on the quality\nof therapeutic relationships, and provides clear, actionable insights to\nimprove the effectiveness of psychotherapy.\n","authors":["Baihan Lin","Djallel Bouneffouf","Yulia Landa","Rachel Jespersen","Cheryl Corcoran","Guillermo Cecchi"],"pdf_url":"https://arxiv.org/pdf/2402.14701v3.pdf","comment":"Translational Psychiatry, in press. This work extends our research\n  series in computational psychiatry (e.g auto annotation in arXiv:2204.05522,\n  topic extraction in arXiv:2204.10189, and diagnosis in arXiv:2210.15603) with\n  the introduction of LLMs to complete the full cycle of interpreting and\n  understanding psychotherapy strategies as a comprehensive analytical\n  framework"},{"id":"http://arxiv.org/abs/2503.04350v2","updated":"2025-04-14T16:57:26Z","published":"2025-03-06T11:46:07Z","title":"EDCA - An Evolutionary Data-Centric AutoML Framework for Efficient\n  Pipelines","summary":"  Automated Machine Learning (AutoML) gained popularity due to the increased\ndemand for Machine Learning (ML) specialists, allowing them to apply ML\ntechniques effortlessly and quickly. AutoML implementations use optimisation\nmethods to identify the most effective ML solution for a given dataset, aiming\nto improve one or more predefined metrics. However, most implementations focus\non model selection and hyperparameter tuning. Despite being an important factor\nin obtaining high-performance ML systems, data quality is usually an overlooked\npart of AutoML and continues to be a manual and time-consuming task. This work\npresents EDCA, an Evolutionary Data Centric AutoML framework. In addition to\nthe traditional tasks such as selecting the best models and hyperparameters,\nEDCA enhances the given data by optimising data processing tasks such as data\nreduction and cleaning according to the problems' needs. All these steps create\nan ML pipeline that is optimised by an evolutionary algorithm. To assess its\neffectiveness, EDCA was compared to FLAML and TPOT, two frameworks at the top\nof the AutoML benchmarks. The frameworks were evaluated in the same conditions\nusing datasets from AMLB classification benchmarks. EDCA achieved statistically\nsimilar results in performance to FLAML and TPOT but used significantly less\ndata to train the final solutions. Moreover, EDCA experimental results reveal\nthat a good performance can be achieved using less data and efficient ML\nalgorithm aspects that align with Green AutoML guidelines\n","authors":["Joana Simões","João Correia"],"pdf_url":"https://arxiv.org/pdf/2503.04350v2.pdf","comment":"preprint for EvoApplications 2025"},{"id":"http://arxiv.org/abs/2504.10403v1","updated":"2025-04-14T16:52:34Z","published":"2025-04-14T16:52:34Z","title":"Satellite Federated Fine-Tuning for Foundation Models in Space Computing\n  Power Networks","summary":"  Advancements in artificial intelligence (AI) and low-earth orbit (LEO)\nsatellites have promoted the application of large remote sensing foundation\nmodels for various downstream tasks. However, direct downloading of these\nmodels for fine-tuning on the ground is impeded by privacy concerns and limited\nbandwidth. Satellite federated learning (FL) offers a solution by enabling\nmodel fine-tuning directly on-board satellites and aggregating model updates\nwithout data downloading. Nevertheless, for large foundation models, the\ncomputational capacity of satellites is insufficient to support effective\non-board fine-tuning in traditional satellite FL frameworks. To address these\nchallenges, we propose a satellite-ground collaborative federated fine-tuning\nframework. The key of the framework lies in how to reasonably decompose and\nallocate model components to alleviate insufficient on-board computation\ncapabilities. During fine-tuning, satellites exchange intermediate results with\nground stations or other satellites for forward propagation and back\npropagation, which brings communication challenges due to the special\ncommunication topology of space transmission networks, such as intermittent\nsatellite-ground communication, short duration of satellite-ground\ncommunication windows, and unstable inter-orbit inter-satellite links (ISLs).\nTo reduce transmission delays, we further introduce tailored communication\nstrategies that integrate both communication and computing resources.\nSpecifically, we propose a parallel intra-orbit communication strategy, a\ntopology-aware satellite-ground communication strategy, and a\nlatency-minimalization inter-orbit communication strategy to reduce space\ncommunication costs. Simulation results demonstrate significant reductions in\ntraining time with improvements of approximately 33%.\n","authors":["Yan zhu","Jingyang zhu","Ting Wang","Yuanming Shi","Chunxiao Jiang","Khaled Ben Letaief"],"pdf_url":"https://arxiv.org/pdf/2504.10403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10397v1","updated":"2025-04-14T16:45:52Z","published":"2025-04-14T16:45:52Z","title":"Can LLMs Assist Expert Elicitation for Probabilistic Causal Modeling?","summary":"  Objective: This study investigates the potential of Large Language Models\n(LLMs) as an alternative to human expert elicitation for extracting structured\ncausal knowledge and facilitating causal modeling in biometric and healthcare\napplications.\n  Material and Methods: LLM-generated causal structures, specifically Bayesian\nnetworks (BNs), were benchmarked against traditional statistical methods (e.g.,\nBayesian Information Criterion) using healthcare datasets. Validation\ntechniques included structural equation modeling (SEM) to verifying\nrelationships, and measures such as entropy, predictive accuracy, and\nrobustness to compare network structures.\n  Results and Discussion: LLM-generated BNs demonstrated lower entropy than\nexpert-elicited and statistically generated BNs, suggesting higher confidence\nand precision in predictions. However, limitations such as contextual\nconstraints, hallucinated dependencies, and potential biases inherited from\ntraining data require further investigation.\n  Conclusion: LLMs represent a novel frontier in expert elicitation for\nprobabilistic causal modeling, promising to improve transparency and reduce\nuncertainty in the decision-making using such models.\n","authors":["Olha Shaposhnyk","Daria Zahorska","Svetlana Yanushkevich"],"pdf_url":"https://arxiv.org/pdf/2504.10397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09525v2","updated":"2025-04-14T16:41:56Z","published":"2025-02-13T17:37:42Z","title":"Robust Learning of Multi-index Models via Iterative Subspace\n  Approximation","summary":"  We study the task of learning Multi-Index Models (MIMs) with label noise\nunder the Gaussian distribution. A $K$-MIM is any function $f$ that only\ndepends on a $K$-dimensional subspace. We focus on well-behaved MIMs with\nfinite ranges that satisfy certain regularity properties. Our main contribution\nis a general robust learner that is qualitatively optimal in the Statistical\nQuery (SQ) model. Our algorithm iteratively constructs better approximations to\nthe defining subspace by computing low-degree moments conditional on the\nprojection to the subspace computed thus far, and adding directions with\nrelatively large empirical moments. This procedure efficiently finds a subspace\n$V$ so that $f(\\mathbf{x})$ is close to a function of the projection of\n$\\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional\nmoments do not help, we prove an SQ lower bound suggesting that no efficient\nlearner exists. As applications, we provide faster robust learners for the\nfollowing concept classes:\n  * {\\bf Multiclass Linear Classifiers} We give a constant-factor approximate\nagnostic learner with sample complexity $N = O(d)\n2^{\\mathrm{poly}(K/\\epsilon)}$ and computational complexity $\\mathrm{poly}(N\n,d)$. This is the first constant-factor agnostic learner for this class whose\ncomplexity is a fixed-degree polynomial in $d$.\n  * {\\bf Intersections of Halfspaces} We give an approximate agnostic learner\nfor this class achieving 0-1 error $K \\tilde{O}(\\mathrm{OPT}) + \\epsilon$ with\nsample complexity $N=O(d^2) 2^{\\mathrm{poly}(K/\\epsilon)}$ and computational\ncomplexity $\\mathrm{poly}(N ,d)$. This is the first agnostic learner for this\nclass with near-linear error dependence and complexity a fixed-degree\npolynomial in $d$.\n  Furthermore, we show that in the presence of random classification noise, the\ncomplexity of our algorithm scales polynomially with $1/\\epsilon$.\n","authors":["Ilias Diakonikolas","Giannis Iakovidis","Daniel M. Kane","Nikos Zarifis"],"pdf_url":"https://arxiv.org/pdf/2502.09525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06730v2","updated":"2025-04-14T16:39:33Z","published":"2025-03-09T19:03:48Z","title":"Adaptive Test-Time Intervention for Concept Bottleneck Models","summary":"  Concept bottleneck models (CBM) aim to improve model interpretability by\npredicting human level \"concepts\" in a bottleneck within a deep learning model\narchitecture. However, how the predicted concepts are used in predicting the\ntarget still either remains black-box or is simplified to maintain\ninterpretability at the cost of prediction performance. We propose to use Fast\nInterpretable Greedy Sum-Trees (FIGS) to obtain Binary Distillation (BD). This\nnew method, called FIGS-BD, distills a binary-augmented concept-to-target\nportion of the CBM into an interpretable tree-based model, while maintaining\nthe competitive prediction performance of the CBM teacher. FIGS-BD can be used\nin downstream tasks to explain and decompose CBM predictions into interpretable\nbinary-concept-interaction attributions and guide adaptive test-time\nintervention. Across 4 datasets, we demonstrate that our adaptive test-time\nintervention identifies key concepts that significantly improve performance for\nrealistic human-in-the-loop settings that only allow for limited concept\ninterventions.\n","authors":["Matthew Shen","Aliyah Hsu","Abhineet Agarwal","Bin Yu"],"pdf_url":"https://arxiv.org/pdf/2503.06730v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00104v2","updated":"2025-04-14T16:36:07Z","published":"2024-05-31T18:00:12Z","title":"Scalable Bayesian Learning with posteriors","summary":"  Although theoretically compelling, Bayesian learning with modern machine\nlearning models is computationally challenging since it requires approximating\na high dimensional posterior distribution. In this work, we (i) introduce\nposteriors, an easily extensible PyTorch library hosting general-purpose\nimplementations making Bayesian learning accessible and scalable to large data\nand parameter regimes; (ii) present a tempered framing of stochastic gradient\nMarkov chain Monte Carlo, as implemented in posteriors, that transitions\nseamlessly into optimization and unveils a minor modification to deep ensembles\nto ensure they are asymptotically unbiased for the Bayesian posterior, and\n(iii) demonstrate and compare the utility of Bayesian approximations through\nexperiments including an investigation into the cold posterior effect and\napplications with large language models.\n","authors":["Samuel Duffield","Kaelan Donatella","Johnathan Chiu","Phoebe Klett","Daniel Simpson"],"pdf_url":"https://arxiv.org/pdf/2406.00104v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00234v2","updated":"2025-04-14T16:34:51Z","published":"2025-02-28T22:42:21Z","title":"Towards Fairness for the Right Reasons: Using Saliency Maps to Evaluate\n  Bias Removal in Neural Networks","summary":"  The widespread adoption of machine learning systems has raised critical\nconcerns about fairness and bias, making mitigating harmful biases essential\nfor AI development. In this paper, we investigate the relationship between\nfairness improvement and the removal of harmful biases in neural networks\napplied to computer vision tasks. First, we introduce a set of novel XAI-based\nmetrics that analyze saliency maps to assess shifts in a model's\ndecision-making process. Then, we demonstrate that successful debiasing methods\nsystematically redirect model focus away from protected attributes.\nAdditionally, we show that techniques originally developed for artifact removal\ncan be effectively repurposed for fairness. These findings underscore the\nimportance of ensuring that models are fair for the right reasons, contributing\nto the development of more ethical and trustworthy AI systems.\n","authors":["Lukasz Sztukiewicz","Ignacy Stępka","Michał Wiliński","Jerzy Stefanowski"],"pdf_url":"https://arxiv.org/pdf/2503.00234v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10388v1","updated":"2025-04-14T16:32:17Z","published":"2025-04-14T16:32:17Z","title":"Inferring genotype-phenotype maps using attention models","summary":"  Predicting phenotype from genotype is a central challenge in genetics.\nTraditional approaches in quantitative genetics typically analyze this problem\nusing methods based on linear regression. These methods generally assume that\nthe genetic architecture of complex traits can be parameterized in terms of an\nadditive model, where the effects of loci are independent, plus (in some cases)\npairwise epistatic interactions between loci. However, these models struggle to\nanalyze more complex patterns of epistasis or subtle gene-environment\ninteractions. Recent advances in machine learning, particularly attention-based\nmodels, offer a promising alternative. Initially developed for natural language\nprocessing, attention-based models excel at capturing context-dependent\ninteractions and have shown exceptional performance in predicting protein\nstructure and function. Here, we apply attention-based models to quantitative\ngenetics. We analyze the performance of this attention-based approach in\npredicting phenotype from genotype using simulated data across a range of\nmodels with increasing epistatic complexity, and using experimental data from a\nrecent quantitative trait locus mapping study in budding yeast. We find that\nour model demonstrates superior out-of-sample predictions in epistatic regimes\ncompared to standard methods. We also explore a more general multi-environment\nattention-based model to jointly analyze genotype-phenotype maps across\nmultiple environments and show that such architectures can be used for\n\"transfer learning\" - predicting phenotypes in novel environments with limited\ntraining data.\n","authors":["Krishna Rijal","Caroline M. Holmes","Samantha Petti","Gautam Reddy","Michael M. Desai","Pankaj Mehta"],"pdf_url":"https://arxiv.org/pdf/2504.10388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.01007v2","updated":"2025-04-14T16:30:27Z","published":"2025-04-01T17:46:42Z","title":"Data-Driven Safety Verification using Barrier Certificates and Matrix\n  Zonotopes","summary":"  Ensuring safety in cyber-physical systems (CPSs) is a critical challenge,\nespecially when system models are difficult to obtain or cannot be fully\ntrusted due to uncertainty, modeling errors, or environmental disturbances.\nTraditional model-based approaches rely on precise system dynamics, which may\nnot be available in real-world scenarios. To address this, we propose a\ndata-driven safety verification framework that leverages matrix zonotopes and\nbarrier certificates to verify system safety directly from noisy data. Instead\nof trusting a single unreliable model, we construct a set of models that\ncapture all possible system dynamics that align with the observed data,\nensuring that the true system model is always contained within this set. This\nmodel set is compactly represented using matrix zonotopes, enabling efficient\ncomputation and propagation of uncertainty. By integrating this representation\ninto a barrier certificate framework, we establish rigorous safety guarantees\nwithout requiring an explicit system model. Numerical experiments demonstrate\nthe effectiveness of our approach in verifying safety for dynamical systems\nwith unknown models, showcasing its potential for real-world CPS applications.\n","authors":["Mohammed Adib Oumer","Amr Alanwar","Majid Zamani"],"pdf_url":"https://arxiv.org/pdf/2504.01007v2.pdf","comment":"This manuscript of 11 pages, 2 tables and 3 figures is a preprint\n  under review with a conference"},{"id":"http://arxiv.org/abs/2504.10375v1","updated":"2025-04-14T16:23:15Z","published":"2025-04-14T16:23:15Z","title":"PG-DPIR: An efficient plug-and-play method for high-count\n  Poisson-Gaussian inverse problems","summary":"  Poisson-Gaussian noise describes the noise of various imaging systems thus\nthe need of efficient algorithms for Poisson-Gaussian image restoration. Deep\nlearning methods offer state-of-the-art performance but often require\nsensor-specific training when used in a supervised setting. A promising\nalternative is given by plug-and-play (PnP) methods, which consist in learning\nonly a regularization through a denoiser, allowing to restore images from\nseveral sources with the same network. This paper introduces PG-DPIR, an\nefficient PnP method for high-count Poisson-Gaussian inverse problems, adapted\nfrom DPIR. While DPIR is designed for white Gaussian noise, a naive adaptation\nto Poisson-Gaussian noise leads to prohibitively slow algorithms due to the\nabsence of a closed-form proximal operator. To address this, we adapt DPIR for\nthe specificities of Poisson-Gaussian noise and propose in particular an\nefficient initialization of the gradient descent required for the proximal step\nthat accelerates convergence by several orders of magnitude. Experiments are\nconducted on satellite image restoration and super-resolution problems.\nHigh-resolution realistic Pleiades images are simulated for the experiments,\nwhich demonstrate that PG-DPIR achieves state-of-the-art performance with\nimproved efficiency, which seems promising for on-ground satellite processing\nchains.\n","authors":["Maud Biquard","Marie Chabert","Florence Genin","Christophe Latry","Thomas Oberlin"],"pdf_url":"https://arxiv.org/pdf/2504.10375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10374v1","updated":"2025-04-14T16:22:11Z","published":"2025-04-14T16:22:11Z","title":"Ctrl-Z: Controlling AI Agents via Resampling","summary":"  Control evaluations measure whether monitoring and security protocols for AI\nsystems prevent intentionally subversive AI models from causing harm. Our work\npresents the first control evaluation performed in an agent environment. We\nconstruct BashBench, a dataset of 257 challenging multi-step system\nadministration tasks, and evaluate whether various safety measures can prevent\nan adversarially constructed AI agent from covertly downloading and executing\nmalicious code in this environment. This multi-step setting introduces new\nattack and defense dynamics, which we investigate in order to design novel\ncontrol protocols that prevent safety failures without hindering the ability of\nnon-malicious agents to perform useful work. We introduce a class of control\nprotocols called resample protocols that dynamically take additional samples of\ncertain actions. We find these protocols significantly improve on existing\ntechniques by selectively blocking the AI agent from executing suspicious code\nand incriminating the agent by generating additional examples of dangerous\nbehavior. We measure the tradeoff between attack prevention and usefulness; our\nbest protocol combines resampling with analysis of previous steps, reducing the\nsuccess rate of attacks from 58% to 7% at a 5% cost to the performance of a\nnon-malicious agent.\n","authors":["Aryan Bhatt","Cody Rushing","Adam Kaufman","Tyler Tracy","Vasil Georgiev","David Matolcsi","Akbir Khan","Buck Shlegeris"],"pdf_url":"https://arxiv.org/pdf/2504.10374v1.pdf","comment":"bashcontrol.com"},{"id":"http://arxiv.org/abs/2504.10373v1","updated":"2025-04-14T16:20:55Z","published":"2025-04-14T16:20:55Z","title":"DUE: A Deep Learning Framework and Library for Modeling Unknown\n  Equations","summary":"  Equations, particularly differential equations, are fundamental for\nunderstanding natural phenomena and predicting complex dynamics across various\nscientific and engineering disciplines. However, the governing equations for\nmany complex systems remain unknown due to intricate underlying mechanisms.\nRecent advancements in machine learning and data science offer a new paradigm\nfor modeling unknown equations from measurement or simulation data. This\nparadigm shift, known as data-driven discovery or modeling, stands at the\nforefront of AI for science, with significant progress made in recent years. In\nthis paper, we introduce a systematic framework for data-driven modeling of\nunknown equations using deep learning. This versatile framework is capable of\nlearning unknown ODEs, PDEs, DAEs, IDEs, SDEs, reduced or partially observed\nsystems, and non-autonomous differential equations. Based on this framework, we\nhave developed Deep Unknown Equations (DUE), an open-source software package\ndesigned to facilitate the data-driven modeling of unknown equations using\nmodern deep learning techniques. DUE serves as an educational tool for\nclassroom instruction, enabling students and newcomers to gain hands-on\nexperience with differential equations, data-driven modeling, and contemporary\ndeep learning approaches such as FNN, ResNet, generalized ResNet, operator\nsemigroup networks (OSG-Net), and Transformers. Additionally, DUE is a\nversatile and accessible toolkit for researchers across various scientific and\nengineering fields. It is applicable not only for learning unknown equations\nfrom data but also for surrogate modeling of known, yet complex, equations that\nare costly to solve using traditional numerical methods. We provide detailed\ndescriptions of DUE and demonstrate its capabilities through diverse examples,\nwhich serve as templates that can be easily adapted for other applications.\n","authors":["Junfeng Chen","Kailiang Wu","Dongbin Xiu"],"pdf_url":"https://arxiv.org/pdf/2504.10373v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2504.10369v1","updated":"2025-04-14T16:15:55Z","published":"2025-04-14T16:15:55Z","title":"SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired\n  Symbolic Reasoning","summary":"  Optimizing Register Transfer Level (RTL) code is crucial for improving the\npower, performance, and area (PPA) of digital circuits in the early stages of\nsynthesis. Manual rewriting, guided by synthesis feedback, can yield\nhigh-quality results but is time-consuming and error-prone. Most existing\ncompiler-based approaches have difficulty handling complex design constraints.\nLarge Language Model (LLM)-based methods have emerged as a promising\nalternative to address these challenges. However, LLM-based approaches often\nface difficulties in ensuring alignment between the generated code and the\nprovided prompts. This paper presents SymRTLO, a novel neuron-symbolic RTL\noptimization framework that seamlessly integrates LLM-based code rewriting with\nsymbolic reasoning techniques. Our method incorporates a retrieval-augmented\ngeneration (RAG) system of optimization rules and Abstract Syntax Tree\n(AST)-based templates, enabling LLM-based rewriting that maintains syntactic\ncorrectness while minimizing undesired circuit behaviors. A symbolic module is\nproposed for analyzing and optimizing finite state machine (FSM) logic,\nallowing fine-grained state merging and partial specification handling beyond\nthe scope of pattern-based compilers. Furthermore, a fast verification\npipeline, combining formal equivalence checks with test-driven validation,\nfurther reduces the complexity of verification. Experiments on the RTL-Rewriter\nbenchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves\npower, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%,\nrespectively, compared to the state-of-the-art methods.\n","authors":["Yiting Wang","Wanghao Ye","Ping Guo","Yexiao He","Ziyao Wang","Yexiao He","Bowei Tian","Shwai He","Guoheng Sun","Zheyu Shen","Sihan Chen","Ankur Srivastava","Qingfu Zhang","Gang Qu","Ang Li"],"pdf_url":"https://arxiv.org/pdf/2504.10369v1.pdf","comment":"16 pages, 8 figures, 7 tables. Under Review"},{"id":"http://arxiv.org/abs/2504.08696v2","updated":"2025-04-14T16:02:38Z","published":"2025-04-11T17:03:58Z","title":"SeaView: Software Engineering Agent Visual Interface for Enhanced\n  Workflow","summary":"  Auto-regressive LLM-based software engineering (SWE) agents, henceforth SWE\nagents, have made tremendous progress (>60% on SWE-Bench Verified) on\nreal-world coding challenges including GitHub issue resolution. SWE agents use\na combination of reasoning, environment interaction and self-reflection to\nresolve issues thereby generating \"trajectories\". Analysis of SWE agent\ntrajectories is difficult, not only as they exceed LLM sequence length\n(sometimes, greater than 128k) but also because it involves a relatively\nprolonged interaction between an LLM and the environment managed by the agent.\nIn case of an agent error, it can be hard to decipher, locate and understand\nits scope. Similarly, it can be hard to track improvements or regression over\nmultiple runs or experiments. While a lot of research has gone into making\nthese SWE agents reach state-of-the-art, much less focus has been put into\ncreating tools to help analyze and visualize agent output. We propose a novel\ntool called SeaView: Software Engineering Agent Visual Interface for Enhanced\nWorkflow, with a vision to assist SWE-agent researchers to visualize and\ninspect their experiments. SeaView's novel mechanisms help compare experimental\nruns with varying hyper-parameters or LLMs, and quickly get an understanding of\nLLM or environment related problems. Based on our user study, experienced\nresearchers spend between 10 and 30 minutes to gather the information provided\nby SeaView, while researchers with little experience can spend between 30\nminutes to 1 hour to diagnose their experiment.\n","authors":["Timothy Bula","Saurabh Pujar","Luca Buratti","Mihaela Bornea","Avirup Sil"],"pdf_url":"https://arxiv.org/pdf/2504.08696v2.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2312.10068v2","updated":"2025-04-14T15:58:10Z","published":"2023-12-05T22:54:32Z","title":"Artificial Neural Network for Estimation of Physical Parameters of Sea\n  Water using LiDAR Waveforms","summary":"  Light Detection and Ranging (LiDAR) are fast emerging sensors in the field of\nEarth Observation. It is a remote sensing technology that utilizes laser beams\nto measure distances and create detailed three-dimensional representations of\nobjects and environments. The potential of Full Waveform LiDAR is much greater\nthan just height estimation and 3D reconstruction only. Overall shape of signal\nprovides important information about properties of water body. However, the\nshape of FWL is unexplored as most LiDAR software work on point cloud by\nutilizing the maximum value within the waveform. Existing techniques in the\nfield of LiDAR data analysis include depth estimation through inverse modeling\nand regression of logarithmic intensity and depth for approximating the\nattenuation coefficient. However, these methods suffer from limitations in\naccuracy. Depth estimation through inverse modeling provides only approximate\nvalues and does not account for variations in surface properties, while the\nregression approach for the attenuation coefficient is only able to generalize\na value through several data points which lacks precision and may lead to\nsignificant errors in estimation. Additionally, there is currently no\nestablished modeling method available for predicting bottom reflectance. This\nresearch proposed a novel solution based on neural networks for parameter\nestimation in LIDAR data analysis. By leveraging the power of neural networks,\nthe proposed solution successfully learned the inversion model, was able to do\nprediction of parameters such as depth, attenuation coefficient, and bottom\nreflectance. Performance of model was validated by testing it on real LiDAR\ndata. In future, more data availability would enable more accuracy and\nreliability of such models.\n","authors":["Saad Ahmed Jamal"],"pdf_url":"https://arxiv.org/pdf/2312.10068v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2504.10343v1","updated":"2025-04-14T15:51:39Z","published":"2025-04-14T15:51:39Z","title":"Domain-Adversarial Neural Network and Explainable AI for Reducing\n  Tissue-of-Origin Signal in Pan-cancer Mortality Classification","summary":"  Tissue-of-origin signals dominate pan-cancer gene expression, often obscuring\nmolecular features linked to patient survival. This hampers the discovery of\ngeneralizable biomarkers, as models tend to overfit tissue-specific patterns\nrather than capture survival-relevant signals. To address this, we propose a\nDomain-Adversarial Neural Network (DANN) trained on TCGA RNA-seq data to learn\nrepresentations less biased by tissue and more focused on survival. Identifying\ntissue-independent genetic profiles is key to revealing core cancer programs.\nWe assess the DANN using: (1) Standard SHAP, based on the original input space\nand DANN's mortality classifier; (2) A layer-aware strategy applied to hidden\nactivations, including an unsupervised manifold from raw activations and a\nsupervised manifold from mortality-specific SHAP values. Standard SHAP remains\nconfounded by tissue signals due to biases inherent in its computation. The raw\nactivation manifold was dominated by high-magnitude activations, which masked\nsubtle tissue and mortality-related signals. In contrast, the layer-aware SHAP\nmanifold offers improved low-dimensional representations of both tissue and\nmortality signals, independent of activation strength, enabling subpopulation\nstratification and pan-cancer identification of survival-associated genes.\n","authors":["Cristian Padron-Manrique","Juan José Oropeza Valdez","Osbaldo Resendis-Antonio"],"pdf_url":"https://arxiv.org/pdf/2504.10343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06560v2","updated":"2025-04-14T15:44:36Z","published":"2024-09-10T14:43:03Z","title":"A Primer on Variational Inference for Physics-Informed Deep Generative\n  Modelling","summary":"  Variational inference (VI) is a computationally efficient and scalable\nmethodology for approximate Bayesian inference. It strikes a balance between\naccuracy of uncertainty quantification and practical tractability. It excels at\ngenerative modelling and inversion tasks due to its built-in Bayesian\nregularisation and flexibility, essential qualities for physics related\nproblems. For such problems, the underlying physical model determines the\ndependence between variables of interest, which in turn will require a tailored\nderivation for the central VI learning objective. Furthermore, in many physical\ninference applications this structure has rich meaning and is essential for\naccurately capturing the dynamics of interest. In this paper, we provide an\naccessible and thorough technical introduction to VI for forward and inverse\nproblems, guiding the reader through standard derivations of the VI framework\nand how it can best be realized through deep learning. We then review and unify\nrecent literature exemplifying the flexibility allowed by VI. This paper is\ndesigned for a general scientific audience looking to solve physics-based\nproblems with an emphasis on uncertainty quantification\n","authors":["Alex Glyn-Davies","Arnaud Vadeboncoeur","O. Deniz Akyildiz","Ieva Kazlauskaite","Mark Girolami"],"pdf_url":"https://arxiv.org/pdf/2409.06560v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.23600v2","updated":"2025-04-14T15:12:29Z","published":"2025-03-30T21:48:11Z","title":"Online Convex Optimization and Integral Quadratic Constraints: A new\n  approach to regret analysis","summary":"  We propose a novel approach for analyzing dynamic regret of first-order\nconstrained online convex optimization algorithms for strongly convex and\nLipschitz-smooth objectives. Crucially, we provide a general analysis that is\napplicable to a wide range of first-order algorithms that can be expressed as\nan interconnection of a linear dynamical system in feedback with a first-order\noracle. By leveraging Integral Quadratic Constraints (IQCs), we derive a\nsemi-definite program which, when feasible, provides a regret guarantee for the\nonline algorithm. For this, the concept of variational IQCs is introduced as\nthe generalization of IQCs to time-varying monotone operators. Our bounds\ncapture the temporal rate of change of the problem in the form of the path\nlength of the time-varying minimizer and the objective function variation. In\ncontrast to standard results in OCO, our results do not require nerither the\nassumption of gradient boundedness, nor that of a bounded feasible set.\nNumerical analyses showcase the ability of the approach to capture the\ndependence of the regret on the function class condition number.\n","authors":["Fabian Jakob","Andrea Iannelli"],"pdf_url":"https://arxiv.org/pdf/2503.23600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13843v2","updated":"2025-04-14T15:10:31Z","published":"2024-03-17T17:45:04Z","title":"Machine Learning and Transformers for Thyroid Carcinoma Diagnosis: A\n  Review","summary":"  The growing interest in developing smart diagnostic systems to help medical\nexperts process extensive data for treating incurable diseases has been\nnotable. In particular, the challenge of identifying thyroid cancer (TC) has\nseen progress with the use of machine learning (ML) and big data analysis,\nincorporating Transformers to evaluate TC prognosis and determine the risk of\nmalignancy in individuals. This review article presents a summary of various\nstudies on AI-based approaches, especially those employing Transformers, for\ndiagnosing TC. It introduces a new categorization system for these methods\nbased on artificial intelligence (AI) algorithms, the goals of the framework,\nand the computing environments used. Additionally, it scrutinizes and contrasts\nthe available TC datasets by their features. The paper highlights the\nimportance of AI instruments in aiding the diagnosis and treatment of TC\nthrough supervised, unsupervised, or mixed approaches, with a special focus on\nthe ongoing importance of Transformers and large language models (LLMs) in\nmedical diagnostics and disease management. It further discusses the progress\nmade and the continuing obstacles in this area. Lastly, it explores future\ndirections and focuses within this research field.\n","authors":["Yassine Habchi","Hamza Kheddar","Yassine Himeur","Mohamed Chahine Ghanem"],"pdf_url":"https://arxiv.org/pdf/2403.13843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10296v1","updated":"2025-04-14T15:06:17Z","published":"2025-04-14T15:06:17Z","title":"Siamese Network with Dual Attention for EEG-Driven Social Learning:\n  Bridging the Human-Robot Gap in Long-Tail Autonomous Driving","summary":"  Robots with wheeled, quadrupedal, or humanoid forms are increasingly\nintegrated into built environments. However, unlike human social learning, they\nlack a critical pathway for intrinsic cognitive development, namely, learning\nfrom human feedback during interaction. To understand human ubiquitous\nobservation, supervision, and shared control in dynamic and uncertain\nenvironments, this study presents a brain-computer interface (BCI) framework\nthat enables classification of Electroencephalogram (EEG) signals to detect\ncognitively demanding and safety-critical events. As a timely and motivating\nco-robotic engineering application, we simulate a human-in-the-loop scenario to\nflag risky events in semi-autonomous robotic driving-representative of\nlong-tail cases that pose persistent bottlenecks to the safety performance of\nsmart mobility systems and robotic vehicles. Drawing on recent advances in\nfew-shot learning, we propose a dual-attention Siamese convolutional network\npaired with Dynamic Time Warping Barycenter Averaging approach to generate\nrobust EEG-encoded signal representations. Inverse source localization reveals\nactivation in Broadman areas 4 and 9, indicating perception-action coupling\nduring task-relevant mental imagery. The model achieves 80% classification\naccuracy under data-scarce conditions and exhibits a nearly 100% increase in\nthe utility of salient features compared to state-of-the-art methods, as\nmeasured through integrated gradient attribution. Beyond performance, this\nstudy contributes to our understanding of the cognitive architecture required\nfor BCI agents-particularly the role of attention and memory mechanisms-in\ncategorizing diverse mental states and supporting both inter- and intra-subject\nadaptation. Overall, this research advances the development of cognitive\nrobotics and socially guided learning for service robots in complex built\nenvironments.\n","authors":["Xiaoshan Zhou","Carol C. Menassa","Vineet R. Kamat"],"pdf_url":"https://arxiv.org/pdf/2504.10296v1.pdf","comment":"50 pages, 18 figures"},{"id":"http://arxiv.org/abs/2502.02275v3","updated":"2025-04-14T15:04:59Z","published":"2025-02-04T12:35:40Z","title":"A User's Guide to Sampling Strategies for Sliced Optimal Transport","summary":"  This paper serves as a user's guide to sampling strategies for sliced optimal\ntransport. We provide reminders and additional regularity results on the Sliced\nWasserstein distance. We detail the construction methods, generation time\ncomplexity, theoretical guarantees, and conditions for each strategy.\nAdditionally, we provide insights into their suitability for sliced optimal\ntransport in theory. Extensive experiments on both simulated and real-world\ndata offer a representative comparison of the strategies, culminating in\npractical recommendations for their best usage.\n","authors":["Keanu Sisouk","Julie Delon","Julien Tierny"],"pdf_url":"https://arxiv.org/pdf/2502.02275v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10288v1","updated":"2025-04-14T14:58:19Z","published":"2025-04-14T14:58:19Z","title":"Noise2Ghost: Self-supervised deep convolutional reconstruction for ghost\n  imaging","summary":"  We present a new self-supervised deep-learning-based Ghost Imaging (GI)\nreconstruction method, which provides unparalleled reconstruction performance\nfor noisy acquisitions among unsupervised methods. We present the supporting\nmathematical framework and results from theoretical and real data use cases.\nSelf-supervision removes the need for clean reference data while offering\nstrong noise reduction. This provides the necessary tools for addressing\nsignal-to-noise ratio concerns for GI acquisitions in emerging and cutting-edge\nlow-light GI scenarios. Notable examples include micro- and nano-scale x-ray\nemission imaging, e.g., x-ray fluorescence imaging of dose-sensitive samples.\nTheir applications include in-vivo and in-operando case studies for biological\nsamples and batteries.\n","authors":["Mathieu Manni","Dmitry Karpov","K. Joost Batenburg","Sharon Shwartz","Nicola Viganò"],"pdf_url":"https://arxiv.org/pdf/2504.10288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.08400v2","updated":"2025-04-14T14:54:00Z","published":"2023-04-17T16:06:58Z","title":"ATHEENA: A Toolflow for Hardware Early-Exit Network Automation","summary":"  The continued need for improvements in accuracy, throughput, and efficiency\nof Deep Neural Networks has resulted in a multitude of methods that make the\nmost of custom architectures on FPGAs. These include the creation of\nhand-crafted networks and the use of quantization and pruning to reduce\nextraneous network parameters. However, with the potential of static solutions\nalready well exploited, we propose to shift the focus to using the varying\ndifficulty of individual data samples to further improve efficiency and reduce\naverage compute for classification. Input-dependent computation allows for the\nnetwork to make runtime decisions to finish a task early if the result meets a\nconfidence threshold. Early-Exit network architectures have become an\nincreasingly popular way to implement such behaviour in software.\n  We create: A Toolflow for Hardware Early-Exit Network Automation (ATHEENA),\nan automated FPGA toolflow that leverages the probability of samples exiting\nearly from such networks to scale the resources allocated to different sections\nof the network. The toolflow uses the data-flow model of fpgaConvNet, extended\nto support Early-Exit networks as well as Design Space Exploration to optimize\nthe generated streaming architecture hardware with the goal of increasing\nthroughput/reducing area while maintaining accuracy. Experimental results on\nthree different networks demonstrate a throughput increase of $2.00\\times$ to\n$2.78\\times$ compared to an optimized baseline network implementation with no\nearly exits. Additionally, the toolflow can achieve a throughput matching the\nsame baseline with as low as $46\\%$ of the resources the baseline requires.\n","authors":["Benjamin Biggs","Christos-Savvas Bouganis","George A. Constantinides"],"pdf_url":"https://arxiv.org/pdf/2304.08400v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10283v1","updated":"2025-04-14T14:51:45Z","published":"2025-04-14T14:51:45Z","title":"$α$-Flow: A Unified Framework for Continuous-State Discrete Flow\n  Matching Models","summary":"  Recent efforts have extended the flow-matching framework to discrete\ngenerative modeling. One strand of models directly works with the continuous\nprobabilities instead of discrete tokens, which we colloquially refer to as\nContinuous-State Discrete Flow Matching (CS-DFM). Existing CS-DFM models differ\nsignificantly in their representations and geometric assumptions. This work\npresents a unified framework for CS-DFM models, under which the existing\nvariants can be understood as operating on different $\\alpha$-representations\nof probabilities. Building upon the theory of information geometry, we\nintroduce $\\alpha$-Flow, a family of CS-DFM models that adheres to the\ncanonical $\\alpha$-geometry of the statistical manifold, and demonstrate its\noptimality in minimizing the generalized kinetic energy. Theoretically, we show\nthat the flow matching loss for $\\alpha$-flow establishes a unified variational\nbound for the discrete negative log-likelihood. We comprehensively evaluate\ndifferent instantiations of $\\alpha$-flow on various discrete generation\ndomains to demonstrate their effectiveness in discrete generative modeling,\nincluding intermediate values whose geometries have never been explored before.\n$\\alpha$-flow significantly outperforms its discrete-state counterpart in image\nand protein sequence generation and better captures the entropy in language\nmodeling.\n","authors":["Chaoran Cheng","Jiahan Li","Jiajun Fan","Ge Liu"],"pdf_url":"https://arxiv.org/pdf/2504.10283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10281v1","updated":"2025-04-14T14:49:45Z","published":"2025-04-14T14:49:45Z","title":"Zero-shot Autonomous Microscopy for Scalable and Intelligent\n  Characterization of 2D Materials","summary":"  Characterization of atomic-scale materials traditionally requires human\nexperts with months to years of specialized training. Even for trained human\noperators, accurate and reliable characterization remains challenging when\nexamining newly discovered materials such as two-dimensional (2D) structures.\nThis bottleneck drives demand for fully autonomous experimentation systems\ncapable of comprehending research objectives without requiring large training\ndatasets. In this work, we present ATOMIC (Autonomous Technology for Optical\nMicroscopy & Intelligent Characterization), an end-to-end framework that\nintegrates foundation models to enable fully autonomous, zero-shot\ncharacterization of 2D materials. Our system integrates the vision foundation\nmodel (i.e., Segment Anything Model), large language models (i.e., ChatGPT),\nunsupervised clustering, and topological analysis to automate microscope\ncontrol, sample scanning, image segmentation, and intelligent analysis through\nprompt engineering, eliminating the need for additional training. When\nanalyzing typical MoS2 samples, our approach achieves 99.7% segmentation\naccuracy for single layer identification, which is equivalent to that of human\nexperts. In addition, the integrated model is able to detect grain boundary\nslits that are challenging to identify with human eyes. Furthermore, the system\nretains robust accuracy despite variable conditions including defocus, color\ntemperature fluctuations, and exposure variations. It is applicable to a broad\nspectrum of common 2D materials-including graphene, MoS2, WSe2, SnSe-regardless\nof whether they were fabricated via chemical vapor deposition or mechanical\nexfoliation. This work represents the implementation of foundation models to\nachieve autonomous analysis, establishing a scalable and data-efficient\ncharacterization paradigm that fundamentally transforms the approach to\nnanoscale materials research.\n","authors":["Jingyun Yang","Ruoyan Avery Yin","Chi Jiang","Yuepeng Hu","Xiaokai Zhu","Xingjian Hu","Sutharsika Kumar","Xiao Wang","Xiaohua Zhai","Keran Rong","Yunyue Zhu","Tianyi Zhang","Zongyou Yin","Jing Kong","Neil Zhenqiang Gong","Zhichu Ren","Haozhe Wang"],"pdf_url":"https://arxiv.org/pdf/2504.10281v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.10275v1","updated":"2025-04-14T14:43:46Z","published":"2025-04-14T14:43:46Z","title":"LMFormer: Lane based Motion Prediction Transformer","summary":"  Motion prediction plays an important role in autonomous driving. This study\npresents LMFormer, a lane-aware transformer network for trajectory prediction\ntasks. In contrast to previous studies, our work provides a simple mechanism to\ndynamically prioritize the lanes and shows that such a mechanism introduces\nexplainability into the learning behavior of the network. Additionally,\nLMFormer uses the lane connection information at intersections, lane merges,\nand lane splits, in order to learn long-range dependency in lane structure.\nMoreover, we also address the issue of refining the predicted trajectories and\npropose an efficient method for iterative refinement through stacked\ntransformer layers. For benchmarking, we evaluate LMFormer on the nuScenes\ndataset and demonstrate that it achieves SOTA performance across multiple\nmetrics. Furthermore, the Deep Scenario dataset is used to not only illustrate\ncross-dataset network performance but also the unification capabilities of\nLMFormer to train on multiple datasets and achieve better performance.\n","authors":["Harsh Yadav","Maximilian Schaefer","Kun Zhao","Tobias Meisen"],"pdf_url":"https://arxiv.org/pdf/2504.10275v1.pdf","comment":"Accepted: Autonomous Driving Workshop, CVPR 2025"},{"id":"http://arxiv.org/abs/2504.10273v1","updated":"2025-04-14T14:40:11Z","published":"2025-04-14T14:40:11Z","title":"Sidecar: A Structure-Preserving Framework for Solving Partial\n  Differential Equations with Neural Networks","summary":"  Solving partial differential equations (PDEs) with neural networks (NNs) has\nshown great potential in various scientific and engineering fields. However,\nmost existing NN solvers mainly focus on satisfying the given PDEs, without\nexplicitly considering intrinsic physical properties such as mass conservation\nor energy dissipation. This limitation can result in unstable or nonphysical\nsolutions, particularly in long-term simulations. To address this issue, we\npropose Sidecar, a novel framework that enhances the accuracy and physical\nconsistency of existing NN solvers by incorporating structure-preserving\nknowledge. Inspired by the Time-Dependent Spectral Renormalization (TDSR)\napproach, our Sidecar framework introduces a small copilot network, which is\ntrained to guide the existing NN solver in preserving physical structure. This\nframework is designed to be highly flexible, enabling the incorporation of\nstructure-preserving principles from diverse PDEs into a wide range of NN\nsolvers. Our experimental results on benchmark PDEs demonstrate the improvement\nof the existing neural network solvers in terms of accuracy and consistency\nwith structure-preserving properties.\n","authors":["Gaohang Chen","Zhonghua Qiao"],"pdf_url":"https://arxiv.org/pdf/2504.10273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.00707v2","updated":"2025-04-14T14:40:09Z","published":"2023-06-01T14:16:43Z","title":"Renormalized Graph Representations for Node Classification","summary":"  Graph neural networks process information on graphs represented at a given\nresolution scale. We analyze the effect of using different coarse-grained graph\nresolutions, obtained through the Laplacian renormalization group theory, on\nnode classification tasks. At the theory's core is grouping nodes connected by\nsignificant information flow at a given time scale. Representations of the\ngraph at different scales encode interaction information at different ranges.\nWe specifically experiment using representations at the characteristic scale of\nthe graph's mesoscopic structures. We provide the models with the original\ngraph and the graph represented at the characteristic resolution scale and\ncompare them to models that can only access the original graph. Our results\nshowed that models with access to both the original graph and the\ncharacteristic scale graph can achieve statistically significant improvements\nin test accuracy.\n","authors":["Francesco Caso","Giovanni Trappolini","Andrea Bacciu","Pietro Liò","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2306.00707v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08315v2","updated":"2025-04-14T14:36:00Z","published":"2024-06-12T15:16:26Z","title":"Improving Policy Optimization via $\\varepsilon$-Retrain","summary":"  We present $\\varepsilon$-retrain, an exploration strategy encouraging a\nbehavioral preference while optimizing policies with monotonic improvement\nguarantees. To this end, we introduce an iterative procedure for collecting\nretrain areas -- parts of the state space where an agent did not satisfy the\nbehavioral preference. Our method switches between the typical uniform restart\nstate distribution and the retrain areas using a decaying factor $\\varepsilon$,\nallowing agents to retrain on situations where they violated the preference. We\nalso employ formal verification of neural networks to provably quantify the\ndegree to which agents adhere to these behavioral preferences. Experiments over\nhundreds of seeds across locomotion, power network, and navigation tasks show\nthat our method yields agents that exhibit significant performance and sample\nefficiency improvements.\n","authors":["Luca Marzari","Priya L. Donti","Changliu Liu","Enrico Marchesini"],"pdf_url":"https://arxiv.org/pdf/2406.08315v2.pdf","comment":"Accepted at AAMAS 2025"},{"id":"http://arxiv.org/abs/2504.10266v1","updated":"2025-04-14T14:29:37Z","published":"2025-04-14T14:29:37Z","title":"Vision based driving agent for race car simulation environments","summary":"  In recent years, autonomous driving has become a popular field of study. As\ncontrol at tire grip limit is essential during emergency situations, algorithms\ndeveloped for racecars are useful for road cars too. This paper examines the\nuse of Deep Reinforcement Learning (DRL) to solve the problem of grip limit\ndriving in a simulated environment. Proximal Policy Optimization (PPO) method\nis used to train an agent to control the steering wheel and pedals of the\nvehicle, using only visual inputs to achieve professional human lap times. The\npaper outlines the formulation of the task of time optimal driving on a race\ntrack as a deep reinforcement learning problem, and explains the chosen\nobservations, actions, and reward functions. The results demonstrate human-like\nlearning and driving behavior that utilize maximum tire grip potential.\n","authors":["Gergely Bári","László Palkovics"],"pdf_url":"https://arxiv.org/pdf/2504.10266v1.pdf","comment":"Submitted to ICMCE 2024 (https://icmce.org/2024.html)"},{"id":"http://arxiv.org/abs/2311.13094v3","updated":"2025-04-14T14:25:36Z","published":"2023-11-22T01:50:43Z","title":"Newton-CG methods for nonconvex unconstrained optimization with Hölder\n  continuous Hessian","summary":"  In this paper we consider a nonconvex unconstrained optimization problem\nminimizing a twice differentiable objective function with H\\\"older continuous\nHessian. Specifically, we first propose a Newton-conjugate gradient (Newton-CG)\nmethod for finding an approximate first- and second-order stationary point of\nthis problem, assuming the associated the H\\\"older parameters are explicitly\nknown. Then we develop a parameter-free Newton-CG method without requiring any\nprior knowledge of these parameters. To the best of our knowledge, this method\nis the first parameter-free second-order method achieving the best-known\niteration and operation complexity for finding an approximate first- and\nsecond-order stationary point of this problem. Finally, we present preliminary\nnumerical results to demonstrate the superior practical performance of our\nparameter-free Newton-CG method over a well-known regularized Newton method.\n","authors":["Chuan He","Heng Huang","Zhaosong Lu"],"pdf_url":"https://arxiv.org/pdf/2311.13094v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2301.03139"},{"id":"http://arxiv.org/abs/2504.10253v1","updated":"2025-04-14T14:14:27Z","published":"2025-04-14T14:14:27Z","title":"TinyverseGP: Towards a Modular Cross-domain Benchmarking Framework for\n  Genetic Programming","summary":"  Over the years, genetic programming (GP) has evolved, with many proposed\nvariations, especially in how they represent a solution. Being essentially a\nprogram synthesis algorithm, it is capable of tackling multiple problem\ndomains. Current benchmarking initiatives are fragmented, as the different\nrepresentations are not compared with each other and their performance is not\nmeasured across the different domains. In this work, we propose a unified\nframework, dubbed TinyverseGP (inspired by tinyGP), which provides support to\nmultiple representations and problem domains, including symbolic regression,\nlogic synthesis and policy search.\n","authors":["Roman Kalkreuth","Fabricio Olivetti de França","Julian Dierkes","Marie Anastacio","Anja Jankovic","Zdenek Vasicek","Holger Hoos"],"pdf_url":"https://arxiv.org/pdf/2504.10253v1.pdf","comment":"Accepted for presentation as a poster at the Genetic and Evolutionary\n  Computation Conference (GECCO) and will appear in the GECCO'25 companion.\n  GECCO'25 will be held July 14-18, 2025 in M\\'alaga, Spain"},{"id":"http://arxiv.org/abs/2504.10248v1","updated":"2025-04-14T14:11:00Z","published":"2025-04-14T14:11:00Z","title":"Adaptive Sensor Steering Strategy Using Deep Reinforcement Learning for\n  Dynamic Data Acquisition in Digital Twins","summary":"  This paper introduces a sensor steering methodology based on deep\nreinforcement learning to enhance the predictive accuracy and decision support\ncapabilities of digital twins by optimising the data acquisition process.\nTraditional sensor placement techniques are often constrained by one-off\noptimisation strategies, which limit their applicability for online\napplications requiring continuous informative data assimilation. The proposed\napproach addresses this limitation by offering an adaptive framework for sensor\nplacement within the digital twin paradigm. The sensor placement problem is\nformulated as a Markov decision process, enabling the training and deployment\nof an agent capable of dynamically repositioning sensors in response to the\nevolving conditions of the physical structure as represented by the digital\ntwin. This ensures that the digital twin maintains a highly representative and\nreliable connection to its physical counterpart. The proposed framework is\nvalidated through a series of comprehensive case studies involving a cantilever\nplate structure subjected to diverse conditions, including healthy and damaged\nconditions. The results demonstrate the capability of the deep reinforcement\nlearning agent to adaptively reposition sensors improving the quality of data\nacquisition and hence enhancing the overall accuracy of digital twins.\n","authors":["Collins O. Ogbodo","Timothy J. Rogers","Mattia Dal Borgo","David J. Wagg"],"pdf_url":"https://arxiv.org/pdf/2504.10248v1.pdf","comment":"18 pages, 14 figures"},{"id":"http://arxiv.org/abs/2502.00859v2","updated":"2025-04-14T14:07:27Z","published":"2025-02-02T17:17:29Z","title":"FedRIR: Rethinking Information Representation in Federated Learning","summary":"  Mobile and Web-of-Things (WoT) devices at the network edge generate vast\namounts of data for machine learning applications, yet privacy concerns hinder\ncentralized model training. Federated Learning (FL) allows clients (devices) to\ncollaboratively train a shared model coordinated by a central server without\ntransfer private data, but inherent statistical heterogeneity among clients\npresents challenges, often leading to a dilemma between clients' needs for\npersonalized local models and the server's goal of building a generalized\nglobal model. Existing FL methods typically prioritize either global\ngeneralization or local personalization, resulting in a trade-off between these\ntwo objectives and limiting the full potential of diverse client data. To\naddress this challenge, we propose a novel framework that simultaneously\nenhances global generalization and local personalization by Rethinking\nInformation Representation in the Federated learning process (FedRIR).\nSpecifically, we introduce Masked Client-Specific Learning (MCSL), which\nisolates and extracts fine-grained client-specific features tailored to each\nclient's unique data characteristics, thereby enhancing personalization.\nConcurrently, the Information Distillation Module (IDM) refines the global\nshared features by filtering out redundant client-specific information,\nresulting in a purer and more robust global representation that enhances\ngeneralization. By integrating the refined global features with the isolated\nclient-specific features, we construct enriched representations that\neffectively capture both global patterns and local nuances, thereby improving\nthe performance of downstream tasks on the client. The code is available at\nhttps://github.com/Deep-Imaging-Group/FedRIR.\n","authors":["Yongqiang Huang","Zerui Shao","Ziyuan Yang","Zexin Lu","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.00859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03506v3","updated":"2025-04-14T14:05:31Z","published":"2025-03-05T13:54:13Z","title":"Opinion: Revisiting synthetic data classifications from a privacy\n  perspective","summary":"  Synthetic data is emerging as a cost-effective solution necessary to meet the\nincreasing data demands of AI development, created either from existing\nknowledge or derived from real data. The traditional classification of\nsynthetic data types into hybrid, partial or fully synthetic datasets has\nlimited value and does not reflect the ever-increasing methods to generate\nsynthetic data. The generation method and their source jointly shape the\ncharacteristics of synthetic data, which in turn determines its practical\napplications. We make a case for an alternative approach to grouping synthetic\ndata types that better reflect privacy perspectives in order to facilitate\nregulatory guidance in the generation and processing of synthetic data. This\napproach to classification provides flexibility to new advancements like deep\ngenerative methods and offers a more practical framework for future\napplications.\n","authors":["Vibeke Binz Vallevik","Serena Elizabeth Marshall","Aleksandar Babic","Jan Franz Nygaard"],"pdf_url":"https://arxiv.org/pdf/2503.03506v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10240v1","updated":"2025-04-14T14:02:09Z","published":"2025-04-14T14:02:09Z","title":"GNN-ACLP: Graph Neural Networks based Analog Circuit Link Prediction","summary":"  Circuit link prediction identifying missing component connections from\nincomplete netlists is crucial in automating analog circuit design. However,\nexisting methods face three main challenges: 1) Insufficient use of topological\npatterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to\nthe complexity of annotations hinders model generalization; 3) Limited\nadaptability to various netlist formats. We propose GNN-ACLP, a Graph Neural\nNetworks (GNNs) based framework featuring three innovations to tackle these\nchallenges. First, we introduce the SEAL (Subgraphs, Embeddings, and Attributes\nfor Link Prediction) framework and achieve port-level accuracy in circuit link\nprediction. Second, we propose Netlist Babel Fish, a netlist format conversion\ntool leveraging retrieval-augmented generation (RAG) with large language model\n(LLM) to enhance the compatibility of netlist formats. Finally, we construct\nSpiceNetlist, a comprehensive dataset that contains 775 annotated circuits\nacross 10 different classes of components. The experimental results demonstrate\nan improvement of 15.05% on the SpiceNetlist dataset and 12.01% on the\nImage2Net dataset over the existing approach.\n","authors":["Guanyuan Pan","Tiansheng Zhou","Bingtao Ma","Yaqi Wang","Jianxiang Zhao","Shuai Wang"],"pdf_url":"https://arxiv.org/pdf/2504.10240v1.pdf","comment":"Data will be made available on request"},{"id":"http://arxiv.org/abs/2502.12736v2","updated":"2025-04-14T13:58:21Z","published":"2025-02-18T10:50:47Z","title":"Cross-Domain Continual Learning for Edge Intelligence in Wireless ISAC\n  Networks","summary":"  In wireless networks with integrated sensing and communications (ISAC), edge\nintelligence (EI) is expected to be developed at edge devices (ED) for sensing\nuser activities based on channel state information (CSI). However, due to the\nCSI being highly specific to users' characteristics, the CSI-activity\nrelationship is notoriously domain dependent, essentially demanding EI to learn\nsufficient datasets from various domains in order to gain cross-domain sensing\ncapability. This poses a crucial challenge owing to the EDs' limited resources,\nfor which storing datasets across all domains will be a significant burden. In\nthis paper, we propose the EdgeCL framework, enabling the EI to continually\nlearn-then-discard each incoming dataset, while remaining resilient to\ncatastrophic forgetting. We design a transformer-based discriminator for\nhandling sequences of noisy and nonequispaced CSI samples. Besides, we propose\na distilled core-set based knowledge retention method with robustness-enhanced\noptimization to train the discriminator, preserving its performance for\nprevious domains while preventing future forgetting. Experimental evaluations\nshow that EdgeCL achieves 89% of performance compared to cumulative training\nwhile consuming only 3% of its memory, mitigating forgetting by 79%.\n","authors":["Jingzhi Hu","Xin Li","Zhou Su","Jun Luo"],"pdf_url":"https://arxiv.org/pdf/2502.12736v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12938v2","updated":"2025-04-14T13:57:48Z","published":"2024-03-19T17:43:57Z","title":"Learning Neural Differential Algebraic Equations via Operator Splitting","summary":"  Differential-Algebraic Equations (DAEs) describe the temporal evolution of\nsystems that obey both differential and algebraic constraints. Of particular\ninterest are systems that contain implicit relationships between their\ncomponents, such as conservation relationships. Here, we present an Operator\nSplitting (OS) numerical integration scheme for learning unknown components of\nDifferential-Algebraic Equations from time-series data. This methodology is\nbuilt upon the concept of the Universal Differential Equation; that is, a model\nconstructed as a system of Neural Ordinary Differential Equations informed by\ntheory from particular science domains. In this work, we show that the proposed\nOS-based time-stepping scheme is suitable for relevant system-theoretic\ndata-driven modeling tasks. Presented examples include (i) the inverse problem\nof tank-manifold dynamics and (ii) discrepancy modeling of a network of pumps,\ntanks, and pipes. Our experiments demonstrate the proposed method's robustness\nto noise and extrapolation ability to (i) learn the behaviors of the system\ncomponents and their interaction physics and (ii) disambiguate between data\ntrends and mechanistic relationships contained in the system.\n","authors":["James Koch","Madelyn Shapiro","Himanshu Sharma","Draguna Vrabie","Jan Drgona"],"pdf_url":"https://arxiv.org/pdf/2403.12938v2.pdf","comment":"Updated version of the article now includes problem statement"},{"id":"http://arxiv.org/abs/2503.12899v2","updated":"2025-04-14T13:57:28Z","published":"2025-03-17T07:59:42Z","title":"A Semantic-based Optimization Approach for Repairing LLMs: Case Study on\n  Code Generation","summary":"  Language Models (LMs) are widely used in software engineering for code\ngeneration, but they may produce code with errors. Rather than repairing the\ngenerated code, an alternative way is to address the underlying failures of\nmodels. LM repair offers a lightweight solution to this challenge: it requires\nminimal data, reduces computational costs, and reduces the side effects. Unlike\nretraining, LM repair focuses on applying tailored updates to targeted neurons,\nmaking it ideal for scenarios with limited resources, high-performance demands,\nor strict safety requirements. In this paper, we propose \\ul{S}emantic\n\\ul{T}argeting for \\ul{A}nalytical \\ul{R}epair (\\textsc{STAR}), a pioneering\nand novel semantic-based optimization approach for repairing LLMs.\n\\textsc{STAR} realizes main operations in LM repair methods in an optimization\nprocess, including locating ``buggy neurons'', solving ``neuron patches'', and\npatching ``buggy neurons''. Correspondingly, it computes the deltas of weight\nmatrix as the prior information to guide optimization; and attributes the\ntargeted layers and neurons leveraging statistical insights. The neuron patches\nare computed with a solid semantic-based analytical formula, which directly\nbridges the changes to logits with the deltas of neurons, by steering latent\nrepresentations. Compared to the prior work of LM repair (\\textsc{MINT}) and\noptimization methods (\\textsc{SGD}), \\textsc{STAR} integrates their strengths\nwhile mitigating their limitations. \\textsc{STAR} supports solving multiple\nfailures together, significantly improving the usefulness. Evaluated on three\ncode generation tasks using popular code LMs, \\textsc{STAR} demonstrates\nsuperior effectiveness. Additionally, \\textsc{STAR} exhibits better efficiency.\nIn terms of side effects, namely the balance between generalization and\nspecificity, \\textsc{STAR} outperforms prior work by a significant margin.\n","authors":["Jian Gu","Aldeida Aleti","Chunyang Chen","Hongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.12899v2.pdf","comment":"12 pages, 6 figure, 6 tables, under peer-review"},{"id":"http://arxiv.org/abs/2504.10231v1","updated":"2025-04-14T13:52:26Z","published":"2025-04-14T13:52:26Z","title":"A Model Zoo of Vision Transformers","summary":"  The availability of large, structured populations of neural networks - called\n'model zoos' - has led to the development of a multitude of downstream tasks\nranging from model analysis, to representation learning on model weights or\ngenerative modeling of neural network parameters. However, existing model zoos\nare limited in size and architecture and neglect the transformer, which is\namong the currently most successful neural network architectures. We address\nthis gap by introducing the first model zoo of vision transformers (ViT). To\nbetter represent recent training approaches, we develop a new blueprint for\nmodel zoo generation that encompasses both pre-training and fine-tuning steps,\nand publish 250 unique models. They are carefully generated with a large span\nof generating factors, and their diversity is validated using a thorough choice\nof weight-space and behavioral metrics. To further motivate the utility of our\nproposed dataset, we suggest multiple possible applications grounded in both\nextensive exploratory experiments and a number of examples from the existing\nliterature. By extending previous lines of similar work, our model zoo allows\nresearchers to push their model population-based methods from the small model\nregime to state-of-the-art architectures. We make our model zoo available at\ngithub.com/ModelZoos/ViTModelZoo.\n","authors":["Damian Falk","Léo Meynent","Florence Pfammatter","Konstantin Schürholt","Damian Borth"],"pdf_url":"https://arxiv.org/pdf/2504.10231v1.pdf","comment":"Accepted at the ICLR Workshop on Neural Network Weights as a New Data\n  Modality 2025"},{"id":"http://arxiv.org/abs/2411.00246v2","updated":"2025-04-14T13:51:08Z","published":"2024-10-31T22:51:45Z","title":"ResiDual Transformer Alignment with Spectral Decomposition","summary":"  When examined through the lens of their residual streams, a puzzling property\nemerges in transformer networks: residual contributions (e.g., attention heads)\nsometimes specialize in specific tasks or input attributes. In this paper, we\nanalyze this phenomenon in vision transformers, focusing on the spectral\ngeometry of residuals, and explore its implications for modality alignment in\nvision-language models. First, we link it to the intrinsically low-dimensional\nstructure of visual head representations, zooming into their principal\ncomponents and showing that they encode specialized roles across a wide variety\nof input data distributions. Then, we analyze the effect of head specialization\nin multimodal models, focusing on how improved alignment between text and\nspecialized heads impacts zero-shot classification performance. This\nspecialization-performance link consistently holds across diverse pre-training\ndata, network sizes, and objectives, demonstrating a powerful new mechanism for\nboosting zero-shot classification through targeted alignment. Ultimately, we\ntranslate these insights into actionable terms by introducing ResiDual, a\ntechnique for spectral alignment of the residual stream. Much like panning for\ngold, it lets the noise from irrelevant unit principal components (i.e.,\nattributes) wash away to amplify task-relevant ones. Remarkably, this dual\nperspective on modality alignment yields fine-tuning level performance on\ndifferent data distributions while modelling an extremely interpretable and\nparameter-efficient transformation, as we extensively show on 70 pre-trained\nnetwork-dataset combinations (7 models, 10 datasets).\n","authors":["Lorenzo Basile","Valentino Maiorca","Luca Bortolussi","Emanuele Rodolà","Francesco Locatello"],"pdf_url":"https://arxiv.org/pdf/2411.00246v2.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2504.10229v1","updated":"2025-04-14T13:50:23Z","published":"2025-04-14T13:50:23Z","title":"ROSFD: Robust Online Streaming Fraud Detection with Resilience to\n  Concept Drift in Data Streams","summary":"  Continuous generation of streaming data from diverse sources, such as online\ntransactions and digital interactions, necessitates timely fraud detection.\nTraditional batch processing methods often struggle to capture the rapidly\nevolving patterns of fraudulent activities. This paper highlights the critical\nimportance of processing streaming data for effective fraud detection. To\naddress the inherent challenges of latency, scalability, and concept drift in\nstreaming environments, we propose a robust online streaming fraud detection\n(ROSFD) framework. Our proposed framework comprises two key stages: (i) Stage\nOne: Offline Model Initialization. In this initial stage, a model is built in\noffline settings using incremental learning principles to overcome the\n\"cold-start\" problem. (ii) Stage Two: Real-time Model Adaptation. In this\ndynamic stage, drift detection algorithms (viz.,, DDM, EDDM, and ADWIN) are\nemployed to identify concept drift in the incoming data stream and\nincrementally train the model accordingly. This \"train-only-when-required\"\nstrategy drastically reduces the number of retrains needed without\nsignificantly impacting the area under the receiver operating characteristic\ncurve (AUC). Overall, ROSFD utilizing ADWIN as the drift detection method\ndemonstrated the best performance among the employed methods. In terms of model\nefficacy, Adaptive Random Forest consistently outperformed other models,\nachieving the highest AUC in four out of five datasets.\n","authors":["Vivek Yelleti"],"pdf_url":"https://arxiv.org/pdf/2504.10229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16519v3","updated":"2025-04-14T13:33:41Z","published":"2024-05-26T11:04:41Z","title":"Fourier Sliced-Wasserstein Embedding for Multisets and Measures","summary":"  We present the Fourier Sliced-Wasserstein (FSW) embedding - a novel method to\nembed multisets and measures over R^d into Euclidean space.\n  Our proposed embedding approximately preserves the sliced Wasserstein\ndistance on distributions, thereby yielding geometrically meaningful\nrepresentations that better capture the structure of the input. Moreover, it is\ninjective on measures and bi-Lipschitz on multisets - a significant advantage\nover prevalent methods based on sum- or max-pooling, which are provably not\nbi-Lipschitz, and, in many cases, not even injective. The required output\ndimension for these guarantees is near-optimal: roughly 2Nd, where N is the\nmaximal input multiset size.\n  Furthermore, we prove that it is impossible to embed distributions over R^d\ninto Euclidean space in a bi-Lipschitz manner. Thus, the metric properties of\nour embedding are, in a sense, the best possible.\n  Through numerical experiments, we demonstrate that our method yields superior\nmultiset representations that improve performance in practical learning tasks.\nSpecifically, we show that (a) a simple combination of the FSW embedding with\nan MLP achieves state-of-the-art performance in learning the (non-sliced)\nWasserstein distance; and (b) replacing max-pooling with the FSW embedding\nmakes PointNet significantly more robust to parameter reduction, with only\nminor performance degradation even after a 40-fold reduction.\n","authors":["Tal Amir","Nadav Dym"],"pdf_url":"https://arxiv.org/pdf/2405.16519v3.pdf","comment":"ICLR 2025 camera-ready"},{"id":"http://arxiv.org/abs/2504.08178v2","updated":"2025-04-14T13:32:35Z","published":"2025-04-11T00:20:37Z","title":"A Piecewise Lyapunov Analysis of Sub-quadratic SGD: Applications to\n  Robust and Quantile Regression","summary":"  Motivated by robust and quantile regression problems, we investigate the\nstochastic gradient descent (SGD) algorithm for minimizing an objective\nfunction $f$ that is locally strongly convex with a sub--quadratic tail. This\nsetting covers many widely used online statistical methods. We introduce a\nnovel piecewise Lyapunov function that enables us to handle functions $f$ with\nonly first-order differentiability, which includes a wide range of popular loss\nfunctions such as Huber loss. Leveraging our proposed Lyapunov function, we\nderive finite-time moment bounds under general diminishing stepsizes, as well\nas constant stepsizes. We further establish the weak convergence, central limit\ntheorem and bias characterization under constant stepsize, providing the first\ngeometrical convergence result for sub--quadratic SGD. Our results have wide\napplications, especially in online statistical methods. In particular, we\ndiscuss two applications of our results. 1) Online robust regression: We\nconsider a corrupted linear model with sub--exponential covariates and\nheavy--tailed noise. Our analysis provides convergence rates comparable to\nthose for corrupted models with Gaussian covariates and noise. 2) Online\nquantile regression: Importantly, our results relax the common assumption in\nprior work that the conditional density is continuous and provide a more\nfine-grained analysis for the moment bounds.\n","authors":["Yixuan Zhang"," Dongyan"," Huo","Yudong Chen","Qiaomin Xie"],"pdf_url":"https://arxiv.org/pdf/2504.08178v2.pdf","comment":"ACM SIGMETRICS 2025. 40 pages, 12 figures"},{"id":"http://arxiv.org/abs/2503.24157v2","updated":"2025-04-14T13:31:18Z","published":"2025-03-31T14:40:31Z","title":"LLM4FS: Leveraging Large Language Models for Feature Selection and How\n  to Improve It","summary":"  Recent advances in large language models (LLMs) have provided new\nopportunities for decision-making, particularly in the task of automated\nfeature selection. In this paper, we first comprehensively evaluate LLM-based\nfeature selection methods, covering the state-of-the-art DeepSeek-R1,\nGPT-o3-mini, and GPT-4.5. Then, we propose a novel hybrid strategy called\nLLM4FS that integrates LLMs with traditional data-driven methods. Specifically,\ninput data samples into LLMs, and directly call traditional data-driven\ntechniques such as random forest and forward sequential selection. Notably, our\nanalysis reveals that the hybrid strategy leverages the contextual\nunderstanding of LLMs and the high statistical reliability of traditional\ndata-driven methods to achieve excellent feature selection performance, even\nsurpassing LLMs and traditional data-driven methods. Finally, we point out the\nlimitations of its application in decision-making.\n","authors":["Jianhao Li","Xianchao Xiu"],"pdf_url":"https://arxiv.org/pdf/2503.24157v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00664v2","updated":"2025-04-14T13:25:03Z","published":"2024-12-01T03:57:21Z","title":"Improving Decoupled Posterior Sampling for Inverse Problems using Data\n  Consistency Constraint","summary":"  Diffusion models have shown strong performances in solving inverse problems\nthrough posterior sampling while they suffer from errors during earlier steps.\nTo mitigate this issue, several Decoupled Posterior Sampling methods have been\nrecently proposed. However, the reverse process in these methods ignores\nmeasurement information, leading to errors that impede effective optimization\nin subsequent steps. To solve this problem, we propose Guided Decoupled\nPosterior Sampling (GDPS) by integrating a data consistency constraint in the\nreverse process. The constraint performs a smoother transition within the\noptimization process, facilitating a more effective convergence toward the\ntarget distribution. Furthermore, we extend our method to latent diffusion\nmodels and Tweedie's formula, demonstrating its scalability. We evaluate GDPS\non the FFHQ and ImageNet datasets across various linear and nonlinear tasks\nunder both standard and challenging conditions. Experimental results\ndemonstrate that GDPS achieves state-of-the-art performance, improving accuracy\nover existing methods.\n","authors":["Zhi Qi","Shihong Yuan","Yulin Yuan","Linling Kuang","Yoshiyuki Kabashima","Xiangming Meng"],"pdf_url":"https://arxiv.org/pdf/2412.00664v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10208v1","updated":"2025-04-14T13:21:29Z","published":"2025-04-14T13:21:29Z","title":"From Prompting to Alignment: A Generative Framework for Query\n  Recommendation","summary":"  In modern search systems, search engines often suggest relevant queries to\nusers through various panels or components, helping refine their information\nneeds. Traditionally, these recommendations heavily rely on historical search\nlogs to build models, which suffer from cold-start or long-tail issues.\nFurthermore, tasks such as query suggestion, completion or clarification are\nstudied separately by specific design, which lacks generalizability and hinders\nadaptation to novel applications. Despite recent attempts to explore the use of\nLLMs for query recommendation, these methods mainly rely on the inherent\nknowledge of LLMs or external sources like few-shot examples, retrieved\ndocuments, or knowledge bases, neglecting the importance of the calibration and\nalignment with user feedback, thus limiting their practical utility. To address\nthese challenges, we first propose a general Generative Query Recommendation\n(GQR) framework that aligns LLM-based query generation with user preference.\nSpecifically, we unify diverse query recommendation tasks by a universal prompt\nframework, leveraging the instruct-following capability of LLMs for effective\ngeneration. Secondly, we align LLMs with user feedback via presenting a\nCTR-alignment framework, which involves training a query-wise CTR predictor as\na process reward model and employing list-wise preference alignment to maximize\nthe click probability of the generated query list. Furthermore, recognizing the\ninconsistency between LLM knowledge and proactive search intents arising from\nthe separation of user-initiated queries from models, we align LLMs with user\ninitiative via retrieving co-occurrence queries as side information when\nhistorical logs are available.\n","authors":["Erxue Min","Hsiu-Yuan Huang","Min Yang","Xihong Yang","Xin Jia","Yunfang Wu","Hengyi Cai","Shuaiqiang Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2504.10208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.02544v2","updated":"2025-04-14T13:02:13Z","published":"2025-04-03T12:51:40Z","title":"Fourier Sliced-Wasserstein Embedding for Multisets and Measures","summary":"  We present the Fourier Sliced-Wasserstein (FSW) embedding - a novel method to\nembed multisets and measures over $\\mathbb{R}^d$ into Euclidean space.\n  Our proposed embedding approximately preserves the sliced Wasserstein\ndistance on distributions, thereby yielding geometrically meaningful\nrepresentations that better capture the structure of the input. Moreover, it is\ninjective on measures and bi-Lipschitz on multisets - a significant advantage\nover prevalent methods based on sum- or max-pooling, which are provably not\nbi-Lipschitz, and, in many cases, not even injective. The required output\ndimension for these guarantees is near-optimal: roughly $2 N d$, where $N$ is\nthe maximal input multiset size.\n  Furthermore, we prove that it is impossible to embed distributions over\n$\\mathbb{R}^d$ into Euclidean space in a bi-Lipschitz manner. Thus, the metric\nproperties of our embedding are, in a sense, the best possible.\n  Through numerical experiments, we demonstrate that our method yields superior\nmultiset representations that improve performance in practical learning tasks.\nSpecifically, we show that (a) a simple combination of the FSW embedding with\nan MLP achieves state-of-the-art performance in learning the (non-sliced)\nWasserstein distance; and (b) replacing max-pooling with the FSW embedding\nmakes PointNet significantly more robust to parameter reduction, with only\nminor performance degradation even after a 40-fold reduction.\n","authors":["Tal Amir","Nadav Dym"],"pdf_url":"https://arxiv.org/pdf/2504.02544v2.pdf","comment":"This is an erroneous submission that duplicates arXiv:2405.16519. It\n  has been withdrawn; please see arXiv:2405.16519 for the intended version and\n  all future updates"},{"id":"http://arxiv.org/abs/2504.10188v1","updated":"2025-04-14T12:43:17Z","published":"2025-04-14T12:43:17Z","title":"Efficient Generative Model Training via Embedded Representation Warmup","summary":"  Diffusion models excel at generating high-dimensional data but fall short in\ntraining efficiency and representation quality compared to self-supervised\nmethods. We identify a key bottleneck: the underutilization of high-quality,\nsemantically rich representations during training notably slows down\nconvergence. Our systematic analysis reveals a critical representation\nprocessing region -- primarily in the early layers -- where semantic and\nstructural pattern learning takes place before generation can occur. To address\nthis, we propose Embedded Representation Warmup (ERW), a plug-and-play\nframework where in the first stage we get the ERW module serves as a warmup\nthat initializes the early layers of the diffusion model with high-quality,\npretrained representations. This warmup minimizes the burden of learning\nrepresentations from scratch, thereby accelerating convergence and boosting\nperformance. Our theoretical analysis demonstrates that ERW's efficacy depends\non its precise integration into specific neural network layers -- termed the\nrepresentation processing region -- where the model primarily processes and\ntransforms feature representations for later generation. We further establish\nthat ERW not only accelerates training convergence but also enhances\nrepresentation quality: empirically, our method achieves a 40$\\times$\nacceleration in training speed compared to REPA, the current state-of-the-art\nmethods. Code is available at https://github.com/LINs-lab/ERW.\n","authors":["Deyuan Liu","Peng Sun","Xufeng Li","Tao Lin"],"pdf_url":"https://arxiv.org/pdf/2504.10188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10185v1","updated":"2025-04-14T12:38:37Z","published":"2025-04-14T12:38:37Z","title":"LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in\n  Current Benchmarks","summary":"  Large language model unlearning has become a critical challenge in ensuring\nsafety and controlled model behavior by removing undesired data-model\ninfluences from the pretrained model while preserving general utility.\nSignificant recent efforts have been dedicated to developing LLM unlearning\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\nUnlearning Six-way Evaluation), facilitating standardized unlearning\nperformance assessment and method comparison. Despite their usefulness, we\nuncover for the first time a novel coreset effect within these benchmarks.\nSpecifically, we find that LLM unlearning achieved with the original (full)\nforget set can be effectively maintained using a significantly smaller subset\n(functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\ncan be performed surprisingly easily, even in an extremely low-data regime. We\ndemonstrate that this coreset effect remains strong, regardless of the LLM\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\nThe surprisingly strong coreset effect is also robust across various data\nselection methods, ranging from random selection to more sophisticated\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\nkeyword-based perspective, showing that keywords extracted from the forget set\nalone contribute significantly to unlearning effectiveness and indicating that\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\nmodels along additional dimensions, such as mode connectivity and robustness to\njailbreaking attacks. Codes are available at\nhttps://github.com/OPTML-Group/MU-Coreset.\n","authors":["Soumyadeep Pal","Changsheng Wang","James Diffenderfer","Bhavya Kailkhura","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2504.10185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.00698v2","updated":"2025-04-14T12:37:51Z","published":"2025-04-01T12:08:07Z","title":"Command A: An Enterprise-Ready Large Language Model","summary":"  In this report we describe the development of Command A, a powerful large\nlanguage model purpose-built to excel at real-world enterprise use cases.\nCommand A is an agent-optimised and multilingual-capable model, with support\nfor 23 languages of global business, and a novel hybrid architecture balancing\nefficiency with top of the range performance. It offers best-in-class Retrieval\nAugmented Generation (RAG) capabilities with grounding and tool use to automate\nsophisticated business processes. These abilities are achieved through a\ndecentralised training approach, including self-refinement algorithms and model\nmerging techniques. We also include results for Command R7B which shares\ncapability and architectural similarities to Command A. Weights for both models\nhave been released for research purposes. This technical report details our\noriginal training pipeline and presents an extensive evaluation of our models\nacross a suite of enterprise-relevant tasks and public benchmarks,\ndemonstrating excellent performance and efficiency.\n","authors":["Team Cohere"," :"," Aakanksha","Arash Ahmadian","Marwan Ahmed","Jay Alammar","Milad Alizadeh","Yazeed Alnumay","Sophia Althammer","Arkady Arkhangorodsky","Viraat Aryabumi","Dennis Aumiller","Raphaël Avalos","Zahara Aviv","Sammie Bae","Saurabh Baji","Alexandre Barbet","Max Bartolo","Björn Bebensee","Neeral Beladia","Walter Beller-Morales","Alexandre Bérard","Andrew Berneshawi","Anna Bialas","Phil Blunsom","Matt Bobkin","Adi Bongale","Sam Braun","Maxime Brunet","Samuel Cahyawijaya","David Cairuz","Jon Ander Campos","Cassie Cao","Kris Cao","Roman Castagné","Julián Cendrero","Leila Chan Currie","Yash Chandak","Diane Chang","Giannis Chatziveroglou","Hongyu Chen","Claire Cheng","Alexis Chevalier","Justin T. Chiu","Eugene Cho","Eugene Choi","Eujeong Choi","Tim Chung","Volkan Cirik","Ana Cismaru","Pierre Clavier","Henry Conklin","Lucas Crawhall-Stein","Devon Crouse","Andres Felipe Cruz-Salinas","Ben Cyrus","Daniel D'souza","Hugo Dalla-Torre","John Dang","William Darling","Omar Darwiche Domingues","Saurabh Dash","Antoine Debugne","Théo Dehaze","Shaan Desai","Joan Devassy","Rishit Dholakia","Kyle Duffy","Ali Edalati","Ace Eldeib","Abdullah Elkady","Sarah Elsharkawy","Irem Ergün","Beyza Ermis","Marzieh Fadaee","Boyu Fan","Lucas Fayoux","Yannis Flet-Berliac","Nick Frosst","Matthias Gallé","Wojciech Galuba","Utsav Garg","Matthieu Geist","Mohammad Gheshlaghi Azar","Ellen Gilsenan-McMahon","Seraphina Goldfarb-Tarrant","Tomas Goldsack","Aidan Gomez","Victor Machado Gonzaga","Nithya Govindarajan","Manoj Govindassamy","Nathan Grinsztajn","Nikolas Gritsch","Patrick Gu","Shangmin Guo","Kilian Haefeli","Rod Hajjar","Tim Hawes","Jingyi He","Sebastian Hofstätter","Sungjin Hong","Sara Hooker","Tom Hosking","Stephanie Howe","Eric Hu","Renjie Huang","Hemant Jain","Ritika Jain","Nick Jakobi","Madeline Jenkins","JJ Jordan","Dhruti Joshi","Jason Jung","Trushant Kalyanpur","Siddhartha Rao Kamalakara","Julia Kedrzycki","Gokce Keskin","Edward Kim","Joon Kim","Wei-Yin Ko","Tom Kocmi","Michael Kozakov","Wojciech Kryściński","Arnav Kumar Jain","Komal Kumar Teru","Sander Land","Michael Lasby","Olivia Lasche","Justin Lee","Patrick Lewis","Jeffrey Li","Jonathan Li","Hangyu Lin","Acyr Locatelli","Kevin Luong","Raymond Ma","Lukáš Mach","Marina Machado","Joanne Magbitang","Brenda Malacara Lopez","Aryan Mann","Kelly Marchisio","Olivia Markham","Alexandre Matton","Alex McKinney","Dominic McLoughlin","Jozef Mokry","Adrien Morisot","Autumn Moulder","Harry Moynehan","Maximilian Mozes","Vivek Muppalla","Lidiya Murakhovska","Hemangani Nagarajan","Alekhya Nandula","Hisham Nasir","Shauna Nehra","Josh Netto-Rosen","Daniel Ohashi","James Owers-Bardsley","Jason Ozuzu","Dennis Padilla","Gloria Park","Sam Passaglia","Jeremy Pekmez","Laura Penstone","Aleksandra Piktus","Case Ploeg","Andrew Poulton","Youran Qi","Shubha Raghvendra","Miguel Ramos","Ekagra Ranjan","Pierre Richemond","Cécile Robert-Michon","Aurélien Rodriguez","Sudip Roy","Sebastian Ruder","Laura Ruis","Louise Rust","Anubhav Sachan","Alejandro Salamanca","Kailash Karthik Saravanakumar","Isha Satyakam","Alice Schoenauer Sebag","Priyanka Sen","Sholeh Sepehri","Preethi Seshadri","Ye Shen","Tom Sherborne","Sylvie Shang Shi","Sanal Shivaprasad","Vladyslav Shmyhlo","Anirudh Shrinivason","Inna Shteinbuk","Amir Shukayev","Mathieu Simard","Ella Snyder","Ava Spataru","Victoria Spooner","Trisha Starostina","Florian Strub","Yixuan Su","Jimin Sun","Dwarak Talupuru","Eugene Tarassov","Elena Tommasone","Jennifer Tracey","Billy Trend","Evren Tumer","Ahmet Üstün","Bharat Venkitesh","David Venuto","Pat Verga","Maxime Voisin","Alex Wang","Donglu Wang","Shijian Wang","Edmond Wen","Naomi White","Jesse Willman","Marysia Winkels","Chen Xia","Jessica Xie","Minjie Xu","Bowen Yang","Tan Yi-Chern","Ivan Zhang","Zhenyu Zhao","Zhoujie Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.00698v2.pdf","comment":"55 pages"},{"id":"http://arxiv.org/abs/2503.23174v3","updated":"2025-04-14T12:34:14Z","published":"2025-03-29T18:06:28Z","title":"TRA: Better Length Generalisation with Threshold Relative Attention","summary":"  Transformers struggle with length generalisation, displaying poor performance\neven on basic tasks. We test whether these limitations can be explained through\ntwo key failures of the self-attention mechanism. The first is the inability to\nfully remove irrelevant information. The second is tied to position, even if\nthe dot product between a key and query is highly negative (i.e. an irrelevant\nkey) learned positional biases may unintentionally up-weight such information -\ndangerous when distances become out of distribution. Put together, these two\nfailure cases lead to compounding generalisation difficulties. We test whether\nthey can be mitigated through the combination of a) selective sparsity -\ncompletely removing irrelevant keys from the attention softmax and b)\ncontextualised relative distance - distance is only considered as between the\nquery and the keys that matter. We show how refactoring the attention mechanism\nwith these two mitigations in place can substantially improve generalisation\ncapabilities of decoder only transformers.\n","authors":["Mattia Opper","Roland Fernandez","Paul Smolensky","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2503.23174v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10169v1","updated":"2025-04-14T12:24:17Z","published":"2025-04-14T12:24:17Z","title":"Challenges in interpretability of additive models","summary":"  We review generalized additive models as a type of ``transparent'' model that\nhas recently seen renewed interest in the deep learning community as neural\nadditive models. We highlight multiple types of nonidentifiability in this\nmodel class and discuss challenges in interpretability, arguing for restraint\nwhen claiming ``interpretability'' or ``suitability for safety-critical\napplications'' of such models.\n","authors":["Xinyu Zhang","Julien Martinelli","ST John"],"pdf_url":"https://arxiv.org/pdf/2504.10169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00466v2","updated":"2025-04-14T12:22:21Z","published":"2024-03-30T20:19:28Z","title":"Computation and Communication Efficient Lightweighting Vertical\n  Federated Learning for Smart Building IoT","summary":"  With the increasing number and enhanced capabilities of IoT devices in smart\nbuildings, these devices are evolving beyond basic data collection and control\nto actively participate in deep learning tasks. Federated Learning (FL), as a\ndecentralized learning paradigm, is well-suited for such scenarios. However,\nthe limited computational and communication resources of IoT devices present\nsignificant challenges. While existing research has extensively explored\nefficiency improvements in Horizontal FL, these techniques cannot be directly\napplied to Vertical FL due to fundamental differences in data partitioning and\nmodel structure. To address this gap, we propose a Lightweight Vertical\nFederated Learning (LVFL) framework that jointly optimizes computational and\ncommunication efficiency. Our approach introduces two distinct lightweighting\nstrategies: one for reducing the complexity of the feature model to improve\nlocal computation, and another for compressing feature embeddings to reduce\ncommunication overhead. Furthermore, we derive a convergence bound for the\nproposed LVFL algorithm that explicitly incorporates both computation and\ncommunication lightweighting ratios. Experimental results on an image\nclassification task demonstrate that LVFL effectively mitigates resource\ndemands while maintaining competitive learning performance.\n","authors":["Heqiang Wang","Xiang Liu","Yucheng Liu","Jia Zhou","Weihong Yang","Xiaoxiong Zhong"],"pdf_url":"https://arxiv.org/pdf/2404.00466v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19460v2","updated":"2025-04-14T12:22:06Z","published":"2025-02-26T10:28:44Z","title":"Practical Evaluation of Copula-based Survival Metrics: Beyond the\n  Independent Censoring Assumption","summary":"  Conventional survival metrics, such as Harrell's concordance index and the\nBrier Score, rely on the independent censoring assumption for valid inference\nin the presence of right-censored data. However, when instances are censored\nfor reasons related to the event of interest, this assumption no longer holds,\nas this kind of dependent censoring biases the marginal survival estimates of\npopular nonparametric estimators. In this paper, we propose three copula-based\nmetrics to evaluate survival models in the presence of dependent censoring, and\ndesign a framework to create realistic, semi-synthetic datasets with dependent\ncensoring to facilitate the evaluation of the metrics. Our empirical analyses\nin synthetic and semi-synthetic datasets show that our metrics can give error\nestimates that are closer to the true error, mainly in terms of prediction\naccuracy.\n","authors":["Christian Marius Lillelund","Shi-ang Qi","Russell Greiner"],"pdf_url":"https://arxiv.org/pdf/2502.19460v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03783v2","updated":"2025-04-14T12:21:49Z","published":"2024-12-05T00:12:50Z","title":"Expressivity of Representation Learning on Continuous-Time Dynamic\n  Graphs: An Information-Flow Centric Review","summary":"  Graphs are ubiquitous in real-world applications, ranging from social\nnetworks to biological systems, and have inspired the development of Graph\nNeural Networks (GNNs) for learning expressive representations. While most\nresearch has centered on static graphs, many real-world scenarios involve\ndynamic, temporally evolving graphs, motivating the need for Continuous-Time\nDynamic Graph (CTDG) models. This paper provides a comprehensive review of\nGraph Representation Learning (GRL) on CTDGs with a focus on Self-Supervised\nRepresentation Learning (SSRL). We introduce a novel theoretical framework that\nanalyzes the expressivity of CTDG models through an Information-Flow (IF) lens,\nquantifying their ability to propagate and encode temporal and structural\ninformation. Leveraging this framework, we categorize existing CTDG methods\nbased on their suitability for different graph types and application scenarios.\nWithin the same scope, we examine the design of SSRL methods tailored to CTDGs,\nsuch as predictive and contrastive approaches, highlighting their potential to\nmitigate the reliance on labeled data. Empirical evaluations on synthetic and\nreal-world datasets validate our theoretical insights, demonstrating the\nstrengths and limitations of various methods across long-range, bi-partite and\ncommunity-based graphs. This work offers both a theoretical foundation and\npractical guidance for selecting and developing CTDG models, advancing the\nunderstanding of GRL in dynamic settings.\n","authors":["Sofiane Ennadir","Gabriela Zarzar Gandler","Filip Cornell","Lele Cao","Oleg Smirnov","Tianze Wang","Levente Zólyomi","Björn Brinne","Sahar Asadi"],"pdf_url":"https://arxiv.org/pdf/2412.03783v2.pdf","comment":"Accepted by TMLR. Source code: https://github.com/king/ctdg-info-flow"},{"id":"http://arxiv.org/abs/2406.18739v2","updated":"2025-04-14T12:15:24Z","published":"2024-06-26T20:10:03Z","title":"RetroGFN: Diverse and Feasible Retrosynthesis using GFlowNets","summary":"  Single-step retrosynthesis aims to predict a set of reactions that lead to\nthe creation of a target molecule, which is a crucial task in molecular\ndiscovery. Although a target molecule can often be synthesized with multiple\ndifferent reactions, it is not clear how to verify the feasibility of a\nreaction, because the available datasets cover only a tiny fraction of the\npossible solutions. Consequently, the existing models are not encouraged to\nexplore the space of possible reactions sufficiently. In this paper, we propose\na novel single-step retrosynthesis model, RetroGFN, that can explore outside\nthe limited dataset and return a diverse set of feasible reactions by\nleveraging a feasibility proxy model during the training. We show that RetroGFN\nachieves competitive results on standard top-k accuracy while outperforming\nexisting methods on round-trip accuracy. Moreover, we provide empirical\narguments in favor of using round-trip accuracy, which expands the notion of\nfeasibility with respect to the standard top-k accuracy metric.\n","authors":["Piotr Gaiński","Michał Koziarski","Krzysztof Maziarz","Marwin Segler","Jacek Tabor","Marek Śmieja"],"pdf_url":"https://arxiv.org/pdf/2406.18739v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10160v1","updated":"2025-04-14T12:14:18Z","published":"2025-04-14T12:14:18Z","title":"MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like\n  Reinforcement Learning","summary":"  Large-scale reinforcement learning (RL) methods have proven highly effective\nin enhancing the reasoning abilities of large language models (LLMs),\nparticularly for tasks with verifiable solutions such as mathematics and\ncoding. However, applying this idea to machine translation (MT), where outputs\nare flexibly formatted and difficult to automatically evaluate with explicit\nrules, remains underexplored. In this work, we introduce MT-R1-Zero, the first\nopen-source adaptation of the R1-Zero RL framework for MT without supervised\nfine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to\nguide LLMs towards improved translation quality via emergent reasoning. On the\nWMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive\nperformance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points.\nMeanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across\nall metrics, placing it on par with advanced proprietary models such as GPT-4o\nand Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves\nstate-of-the-art scores on semantic metrics. Moreover, our work exhibits strong\ngeneralization capabilities on out-of-distribution MT tasks, robustly\nsupporting multilingual and low-resource settings. Extensive analysis of model\nbehavior across different initializations and reward metrics offers pioneering\ninsight into the critical role of reward design, LLM adaptability, training\ndynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT.\nOur code is available at https://github.com/fzp0424/MT-R1-Zero.\n","authors":["Zhaopeng Feng","Shaosheng Cao","Jiahan Ren","Jiayuan Su","Ruizhe Chen","Yan Zhang","Zhe Xu","Yao Hu","Jian Wu","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2504.10160v1.pdf","comment":"Work in progress. Our code is available at\n  https://github.com/fzp0424/MT-R1-Zero"},{"id":"http://arxiv.org/abs/2504.10151v1","updated":"2025-04-14T12:02:32Z","published":"2025-04-14T12:02:32Z","title":"Continual learning for rotating machinery fault diagnosis with\n  cross-domain environmental and operational variations","summary":"  Although numerous machine learning models exist to detect issues like rolling\nbearing strain and deformation, typically caused by improper mounting,\noverloading, or poor lubrication, these models often struggle to isolate faults\nfrom the noise of real-world operational and environmental variability.\nConditions such as variable loads, high temperatures, stress, and rotational\nspeeds can mask early signs of failure, making reliable detection challenging.\nTo address these limitations, this work proposes a continual deep learning\napproach capable of learning across domains that share underlying structure\nover time. This approach goes beyond traditional accuracy metrics by addressing\nfour second-order challenges: catastrophic forgetting (where new learning\noverwrites past knowledge), lack of plasticity (where models fail to adapt to\nnew data), forward transfer (using past knowledge to improve future learning),\nand backward transfer (refining past knowledge with insights from new domains).\nThe method comprises a feature generator and domain-specific classifiers,\nallowing capacity to grow as new domains emerge with minimal interference,\nwhile an experience replay mechanism selectively revisits prior domains to\nmitigate forgetting. Moreover, nonlinear dependencies across domains are\nexploited by prioritizing replay from those with the highest prior errors,\nrefining models based on most informative past experiences. Experiments show\nhigh average domain accuracy (up to 88.96%), with forgetting measures as low as\n.0027 across non-stationary class-incremental environments.\n","authors":["Diogo Risca","Afonso Lourenço","Goreti Marreiros"],"pdf_url":"https://arxiv.org/pdf/2504.10151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10149v1","updated":"2025-04-14T12:00:00Z","published":"2025-04-14T12:00:00Z","title":"BoTTA: Benchmarking on-device Test Time Adaptation","summary":"  The performance of deep learning models depends heavily on test samples at\nruntime, and shifts from the training data distribution can significantly\nreduce accuracy. Test-time adaptation (TTA) addresses this by adapting models\nduring inference without requiring labeled test data or access to the original\ntraining set. While research has explored TTA from various perspectives like\nalgorithmic complexity, data and class distribution shifts, model\narchitectures, and offline versus continuous learning, constraints specific to\nmobile and edge devices remain underexplored. We propose BoTTA, a benchmark\ndesigned to evaluate TTA methods under practical constraints on mobile and edge\ndevices. Our evaluation targets four key challenges caused by limited resources\nand usage conditions: (i) limited test samples, (ii) limited exposure to\ncategories, (iii) diverse distribution shifts, and (iv) overlapping shifts\nwithin a sample. We assess state-of-the-art TTA methods under these scenarios\nusing benchmark datasets and report system-level metrics on a real testbed.\nFurthermore, unlike prior work, we align with on-device requirements by\nadvocating periodic adaptation instead of continuous inference-time adaptation.\nExperiments reveal key insights: many recent TTA algorithms struggle with small\ndatasets, fail to generalize to unseen categories, and depend on the diversity\nand complexity of distribution shifts. BoTTA also reports device-specific\nresource use. For example, while SHOT improves accuracy by $2.25\\times$ with\n$512$ adaptation samples, it uses $1.08\\times$ peak memory on Raspberry Pi\nversus the base model. BoTTA offers actionable guidance for TTA in real-world,\nresource-constrained deployments.\n","authors":["Michal Danilowski","Soumyajit Chatterjee","Abhirup Ghosh"],"pdf_url":"https://arxiv.org/pdf/2504.10149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01614v3","updated":"2025-04-14T11:57:40Z","published":"2024-05-02T16:17:29Z","title":"RULSurv: A probabilistic survival-based method for early censoring-aware\n  prediction of remaining useful life in ball bearings","summary":"  Predicting the remaining useful life (RUL) of ball bearings is an active area\nof research, where novel machine learning techniques are continuously being\napplied to predict degradation trends and anticipate failures before they\noccur. However, few studies have explicitly addressed the challenge of handling\ncensored data, where information about a specific event (\\eg mechanical\nfailure) is incomplete or only partially observed. To address this issue, we\nintroduce a novel and flexible method for early fault detection using\nKullback-Leibler (KL) divergence and RUL estimation using survival analysis\nthat naturally supports censored data. We demonstrate our approach in the\nXJTU-SY dataset using a 5-fold cross-validation strategy across three different\noperating conditions. When predicting the time to failure for bearings under\nthe highest load (C1, 12.0 kN and 2100 RPM) with 25% random censoring, our\napproach achieves a mean absolute error (MAE) of 14.7 minutes (95% CI =\n13.6-15.8) using a linear CoxPH model, and an MAE of 12.6 minutes (95% CI =\n11.8-13.4) using a nonlinear Random Survival Forests model, compared to an MAE\nof 18.5 minutes (95% CI = 17.4-19.6) using a linear LASSO model that does not\nsupport censoring. Moreover, our approach achieves a mean cumulative relative\naccuracy (CRA) of 0.7586 over 5 bearings under the highest load, which improves\nover several state-of-the-art baselines. Our work highlights the importance of\nconsidering censored data as part of the model design when building predictive\nmodels for early fault detection and RUL estimation.\n","authors":["Christian Marius Lillelund","Fernando Pannullo","Morten Opprud Jakobsen","Manuel Morante","Christian Fischer Pedersen"],"pdf_url":"https://arxiv.org/pdf/2405.01614v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10146v1","updated":"2025-04-14T11:56:55Z","published":"2025-04-14T11:56:55Z","title":"GeoUni: A Unified Model for Generating Geometry Diagrams, Problems and\n  Problem Solutions","summary":"  We propose GeoUni, the first unified geometry expert model capable of\ngenerating problem solutions and diagrams within a single framework in a way\nthat enables the creation of unique and individualized geometry problems.\nTraditionally, solving geometry problems and generating diagrams have been\ntreated as separate tasks in machine learning, with no models successfully\nintegrating both to support problem creation. However, we believe that mastery\nin geometry requires frictionless integration of all of these skills, from\nsolving problems to visualizing geometric relationships, and finally, crafting\ntailored problems. Our extensive experiments demonstrate that GeoUni, with only\n1.5B parameters, achieves performance comparable to larger models such as\nDeepSeek-R1 with 671B parameters in geometric reasoning tasks. GeoUni also\nexcels in generating precise geometric diagrams, surpassing both text-to-image\nmodels and unified models, including the GPT-4o image generation. Most\nimportantly, GeoUni is the only model capable of successfully generating\ntextual problems with matching diagrams based on specific knowledge points,\nthus offering a wider range of capabilities that extend beyond current models.\n","authors":["Jo-Ku Cheng","Zeren Zhang","Ran Chen","Jingyang Deng","Ziran Qin","Jinwen Ma"],"pdf_url":"https://arxiv.org/pdf/2504.10146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10143v1","updated":"2025-04-14T11:54:19Z","published":"2025-04-14T11:54:19Z","title":"Negate or Embrace: On How Misalignment Shapes Multimodal Representation\n  Learning","summary":"  Multimodal representation learning, exemplified by multimodal contrastive\nlearning (MMCL) using image-text pairs, aims to learn powerful representations\nby aligning cues across modalities. This approach relies on the core assumption\nthat the exemplar image-text pairs constitute two representations of an\nidentical concept. However, recent research has revealed that real-world\ndatasets often exhibit misalignment. There are two distinct viewpoints on how\nto address this issue: one suggests mitigating the misalignment, and the other\nleveraging it. We seek here to reconcile these seemingly opposing perspectives,\nand to provide a practical guide for practitioners. Using latent variable\nmodels we thus formalize misalignment by introducing two specific mechanisms:\nselection bias, where some semantic variables are missing, and perturbation\nbias, where semantic variables are distorted -- both affecting latent variables\nshared across modalities. Our theoretical analysis demonstrates that, under\nmild assumptions, the representations learned by MMCL capture exactly the\ninformation related to the subset of the semantic variables invariant to\nselection and perturbation biases. This provides a unified perspective for\nunderstanding misalignment. Based on this, we further offer actionable insights\ninto how misalignment should inform the design of real-world ML systems. We\nvalidate our theoretical findings through extensive empirical studies on both\nsynthetic data and real image-text datasets, shedding light on the nuanced\nimpact of misalignment on multimodal representation learning.\n","authors":["Yichao Cai","Yuhang Liu","Erdun Gao","Tianjiao Jiang","Zhen Zhang","Anton van den Hengel","Javen Qinfeng Shi"],"pdf_url":"https://arxiv.org/pdf/2504.10143v1.pdf","comment":"38 pages"},{"id":"http://arxiv.org/abs/2504.10141v1","updated":"2025-04-14T11:54:06Z","published":"2025-04-14T11:54:06Z","title":"The Impact of Model Zoo Size and Composition on Weight Space Learning","summary":"  Re-using trained neural network models is a common strategy to reduce\ntraining cost and transfer knowledge. Weight space learning - using the weights\nof trained models as data modality - is a promising new field to re-use\npopulations of pre-trained models for future tasks. Approaches in this field\nhave demonstrated high performance both on model analysis and weight generation\ntasks. However, until now their learning setup requires homogeneous model zoos\nwhere all models share the same exact architecture, limiting their capability\nto generalize beyond the population of models they saw during training. In this\nwork, we remove this constraint and propose a modification to a common weight\nspace learning method to accommodate training on heterogeneous populations of\nmodels. We further investigate the resulting impact of model diversity on\ngenerating unseen neural network model weights for zero-shot knowledge\ntransfer. Our extensive experimental evaluation shows that including models\nwith varying underlying image datasets has a high impact on performance and\ngeneralization, for both in- and out-of-distribution settings. Code is\navailable on github.com/HSG-AIML/MultiZoo-SANE.\n","authors":["Damian Falk","Konstantin Schürholt","Damian Borth"],"pdf_url":"https://arxiv.org/pdf/2504.10141v1.pdf","comment":"Accepted at the ICLR Workshop on Neural Network Weights as a New Data\n  Modality 2025"},{"id":"http://arxiv.org/abs/2504.10139v1","updated":"2025-04-14T11:53:29Z","published":"2025-04-14T11:53:29Z","title":"Conditional Distribution Compression via the Kernel Conditional Mean\n  Embedding","summary":"  Existing distribution compression methods, like Kernel Herding (KH), were\noriginally developed for unlabelled data. However, no existing approach\ndirectly compresses the conditional distribution of labelled data. To address\nthis gap, we first introduce the Average Maximum Conditional Mean Discrepancy\n(AMCMD), a natural metric for comparing conditional distributions. We then\nderive a consistent estimator for the AMCMD and establish its rate of\nconvergence. Next, we make a key observation: in the context of distribution\ncompression, the cost of constructing a compressed set targeting the AMCMD can\nbe reduced from $\\mathcal{O}(n^3)$ to $\\mathcal{O}(n)$. Building on this, we\nextend the idea of KH to develop Average Conditional Kernel Herding (ACKH), a\nlinear-time greedy algorithm that constructs a compressed set targeting the\nAMCMD. To better understand the advantages of directly compressing the\nconditional distribution rather than doing so via the joint distribution, we\nintroduce Joint Kernel Herding (JKH), a straightforward adaptation of KH\ndesigned to compress the joint distribution of labelled data. While herding\nmethods provide a simple and interpretable selection process, they rely on a\ngreedy heuristic. To explore alternative optimisation strategies, we propose\nJoint Kernel Inducing Points (JKIP) and Average Conditional Kernel Inducing\nPoints (ACKIP), which jointly optimise the compressed set while maintaining\nlinear complexity. Experiments show that directly preserving conditional\ndistributions with ACKIP outperforms both joint distribution compression (via\nJKH and JKIP) and the greedy selection used in ACKH. Moreover, we see that JKIP\nconsistently outperforms JKH.\n","authors":["Dominic Broadbent","Nick Whiteley","Robert Allison","Tom Lovett"],"pdf_url":"https://arxiv.org/pdf/2504.10139v1.pdf","comment":"68 pages, 28 figures"},{"id":"http://arxiv.org/abs/2504.10136v1","updated":"2025-04-14T11:47:42Z","published":"2025-04-14T11:47:42Z","title":"Uncertainty Propagation in the Fast Fourier Transform","summary":"  We address the problem of uncertainty propagation in the discrete Fourier\ntransform by modeling the fast Fourier transform as a factor graph. Building on\nthis representation, we propose an efficient framework for approximate Bayesian\ninference using belief propagation (BP) and expectation propagation, extending\nits applicability beyond Gaussian assumptions. By leveraging an appropriate BP\nmessage representation and a suitable schedule, our method achieves stable\nconvergence with accurate mean and variance estimates. Numerical experiments in\nrepresentative scenarios from communications demonstrate the practical\npotential of the proposed framework for uncertainty-aware inference in\nprobabilistic systems operating across both time and frequency domain.\n","authors":["Luca Schmid","Charlotte Muth","Laurent Schmalen"],"pdf_url":"https://arxiv.org/pdf/2504.10136v1.pdf","comment":"Submitted to IEEE"},{"id":"http://arxiv.org/abs/2405.08825v4","updated":"2025-04-14T11:44:36Z","published":"2024-05-12T17:57:25Z","title":"Thermodynamic limit in learning period three","summary":"  A continuous one-dimensional map with period three includes all periods. This\nraises the following question: Can we obtain any types of periodic orbits\nsolely by learning three data points? In this paper, we report the answer to be\nyes. Considering a random neural network in its thermodynamic limit, we first\nshow that almost all learned periods are unstable, and each network has its own\ncharacteristic attractors (which can even be untrained ones). The latently\nacquired dynamics, which are unstable within the trained network, serve as a\nfoundation for the diversity of characteristic attractors and may even lead to\nthe emergence of attractors of all periods after learning. When the neural\nnetwork interpolation is quadratic, a universal post-learning bifurcation\nscenario appears, which is consistent with a topological conjugacy between the\ntrained network and the classical logistic map. In addition to universality, we\nexplore specific properties of certain networks, including the singular\nbehavior of the scale of weight at the infinite limit, the finite-size effects,\nand the symmetry in learning period three.\n","authors":["Yuichiro Terasaki","Kohei Nakajima"],"pdf_url":"https://arxiv.org/pdf/2405.08825v4.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2411.02253v3","updated":"2025-04-14T11:36:12Z","published":"2024-11-04T16:43:16Z","title":"Towards safe Bayesian optimization with Wiener kernel regression","summary":"  Bayesian Optimization (BO) is a data-driven strategy for\nminimizing/maximizing black-box functions based on probabilistic surrogate\nmodels. In the presence of safety constraints, the performance of BO crucially\nrelies on tight probabilistic error bounds related to the uncertainty\nsurrounding the surrogate model. For the case of Gaussian Process surrogates\nand Gaussian measurement noise, we present a novel error bound based on the\nrecently proposed Wiener kernel regression. We prove that under rather mild\nassumptions, the proposed error bound is tighter than bounds previously\ndocumented in the literature, leading to enlarged safety regions. We draw upon\na numerical example to demonstrate the efficacy of the proposed error bound in\nsafe BO.\n","authors":["Oleksii Molodchyk","Johannes Teutsch","Timm Faulwasser"],"pdf_url":"https://arxiv.org/pdf/2411.02253v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13116v2","updated":"2025-04-14T11:28:43Z","published":"2025-03-17T12:38:03Z","title":"VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for\n  LLM-Driven Verilog Coding","summary":"  Large language models (LLMs) offer significant potential for coding, yet\nfine-tuning (FT) with curated data is essential for niche languages like\nVerilog. Using proprietary intellectual property (IP) for FT presents a serious\nrisk, as FT data can be leaked through LLM inference. This leads to a critical\ndilemma for design houses: seeking to build externally accessible LLMs offering\ncompetitive Verilog coding, how can they leverage in-house IP to enhance FT\nutility while ensuring IP protection?\n  For the first time in the literature, we study this dilemma. Using LLaMA\n3.1-8B, we conduct in-house FT on a baseline Verilog dataset (RTLCoder)\nsupplemented with our own in-house IP, which is validated through multiple\ntape-outs. To rigorously assess IP leakage, we quantify structural similarity\n(AST/Dolos) and functional equivalence (Synopsys Formality) between generated\ncodes and our in-house IP. We show that our IP can indeed be leaked, confirming\nthe threat. As defense, we evaluate logic locking of Verilog codes (ASSURE).\nThis offers some level of protection, yet reduces the IP's utility for FT and\ndegrades the LLM's performance. Our study shows the need for novel strategies\nthat are both effective and minimally disruptive to FT, an essential effort for\nenabling design houses to fully utilize their proprietary IP toward LLM-driven\nVerilog coding.\n","authors":["Zeng Wang","Minghao Shao","Mohammed Nabeel","Prithwish Basu Roy","Likhitha Mankali","Jitendra Bhandari","Ramesh Karri","Ozgur Sinanoglu","Muhammad Shafique","Johann Knechtel"],"pdf_url":"https://arxiv.org/pdf/2503.13116v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21491v2","updated":"2025-04-14T11:19:21Z","published":"2024-10-28T20:02:05Z","title":"Trustworthiness of Stochastic Gradient Descent in Distributed Learning","summary":"  Distributed learning (DL) uses multiple nodes to accelerate training,\nenabling efficient optimization of large-scale models. Stochastic Gradient\nDescent (SGD), a key optimization algorithm, plays a central role in this\nprocess. However, communication bottlenecks often limit scalability and\nefficiency, leading to increasing adoption of compressed SGD techniques to\nalleviate these challenges. Despite addressing communication overheads,\ncompressed SGD introduces trustworthiness concerns, as gradient exchanges among\nnodes are vulnerable to attacks like gradient inversion (GradInv) and\nmembership inference attacks (MIA). The trustworthiness of compressed SGD\nremains unexplored, leaving important questions about its reliability\nunanswered.\n  In this paper, we provide a trustworthiness evaluation of compressed versus\nuncompressed SGD. Specifically, we conducted empirical studies using GradInv\nattacks, revealing that compressed SGD demonstrates significantly higher\nresistance to privacy leakage compared to uncompressed SGD. In addition, our\nfindings suggest that MIA may not be a reliable metric for assessing privacy\nrisks in distributed learning.\n","authors":["Hongyang Li","Caesar Wu","Mohammed Chadli","Said Mammar","Pascal Bouvry"],"pdf_url":"https://arxiv.org/pdf/2410.21491v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10097v1","updated":"2025-04-14T11:03:19Z","published":"2025-04-14T11:03:19Z","title":"STaRFormer: Semi-Supervised Task-Informed Representation Learning via\n  Dynamic Attention-Based Regional Masking for Sequential Data","summary":"  Accurate predictions using sequential spatiotemporal data are crucial for\nvarious applications. Utilizing real-world data, we aim to learn the intent of\na smart device user within confined areas of a vehicle's surroundings. However,\nin real-world scenarios, environmental factors and sensor limitations result in\nnon-stationary and irregularly sampled data, posing significant challenges. To\naddress these issues, we developed a Transformer-based approach, STaRFormer,\nwhich serves as a universal framework for sequential modeling. STaRFormer\nemploys a novel, dynamic attention-based regional masking scheme combined with\nsemi-supervised contrastive learning to enhance task-specific latent\nrepresentations. Comprehensive experiments on 15 datasets varying in types\n(including non-stationary and irregularly sampled), domains, sequence lengths,\ntraining samples, and applications, demonstrate the efficacy and practicality\nof STaRFormer. We achieve notable improvements over state-of-the-art\napproaches. Code and data will be made available.\n","authors":["Maxmilian Forstenhäusler","Daniel Külzer","Christos Anagnostopoulos","Shameem Puthiya Parambath","Natascha Weber"],"pdf_url":"https://arxiv.org/pdf/2504.10097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08980v2","updated":"2025-04-14T11:00:31Z","published":"2025-03-12T01:21:17Z","title":"I Predict Therefore I Am: Is Next Token Prediction Enough to Learn\n  Human-Interpretable Concepts from Data?","summary":"  The remarkable achievements of large language models (LLMs) have led many to\nconclude that they exhibit a form of intelligence. This is as opposed to\nexplanations of their capabilities based on their ability to perform relatively\nsimple manipulations of vast volumes of data. To illuminate the distinction\nbetween these explanations, we introduce a novel generative model that\ngenerates tokens on the basis of human interpretable concepts represented as\nlatent discrete variables. Under mild conditions, even when the mapping from\nthe latent space to the observed space is non-invertible, we establish an\nidentifiability result: the representations learned by LLMs through next-token\nprediction can be approximately modeled as the logarithm of the posterior\nprobabilities of these latent discrete concepts, up to an invertible linear\ntransformation. This theoretical finding not only provides evidence that LLMs\ncapture underlying generative factors, but also strongly reinforces the linear\nrepresentation hypothesis, which posits that LLMs learn linear representations\nof human-interpretable concepts. Empirically, we validate our theoretical\nresults through evaluations on both simulation data and the Pythia, Llama, and\nDeepSeek model families.\n","authors":["Yuhang Liu","Dong Gong","Erdun Gao","Zhen Zhang","Biwei Huang","Mingming Gong","Anton van den Hengel","Javen Qinfeng Shi"],"pdf_url":"https://arxiv.org/pdf/2503.08980v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11709v3","updated":"2025-04-14T10:54:19Z","published":"2024-10-15T15:46:03Z","title":"GeOT: A spatially explicit framework for evaluating spatio-temporal\n  predictions","summary":"  When predicting observations across space and time, the spatial layout of\nerrors impacts a model's real-world utility. For instance, in bike sharing\ndemand prediction, error patterns translate to relocation costs. However,\ncommonly used error metrics in GeoAI evaluate predictions point-wise,\nneglecting effects such as spatial heterogeneity, autocorrelation, and the\nModifiable Areal Unit Problem. We put forward Optimal Transport (OT) as a\nspatial evaluation metric and loss function. The proposed framework, called\nGeOT, assesses the performance of prediction models by quantifying the\ntransport costs associated with their prediction errors. Through experiments on\nreal and synthetic data, we demonstrate that 1) the spatial distribution of\nprediction errors relates to real-world costs in many applications, 2) OT\ncaptures these spatial costs more accurately than existing metrics, and 3) OT\nenhances comparability across spatial and temporal scales. Finally, we advocate\nfor leveraging OT as a loss function in neural networks to improve the spatial\naccuracy of predictions. Experiments with bike sharing, charging station, and\ntraffic datasets show that spatial costs are significantly reduced with only\nmarginal changes to non-spatial error metrics. Thus, this approach not only\noffers a spatially explicit tool for model evaluation and selection, but also\nintegrates spatial considerations into model training. All code is available at\nhttps://github.com/mie-lab/geospatialOT.\n","authors":["Nina Wiedemann","Théo Uscidda","Martin Raubal"],"pdf_url":"https://arxiv.org/pdf/2410.11709v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09329v2","updated":"2025-04-14T10:53:19Z","published":"2025-03-12T12:23:15Z","title":"Energy Optimized Piecewise Polynomial Approximation Utilizing Modern\n  Machine Learning Optimizers","summary":"  This work explores an extension of machine learning-optimized piecewise\npolynomial approximation by incorporating energy optimization as an additional\nobjective. Traditional closed-form solutions enable continuity and\napproximation targets but lack flexibility in accommodating complex\noptimization goals. By leveraging modern gradient descent optimizers within\nTensorFlow, we introduce a framework that minimizes elastic strain energy in\ncam profiles, leading to smoother motion. Experimental results confirm the\neffectiveness of this approach, demonstrating its potential to\nPareto-efficiently trade approximation quality against energy consumption.\n","authors":["Hannes Waclawek","Stefan Huber"],"pdf_url":"https://arxiv.org/pdf/2503.09329v2.pdf","comment":"Submitted to AI4IP 2025"},{"id":"http://arxiv.org/abs/2306.00037v5","updated":"2025-04-14T10:27:14Z","published":"2023-05-31T09:12:35Z","title":"BotArtist: Generic approach for bot detection in Twitter via\n  semi-automatic machine learning pipeline","summary":"  Twitter, as one of the most popular social networks, provides a platform for\ncommunication and online discourse. Unfortunately, it has also become a target\nfor bots and fake accounts, resulting in the spread of false information and\nmanipulation. This paper introduces a semi-automatic machine learning pipeline\n(SAMLP) designed to address the challenges associated with machine learning\nmodel development. Through this pipeline, we develop a comprehensive bot\ndetection model named BotArtist, based on user profile features. SAMLP\nleverages nine distinct publicly available datasets to train the BotArtist\nmodel. To assess BotArtist's performance against current state-of-the-art\nsolutions, we evaluate 35 existing Twitter bot detection methods, each\nutilizing a diverse range of features. Our comparative evaluation of BotArtist\nand these existing methods, conducted across nine public datasets under\nstandardized conditions, reveals that the proposed model outperforms existing\nsolutions by almost 10% in terms of F1-score, achieving an average score of\n83.19% and 68.5% over specific and general approaches, respectively. As a\nresult of this research, we provide one of the largest labeled Twitter bot\ndatasets. The dataset contains extracted features combined with BotArtist\npredictions for 10,929,533 Twitter user profiles, collected via Twitter API\nduring the 2022 Russo-Ukrainian War over a 16-month period. This dataset was\ncreated based on [Shevtsov et al., 2022a] where the original authors share\nanonymized tweets discussing the Russo-Ukrainian war, totaling 127,275,386\ntweets. The combination of the existing textual dataset and the provided\nlabeled bot and human profiles will enable future development of more advanced\nbot detection large language models in the post-Twitter API era.\n","authors":["Alexander Shevtsov","Despoina Antonakaki","Ioannis Lamprou","Polyvios Pratikakis","Sotiris Ioannidis"],"pdf_url":"https://arxiv.org/pdf/2306.00037v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01894v2","updated":"2025-04-14T10:25:47Z","published":"2024-11-04T08:50:52Z","title":"Efficient Active Imitation Learning with Random Network Distillation","summary":"  Developing agents for complex and underspecified tasks, where no clear\nobjective exists, remains challenging but offers many opportunities. This is\nespecially true in video games, where simulated players (bots) need to play\nrealistically, and there is no clear reward to evaluate them. While imitation\nlearning has shown promise in such domains, these methods often fail when\nagents encounter out-of-distribution scenarios during deployment. Expanding the\ntraining dataset is a common solution, but it becomes impractical or costly\nwhen relying on human demonstrations. This article addresses active imitation\nlearning, aiming to trigger expert intervention only when necessary, reducing\nthe need for constant expert input along training. We introduce Random Network\nDistillation DAgger (RND-DAgger), a new active imitation learning method that\nlimits expert querying by using a learned state-based out-of-distribution\nmeasure to trigger interventions. This approach avoids frequent expert-agent\naction comparisons, thus making the expert intervene only when it is useful. We\nevaluate RND-DAgger against traditional imitation learning and other active\napproaches in 3D video games (racing and third-person navigation) and in a\nrobotic locomotion task and show that RND-DAgger surpasses previous methods by\nreducing expert queries. https://sites.google.com/view/rnd-dagger\n","authors":["Emilien Biré","Anthony Kobanda","Ludovic Denoyer","Rémy Portelas"],"pdf_url":"https://arxiv.org/pdf/2411.01894v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2504.10077v1","updated":"2025-04-14T10:21:59Z","published":"2025-04-14T10:21:59Z","title":"Towards Quantifying Commonsense Reasoning with Mechanistic Insights","summary":"  Commonsense reasoning deals with the implicit knowledge that is well\nunderstood by humans and typically acquired via interactions with the world. In\nrecent times, commonsense reasoning and understanding of various LLMs have been\nevaluated using text-based tasks. In this work, we argue that a proxy of this\nunderstanding can be maintained as a graphical structure that can further help\nto perform a rigorous evaluation of commonsense reasoning abilities about\nvarious real-world activities. We create an annotation scheme for capturing\nthis implicit knowledge in the form of a graphical structure for 37 daily human\nactivities. We find that the created resource can be used to frame an enormous\nnumber of commonsense queries (~ 10^{17}), facilitating rigorous evaluation of\ncommonsense reasoning in LLMs. Moreover, recently, the remarkable performance\nof LLMs has raised questions about whether these models are truly capable of\nreasoning in the wild and, in general, how reasoning occurs inside these\nmodels. In this resource paper, we bridge this gap by proposing design\nmechanisms that facilitate research in a similar direction. Our findings\nsuggest that the reasoning components are localized in LLMs that play a\nprominent role in decision-making when prompted with a commonsense query.\n","authors":["Abhinav Joshi","Areeb Ahmad","Divyaksh Shukla","Ashutosh Modi"],"pdf_url":"https://arxiv.org/pdf/2504.10077v1.pdf","comment":"Accepted at NAACL 2025; 28 pages (9 pages + 7 pages references + 12\n  pages appendix)"},{"id":"http://arxiv.org/abs/2504.10076v1","updated":"2025-04-14T10:21:08Z","published":"2025-04-14T10:21:08Z","title":"Towards Scalable Bayesian Optimization via Gradient-Informed Bayesian\n  Neural Networks","summary":"  Bayesian optimization (BO) is a widely used method for data-driven\noptimization that generally relies on zeroth-order data of objective function\nto construct probabilistic surrogate models. These surrogates guide the\nexploration-exploitation process toward finding global optimum. While Gaussian\nprocesses (GPs) are commonly employed as surrogates of the unknown objective\nfunction, recent studies have highlighted the potential of Bayesian neural\nnetworks (BNNs) as scalable and flexible alternatives. Moreover, incorporating\ngradient observations into GPs, when available, has been shown to improve BO\nperformance. However, the use of gradients within BNN surrogates remains\nunexplored. By leveraging automatic differentiation, gradient information can\nbe seamlessly integrated into BNN training, resulting in more informative\nsurrogates for BO. We propose a gradient-informed loss function for BNN\ntraining, effectively augmenting function observations with local gradient\ninformation. The effectiveness of this approach is demonstrated on well-known\nbenchmarks in terms of improved BNN predictions and faster BO convergence as\nthe number of decision variables increases.\n","authors":["Georgios Makrygiorgos","Joshua Hang Sai Ip","Ali Mesbah"],"pdf_url":"https://arxiv.org/pdf/2504.10076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00448v3","updated":"2025-04-14T10:11:13Z","published":"2023-12-31T10:53:58Z","title":"Beyond Chinchilla-Optimal: Accounting for Inference in Language Model\n  Scaling Laws","summary":"  Large language model (LLM) scaling laws are empirical formulas that estimate\nchanges in model quality as a result of increasing parameter count and training\ndata. However, these formulas, including the popular Deepmind Chinchilla\nscaling laws, neglect to include the cost of inference. We modify the\nChinchilla scaling laws to calculate the optimal LLM parameter count and\npre-training data size to train and deploy a model of a given quality and\ninference demand. We conduct our analysis both in terms of a compute budget and\nreal-world costs and find that LLM researchers expecting reasonably large\ninference demand (~1B requests) should train models smaller and longer than\nChinchilla-optimal. Furthermore, we train 47 models of varying sizes and\nparameter counts to validate our formula and find that model quality continues\nto improve as we scale tokens per parameter to extreme ranges (up to 10,000).\nFinally, we ablate the procedure used to fit the Chinchilla scaling law\ncoefficients and find that developing scaling laws only from data collected at\ntypical token/parameter ratios overestimates the impact of additional tokens at\nthese extreme ranges.\n","authors":["Nikhil Sardana","Jacob Portes","Sasha Doubov","Jonathan Frankle"],"pdf_url":"https://arxiv.org/pdf/2401.00448v3.pdf","comment":"16 pages, 7 figures, In the 41st International Conference on Machine\n  Learning, 2024"},{"id":"http://arxiv.org/abs/2504.10067v1","updated":"2025-04-14T10:09:38Z","published":"2025-04-14T10:09:38Z","title":"Undermining Federated Learning Accuracy in EdgeIoT via Variational Graph\n  Auto-Encoders","summary":"  EdgeIoT represents an approach that brings together mobile edge computing\nwith Internet of Things (IoT) devices, allowing for data processing close to\nthe data source. Sending source data to a server is bandwidth-intensive and may\ncompromise privacy. Instead, federated learning allows each device to upload a\nshared machine-learning model update with locally processed data. However, this\ntechnique, which depends on aggregating model updates from various IoT devices,\nis vulnerable to attacks from malicious entities that may inject harmful data\ninto the learning process. This paper introduces a new attack method targeting\nfederated learning in EdgeIoT, known as data-independent model manipulation\nattack. This attack does not rely on training data from the IoT devices but\ninstead uses an adversarial variational graph auto-encoder (AV-GAE) to create\nmalicious model updates by analyzing benign model updates intercepted during\ncommunication. AV-GAE identifies and exploits structural relationships between\nbenign models and their training data features. By manipulating these\nstructural correlations, the attack maximizes the training loss of the\nfederated learning system, compromising its overall effectiveness.\n","authors":["Kai Li","Shuyan Hu","Bochun Wu","Sai Zou","Wei Ni","Falko Dressler"],"pdf_url":"https://arxiv.org/pdf/2504.10067v1.pdf","comment":"7 pages and 6 figures. Accepted in IEEE IWCMC 2025"},{"id":"http://arxiv.org/abs/2503.13572v2","updated":"2025-04-14T10:02:07Z","published":"2025-03-17T12:26:49Z","title":"VeriContaminated: Assessing LLM-Driven Verilog Coding for Data\n  Contamination","summary":"  Large Language Models (LLMs) have revolutionized code generation, achieving\nexceptional results on various established benchmarking frameworks. However,\nconcerns about data contamination - where benchmark data inadvertently leaks\ninto pre-training or fine-tuning datasets - raise questions about the validity\nof these evaluations. While this issue is known, limiting the industrial\nadoption of LLM-driven software engineering, hardware coding has received\nlittle to no attention regarding these risks. For the first time, we analyze\nstate-of-the-art (SOTA) evaluation frameworks for Verilog code generation\n(VerilogEval and RTLLM), using established methods for contamination detection\n(CCD and Min-K% Prob). We cover SOTA commercial and open-source LLMs\n(CodeGen2.5, Minitron 4b, Mistral 7b, phi-4 mini, LLaMA-{1,2,3.1},\nGPT-{2,3.5,4o}, Deepseek-Coder, and CodeQwen 1.5), in baseline and fine-tuned\nmodels (RTLCoder and Verigen). Our study confirms that data contamination is a\ncritical concern. We explore mitigations and the resulting trade-offs for code\nquality vs fairness (i.e., reducing contamination toward unbiased\nbenchmarking).\n","authors":["Zeng Wang","Minghao Shao","Jitendra Bhandari","Likhitha Mankali","Ramesh Karri","Ozgur Sinanoglu","Muhammad Shafique","Johann Knechtel"],"pdf_url":"https://arxiv.org/pdf/2503.13572v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01708v2","updated":"2025-04-14T09:56:26Z","published":"2024-06-03T18:04:37Z","title":"SnatchML: Hijacking ML models without Training Access","summary":"  Model hijacking can cause significant accountability and security risks since\nthe owner of a hijacked model can be framed for having their model offer\nillegal or unethical services. Prior works consider model hijacking as a\ntraining time attack, whereby an adversary requires full access to the ML model\ntraining. In this paper, we consider a stronger threat model for an\ninference-time hijacking attack, where the adversary has no access to the\ntraining phase of the victim model. Our intuition is that ML models, which are\ntypically over-parameterized, might have the capacity to (unintentionally)\nlearn more than the intended task they are trained for. We propose SnatchML, a\nnew training-free model hijacking attack, that leverages the extra capacity\nlearnt by the victim model to infer different tasks that can be semantically\nrelated or unrelated to the original one. Our results on models deployed on AWS\nSagemaker showed that SnatchML can deliver high accuracy on hijacking tasks.\nInterestingly, while all previous approaches are limited by the number of\nclasses in the benign task, SnatchML can hijack models for tasks that contain\nmore classes than the original. We explore different methods to mitigate this\nrisk; We propose meta-unlearning, which is designed to help the model unlearn a\npotentially malicious task while training for the original task. We also\nprovide insights on over-parametrization as a possible inherent factor that\nfacilitates model hijacking, and accordingly, we propose a compression-based\ncountermeasure to counteract this attack. We believe this work offers a\npreviously overlooked perspective on model hijacking attacks, presenting a\nstronger threat model and higher applicability in real-world contexts.\n","authors":["Mahmoud Ghorbel","Halima Bouzidi","Ioan Marius Bilasco","Ihsen Alouani"],"pdf_url":"https://arxiv.org/pdf/2406.01708v2.pdf","comment":"17 pages, 14 figures, 7 tables"},{"id":"http://arxiv.org/abs/2407.18449v3","updated":"2025-04-14T09:55:21Z","published":"2024-07-26T01:12:54Z","title":"Towards A Generalizable Pathology Foundation Model via Unified Knowledge\n  Distillation","summary":"  Foundation models pretrained on large-scale datasets are revolutionizing the\nfield of computational pathology (CPath). The generalization ability of\nfoundation models is crucial for the success in various downstream clinical\ntasks. However, current foundation models have only been evaluated on a limited\ntype and number of tasks, leaving their generalization ability and overall\nperformance unclear. To address this gap, we established a most comprehensive\nbenchmark to evaluate the performance of off-the-shelf foundation models across\nsix distinct clinical task types, encompassing a total of 72 specific tasks,\nincluding slide-level classification, survival prediction, ROI-tissue\nclassification, ROI retrieval, visual question answering, and report\ngeneration. Our findings reveal that existing foundation models excel at\ncertain task types but struggle to effectively handle the full breadth of\nclinical tasks. To improve the generalization of pathology foundation models,\nwe propose a unified knowledge distillation framework consisting of both expert\nand self-knowledge distillation, where the former allows the model to learn\nfrom the knowledge of multiple expert models, while the latter leverages\nself-distillation to enable image representation learning via local-global\nalignment. Based on this framework, we curated a dataset of 96,000 whole slide\nimages (WSIs) and developed a Generalizable Pathology Foundation Model (GPFM).\nThis advanced model was trained on a substantial dataset comprising 190 million\nimages extracted from approximately 72,000 publicly available slides,\nencompassing 34 major tissue types. Evaluated on the established benchmark,\nGPFM achieves an impressive average rank of 1.6, with 42 tasks ranked 1st,\nwhile the second-best model, UNI, attains an average rank of 3.7, with only 6\ntasks ranked 1st.\n","authors":["Jiabo Ma","Zhengrui Guo","Fengtao Zhou","Yihui Wang","Yingxue Xu","Jinbang Li","Fang Yan","Yu Cai","Zhengjie Zhu","Cheng Jin","Yi Lin","Xinrui Jiang","Chenglong Zhao","Danyi Li","Anjia Han","Zhenhui Li","Ronald Cheong Kin Chan","Jiguang Wang","Peng Fei","Kwang-Ting Cheng","Shaoting Zhang","Li Liang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2407.18449v3.pdf","comment":"update"},{"id":"http://arxiv.org/abs/2504.10045v1","updated":"2025-04-14T09:51:09Z","published":"2025-04-14T09:51:09Z","title":"CHARM: Calibrating Reward Models With Chatbot Arena Scores","summary":"  Reward models (RMs) play a crucial role in Reinforcement Learning from Human\nFeedback by serving as proxies for human preferences in aligning large language\nmodels. In this paper, we identify a model preference bias in RMs, where they\nsystematically assign disproportionately high scores to responses from certain\npolicy models. This bias distorts ranking evaluations and leads to unfair\njudgments. To address this issue, we propose a calibration method named CHatbot\nArena calibrated Reward Modeling (CHARM) that leverages Elo scores from the\nChatbot Arena leaderboard to mitigate RM overvaluation. We also introduce a\nMismatch Degree metric to measure this preference bias. Our approach is\ncomputationally efficient, requiring only a small preference dataset for\ncontinued training of the RM. We conduct extensive experiments on reward model\nbenchmarks and human preference alignment. Results demonstrate that our\ncalibrated RMs (1) achieve improved evaluation accuracy on RM-Bench and the\nChat-Hard domain of RewardBench, and (2) exhibit a stronger correlation with\nhuman preferences by producing scores more closely aligned with Elo rankings.\nBy mitigating model preference bias, our method provides a generalizable and\nefficient solution for building fairer and more reliable reward models.\n","authors":["Xiao Zhu","Chenmien Tan","Pinzhen Chen","Rico Sennrich","Yanlin Zhang","Hanxu Hu"],"pdf_url":"https://arxiv.org/pdf/2504.10045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11267v2","updated":"2025-04-14T09:44:22Z","published":"2024-09-17T15:17:16Z","title":"Integrating Reinforcement Learning and Model Predictive Control with\n  Applications to Microgrids","summary":"  This work proposes an approach that integrates reinforcement learning and\nmodel predictive control (MPC) to solve finite-horizon optimal control problems\nin mixed-logical dynamical systems efficiently. Optimization-based control of\nsuch systems with discrete and continuous decision variables entails the online\nsolution of mixed-integer linear programs, which suffer from the curse of\ndimensionality. Our approach aims to mitigate this issue by decoupling the\ndecision on the discrete variables from the decision on the continuous\nvariables. In the proposed approach, reinforcement learning determines the\ndiscrete decision variables and simplifies the online optimization problem of\nthe MPC controller from a mixed-integer linear program to a linear program,\nsignificantly reducing the computational time. A fundamental contribution of\nthis work is the definition of the decoupled Q-function, which plays a crucial\nrole in making the learning problem tractable in a combinatorial action space.\nWe motivate the use of recurrent neural networks to approximate the decoupled\nQ-function and show how they can be employed in a reinforcement learning\nsetting. Simulation experiments on a microgrid system using real-world data\ndemonstrate that the proposed method substantially reduces the online\ncomputation time of MPC while maintaining high feasibility and low\nsuboptimality.\n","authors":["Caio Fabio Oliveira da Silva","Azita Dabiri","Bart De Schutter"],"pdf_url":"https://arxiv.org/pdf/2409.11267v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05045v3","updated":"2025-04-14T09:42:55Z","published":"2025-04-07T13:14:45Z","title":"Attention-Augmented Inverse Reinforcement Learning with Graph\n  Convolutions for Multi-Agent Task Allocation","summary":"  This work has been submitted to the IEEE for possible publication. Copyright\nmay be transferred without notice, after which this version may no longer be\naccessible.\n  Multi-agent task allocation (MATA) plays a vital role in cooperative\nmulti-agent systems, with significant implications for applications such as\nlogistics, search and rescue, and robotic coordination. Although traditional\ndeep reinforcement learning (DRL) methods have been shown to be promising,\ntheir effectiveness is hindered by a reliance on manually designed reward\nfunctions and inefficiencies in dynamic environments. In this paper, an inverse\nreinforcement learning (IRL)-based framework is proposed, in which multi-head\nself-attention (MHSA) and graph attention mechanisms are incorporated to\nenhance reward function learning and task execution efficiency. Expert\ndemonstrations are utilized to infer optimal reward densities, allowing\ndependence on handcrafted designs to be reduced and adaptability to be\nimproved. Extensive experiments validate the superiority of the proposed method\nover widely used multi-agent reinforcement learning (MARL) algorithms in terms\nof both cumulative rewards and task execution efficiency.\n","authors":["Huilin Yin","Zhikun Yang","Linchuan Zhang","Daniel Watzenig"],"pdf_url":"https://arxiv.org/pdf/2504.05045v3.pdf","comment":"This version includes changes made to meet the submission\n  requirements of IEEE Transactions on Vehicular Technology (TVT): author\n  biographies and IEEE copyright footer removed; acknowledgment anonymized;\n  author footnotes updated; a co-author added for figure illustration and minor\n  edits"},{"id":"http://arxiv.org/abs/2504.10039v1","updated":"2025-04-14T09:41:47Z","published":"2025-04-14T09:41:47Z","title":"Investigating the Role of Bilateral Symmetry for Inpainting Brain MRI","summary":"  Inpainting has recently emerged as a valuable and interesting technology to\nemploy in the analysis of medical imaging data, in particular brain MRI. A wide\nvariety of methodologies for inpainting MRI have been proposed and demonstrated\non tasks including anomaly detection. In this work we investigate the\nstatistical relationship between inpainted brain structures and the amount of\nsubject-specific conditioning information, i.e. the other areas of the image\nthat are masked. In particular, we analyse the distribution of inpainting\nresults when masking additional regions of the image, specifically the\ncontra-lateral structure. This allows us to elucidate where in the brain the\nmodel is drawing information from, and in particular, what is the importance of\nhemispherical symmetry? Our experiments interrogate a diffusion inpainting\nmodel through analysing the inpainting of subcortical brain structures based on\nintensity and estimated area change. We demonstrate that some structures show a\nstrong influence of symmetry in the conditioning of the inpainting process.\n","authors":["Sergey Kuznetsov","Sanduni Pinnawala","Peter A. Wijeratne","Ivor J. A. Simpson"],"pdf_url":"https://arxiv.org/pdf/2504.10039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10031v1","updated":"2025-04-14T09:34:12Z","published":"2025-04-14T09:34:12Z","title":"Using Reinforcement Learning to Integrate Subjective Wellbeing into\n  Climate Adaptation Decision Making","summary":"  Subjective wellbeing is a fundamental aspect of human life, influencing life\nexpectancy and economic productivity, among others. Mobility plays a critical\nrole in maintaining wellbeing, yet the increasing frequency and intensity of\nboth nuisance and high-impact floods due to climate change are expected to\nsignificantly disrupt access to activities and destinations, thereby affecting\noverall wellbeing. Addressing climate adaptation presents a complex challenge\nfor policymakers, who must select and implement policies from a broad set of\noptions with varying effects while managing resource constraints and uncertain\nclimate projections. In this work, we propose a multi-modular framework that\nuses reinforcement learning as a decision-support tool for climate adaptation\nin Copenhagen, Denmark. Our framework integrates four interconnected\ncomponents: long-term rainfall projections, flood modeling, transport\naccessibility, and wellbeing modeling. This approach enables decision-makers to\nidentify spatial and temporal policy interventions that help sustain or enhance\nsubjective wellbeing over time. By modeling climate adaptation as an open-ended\nsystem, our framework provides a structured framework for exploring and\nevaluating adaptation policy pathways. In doing so, it supports policymakers to\nmake informed decisions that maximize wellbeing in the long run.\n","authors":["Arthur Vandervoort","Miguel Costa","Morten W. Petersen","Martin Drews","Sonja Haustein","Karyn Morrissey","Francisco C. Pereira"],"pdf_url":"https://arxiv.org/pdf/2504.10031v1.pdf","comment":"Accepted for presentation at Tackling Climate Change with Machine\n  Learning workshop at ICLR 2025"},{"id":"http://arxiv.org/abs/2503.14301v2","updated":"2025-04-14T09:20:57Z","published":"2025-03-18T14:42:38Z","title":"FeNeC: Enhancing Continual Learning via Feature Clustering with\n  Neighbor- or Logit-Based Classification","summary":"  The ability of deep learning models to learn continuously is essential for\nadapting to new data categories and evolving data distributions. In recent\nyears, approaches leveraging frozen feature extractors after an initial\nlearning phase have been extensively studied. Many of these methods estimate\nper-class covariance matrices and prototypes based on backbone-derived feature\nrepresentations. Within this paradigm, we introduce FeNeC (Feature Neighborhood\nClassifier) and FeNeC-Log, its variant based on the log-likelihood function.\nOur approach generalizes the existing concept by incorporating data clustering\nto capture greater intra-class variability. Utilizing the Mahalanobis distance,\nour models classify samples either through a nearest neighbor approach or\ntrainable logit values assigned to consecutive classes. Our proposition may be\nreduced to the existing approaches in a special case while extending them with\nthe ability of more flexible adaptation to data. We demonstrate that two FeNeC\nvariants achieve competitive performance in scenarios where task identities are\nunknown and establish state-of-the-art results on several benchmarks.\n","authors":["Kamil Książek","Hubert Jastrzębski","Bartosz Trojan","Krzysztof Pniaczek","Michał Karp","Jacek Tabor"],"pdf_url":"https://arxiv.org/pdf/2503.14301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10014v1","updated":"2025-04-14T09:18:11Z","published":"2025-04-14T09:18:11Z","title":"Air Quality Prediction with A Meteorology-Guided Modality-Decoupled\n  Spatio-Temporal Network","summary":"  Air quality prediction plays a crucial role in public health and\nenvironmental protection. Accurate air quality prediction is a complex\nmultivariate spatiotemporal problem, that involves interactions across temporal\npatterns, pollutant correlations, spatial station dependencies, and\nparticularly meteorological influences that govern pollutant dispersion and\nchemical transformations. Existing works underestimate the critical role of\natmospheric conditions in air quality prediction and neglect comprehensive\nmeteorological data utilization, thereby impairing the modeling of dynamic\ninterdependencies between air quality and meteorological data. To overcome\nthis, we propose MDSTNet, an encoder-decoder framework that explicitly models\nair quality observations and atmospheric conditions as distinct modalities,\nintegrating multi-pressure-level meteorological data and weather forecasts to\ncapture atmosphere-pollution dependencies for prediction. Meantime, we\nconstruct ChinaAirNet, the first nationwide dataset combining air quality\nrecords with multi-pressure-level meteorological observations. Experimental\nresults on ChinaAirNet demonstrate MDSTNet's superiority, substantially\nreducing 48-hour prediction errors by 17.54\\% compared to the state-of-the-art\nmodel. The source code and dataset will be available on github.\n","authors":["Hang Yin","Yan-Ming Zhang","Jian Xu","Jian-Long Chang","Yin Li","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2504.10014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10007v1","updated":"2025-04-14T09:09:01Z","published":"2025-04-14T09:09:01Z","title":"Balancing Two Classifiers via A Simplex ETF Structure for Model\n  Calibration","summary":"  In recent years, deep neural networks (DNNs) have demonstrated\nstate-of-the-art performance across various domains. However, despite their\nsuccess, they often face calibration issues, particularly in safety-critical\napplications such as autonomous driving and healthcare, where unreliable\npredictions can have serious consequences. Recent research has started to\nimprove model calibration from the view of the classifier. However, the\nexploration of designing the classifier to solve the model calibration problem\nis insufficient. Let alone most of the existing methods ignore the calibration\nerrors arising from underconfidence. In this work, we propose a novel method by\nbalancing learnable and ETF classifiers to solve the overconfidence or\nunderconfidence problem for model Calibration named BalCAL. By introducing a\nconfidence-tunable module and a dynamic adjustment method, we ensure better\nalignment between model confidence and its true accuracy. Extensive\nexperimental validation shows that ours significantly improves model\ncalibration performance while maintaining high predictive accuracy,\noutperforming existing techniques. This provides a novel solution to the\ncalibration challenges commonly encountered in deep learning.\n","authors":["Jiani Ni","He Zhao","Jintong Gao","Dandan Guo","Hongyuan Zha"],"pdf_url":"https://arxiv.org/pdf/2504.10007v1.pdf","comment":"CVPR2025"},{"id":"http://arxiv.org/abs/2504.10006v1","updated":"2025-04-14T09:08:53Z","published":"2025-04-14T09:08:53Z","title":"Improving Controller Generalization with Dimensionless Markov Decision\n  Processes","summary":"  Controllers trained with Reinforcement Learning tend to be very specialized\nand thus generalize poorly when their testing environment differs from their\ntraining one. We propose a Model-Based approach to increase generalization\nwhere both world model and policy are trained in a dimensionless state-action\nspace. To do so, we introduce the Dimensionless Markov Decision Process\n($\\Pi$-MDP): an extension of Contextual-MDPs in which state and action spaces\nare non-dimensionalized with the Buckingham-$\\Pi$ theorem. This procedure\ninduces policies that are equivariant with respect to changes in the context of\nthe underlying dynamics. We provide a generic framework for this approach and\napply it to a model-based policy search algorithm using Gaussian Process\nmodels. We demonstrate the applicability of our method on simulated actuated\npendulum and cartpole systems, where policies trained on a single environment\nare robust to shifts in the distribution of the context.\n","authors":["Valentin Charvet","Sebastian Stein","Roderick Murray-Smith"],"pdf_url":"https://arxiv.org/pdf/2504.10006v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.10005v1","updated":"2025-04-14T09:08:40Z","published":"2025-04-14T09:08:40Z","title":"Session-based Recommender Systems: User Interest as a Stochastic Process\n  in the Latent Space","summary":"  This paper jointly addresses the problem of data uncertainty, popularity\nbias, and exposure bias in session-based recommender systems. We study the\nsymptoms of this bias both in item embeddings and in recommendations. We\npropose treating user interest as a stochastic process in the latent space and\nproviding a model-agnostic implementation of this mathematical concept. The\nproposed stochastic component consists of elements: debiasing item embeddings\nwith regularization for embedding uniformity, modeling dense user interest from\nsession prefixes, and introducing fake targets in the data to simulate extended\nexposure. We conducted computational experiments on two popular benchmark\ndatasets, Diginetica and YooChoose 1/64, as well as several modifications of\nthe YooChoose dataset with different ratios of popular items. The results show\nthat the proposed approach allows us to mitigate the challenges mentioned.\n","authors":["Klaudia Balcer","Piotr Lipinski"],"pdf_url":"https://arxiv.org/pdf/2504.10005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18826v2","updated":"2025-04-14T09:08:36Z","published":"2025-03-24T16:06:43Z","title":"Interpretable and Fair Mechanisms for Abstaining Classifiers","summary":"  Abstaining classifiers have the option to refrain from providing a prediction\nfor instances that are difficult to classify. The abstention mechanism is\ndesigned to trade off the classifier's performance on the accepted data while\nensuring a minimum number of predictions. In this setting, often fairness\nconcerns arise when the abstention mechanism solely reduces errors for the\nmajority groups of the data, resulting in increased performance differences\nacross demographic groups. While there exist a bunch of methods that aim to\nreduce discrimination when abstaining, there is no mechanism that can do so in\nan explainable way. In this paper, we fill this gap by introducing\nInterpretable and Fair Abstaining Classifier IFAC, an algorithm that can reject\npredictions both based on their uncertainty and their unfairness. By rejecting\npossibly unfair predictions, our method reduces error and positive decision\nrate differences across demographic groups of the non-rejected data. Since the\nunfairness-based rejections are based on an interpretable-by-design method,\ni.e., rule-based fairness checks and situation testing, we create a transparent\nprocess that can empower human decision-makers to review the unfair predictions\nand make more just decisions for them. This explainable aspect is especially\nimportant in light of recent AI regulations, mandating that any high-risk\ndecision task should be overseen by human experts to reduce discrimination\nrisks.\n","authors":["Daphne Lenders","Andrea Pugnana","Roberto Pellungrini","Toon Calders","Dino Pedreschi","Fosca Giannotti"],"pdf_url":"https://arxiv.org/pdf/2503.18826v2.pdf","comment":"25 pages, 8 figures. In: Machine Learning and Knowledge Discovery in\n  Databases. Research Track. ECML PKDD 2024"},{"id":"http://arxiv.org/abs/2410.12877v2","updated":"2025-04-14T09:04:45Z","published":"2024-10-15T08:38:20Z","title":"Improving Instruction-Following in Language Models through Activation\n  Steering","summary":"  The ability to follow instructions is crucial for numerous real-world\napplications of language models. In pursuit of deeper insights and more\npowerful capabilities, we derive instruction-specific vector representations\nfrom language models and use them to steer models accordingly. These vectors\nare computed as the difference in activations between inputs with and without\ninstructions, enabling a modular approach to activation steering. We\ndemonstrate how this method can enhance model adherence to constraints such as\noutput format, length, and word inclusion, providing inference-time control\nover instruction following. Our experiments across four models demonstrate how\nwe can use the activation vectors to guide models to follow constraints even\nwithout explicit instructions and to enhance performance when instructions are\npresent. Additionally, we explore the compositionality of activation steering,\nsuccessfully applying multiple instructions simultaneously. Finally, we\ndemonstrate that steering vectors computed on instruction-tuned models can\ntransfer to improve base models. Our findings demonstrate that activation\nsteering offers a practical and scalable approach for fine-grained control in\nlanguage generation. Our code and data are available at\nhttps://github.com/microsoft/llm-steer-instruct.\n","authors":["Alessandro Stolfo","Vidhisha Balachandran","Safoora Yousefi","Eric Horvitz","Besmira Nushi"],"pdf_url":"https://arxiv.org/pdf/2410.12877v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2504.10002v1","updated":"2025-04-14T09:04:14Z","published":"2025-04-14T09:04:14Z","title":"FLoRA: Sample-Efficient Preference-based RL via Low-Rank Style\n  Adaptation of Reward Functions","summary":"  Preference-based reinforcement learning (PbRL) is a suitable approach for\nstyle adaptation of pre-trained robotic behavior: adapting the robot's policy\nto follow human user preferences while still being able to perform the original\ntask. However, collecting preferences for the adaptation process in robotics is\noften challenging and time-consuming. In this work we explore the adaptation of\npre-trained robots in the low-preference-data regime. We show that, in this\nregime, recent adaptation approaches suffer from catastrophic reward forgetting\n(CRF), where the updated reward model overfits to the new preferences, leading\nthe agent to become unable to perform the original task. To mitigate CRF, we\npropose to enhance the original reward model with a small number of parameters\n(low-rank matrices) responsible for modeling the preference adaptation. Our\nevaluation shows that our method can efficiently and effectively adjust robotic\nbehavior to human preferences across simulation benchmark tasks and multiple\nreal-world robotic tasks.\n","authors":["Daniel Marta","Simon Holk","Miguel Vasco","Jens Lundell","Timon Homberger","Finn Busch","Olov Andersson","Danica Kragic","Iolanda Leite"],"pdf_url":"https://arxiv.org/pdf/2504.10002v1.pdf","comment":"Accepted at 2025 IEEE International Conference on Robotics &\n  Automation (ICRA). We provide videos of our results and source code at\n  https://sites.google.com/view/preflora/"},{"id":"http://arxiv.org/abs/2504.10000v1","updated":"2025-04-14T09:03:51Z","published":"2025-04-14T09:03:51Z","title":"Do We Really Need Curated Malicious Data for Safety Alignment in\n  Multi-modal Large Language Models?","summary":"  Multi-modal large language models (MLLMs) have made significant progress, yet\ntheir safety alignment remains limited. Typically, current open-source MLLMs\nrely on the alignment inherited from their language module to avoid harmful\ngenerations. However, the lack of safety measures specifically designed for\nmulti-modal inputs creates an alignment gap, leaving MLLMs vulnerable to\nvision-domain attacks such as typographic manipulation. Current methods utilize\na carefully designed safety dataset to enhance model defense capability, while\nthe specific knowledge or patterns acquired from the high-quality dataset\nremain unclear. Through comparison experiments, we find that the alignment gap\nprimarily arises from data distribution biases, while image content, response\nquality, or the contrastive behavior of the dataset makes little contribution\nto boosting multi-modal safety. To further investigate this and identify the\nkey factors in improving MLLM safety, we propose finetuning MLLMs on a small\nset of benign instruct-following data with responses replaced by simple, clear\nrejection sentences. Experiments show that, without the need for\nlabor-intensive collection of high-quality malicious data, model safety can\nstill be significantly improved, as long as a specific fraction of rejection\ndata exists in the finetuning set, indicating the security alignment is not\nlost but rather obscured during multi-modal pretraining or instruction\nfinetuning. Simply correcting the underlying data bias could narrow the safety\ngap in the vision domain.\n","authors":["Yanbo Wang","Jiyang Guan","Jian Liang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2504.10000v1.pdf","comment":"Accepted to CVPR 2025, codes in process"},{"id":"http://arxiv.org/abs/2504.09994v1","updated":"2025-04-14T08:57:59Z","published":"2025-04-14T08:57:59Z","title":"Physical Scales Matter: The Role of Receptive Fields and Advection in\n  Satellite-Based Thunderstorm Nowcasting with Convolutional Neural Networks","summary":"  The focus of nowcasting development is transitioning from physically\nmotivated advection methods to purely data-driven Machine Learning (ML)\napproaches. Nevertheless, recent work indicates that incorporating advection\ninto the ML value chain has improved skill for radar-based precipitation\nnowcasts. However, the generality of this approach and the underlying causes\nremain unexplored. This study investigates the generality by probing the\napproach on satellite-based thunderstorm nowcasts for the first time. Resorting\nto a scale argument, we then put forth an explanation when and why skill\nimprovements can be expected. In essence, advection guarantees that\nthunderstorm patterns relevant for nowcasting are contained in the receptive\nfield at long lead times. To test our hypotheses, we train ResU-Nets solving\nsegmentation tasks with lightning observations as ground truth. The input of\nthe Baseline Neural Network (BNN) are short time series of multispectral\nsatellite imagery and lightning observations, whereas the Advection-Informed\nNeural Network (AINN) additionally receives the Lagrangian persistence nowcast\nof all input channels at the desired lead time. Overall, we find only a minor\nskill improvement of the AINN over the BNN when considering fully averaged\nscores. However, assessing skill conditioned on lead time and wind speed, we\ndemonstrate that our scale argument correctly predicts the onset of skill\nimprovement of the AINN over the BNN after 2h lead time. We confirm that\ngenerally advection becomes gradually more important with longer lead times and\nhigher wind speeds. Our work accentuates the importance of considering and\nincorporating the underlying physical scales when designing ML based\nforecasting models.\n","authors":["Christoph Metzl","Kianusch Vahid Yousefnia","Richard Müller","Virginia Poli","Miria Celano","Tobias Bölle"],"pdf_url":"https://arxiv.org/pdf/2504.09994v1.pdf","comment":"11 pages, 10 figures, 1 table. This work has been submitted to\n  Artificial Intelligence for the Earth Systems (AIES). Copyright in this work\n  may be transferred without further notice"},{"id":"http://arxiv.org/abs/2504.09993v1","updated":"2025-04-14T08:55:16Z","published":"2025-04-14T08:55:16Z","title":"AimTS: Augmented Series and Image Contrastive Learning for Time Series\n  Classification","summary":"  Time series classification (TSC) is an important task in time series\nanalysis. Existing TSC methods mainly train on each single domain separately,\nsuffering from a degradation in accuracy when the samples for training are\ninsufficient in certain domains. The pre-training and fine-tuning paradigm\nprovides a promising direction for solving this problem. However, time series\nfrom different domains are substantially divergent, which challenges the\neffective pre-training on multi-source data and the generalization ability of\npre-trained models. To handle this issue, we introduce Augmented Series and\nImage Contrastive Learning for Time Series Classification (AimTS), a\npre-training framework that learns generalizable representations from\nmulti-source time series data. We propose a two-level prototype-based\ncontrastive learning method to effectively utilize various augmentations in\nmulti-source pre-training, which learns representations for TSC that can be\ngeneralized to different domains. In addition, considering augmentations within\nthe single time series modality are insufficient to fully address\nclassification problems with distribution shift, we introduce the image\nmodality to supplement structural information and establish a series-image\ncontrastive learning to improve the generalization of the learned\nrepresentations for TSC tasks. Extensive experiments show that after\nmulti-source pre-training, AimTS achieves good generalization performance,\nenabling efficient learning and even few-shot learning on various downstream\nTSC datasets.\n","authors":["Yuxuan Chen","Shanshan Huang","Yunyao Cheng","Peng Chen","Zhongwen Rao","Yang Shu","Bin Yang","Lujia Pan","Chenjuan Guo"],"pdf_url":"https://arxiv.org/pdf/2504.09993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13721v2","updated":"2025-04-14T08:49:39Z","published":"2024-05-22T15:12:14Z","title":"Connectivity Shapes Implicit Regularization in Matrix Factorization\n  Models for Matrix Completion","summary":"  Matrix factorization models have been extensively studied as a valuable\ntest-bed for understanding the implicit biases of overparameterized models.\nAlthough both low nuclear norm and low rank regularization have been studied\nfor these models, a unified understanding of when, how, and why they achieve\ndifferent implicit regularization effects remains elusive. In this work, we\nsystematically investigate the implicit regularization of matrix factorization\nfor solving matrix completion problems. We empirically discover that the\nconnectivity of observed data plays a crucial role in the implicit bias, with a\ntransition from low nuclear norm to low rank as data shifts from disconnected\nto connected with increased observations. We identify a hierarchy of intrinsic\ninvariant manifolds in the loss landscape that guide the training trajectory to\nevolve from low-rank to higher-rank solutions. Based on this finding, we\ntheoretically characterize the training trajectory as following the\nhierarchical invariant manifold traversal process, generalizing the\ncharacterization of Li et al. (2020) to include the disconnected case.\nFurthermore, we establish conditions that guarantee minimum nuclear norm,\nclosely aligning with our experimental findings, and we provide a dynamics\ncharacterization condition for ensuring minimum rank. Our work reveals the\nintricate interplay between data connectivity, training dynamics, and implicit\nregularization in matrix factorization models.\n","authors":["Zhiwei Bai","Jiajie Zhao","Yaoyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.13721v2.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2504.05172v2","updated":"2025-04-14T08:47:52Z","published":"2025-04-07T15:16:22Z","title":"Attention-Based Multiscale Temporal Fusion Network for Uncertain-Mode\n  Fault Diagnosis in Multimode Processes","summary":"  Fault diagnosis in multimode processes plays a critical role in ensuring the\nsafe operation of industrial systems across multiple modes. It faces a great\nchallenge yet to be addressed - that is, the significant distributional\ndifferences among monitoring data from multiple modes make it difficult for the\nmodels to extract shared feature representations related to system health\nconditions. In response to this problem, this paper introduces a novel method\ncalled attention-based multiscale temporal fusion network. The multiscale\ndepthwise convolution and gated recurrent unit are employed to extract\nmultiscale contextual local features and long-short-term features. Instance\nnormalization is applied to suppress mode-specific information. Furthermore, a\ntemporal attention mechanism is designed to focus on critical time points with\nhigher cross-mode shared information, thereby enhancing the accuracy of fault\ndiagnosis. The proposed model is applied to Tennessee Eastman process dataset\nand three-phase flow facility dataset. The experiments demonstrate that the\nproposed model achieves superior diagnostic performance and maintains a small\nmodel size. The source code will be available on GitHub at\nhttps://github.com/GuangqiangLi/AMTFNet.\n","authors":["Guangqiang Li","M. Amine Atoui","Xiangshun Li"],"pdf_url":"https://arxiv.org/pdf/2504.05172v2.pdf","comment":"31 pages,11 figures"},{"id":"http://arxiv.org/abs/2408.14358v3","updated":"2025-04-14T08:43:34Z","published":"2024-08-26T15:32:31Z","title":"An Embedding is Worth a Thousand Noisy Labels","summary":"  The performance of deep neural networks scales with dataset size and label\nquality, rendering the efficient mitigation of low-quality data annotations\ncrucial for building robust and cost-effective systems. Existing strategies to\naddress label noise exhibit severe limitations due to computational complexity\nand application dependency. In this work, we propose WANN, a Weighted Adaptive\nNearest Neighbor approach that builds on self-supervised feature\nrepresentations obtained from foundation models. To guide the weighted voting\nscheme, we introduce a reliability score $\\eta$, which measures the likelihood\nof a data label being correct. WANN outperforms reference methods, including a\nlinear layer trained with robust loss functions, on diverse datasets of varying\nsize and under various noise types and severities. WANN also exhibits superior\ngeneralization on imbalanced data compared to both Adaptive-NNs (ANN) and fixed\nk-NNs. Furthermore, the proposed weighting scheme enhances supervised\ndimensionality reduction under noisy labels. This yields a significant boost in\nclassification performance with 10x and 100x smaller image embeddings,\nminimizing latency and storage requirements. Our approach, emphasizing\nefficiency and explainability, emerges as a simple, robust solution to overcome\ninherent limitations of deep neural network training. The code is available at\nhttps://github.com/francescodisalvo05/wann-noisy-labels .\n","authors":["Francesco Di Salvo","Sebastian Doerrich","Ines Rieger","Christian Ledig"],"pdf_url":"https://arxiv.org/pdf/2408.14358v3.pdf","comment":"Accepted to Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2503.15166v2","updated":"2025-04-14T08:38:46Z","published":"2025-03-19T12:47:37Z","title":"Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive\n  Learning: Adapting Alignment Calibration to MERU","summary":"  Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC\n","authors":["Àlex Pujol Vidal","Sergio Escalera","Kamal Nasrollahi","Thomas B. Moeslund"],"pdf_url":"https://arxiv.org/pdf/2503.15166v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2504.03162v2","updated":"2025-04-14T08:32:27Z","published":"2025-04-04T04:42:38Z","title":"Beyond Progress Measures: Theoretical Insights into the Mechanism of\n  Grokking","summary":"  Grokking, referring to the abrupt improvement in test accuracy after extended\noverfitting, offers valuable insights into the mechanisms of model\ngeneralization. Existing researches based on progress measures imply that\ngrokking relies on understanding the optimization dynamics when the loss\nfunction is dominated solely by the weight decay term. However, we find that\nthis optimization merely leads to token uniformity, which is not a sufficient\ncondition for grokking. In this work, we investigate the grokking mechanism\nunderlying the Transformer in the task of prime number operations. Based on\ntheoretical analysis and experimental validation, we present the following\ninsights: (i) The weight decay term encourages uniformity across all tokens in\nthe embedding space when it is minimized. (ii) The occurrence of grokking is\njointly determined by the uniformity of the embedding space and the\ndistribution of the training dataset. Building on these insights, we provide a\nunified perspective for understanding various previously proposed progress\nmeasures and introduce a novel, concise, and effective progress measure that\ncould trace the changes in test loss more accurately. Finally, to demonstrate\nthe versatility of our theoretical framework, we design a dedicated dataset to\nvalidate our theory on ResNet-18, successfully showcasing the occurrence of\ngrokking. The code is released at https://github.com/Qihuai27/Grokking-Insight.\n","authors":["Zihan Gu","Ruoyu Chen","Hua Zhang","Yue Hu","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2504.03162v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.13283v4","updated":"2025-04-14T08:23:31Z","published":"2022-05-26T11:42:44Z","title":"Embedding Principle in Depth for the Loss Landscape Analysis of Deep\n  Neural Networks","summary":"  Understanding the relation between deep and shallow neural networks is\nextremely important for the theoretical study of deep learning. In this work,\nwe discover an embedding principle in depth that loss landscape of an NN\n\"contains\" all critical points of the loss landscapes for shallower NNs. The\nkey tool for our discovery is the critical lifting operator proposed in this\nwork that maps any critical point of a network to critical manifolds of any\ndeeper network while preserving the outputs. This principle provides new\ninsights to many widely observed behaviors of DNNs. Regarding the easy training\nof deep networks, we show that local minimum of an NN can be lifted to strict\nsaddle points of a deeper NN. Regarding the acceleration effect of batch\nnormalization, we demonstrate that batch normalization helps avoid the critical\nmanifolds lifted from shallower NNs by suppressing layer linearization. We also\nprove that increasing training data shrinks the lifted critical manifolds,\nwhich can result in acceleration of training as demonstrated in experiments.\nOverall, our discovery of the embedding principle in depth uncovers the\ndepth-wise hierarchical structure of deep learning loss landscape, which serves\nas a solid foundation for the further study about the role of depth for DNNs.\n","authors":["Zhiwei Bai","Tao Luo","Zhi-Qin John Xu","Yaoyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2205.13283v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09970v1","updated":"2025-04-14T08:21:41Z","published":"2025-04-14T08:21:41Z","title":"IsoSEL: Isometric Structural Entropy Learning for Deep Graph Clustering\n  in Hyperbolic Space","summary":"  Graph clustering is a longstanding topic in machine learning. In recent\nyears, deep learning methods have achieved encouraging results, but they still\nrequire predefined cluster numbers K, and typically struggle with imbalanced\ngraphs, especially in identifying minority clusters. The limitations motivate\nus to study a challenging yet practical problem: deep graph clustering without\nK considering the imbalance in reality. We approach this problem from a fresh\nperspective of information theory (i.e., structural information). In the\nliterature, structural information has rarely been touched in deep clustering,\nand the classic definition falls short in its discrete formulation, neglecting\nnode attributes and exhibiting prohibitive complexity. In this paper, we first\nestablish a new Differentiable Structural Information, generalizing the\ndiscrete formalism to continuous realm, so that the optimal partitioning tree,\nrevealing the cluster structure, can be created by the gradient\nbackpropagation. Theoretically, we demonstrate its capability in clustering\nwithout requiring K and identifying the minority clusters in imbalanced graphs,\nwhile reducing the time complexity to O(N) w.r.t. the number of nodes.\nSubsequently, we present a novel IsoSEL framework for deep graph clustering,\nwhere we design a hyperbolic neural network to learn the partitioning tree in\nthe Lorentz model of hyperbolic space, and further conduct Lorentz Tree\nContrastive Learning with isometric augmentation. As a result, the partitioning\ntree incorporates node attributes via mutual information maximization, while\nthe cluster assignment is refined by the proposed tree contrastive learning.\nExtensive experiments on five benchmark datasets show the IsoSEL outperforms 14\nrecent baselines by an average of +1.3% in NMI.\n","authors":["Li Sun","Zhenhao Huang","Yujie Wang","Hongbo Lv","Chunyang Liu","Hao Peng","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2504.09970v1.pdf","comment":"submitted to IEEE TPAMI, 33 pages, including technical appendix of 16\n  pages"},{"id":"http://arxiv.org/abs/2504.09967v1","updated":"2025-04-14T08:09:37Z","published":"2025-04-14T08:09:37Z","title":"Enhancing Multi-task Learning Capability of Medical Generalist\n  Foundation Model via Image-centric Multi-annotation Data","summary":"  The emergence of medical generalist foundation models has revolutionized\nconventional task-specific model development paradigms, aiming to better handle\nmultiple tasks through joint training on large-scale medical datasets. However,\nrecent advances prioritize simple data scaling or architectural component\nenhancement, while neglecting to re-examine multi-task learning from a\ndata-centric perspective. Critically, simply aggregating existing data\nresources leads to decentralized image-task alignment, which fails to cultivate\ncomprehensive image understanding or align with clinical needs for\nmulti-dimensional image interpretation. In this paper, we introduce the\nimage-centric multi-annotation X-ray dataset (IMAX), the first attempt to\nenhance the multi-task learning capabilities of medical multi-modal large\nlanguage models (MLLMs) from the data construction level. To be specific, IMAX\nis featured from the following attributes: 1) High-quality data curation. A\ncomprehensive collection of more than 354K entries applicable to seven\ndifferent medical tasks. 2) Image-centric dense annotation. Each X-ray image is\nassociated with an average of 4.10 tasks and 7.46 training entries, ensuring\nmulti-task representation richness per image. Compared to the general\ndecentralized multi-annotation X-ray dataset (DMAX), IMAX consistently\ndemonstrates significant multi-task average performance gains ranging from\n3.20% to 21.05% across seven open-source state-of-the-art medical MLLMs.\nMoreover, we investigate differences in statistical patterns exhibited by IMAX\nand DMAX training processes, exploring potential correlations between\noptimization dynamics and multi-task performance. Finally, leveraging the core\nconcept of IMAX data construction, we propose an optimized DMAX-based training\nstrategy to alleviate the dilemma of obtaining high-quality IMAX data in\npractical scenarios.\n","authors":["Xun Zhu","Fanbin Mo","Zheng Zhang","Jiaxi Wang","Yiming Shi","Ming Wu","Chuang Zhang","Miao Li","Ji Wu"],"pdf_url":"https://arxiv.org/pdf/2504.09967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09963v1","updated":"2025-04-14T08:00:20Z","published":"2025-04-14T08:00:20Z","title":"Towards Unbiased Federated Graph Learning: Label and Topology\n  Perspectives","summary":"  Federated Graph Learning (FGL) enables privacy-preserving, distributed\ntraining of graph neural networks without sharing raw data. Among its\napproaches, subgraph-FL has become the dominant paradigm, with most work\nfocused on improving overall node classification accuracy. However, these\nmethods often overlook fairness due to the complexity of node features, labels,\nand graph structures. In particular, they perform poorly on nodes with\ndisadvantaged properties, such as being in the minority class within subgraphs\nor having heterophilous connections (neighbors with dissimilar labels or\nmisleading features). This reveals a critical issue: high accuracy can mask\ndegraded performance on structurally or semantically marginalized nodes. To\naddress this, we advocate for two fairness goals: (1) improving representation\nof minority class nodes for class-wise fairness and (2) mitigating topological\nbias from heterophilous connections for topology-aware fairness. We propose\nFairFGL, a novel framework that enhances fairness through fine-grained graph\nmining and collaborative learning. On the client side, the History-Preserving\nModule prevents overfitting to dominant local classes, while the Majority\nAlignment Module refines representations of heterophilous majority-class nodes.\nThe Gradient Modification Module transfers minority-class knowledge from\nstructurally favorable clients to improve fairness. On the server side, FairFGL\nuploads only the most influenced subset of parameters to reduce communication\ncosts and better reflect local distributions. A cluster-based aggregation\nstrategy reconciles conflicting updates and curbs global majority dominance .\nExtensive evaluations on eight benchmarks show FairFGL significantly improves\nminority-group performance , achieving up to a 22.62 percent Macro-F1 gain\nwhile enhancing convergence over state-of-the-art baselines.\n","authors":["Zhengyu Wu","Boyang Pang","Xunkai Li","Yinlin Zhu","Daohan Su","Bowen Fan","Rong-Hua Li","Guoren Wang","Chenghu Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.09963v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2405.15540v2","updated":"2025-04-14T07:52:41Z","published":"2024-05-24T13:28:48Z","title":"Bundle Neural Networks for message diffusion on graphs","summary":"  The dominant paradigm for learning on graph-structured data is message\npassing. Despite being a strong inductive bias, the local message passing\nmechanism suffers from pathological issues such as over-smoothing,\nover-squashing, and limited node-level expressivity. To address these\nlimitations we propose Bundle Neural Networks (BuNN), a new type of GNN that\noperates via message diffusion over flat vector bundles - structures analogous\nto connections on Riemannian manifolds that augment the graph by assigning to\neach node a vector space and an orthogonal map. A BuNN layer evolves the\nfeatures according to a diffusion-type partial differential equation. When\ndiscretized, BuNNs are a special case of Sheaf Neural Networks (SNNs), a\nrecently proposed MPNN capable of mitigating over-smoothing. The continuous\nnature of message diffusion enables BuNNs to operate on larger scales of the\ngraph and, therefore, to mitigate over-squashing. Finally, we prove that BuNN\ncan approximate any feature transformation over nodes on any (potentially\ninfinite) family of graphs given injective positional encodings, resulting in\nuniversal node-level expressivity. We support our theory via synthetic\nexperiments and showcase the strong empirical performance of BuNNs over a range\nof real-world tasks, achieving state-of-the-art results on several standard\nbenchmarks in transductive and inductive settings.\n","authors":["Jacob Bamberger","Federico Barbero","Xiaowen Dong","Michael M. Bronstein"],"pdf_url":"https://arxiv.org/pdf/2405.15540v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09956v1","updated":"2025-04-14T07:44:34Z","published":"2025-04-14T07:44:34Z","title":"Semantic Depth Matters: Explaining Errors of Deep Vision Networks\n  through Perceived Class Similarities","summary":"  Understanding deep neural network (DNN) behavior requires more than\nevaluating classification accuracy alone; analyzing errors and their\npredictability is equally crucial. Current evaluation methodologies lack\ntransparency, particularly in explaining the underlying causes of network\nmisclassifications. To address this, we introduce a novel framework that\ninvestigates the relationship between the semantic hierarchy depth perceived by\na network and its real-data misclassification patterns. Central to our\nframework is the Similarity Depth (SD) metric, which quantifies the semantic\nhierarchy depth perceived by a network along with a method of evaluation of how\nclosely the network's errors align with its internally perceived similarity\nstructure. We also propose a graph-based visualization of model semantic\nrelationships and misperceptions. A key advantage of our approach is that\nleveraging class templates -- representations derived from classifier layer\nweights -- is applicable to already trained networks without requiring\nadditional data or experiments. Our approach reveals that deep vision networks\nencode specific semantic hierarchies and that high semantic depth improves the\ncompliance between perceived class similarities and actual errors.\n","authors":["Katarzyna Filus","Michał Romaszewski","Mateusz Żarski"],"pdf_url":"https://arxiv.org/pdf/2504.09956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20021v4","updated":"2025-04-14T07:42:52Z","published":"2024-07-29T13:57:40Z","title":"MimiQ: Low-Bit Data-Free Quantization of Vision Transformers with\n  Encouraging Inter-Head Attention Similarity","summary":"  Data-free quantization (DFQ) is a technique that creates a lightweight\nnetwork from its full-precision counterpart without the original training data,\noften through a synthetic dataset. Although several DFQ methods have been\nproposed for vision transformer (ViT) architectures, they fail to achieve\nefficacy in low-bit settings. Examining the existing methods, we observe that\ntheir synthetic data produce misaligned attention maps, while those of the real\nsamples are highly aligned. From this observation, we find that aligning\nattention maps of synthetic data helps improve the overall performance of\nquantized ViTs. Motivated by this finding, we devise MimiQ, a novel DFQ method\ndesigned for ViTs that enhances inter-head attention similarity. First, we\ngenerate synthetic data by aligning head-wise attention outputs from each\nspatial query patch. Then, we align the attention maps of the quantized network\nto those of the full-precision teacher by applying head-wise structural\nattention distillation. The experimental results show that the proposed method\nsignificantly outperforms baselines, setting a new state-of-the-art for\nViT-DFQ. This paper is an extended version of our work published in the\nproceedings of AAAI 2025, including additional supplementary material.\n","authors":["Kanghyun Choi","Hye Yoon Lee","Dain Kwon","SunJong Park","Kyuyeun Kim","Noseong Park","Jonghyun Choi","Jinho Lee"],"pdf_url":"https://arxiv.org/pdf/2407.20021v4.pdf","comment":"Published to AAAI 2025"},{"id":"http://arxiv.org/abs/2502.17494v5","updated":"2025-04-14T07:39:56Z","published":"2025-02-20T22:35:52Z","title":"External Large Foundation Model: How to Efficiently Serve Trillions of\n  Parameters for Online Ads Recommendation","summary":"  Ads recommendation is a prominent service of online advertising systems and\nhas been actively studied. Recent studies indicate that scaling-up and advanced\ndesign of the recommendation model can bring significant performance\nimprovement. However, with a larger model scale, such prior studies have a\nsignificantly increasing gap from industry as they often neglect two\nfundamental challenges in industrial-scale applications. First, training and\ninference budgets are restricted for the model to be served, exceeding which\nmay incur latency and impair user experience. Second, large-volume data arrive\nin a streaming mode with data distributions dynamically shifting, as new\nusers/ads join and existing users/ads leave the system. We propose the External\nLarge Foundation Model (ExFM) framework to address the overlooked challenges.\nSpecifically, we develop external distillation and a data augmentation system\n(DAS) to control the computational cost of training/inference while maintaining\nhigh performance. We design the teacher in a way like a foundation model (FM)\nthat can serve multiple students as vertical models (VMs) to amortize its\nbuilding cost. We propose Auxiliary Head and Student Adapter to mitigate the\ndata distribution gap between FM and VMs caused by the streaming data issue.\nComprehensive experiments on internal industrial-scale applications and public\ndatasets demonstrate significant performance gain by ExFM.\n","authors":["Mingfu Liang","Xi Liu","Rong Jin","Boyang Liu","Qiuling Suo","Qinghai Zhou","Song Zhou","Laming Chen","Hua Zheng","Zhiyuan Li","Shali Jiang","Jiyan Yang","Xiaozhen Xia","Fan Yang","Yasmine Badr","Ellie Wen","Shuyu Xu","Hansey Chen","Zhengyu Zhang","Jade Nie","Chunzhi Yang","Zhichen Zeng","Weilin Zhang","Xingliang Huang","Qianru Li","Shiquan Wang","Evelyn Lyu","Wenjing Lu","Rui Zhang","Wenjun Wang","Jason Rudy","Mengyue Hang","Kai Wang","Yinbin Ma","Shuaiwen Wang","Sihan Zeng","Tongyi Tang","Xiaohan Wei","Longhao Jin","Jamey Zhang","Marcus Chen","Jiayi Zhang","Angie Huang","Chi Zhang","Zhengli Zhao","Jared Yang","Qiang Jin","Xian Chen","Amit Anand Amlesahwaram","Lexi Song","Liang Luo","Yuchen Hao","Nan Xiao","Yavuz Yetim","Luoshang Pan","Gaoxiang Liu","Yuxi Hu","Yuzhen Huang","Jackie Xu","Rich Zhu","Xin Zhang","Yiqun Liu","Hang Yin","Yuxin Chen","Buyun Zhang","Xiaoyi Liu","Xingyuan Wang","Wenguang Mao","Zhijing Li","Zhehui Zhou","Feifan Gu","Qin Huang","Chonglin Sun","Nancy Yu","Shuo Gu","Shupin Mao","Benjamin Au","Jingzheng Qin","Peggy Yao","Jae-Woo Choi","Bin Gao","Ernest Wang","Lei Zhang","Wen-Yen Chen","Ted Lee","Jay Zha","Yi Meng","Alex Gong","Edison Gao","Alireza Vahdatpour","Yiping Han","Yantao Yao","Toshinari Kureha","Shuo Chang","Musharaf Sultan","John Bocharov","Sagar Chordia","Xiaorui Gan","Peng Sun","Rocky Liu","Bo Long","Wenlin Chen","Santanu Kolay","Huayu Li"],"pdf_url":"https://arxiv.org/pdf/2502.17494v5.pdf","comment":"Accepted by the ACM Web Conference (WWW) 2025 Industrial Track as\n  Oral Presentation"},{"id":"http://arxiv.org/abs/2504.09951v1","updated":"2025-04-14T07:26:34Z","published":"2025-04-14T07:26:34Z","title":"Towards Weaker Variance Assumptions for Stochastic Optimization","summary":"  We revisit a classical assumption for analyzing stochastic gradient\nalgorithms where the squared norm of the stochastic subgradient (or the\nvariance for smooth problems) is allowed to grow as fast as the squared norm of\nthe optimization variable. We contextualize this assumption in view of its\ninception in the 1960s, its seemingly independent appearance in the recent\nliterature, its relationship to weakest-known variance assumptions for\nanalyzing stochastic gradient algorithms, and its relevance in deterministic\nproblems for non-Lipschitz nonsmooth convex optimization. We build on and\nextend a connection recently made between this assumption and the Halpern\niteration. For convex nonsmooth, and potentially stochastic, optimization, we\nanalyze horizon-free, anytime algorithms with last-iterate rates. For problems\nbeyond simple constrained optimization, such as convex problems with functional\nconstraints or regularized convex-concave min-max problems, we obtain rates for\noptimality measures that do not require boundedness of the feasible set.\n","authors":["Ahmet Alacaoglu","Yura Malitsky","Stephen J. Wright"],"pdf_url":"https://arxiv.org/pdf/2504.09951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01990v5","updated":"2025-04-14T07:09:15Z","published":"2024-09-03T15:35:01Z","title":"Designing Large Foundation Models for Efficient Training and Inference:\n  A Survey","summary":"  This paper focuses on modern efficient training and inference technologies on\nfoundation models and illustrates them from two perspectives: model and system\ndesign. Model and System Design optimize LLM training and inference from\ndifferent aspects to save computational resources, making LLMs more efficient,\naffordable, and more accessible. The paper list repository is available at\nhttps://github.com/NoakLiu/Efficient-Foundation-Models-Survey.\n","authors":["Dong Liu","Yanxuan Yu","Yite Wang","Jing Wu","Zhongwei Wan","Sina Alinejad","Benjamin Lengerich","Ying Nian Wu"],"pdf_url":"https://arxiv.org/pdf/2409.01990v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09941v1","updated":"2025-04-14T07:04:10Z","published":"2025-04-14T07:04:10Z","title":"FedRecon: Missing Modality Reconstruction in Distributed Heterogeneous\n  Environments","summary":"  Multimodal data are often incomplete and exhibit Non-Independent and\nIdentically Distributed (Non-IID) characteristics in real-world scenarios.\nThese inherent limitations lead to both modality heterogeneity through partial\nmodality absence and data heterogeneity from distribution divergence, creating\nfundamental challenges for effective federated learning (FL). To address these\ncoupled challenges, we propose FedRecon, the first method targeting\nsimultaneous missing modality reconstruction and Non-IID adaptation in\nmultimodal FL. Our approach first employs a lightweight Multimodal Variational\nAutoencoder (MVAE) to reconstruct missing modalities while preserving\ncross-modal consistency. Distinct from conventional imputation methods, we\nachieve sample-level alignment through a novel distribution mapping mechanism\nthat guarantees both data consistency and completeness. Additionally, we\nintroduce a strategy employing global generator freezing to prevent\ncatastrophic forgetting, which in turn mitigates Non-IID fluctuations.\nExtensive evaluations on multimodal datasets demonstrate FedRecon's superior\nperformance in modality reconstruction under Non-IID conditions, surpassing\nstate-of-the-art methods.\n","authors":["Junming Liu","Guosun Zeng","Ding Wang","Yanting Gao","Yufei Jin"],"pdf_url":"https://arxiv.org/pdf/2504.09941v1.pdf","comment":"18 pages, 32 figures"},{"id":"http://arxiv.org/abs/2504.09940v1","updated":"2025-04-14T07:02:34Z","published":"2025-04-14T07:02:34Z","title":"TianQuan-Climate: A Subseasonal-to-Seasonal Global Weather Model via\n  Incorporate Climatology State","summary":"  Subseasonal forecasting serves as an important support for Sustainable\nDevelopment Goals (SDGs), such as climate challenges, agricultural yield and\nsustainable energy production. However, subseasonal forecasting is a complex\ntask in meteorology due to dissipating initial conditions and delayed external\nforces. Although AI models are increasingly pushing the boundaries of this\nforecasting limit, they face two major challenges: error accumulation and\nSmoothness. To address these two challenges, we propose Climate Furnace\nSubseasonal-to-Seasonal (TianQuan-Climate), a novel machine learning model\ndesigned to provide global daily mean forecasts up to 45 days, covering five\nupper-air atmospheric variables at 13 pressure levels and two surface\nvariables. Our proposed TianQuan-Climate has two advantages: 1) it utilizes a\nmulti-model prediction strategy to reduce system error impacts in long-term\nsubseasonal forecasts; 2) it incorporates a Content Fusion Module for\nclimatological integration and extends ViT with uncertainty blocks (UD-ViT) to\nimprove generalization by learning from uncertainty. We demonstrate the\neffectiveness of TianQuan-Climate on benchmarks for weather forecasting and\nclimate projections within the 15 to 45-day range, where TianQuan-Climate\noutperforms existing numerical and AI methods.\n","authors":["Guowen Li","Xintong Liu","Shilei Cao","Haoyuan Liang","Mengxuan Chen","Lixian Zhang","Jinxiao Zhang","Jiuke Wang","Meng Jin","Juepeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2504.09940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07993v2","updated":"2025-04-14T06:59:33Z","published":"2025-04-08T12:45:01Z","title":"Towards Simple Machine Learning Baselines for GNSS RFI Detection","summary":"  Machine learning research in GNSS radio frequency interference (RFI)\ndetection often lacks a clear empirical justification for the choice of deep\nlearning architectures over simpler machine learning approaches. In this work,\nwe argue for a change in research direction-from developing ever more complex\ndeep learning models to carefully assessing their real-world effectiveness in\ncomparison to interpretable and lightweight machine learning baselines. Our\nfindings reveal that state-of-the-art deep learning models frequently fail to\noutperform simple, well-engineered machine learning methods in the context of\nGNSS RFI detection. Leveraging a unique large-scale dataset collected by the\nSwiss Air Force and Swiss Air-Rescue (Rega), and preprocessed by Swiss Air\nNavigation Services Ltd. (Skyguide), we demonstrate that a simple baseline\nmodel achieves 91\\% accuracy in detecting GNSS RFI, outperforming more complex\ndeep learning counterparts. These results highlight the effectiveness of\npragmatic solutions and offer valuable insights to guide future research in\nthis critical application domain.\n","authors":["Viktor Ivanov","Richard C. Wilson","Maurizio Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2504.07993v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09936v1","updated":"2025-04-14T06:58:00Z","published":"2025-04-14T06:58:00Z","title":"KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference","summary":"  Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets.\n","authors":["Yuxuan Tian","Zihan Wang","Yebo Peng","Aomufei Yuan","Zhiming Wang","Bairen Yi","Xin Liu","Yong Cui","Tong Yang"],"pdf_url":"https://arxiv.org/pdf/2504.09936v1.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.17417v2","updated":"2025-04-14T06:45:58Z","published":"2025-03-21T01:17:44Z","title":"Generative Modeling of Class Probability for Multi-Modal Representation\n  Learning","summary":"  Multi-modal understanding plays a crucial role in artificial intelligence by\nenabling models to jointly interpret inputs from different modalities. However,\nconventional approaches such as contrastive learning often struggle with\nmodality discrepancies, leading to potential misalignments. In this paper, we\npropose a novel class anchor alignment approach that leverages class\nprobability distributions for multi-modal representation learning. Our method,\nClass-anchor-ALigned generative Modeling (CALM), encodes class anchors as\nprompts to generate and align class probability distributions for each\nmodality, enabling more effective alignment. Furthermore, we introduce a\ncross-modal probabilistic variational autoencoder to model uncertainty in the\nalignment, enhancing the ability to capture deeper relationships between\nmodalities and data variations. Extensive experiments on four benchmark\ndatasets demonstrate that our approach significantly outperforms\nstate-of-the-art methods, especially in out-of-domain evaluations. This\nhighlights its superior generalization capabilities in multi-modal\nrepresentation learning.\n","authors":["Jungkyoo Shin","Bumsoo Kim","Eunwoo Kim"],"pdf_url":"https://arxiv.org/pdf/2503.17417v2.pdf","comment":"To appear in CVPR 2025 (Highlight)"},{"id":"http://arxiv.org/abs/2504.09930v1","updated":"2025-04-14T06:44:13Z","published":"2025-04-14T06:44:13Z","title":"Multi-objective Bayesian Optimization With Mixed-categorical Design\n  Variables for Expensive-to-evaluate Aeronautical Applications","summary":"  This work aims at developing new methodologies to optimize computational\ncostly complex systems (e.g., aeronautical engineering systems). The proposed\nsurrogate-based method (often called Bayesian optimization) uses adaptive\nsampling to promote a trade-off between exploration and exploitation. Our\nin-house implementation, called SEGOMOE, handles a high number of design\nvariables (continuous, discrete or categorical) and nonlinearities by combining\nmixtures of experts for the objective and/or the constraints. Additionally, the\nmethod handles multi-objective optimization settings, as it allows the\nconstruction of accurate Pareto fronts with a minimal number of function\nevaluations. Different infill criteria have been implemented to handle multiple\nobjectives with or without constraints. The effectiveness of the proposed\nmethod was tested on practical aeronautical applications within the context of\nthe European Project AGILE 4.0 and demonstrated favorable results. A first\nexample concerns a retrofitting problem where a comparison between two\noptimizers have been made. A second example introduces hierarchical variables\nto deal with architecture system in order to design an aircraft family. The\nthird example increases drastically the number of categorical variables as it\ncombines aircraft design, supply chain and manufacturing process. In this\narticle, we show, on three different realistic problems, various aspects of our\noptimization codes thanks to the diversity of the treated aircraft problems.\n","authors":["Nathalie Bartoli","Thierry Lefebvre","Rémi Lafage","Paul Saves","Youssef Diouane","Joseph Morlier","Jasper Bussemaker","Giuseppa Donelli","Joao Marcos Gomes de Mello","Massimo Mandorino","Pierluigi Della Vecchia"],"pdf_url":"https://arxiv.org/pdf/2504.09930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09929v1","updated":"2025-04-14T06:41:15Z","published":"2025-04-14T06:41:15Z","title":"Moderate Actor-Critic Methods: Controlling Overestimation Bias via\n  Expectile Loss","summary":"  Overestimation is a fundamental characteristic of model-free reinforcement\nlearning (MF-RL), arising from the principles of temporal difference learning\nand the approximation of the Q-function. To address this challenge, we propose\na novel moderate target in the Q-function update, formulated as a convex\noptimization of an overestimated Q-function and its lower bound. Our primary\ncontribution lies in the efficient estimation of this lower bound through the\nlower expectile of the Q-value distribution conditioned on a state. Notably,\nour moderate target integrates seamlessly into state-of-the-art (SOTA) MF-RL\nalgorithms, including Deep Deterministic Policy Gradient (DDPG) and Soft Actor\nCritic (SAC). Experimental results validate the effectiveness of our moderate\ntarget in mitigating overestimation bias in DDPG, SAC, and distributional RL\nalgorithms.\n","authors":["Ukjo Hwang","Songnam Hong"],"pdf_url":"https://arxiv.org/pdf/2504.09929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13019v5","updated":"2025-04-14T06:22:56Z","published":"2023-10-18T18:50:39Z","title":"Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class\n  Manipulation Using DeepFool Algorithm","summary":"  The susceptibility of deep neural networks (DNNs) to adversarial attacks\nundermines their reliability across numerous applications, underscoring the\nnecessity for an in-depth exploration of these vulnerabilities and the\nformulation of robust defense strategies. The DeepFool algorithm by\nMoosavi-Dezfooli et al. (2016) represents a pivotal step in identifying minimal\nperturbations required to induce misclassification of input images.\nNonetheless, its generic methodology falls short in scenarios necessitating\ntargeted interventions. Additionally, previous research studies have\npredominantly concentrated on the success rate of attacks without adequately\naddressing the consequential distortion of images, the maintenance of image\nquality, or the confidence threshold required for misclassification. To bridge\nthese gaps, we introduce the Enhanced Targeted DeepFool (ET DeepFool)\nalgorithm, an evolution of DeepFool that not only facilitates the specification\nof desired misclassification targets but also incorporates a configurable\nminimum confidence score. Our empirical investigations demonstrate the\nsuperiority of this refined approach in maintaining the integrity of images and\nminimizing perturbations across a variety of DNN architectures. Unlike previous\niterations, such as the Targeted DeepFool by Gajjar et al. (2022), our method\ngrants unparalleled control over the perturbation process, enabling precise\nmanipulation of model responses. Preliminary outcomes reveal that certain\nmodels, including AlexNet and the advanced Vision Transformer, display\ncommendable robustness to such manipulations. This discovery of varying levels\nof model robustness, as unveiled through our confidence level adjustments,\ncould have far-reaching implications for the field of image recognition. Our\ncode is available at https://github.com/FazleLabib/et_deepfool.\n","authors":["S. M. Fazle Rabby Labib","Joyanta Jyoti Mondal","Meem Arafat Manab","Xi Xiao","Sarfaraz Newaz"],"pdf_url":"https://arxiv.org/pdf/2310.13019v5.pdf","comment":"18 pages, 5 figures. Accepted by Nature Scientific Reports"},{"id":"http://arxiv.org/abs/2406.18145v4","updated":"2025-04-14T06:19:02Z","published":"2024-06-26T07:53:48Z","title":"Beyond Statistical Estimation: Differentially Private Individual\n  Computation via Shuffling","summary":"  In data-driven applications, preserving user privacy while enabling valuable\ncomputations remains a critical challenge. Technologies like differential\nprivacy have been pivotal in addressing these concerns. The shuffle model of DP\nrequires no trusted curators and can achieve high utility by leveraging the\nprivacy amplification effect yielded from shuffling. These benefits have led to\nsignificant interest in the shuffle model. However, the computation tasks in\nthe shuffle model are limited to statistical estimation, making it inapplicable\nto real-world scenarios in which each user requires a personalized output. This\npaper introduces a novel paradigm termed Private Individual Computation (PIC),\nexpanding the shuffle model to support a broader range of\npermutation-equivariant computations. PIC enables personalized outputs while\npreserving privacy, and enjoys privacy amplification through shuffling. We\npropose a concrete protocol that realizes PIC. By using one-time public keys,\nour protocol enables users to receive their outputs without compromising\nanonymity, which is essential for privacy amplification. Additionally, we\npresent an optimal randomizer, the Minkowski Response, designed for the PIC\nmodel to enhance utility. We formally prove the security and privacy properties\nof the PIC protocol. Theoretical analysis and empirical evaluations demonstrate\nPIC's capability in handling non-statistical computation tasks, and the\nefficacy of PIC and the Minkowski randomizer in achieving superior utility\ncompared to existing solutions.\n","authors":["Shaowei Wang","Changyu Dong","Xiangfu Song","Jin Li","Zhili Zhou","Di Wang","Han Wu"],"pdf_url":"https://arxiv.org/pdf/2406.18145v4.pdf","comment":"USENIX Security 2025. Full version with corrected consts in Theorem\n  5.2 and Appendix A"},{"id":"http://arxiv.org/abs/2504.09895v1","updated":"2025-04-14T05:43:21Z","published":"2025-04-14T05:43:21Z","title":"Learning from Reference Answers: Versatile Language Model Alignment\n  without Binary Human Preference Data","summary":"  Large language models~(LLMs) are expected to be helpful, harmless, and\nhonest. In various alignment scenarios, such as general human preference,\nsafety, and confidence alignment, binary preference data collection and reward\nmodeling are resource-intensive but necessary for human preference\ntransferring. In this work, we explore using the similarity between sampled\ngenerations and high-quality reference answers as an alternative reward\nfunction for LLM alignment. Using similarity as a reward circumvents training\nreward models, and collecting a single reference answer potentially costs less\ntime than constructing binary preference pairs when multiple candidates are\navailable. Specifically, we develop \\textit{RefAlign}, a versatile\nREINFORCE-style alignment algorithm, which is free of reference and reward\nmodels. Instead, RefAlign utilizes BERTScore between sampled generations and\nhigh-quality reference answers as the surrogate reward. Beyond general human\npreference optimization, RefAlign can be readily extended to diverse scenarios,\nsuch as safety and confidence alignment, by incorporating the similarity reward\nwith task-related objectives. In various scenarios, {RefAlign} demonstrates\ncomparable performance to previous alignment methods while offering high\nefficiency.\n","authors":["Shuai Zhao","Linchao Zhu","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2504.09895v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2410.01432v2","updated":"2025-04-14T05:09:11Z","published":"2024-10-02T11:33:13Z","title":"Adaptive teachers for amortized samplers","summary":"  Amortized inference is the task of training a parametric model, such as a\nneural network, to approximate a distribution with a given unnormalized density\nwhere exact sampling is intractable. When sampling is implemented as a\nsequential decision-making process, reinforcement learning (RL) methods, such\nas generative flow networks, can be used to train the sampling policy.\nOff-policy RL training facilitates the discovery of diverse, high-reward\ncandidates, but existing methods still face challenges in efficient\nexploration. We propose to use an adaptive training distribution (the \\teacher)\nto guide the training of the primary amortized sampler (the \\student). The\n\\teacher, an auxiliary behavior model, is trained to sample high-loss regions\nof the \\student and can generalize across unexplored modes, thereby enhancing\nmode coverage by providing an efficient training curriculum. We validate the\neffectiveness of this approach in a synthetic environment designed to present\nan exploration challenge, two diffusion-based sampling tasks, and four\nbiochemical discovery tasks demonstrating its ability to improve sample\nefficiency and mode coverage. Source code is available at\nhttps://github.com/alstn12088/adaptive-teacher.\n","authors":["Minsu Kim","Sanghyeok Choi","Taeyoung Yun","Emmanuel Bengio","Leo Feng","Jarrid Rector-Brooks","Sungsoo Ahn","Jinkyoo Park","Nikolay Malkin","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2410.01432v2.pdf","comment":"ICLR 2025, 27 pages, 12 figures"},{"id":"http://arxiv.org/abs/2501.06089v2","updated":"2025-04-14T04:58:27Z","published":"2025-01-10T16:39:01Z","title":"Towards Developing Socially Compliant Automated Vehicles: Advances,\n  Expert Insights, and A Conceptual Framework","summary":"  Automated Vehicles (AVs) hold promise for revolutionizing transportation by\nimproving road safety, traffic efficiency, and overall mobility. Despite the\nsteady advancement in high-level AVs in recent years, the transition to full\nautomation entails a period of mixed traffic, where AVs of varying automation\nlevels coexist with human-driven vehicles (HDVs). Making AVs socially compliant\nand understood by human drivers is expected to improve the safety and\nefficiency of mixed traffic. Thus, ensuring AVs' compatibility with HDVs and\nsocial acceptance is crucial for their successful and seamless integration into\nmixed traffic. However, research in this critical area of developing Socially\nCompliant AVs (SCAVs) remains sparse. This study carries out the first\ncomprehensive scoping review to assess the current state of the art in\ndeveloping SCAVs, identifying key concepts, methodological approaches, and\nresearch gaps. An informal expert interview was also conducted to discuss the\nliterature review results and identify critical research gaps and expectations\ntowards SCAVs. Based on the scoping review and expert interview input, a\nconceptual framework is proposed for the development of SCAVs. The conceptual\nframework is evaluated using an online survey targeting researchers,\ntechnicians, policymakers, and other relevant professionals worldwide. The\nsurvey results provide valuable validation and insights, affirming the\nsignificance of the proposed conceptual framework in tackling the challenges of\nintegrating AVs into mixed-traffic environments. Additionally, future research\nperspectives and suggestions are discussed, contributing to the research and\ndevelopment agenda of SCAVs.\n","authors":["Yongqi Dong","Bart van Arem","Haneen Farah"],"pdf_url":"https://arxiv.org/pdf/2501.06089v2.pdf","comment":"58 pages, 13 figures, accepted by the Journal of Communications in\n  Transportation Research"},{"id":"http://arxiv.org/abs/2504.09873v1","updated":"2025-04-14T04:42:00Z","published":"2025-04-14T04:42:00Z","title":"Truncated Matrix Completion - An Empirical Study","summary":"  Low-rank Matrix Completion (LRMC) describes the problem where we wish to\nrecover missing entries of partially observed low-rank matrix. Most existing\nmatrix completion work deals with sampling procedures that are independent of\nthe underlying data values. While this assumption allows the derivation of nice\ntheoretical guarantees, it seldom holds in real-world applications. In this\npaper, we consider various settings where the sampling mask is dependent on the\nunderlying data values, motivated by applications in sensing, sequential\ndecision-making, and recommender systems. Through a series of experiments, we\nstudy and compare the performance of various LRMC algorithms that were\noriginally successful for data-independent sampling patterns.\n","authors":["Rishhabh Naik","Nisarg Trivedi","Davoud Ataee Tarzanagh","Laura Balzano"],"pdf_url":"https://arxiv.org/pdf/2504.09873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09870v1","updated":"2025-04-14T04:29:46Z","published":"2025-04-14T04:29:46Z","title":"Ember: A Compiler for Efficient Embedding Operations on Decoupled\n  Access-Execute Architectures","summary":"  Irregular embedding lookups are a critical bottleneck in recommender models,\nsparse large language models, and graph learning models. In this paper, we\nfirst demonstrate that, by offloading these lookups to specialized access\nunits, Decoupled Access-Execute (DAE) processors achieve 2.6$\\times$ higher\nperformance and 6.4$\\times$ higher performance/watt than GPUs on end-to-end\nmodels. Then, we propose the Ember compiler for automatically generating\noptimized DAE code from PyTorch and TensorFlow. Conversely from other DAE\ncompilers, Ember features multiple intermediate representations specifically\ndesigned for different optimization levels. In this way, Ember can implement\nall optimizations to match the performance of hand-written code, unlocking the\nfull potential of DAE architectures at scale.\n","authors":["Marco Siracusa","Olivia Hsu","Victor Soria-Pardos","Joshua Randall","Arnaud Grasset","Eric Biscondi","Doug Joseph","Randy Allen","Fredrik Kjolstad","Miquel Moretó Planas","Adrià Armejach"],"pdf_url":"https://arxiv.org/pdf/2504.09870v1.pdf","comment":"14 pages, 19 figures, under review"},{"id":"http://arxiv.org/abs/2504.09862v1","updated":"2025-04-14T04:18:25Z","published":"2025-04-14T04:18:25Z","title":"RadarLLM: Empowering Large Language Models to Understand Human Motion\n  from Millimeter-wave Point Cloud Sequence","summary":"  Millimeter-wave radar provides a privacy-preserving solution for human motion\nanalysis, yet its sparse point clouds pose significant challenges for semantic\nunderstanding. We present Radar-LLM, the first framework that leverages large\nlanguage models (LLMs) for human motion understanding using millimeter-wave\nradar as the sensing modality. Our approach introduces two key innovations: (1)\na motion-guided radar tokenizer based on our Aggregate VQ-VAE architecture that\nincorporates deformable body templates and masked trajectory modeling to encode\nspatiotemporal point clouds into compact semantic tokens, and (2) a radar-aware\nlanguage model that establishes cross-modal alignment between radar and text in\na shared embedding space. To address data scarcity, we introduce a\nphysics-aware synthesis pipeline that generates realistic radar-text pairs from\nmotion-text datasets. Extensive experiments demonstrate that Radar-LLM achieves\nstate-of-the-art performance across both synthetic and real-world benchmarks,\nenabling accurate translation of millimeter-wave signals to natural language\ndescriptions. This breakthrough facilitates comprehensive motion understanding\nin privacy-sensitive applications like healthcare and smart homes. We will\nrelease the full implementation to support further research on\nhttps://inowlzy.github.io/RadarLLM/.\n","authors":["Zengyuan Lai","Jiarui Yang","Songpengcheng Xia","Lizhou Lin","Lan Sun","Renwen Wang","Jianran Liu","Qi Wu","Ling Pei"],"pdf_url":"https://arxiv.org/pdf/2504.09862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03974v2","updated":"2025-04-14T04:16:25Z","published":"2024-10-04T23:27:33Z","title":"Robust Barycenter Estimation using Semi-Unbalanced Neural Optimal\n  Transport","summary":"  Aggregating data from multiple sources can be formalized as an Optimal\nTransport (OT) barycenter problem, which seeks to compute the average of\nprobability distributions with respect to OT discrepancies. However, in\nreal-world scenarios, the presence of outliers and noise in the data measures\ncan significantly hinder the performance of traditional statistical methods for\nestimating OT barycenters. To address this issue, we propose a novel scalable\napproach for estimating the robust continuous barycenter, leveraging the dual\nformulation of the (semi-)unbalanced OT problem. To the best of our knowledge,\nthis paper is the first attempt to develop an algorithm for robust barycenters\nunder the continuous distribution setup. Our method is framed as a min-max\noptimization problem and is adaptable to general cost functions. We rigorously\nestablish the theoretical underpinnings of the proposed method and demonstrate\nits robustness to outliers and class imbalance through a number of illustrative\nexperiments. Our source code is publicly available at\nhttps://github.com/milenagazdieva/U-NOTBarycenters.\n","authors":["Milena Gazdieva","Jaemoo Choi","Alexander Kolesov","Jaewoong Choi","Petr Mokrov","Alexander Korotin"],"pdf_url":"https://arxiv.org/pdf/2410.03974v2.pdf","comment":"30 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.08201v2","updated":"2025-04-14T04:08:40Z","published":"2025-04-11T02:06:20Z","title":"Neural Encoding and Decoding at Scale","summary":"  Recent work has demonstrated that large-scale, multi-animal models are\npowerful tools for characterizing the relationship between neural activity and\nbehavior. Current large-scale approaches, however, focus exclusively on either\npredicting neural activity from behavior (encoding) or predicting behavior from\nneural activity (decoding), limiting their ability to capture the bidirectional\nrelationship between neural activity and behavior. To bridge this gap, we\nintroduce a multimodal, multi-task model that enables simultaneous Neural\nEncoding and Decoding at Scale (NEDS). Central to our approach is a novel\nmulti-task-masking strategy, which alternates between neural, behavioral,\nwithin-modality, and cross-modality masking. We pretrain our method on the\nInternational Brain Laboratory (IBL) repeated site dataset, which includes\nrecordings from 83 animals performing the same visual decision-making task. In\ncomparison to other large-scale models, we demonstrate that NEDS achieves\nstate-of-the-art performance for both encoding and decoding when pretrained on\nmulti-animal data and then fine-tuned on new animals. Surprisingly, NEDS's\nlearned embeddings exhibit emergent properties: even without explicit training,\nthey are highly predictive of the brain regions in each recording. Altogether,\nour approach is a step towards a foundation model of the brain that enables\nseamless translation between neural activity and behavior.\n","authors":["Yizi Zhang","Yanchen Wang","Mehdi Azabou","Alexandre Andre","Zixuan Wang","Hanrui Lyu","The International Brain Laboratory","Eva Dyer","Liam Paninski","Cole Hurwitz"],"pdf_url":"https://arxiv.org/pdf/2504.08201v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16340v4","updated":"2025-04-14T04:08:16Z","published":"2025-03-20T16:57:15Z","title":"Deep learning framework for action prediction reveals multi-timescale\n  locomotor control","summary":"  Modeling human movement in real-world tasks is a fundamental goal for motor\ncontrol, biomechanics, and rehabilitation engineering. However, existing models\nof essential tasks like locomotion are not applicable across varying terrain,\nmechanical conditions, and sensory contexts. This is at least in part due to\nsimplifying assumptions like linear and fixed timescales mappings between\ninputs and future actions, which may not be broadly applicable. Here, we\ndevelop a deep learning-based framework for action prediction, outperforming\ntraditional models across multiple contexts (walking and running, treadmill and\noverground, varying terrains) and input modalities (multiple body states,\nvisual gaze). We find that neural network architectures with flexible input\nhistory-dependence, like GRU and Transformer, and with architecture-dependent\ntrial embeddings perform best overall. By quantifying the model's predictions\nrelative to an autoregressive baseline, we identify context- and\nmodality-dependent timescales. These analyses reveal that there is greater\nreliance on fast-timescale predictions in complex terrain, gaze predicts future\nfoot placement before body states, and the full-body state predictions precede\nthose by center-of-mass states. This deep learning framework for human action\nprediction provides quantifiable insights into the control of real-world\nlocomotion and can be extended to other actions, contexts, and populations.\n","authors":["Wei-Chen Wang","Antoine De Comite","Alexandra Voloshina","Monica Daley","Nidhi Seethapathi"],"pdf_url":"https://arxiv.org/pdf/2503.16340v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09850v1","updated":"2025-04-14T03:43:27Z","published":"2025-04-14T03:43:27Z","title":"Accelerating Differentially Private Federated Learning via Adaptive\n  Extrapolation","summary":"  The federated learning (FL) framework enables multiple clients to\ncollaboratively train machine learning models without sharing their raw data,\nbut it remains vulnerable to privacy attacks. One promising approach is to\nincorporate differential privacy (DP)-a formal notion of privacy-into the FL\nframework. DP-FedAvg is one of the most popular algorithms for DP-FL, but it is\nknown to suffer from the slow convergence in the presence of heterogeneity\namong clients' data. Most of the existing methods to accelerate DP-FL require\n1) additional hyperparameters or 2) additional computational cost for clients,\nwhich is not desirable since 1) hyperparameter tuning is computationally\nexpensive and data-dependent choice of hyperparameters raises the risk of\nprivacy leakage, and 2) clients are often resource-constrained. To address this\nissue, we propose DP-FedEXP, which adaptively selects the global step size\nbased on the diversity of the local updates without requiring any additional\nhyperparameters or client computational cost. We show that DP-FedEXP provably\naccelerates the convergence of DP-FedAvg and it empirically outperforms\nexisting methods tailored for DP-FL.\n","authors":["Shokichi Takakura","Seng Pei Liew","Satoshi Hasegawa"],"pdf_url":"https://arxiv.org/pdf/2504.09850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09846v1","updated":"2025-04-14T03:32:39Z","published":"2025-04-14T03:32:39Z","title":"GlyTwin: Digital Twin for Glucose Control in Type 1 Diabetes Through\n  Optimal Behavioral Modifications Using Patient-Centric Counterfactuals","summary":"  Frequent and long-term exposure to hyperglycemia (i.e., high blood glucose)\nincreases the risk of chronic complications such as neuropathy, nephropathy,\nand cardiovascular disease. Current technologies like continuous subcutaneous\ninsulin infusion (CSII) and continuous glucose monitoring (CGM) primarily model\nspecific aspects of glycemic control-like hypoglycemia prediction or insulin\ndelivery. Similarly, most digital twin approaches in diabetes management\nsimulate only physiological processes. These systems lack the ability to offer\nalternative treatment scenarios that support proactive behavioral\ninterventions. To address this, we propose GlyTwin, a novel digital twin\nframework that uses counterfactual explanations to simulate optimal treatments\nfor glucose regulation. Our approach helps patients and caregivers modify\nbehaviors like carbohydrate intake and insulin dosing to avoid abnormal glucose\nevents. GlyTwin generates behavioral treatment suggestions that proactively\nprevent hyperglycemia by recommending small adjustments to daily choices,\nreducing both frequency and duration of these events. Additionally, it\nincorporates stakeholder preferences into the intervention design, making\nrecommendations patient-centric and tailored. We evaluate GlyTwin on AZT1D, a\nnewly constructed dataset with longitudinal data from 21 type 1 diabetes (T1D)\npatients on automated insulin delivery systems over 26 days. Results show\nGlyTwin outperforms state-of-the-art counterfactual methods, generating 76.6%\nvalid and 86% effective interventions. These findings demonstrate the promise\nof counterfactual-driven digital twins in delivering personalized healthcare.\n","authors":["Asiful Arefeen","Saman Khamesian","Maria Adela Grando","Bithika Thompson","Hassan Ghasemzadeh"],"pdf_url":"https://arxiv.org/pdf/2504.09846v1.pdf","comment":null}],"Computation and Language":[{"id":"http://arxiv.org/abs/2504.10481v1","updated":"2025-04-14T17:59:36Z","published":"2025-04-14T17:59:36Z","title":"xVerify: Efficient Answer Verifier for Reasoning Model Evaluations","summary":"  With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify.\n","authors":["Ding Chen","Qingchen Yu","Pengyuan Wang","Wentao Zhang","Bo Tang","Feiyu Xiong","Xinchi Li","Minchuan Yang","Zhiyu Li"],"pdf_url":"https://arxiv.org/pdf/2504.10481v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2503.13423v2","updated":"2025-04-14T17:59:35Z","published":"2025-03-17T17:53:23Z","title":"SuperBPE: Space Travel for Language Models","summary":"  The assumption across nearly all language model (LM) tokenization schemes is\nthat tokens should be subwords, i.e., contained within word boundaries. While\nproviding a seemingly reasonable inductive bias, is this common practice\nlimiting the potential of modern LMs? Whitespace is not a reliable delimiter of\nmeaning, as evidenced by multi-word expressions (e.g., \"by the way\"),\ncrosslingual variation in the number of words needed to express a concept\n(e.g., \"spacesuit helmet\" in German is \"raumanzughelm\"), and languages that do\nnot use whitespace at all (e.g., Chinese). To explore the potential of\ntokenization beyond subwords, we introduce a \"superword\" tokenizer, SuperBPE,\nwhich incorporates a simple pretokenization curriculum into the byte-pair\nencoding (BPE) algorithm to first learn subwords, then superwords that bridge\nwhitespace. This brings dramatic improvements in encoding efficiency: when\nfixing the vocabulary size to 200k, SuperBPE encodes a fixed piece of text with\nup to 33% fewer tokens than BPE on average. In experiments, we pretrain 8B\ntransformer LMs from scratch while fixing the model size, vocabulary size, and\ntrain compute, varying *only* the algorithm for learning the vocabulary. Our\nmodel trained with SuperBPE achieves an average +4.0% absolute improvement over\nthe BPE baseline across 30 downstream tasks (including +8.2% on MMLU), while\nsimultaneously requiring 27% less compute at inference time. In analysis, we\nfind that SuperBPE results in segmentations of text that are more uniform in\nper-token difficulty. Qualitatively, this may be because SuperBPE tokens often\ncapture common multi-word expressions that function semantically as a single\nunit. SuperBPE is a straightforward, local modification to tokenization that\nimproves both encoding efficiency and downstream performance, yielding better\nlanguage models overall.\n","authors":["Alisa Liu","Jonathan Hayase","Valentin Hofmann","Sewoong Oh","Noah A. Smith","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2503.13423v2.pdf","comment":"updated related work"},{"id":"http://arxiv.org/abs/2504.10471v1","updated":"2025-04-14T17:54:28Z","published":"2025-04-14T17:54:28Z","title":"MIEB: Massive Image Embedding Benchmark","summary":"  Image representations are often evaluated through disjointed, task-specific\nprotocols, leading to a fragmented understanding of model capabilities. For\ninstance, it is unclear whether an image embedding model adept at clustering\nimages is equally good at retrieving relevant images given a piece of text. We\nintroduce the Massive Image Embedding Benchmark (MIEB) to evaluate the\nperformance of image and image-text embedding models across the broadest\nspectrum to date. MIEB spans 38 languages across 130 individual tasks, which we\ngroup into 8 high-level categories. We benchmark 50 models across our\nbenchmark, finding that no single method dominates across all task categories.\nWe reveal hidden capabilities in advanced vision models such as their accurate\nvisual representation of texts, and their yet limited capabilities in\ninterleaved encodings and matching images and texts in the presence of\nconfounders. We also show that the performance of vision encoders on MIEB\ncorrelates highly with their performance when used in multimodal large language\nmodels. Our code, dataset, and leaderboard are publicly available at\nhttps://github.com/embeddings-benchmark/mteb.\n","authors":["Chenghao Xiao","Isaac Chung","Imene Kerboua","Jamie Stirling","Xin Zhang","Márton Kardos","Roman Solomatin","Noura Al Moubayed","Kenneth Enevoldsen","Niklas Muennighoff"],"pdf_url":"https://arxiv.org/pdf/2504.10471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07288v2","updated":"2025-04-14T17:48:08Z","published":"2025-04-09T21:28:17Z","title":"MDIT: A Model-free Data Interpolation Method for Diverse Instruction\n  Tuning","summary":"  As Large Language Models (LLMs) are increasingly applied across various\ntasks, instruction tuning has emerged as a critical method for enhancing model\nperformance. However, current data management strategies face substantial\nchallenges in generating diverse and comprehensive data, restricting further\nimprovements in model performance. To address this gap, we propose MDIT, a\nnovel model-free data interpolation method for diverse instruction tuning,\nwhich generates varied and high-quality instruction data by performing task\ninterpolation. Moreover, it contains diversity-based clustering strategies to\nensure the diversity of the training data. Extensive experiments show that our\nmethod achieves superior performance in multiple benchmark tasks. The LLMs\nfinetuned with MDIT show significant improvements in numerous tasks such as\ngeneral question answering, math reasoning, and code generation. MDIT offers an\nefficient and automatic data synthetic method, generating diverse instruction\ndata without depending on external resources while expanding the application\npotential of LLMs in complex environments.\n","authors":["Yangning Li","Zihua Lan","Lv Qingsong","Yinghui Li","Hai-Tao Zheng"],"pdf_url":"https://arxiv.org/pdf/2504.07288v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10458v1","updated":"2025-04-14T17:45:54Z","published":"2025-04-14T17:45:54Z","title":"GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI\n  Agents","summary":"  Existing efforts in building Graphical User Interface (GUI) agents largely\nrely on the training paradigm of supervised fine-tuning on Large\nVision-Language Models (LVLMs). However, this approach not only demands\nextensive amounts of training data but also struggles to effectively understand\nGUI screenshots and generalize to unseen interfaces. The issue significantly\nlimits its application in real-world scenarios, especially for high-level\ntasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models\n(e.g., DeepSeek-R1), which efficiently enhances the problem-solving\ncapabilities of large language models in real-world settings, we propose \\name,\nthe first reinforcement learning framework designed to enhance the GUI\ncapabilities of LVLMs in high-level real-world task scenarios, through unified\naction space rule modeling. By leveraging a small amount of carefully curated\nhigh-quality data across multiple platforms (including Windows, Linux, MacOS,\nAndroid, and Web) and employing policy optimization algorithms such as Group\nRelative Policy Optimization (GRPO) to update the model, \\name achieves\nsuperior performance using only 0.02\\% of the data (3K vs. 13M) compared to\nprevious state-of-the-art methods like OS-Atlas across eight benchmarks\nspanning three different platforms (mobile, desktop, and web). These results\ndemonstrate the immense potential of reinforcement learning based on unified\naction space rule modeling in improving the execution capabilities of LVLMs for\nreal-world GUI agent tasks.\n","authors":["Xiaobo Xia","Run Luo"],"pdf_url":"https://arxiv.org/pdf/2504.10458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15583v2","updated":"2025-04-14T17:42:35Z","published":"2024-06-21T18:31:49Z","title":"Detecting AI-Generated Text: Factors Influencing Detectability with\n  Current Methods","summary":"  Large language models (LLMs) have advanced to a point that even humans have\ndifficulty discerning whether a text was generated by another human, or by a\ncomputer. However, knowing whether a text was produced by human or artificial\nintelligence (AI) is important to determining its trustworthiness, and has\napplications in many domains including detecting fraud and academic dishonesty,\nas well as combating the spread of misinformation and political propaganda. The\ntask of AI-generated text (AIGT) detection is therefore both very challenging,\nand highly critical. In this survey, we summarize state-of-the art approaches\nto AIGT detection, including watermarking, statistical and stylistic analysis,\nand machine learning classification. We also provide information about existing\ndatasets for this task. Synthesizing the research findings, we aim to provide\ninsight into the salient factors that combine to determine how \"detectable\"\nAIGT text is under different scenarios, and to make practical recommendations\nfor future work towards this significant technical and societal challenge.\n","authors":["Kathleen C. Fraser","Hillary Dawkins","Svetlana Kiritchenko"],"pdf_url":"https://arxiv.org/pdf/2406.15583v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10445v1","updated":"2025-04-14T17:36:46Z","published":"2025-04-14T17:36:46Z","title":"RealWebAssist: A Benchmark for Long-Horizon Web Assistance with\n  Real-World Users","summary":"  To achieve successful assistance with long-horizon web-based tasks, AI agents\nmust be able to sequentially follow real-world user instructions over a long\nperiod. Unlike existing web-based agent benchmarks, sequential instruction\nfollowing in the real world poses significant challenges beyond performing a\nsingle, clearly defined task. For instance, real-world human instructions can\nbe ambiguous, require different levels of AI assistance, and may evolve over\ntime, reflecting changes in the user's mental state. To address this gap, we\nintroduce RealWebAssist, a novel benchmark designed to evaluate sequential\ninstruction-following in realistic scenarios involving long-horizon\ninteractions with the web, visual GUI grounding, and understanding ambiguous\nreal-world user instructions. RealWebAssist includes a dataset of sequential\ninstructions collected from real-world human users. Each user instructs a\nweb-based assistant to perform a series of tasks on multiple websites. A\nsuccessful agent must reason about the true intent behind each instruction,\nkeep track of the mental state of the user, understand user-specific routines,\nand ground the intended tasks to actions on the correct GUI elements. Our\nexperimental results show that state-of-the-art models struggle to understand\nand ground user instructions, posing critical challenges in following\nreal-world user instructions for long-horizon web assistance.\n","authors":["Suyu Ye","Haojun Shi","Darren Shih","Hyokun Yun","Tanya Roosta","Tianmin Shu"],"pdf_url":"https://arxiv.org/pdf/2504.10445v1.pdf","comment":"Project Website: https://scai.cs.jhu.edu/projects/RealWebAssist/\n  Code: https://github.com/SCAI-JHU/RealWebAssist"},{"id":"http://arxiv.org/abs/2501.17391v2","updated":"2025-04-14T17:34:19Z","published":"2025-01-29T02:52:32Z","title":"Learning Free Token Reduction for Multi-Modal Large Language Models","summary":"  Vision-Language Models (VLMs) have achieved remarkable success across a range\nof multimodal tasks; however, their practical deployment is often constrained\nby high computational costs and prolonged inference times. Since the vision\nmodality typically carries more information than the text modality, compressing\nvisual prompts offers a promising solution to alleviate these challenges.\nExisting approaches predominantly focus on refining model architectures or\ndirectly reducing the number of visual tokens. However, these methods often\ncompromise inference performance due to a lack of consideration for the unique\nspatial and temporal characteristics of visual data. In this work, we propose a\ntoken compression paradigm that operates on both spatial and temporal\ndimensions. Our approach includes a learning-free, plug-and-play compression\npipeline that can be seamlessly integrated into most Multimodal Large Language\nModel (MLLM) frameworks. By leveraging this method, we enhance the model\ninference capability while simultaneously reducing its computational cost.\nExperimental results on the Video-QA task demonstrate the effectiveness of the\nproposed approach, showcasing significant improvements in efficiency without\nsacrificing performance.\n","authors":["Zihui Zhao","Yingxin Li","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2501.17391v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10443v1","updated":"2025-04-14T17:34:06Z","published":"2025-04-14T17:34:06Z","title":"Multimodal Long Video Modeling Based on Temporal Dynamic Context","summary":"  Recent advances in Large Language Models (LLMs) have led to significant\nbreakthroughs in video understanding. However, existing models still struggle\nwith long video processing due to the context length constraint of LLMs and the\nvast amount of information within the video. Although some recent methods are\ndesigned for long video understanding, they often lose crucial information\nduring token compression and struggle with additional modality like audio. In\nthis work, we propose a dynamic long video encoding method utilizing the\ntemporal relationship between frames, named Temporal Dynamic Context (TDC).\nFirstly, we segment the video into semantically consistent scenes based on\ninter-frame similarities, then encode each frame into tokens using visual-audio\nencoders. Secondly, we propose a novel temporal context compressor to reduce\nthe number of tokens within each segment. Specifically, we employ a query-based\nTransformer to aggregate video, audio, and instruction text tokens into a\nlimited set of temporal context tokens. Finally, we feed the static frame\ntokens and the temporal context tokens into the LLM for video understanding.\nFurthermore, to handle extremely long videos, we propose a training-free\nchain-of-thought strategy that progressively extracts answers from multiple\nvideo segments. These intermediate answers serve as part of the reasoning\nprocess and contribute to the final answer. We conduct extensive experiments on\ngeneral video understanding and audio-video understanding benchmarks, where our\nmethod demonstrates strong performance. The code and models are available at\nhttps://github.com/Hoar012/TDC-Video.\n","authors":["Haoran Hao","Jiaming Han","Yiyuan Zhang","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2504.10443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10430v1","updated":"2025-04-14T17:20:34Z","published":"2025-04-14T17:20:34Z","title":"LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety\n  in Large Language Models","summary":"  Recent advancements in Large Language Models (LLMs) have enabled them to\napproach human-level persuasion capabilities. However, such potential also\nraises concerns about the safety risks of LLM-driven persuasion, particularly\ntheir potential for unethical influence through manipulation, deception,\nexploitation of vulnerabilities, and many other harmful tactics. In this work,\nwe present a systematic investigation of LLM persuasion safety through two\ncritical aspects: (1) whether LLMs appropriately reject unethical persuasion\ntasks and avoid unethical strategies during execution, including cases where\nthe initial persuasion goal appears ethically neutral, and (2) how influencing\nfactors like personality traits and external pressures affect their behavior.\nTo this end, we introduce PersuSafety, the first comprehensive framework for\nthe assessment of persuasion safety which consists of three stages, i.e.,\npersuasion scene creation, persuasive conversation simulation, and persuasion\nsafety assessment. PersuSafety covers 6 diverse unethical persuasion topics and\n15 common unethical strategies. Through extensive experiments across 8 widely\nused LLMs, we observe significant safety concerns in most LLMs, including\nfailing to identify harmful persuasion tasks and leveraging various unethical\npersuasion strategies. Our study calls for more attention to improve safety\nalignment in progressive and goal-driven conversations such as persuasion.\n","authors":["Minqian Liu","Zhiyang Xu","Xinyi Zhang","Heajun An","Sarvech Qadir","Qi Zhang","Pamela J. Wisniewski","Jin-Hee Cho","Sang Won Lee","Ruoxi Jia","Lifu Huang"],"pdf_url":"https://arxiv.org/pdf/2504.10430v1.pdf","comment":"20 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2504.10421v1","updated":"2025-04-14T17:08:20Z","published":"2025-04-14T17:08:20Z","title":"Can We Edit LLMs for Long-Tail Biomedical Knowledge?","summary":"  Knowledge editing has emerged as an effective approach for updating large\nlanguage models (LLMs) by modifying their internal knowledge. However, their\napplication to the biomedical domain faces unique challenges due to the\nlong-tailed distribution of biomedical knowledge, where rare and infrequent\ninformation is prevalent. In this paper, we conduct the first comprehensive\nstudy to investigate the effectiveness of knowledge editing methods for editing\nlong-tail biomedical knowledge. Our results indicate that, while existing\nediting methods can enhance LLMs' performance on long-tail biomedical\nknowledge, their performance on long-tail knowledge remains inferior to that on\nhigh-frequency popular knowledge, even after editing. Our further analysis\nreveals that long-tail biomedical knowledge contains a significant amount of\none-to-many knowledge, where one subject and relation link to multiple objects.\nThis high prevalence of one-to-many knowledge limits the effectiveness of\nknowledge editing in improving LLMs' understanding of long-tail biomedical\nknowledge, highlighting the need for tailored strategies to bridge this\nperformance gap.\n","authors":["Xinhao Yi","Jake Lever","Kevin Bryson","Zaiqiao Meng"],"pdf_url":"https://arxiv.org/pdf/2504.10421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10419v1","updated":"2025-04-14T17:06:59Z","published":"2025-04-14T17:06:59Z","title":"Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large\n  Language Models with CheckboxQA","summary":"  Checkboxes are critical in real-world document processing where the presence\nor absence of ticks directly informs data extraction and decision-making\nprocesses. Yet, despite the strong performance of Large Vision and Language\nModels across a wide range of tasks, they struggle with interpreting checkable\ncontent. This challenge becomes particularly pressing in industries where a\nsingle overlooked checkbox may lead to costly regulatory or contractual\noversights. To address this gap, we introduce the CheckboxQA dataset, a\ntargeted resource designed to evaluate and improve model performance on\ncheckbox-related tasks. It reveals the limitations of current models and serves\nas a valuable tool for advancing document comprehension systems, with\nsignificant implications for applications in sectors such as legal tech and\nfinance.\n  The dataset is publicly available at:\nhttps://github.com/Snowflake-Labs/CheckboxQA\n","authors":["Michał Turski","Mateusz Chiliński","Łukasz Borchmann"],"pdf_url":"https://arxiv.org/pdf/2504.10419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10418v1","updated":"2025-04-14T17:06:47Z","published":"2025-04-14T17:06:47Z","title":"CliniChat: A Multi-Source Knowledge-Driven Framework for Clinical\n  Interview Dialogue Reconstruction and Evaluation","summary":"  Large language models (LLMs) hold great promise for assisting clinical\ninterviews due to their fluent interactive capabilities and extensive medical\nknowledge. However, the lack of high-quality interview dialogue data and widely\naccepted evaluation methods has significantly impeded this process. So we\npropose CliniChat, a framework that integrates multi-source knowledge to enable\nLLMs to simulate real-world clinical interviews. It consists of two modules:\nClini-Recon and Clini-Eval, each responsible for reconstructing and evaluating\ninterview dialogues, respectively. By incorporating three sources of knowledge,\nClini-Recon transforms clinical notes into systematic, professional, and\nempathetic interview dialogues. Clini-Eval combines a comprehensive evaluation\nmetric system with a two-phase automatic evaluation approach, enabling LLMs to\nassess interview performance like experts. We contribute MedQA-Dialog, a\nhigh-quality synthetic interview dialogue dataset, and CliniChatGLM, a model\nspecialized for clinical interviews. Experimental results demonstrate that\nCliniChatGLM's interview capabilities undergo a comprehensive upgrade,\nparticularly in history-taking, achieving state-of-the-art performance.\n","authors":["Jing Chen","Zhihua Wei","Wei Zhang","Yingying Hu","Qiong Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.10418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10415v1","updated":"2025-04-14T17:00:13Z","published":"2025-04-14T17:00:13Z","title":"LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\n  Large Language Models","summary":"  Scientific equation discovery is a fundamental task in the history of\nscientific progress, enabling the derivation of laws governing natural\nphenomena. Recently, Large Language Models (LLMs) have gained interest for this\ntask due to their potential to leverage embedded scientific knowledge for\nhypothesis generation. However, evaluating the true discovery capabilities of\nthese methods remains challenging, as existing benchmarks often rely on common\nequations that are susceptible to memorization by LLMs, leading to inflated\nperformance metrics that do not reflect discovery. In this paper, we introduce\nLLM-SRBench, a comprehensive benchmark with 239 challenging problems across\nfour scientific domains specifically designed to evaluate LLM-based scientific\nequation discovery methods while preventing trivial memorization. Our benchmark\ncomprises two main categories: LSR-Transform, which transforms common physical\nmodels into less common mathematical representations to test reasoning beyond\nmemorized forms, and LSR-Synth, which introduces synthetic, discovery-driven\nproblems requiring data-driven reasoning. Through extensive evaluation of\nseveral state-of-the-art methods, using both open and closed LLMs, we find that\nthe best-performing system so far achieves only 31.5% symbolic accuracy. These\nfindings highlight the challenges of scientific equation discovery, positioning\nLLM-SRBench as a valuable resource for future research.\n","authors":["Parshin Shojaee","Ngoc-Hieu Nguyen","Kazem Meidani","Amir Barati Farimani","Khoa D Doan","Chandan K Reddy"],"pdf_url":"https://arxiv.org/pdf/2504.10415v1.pdf","comment":"Project page:\n  https://github.com/deep-symbolic-mathematics/llm-srbench , Benchmark page:\n  https://huggingface.co/datasets/nnheui/llm-srbench"},{"id":"http://arxiv.org/abs/2502.01436v2","updated":"2025-04-14T16:58:48Z","published":"2025-02-03T15:19:28Z","title":"Towards Safer Chatbots: A Framework for Policy Compliance Evaluation of\n  Custom GPTs","summary":"  Large Language Models (LLMs) have gained unprecedented prominence, achieving\nwidespread adoption across diverse domains and integrating deeply into society.\nThe capability to fine-tune general-purpose LLMs, such as Generative\nPre-trained Transformers (GPT), for specific tasks has facilitated the\nemergence of numerous Custom GPTs. These tailored models are increasingly made\navailable through dedicated marketplaces, such as OpenAI's GPT Store. However,\ntheir black-box nature introduces significant safety and compliance risks. In\nthis work, we present a scalable framework for the automated evaluation of\nCustom GPTs against OpenAI's usage policies, which define the permissible\nbehaviors of these systems. Our framework integrates three core components: (1)\nautomated discovery and data collection of models from the GPT store, (2) a\nred-teaming prompt generator tailored to specific policy categories and the\ncharacteristics of each target GPT, and (3) an LLM-as-a-judge technique to\nanalyze each prompt-response pair for potential policy violations. We validate\nour framework with a manually annotated ground truth, and evaluate it through a\nlarge-scale study with 782 Custom GPTs across three categories: Romantic,\nCybersecurity, and Academic GPTs. Our manual annotation process achieved an F1\nscore of 0.975 in identifying policy violations, confirming the reliability of\nthe framework's assessments. The results reveal that 58.7% of the analyzed\nmodels exhibit indications of non-compliance, exposing weaknesses in the GPT\nstore's review and approval processes. Furthermore, our findings indicate that\na model's popularity does not correlate with compliance, and non-compliance\nissues largely stem from behaviors inherited from base models rather than\nuser-driven customizations. We believe this approach is extendable to other\nchatbot platforms and policy domains, improving LLM-based systems safety.\n","authors":["David Rodriguez","William Seymour","Jose M. Del Alamo","Jose Such"],"pdf_url":"https://arxiv.org/pdf/2502.01436v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14701v3","updated":"2025-04-14T16:58:34Z","published":"2024-02-22T16:56:44Z","title":"COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies\n  with Language Modeling","summary":"  The therapeutic working alliance is a critical predictor of psychotherapy\nsuccess. Traditionally, working alliance assessment relies on questionnaires\ncompleted by both therapists and patients. In this paper, we present COMPASS, a\nnovel framework to directly infer the therapeutic working alliance from the\nnatural language used in psychotherapy sessions. Our approach leverages\nadvanced large language models (LLMs) to analyze session transcripts and map\nthem to distributed representations. These representations capture the semantic\nsimilarities between the dialogues and psychometric instruments, such as the\nWorking Alliance Inventory. Analyzing a dataset of over 950 sessions spanning\ndiverse psychiatric conditions -- including anxiety (N=498), depression\n(N=377), schizophrenia (N=71), and suicidal tendencies (N=12) -- collected\nbetween 1970 and 2012, we demonstrate the effectiveness of our method in\nproviding fine-grained mapping of patient-therapist alignment trajectories,\noffering interpretable insights for clinical practice, and identifying emerging\npatterns related to the condition being treated. By employing various deep\nlearning-based topic modeling techniques in combination with prompting\ngenerative language models, we analyze the topical characteristics of different\npsychiatric conditions and how these topics evolve during each turn of the\nconversation. This integrated framework enhances the understanding of\ntherapeutic interactions, enables timely feedback for therapists on the quality\nof therapeutic relationships, and provides clear, actionable insights to\nimprove the effectiveness of psychotherapy.\n","authors":["Baihan Lin","Djallel Bouneffouf","Yulia Landa","Rachel Jespersen","Cheryl Corcoran","Guillermo Cecchi"],"pdf_url":"https://arxiv.org/pdf/2402.14701v3.pdf","comment":"Translational Psychiatry, in press. This work extends our research\n  series in computational psychiatry (e.g auto annotation in arXiv:2204.05522,\n  topic extraction in arXiv:2204.10189, and diagnosis in arXiv:2210.15603) with\n  the introduction of LLMs to complete the full cycle of interpreting and\n  understanding psychotherapy strategies as a comprehensive analytical\n  framework"},{"id":"http://arxiv.org/abs/2504.10405v1","updated":"2025-04-14T16:53:59Z","published":"2025-04-14T16:53:59Z","title":"Performance of Large Language Models in Supporting Medical Diagnosis and\n  Treatment","summary":"  The integration of Large Language Models (LLMs) into healthcare holds\nsignificant potential to enhance diagnostic accuracy and support medical\ntreatment planning. These AI-driven systems can analyze vast datasets,\nassisting clinicians in identifying diseases, recommending treatments, and\npredicting patient outcomes. This study evaluates the performance of a range of\ncontemporary LLMs, including both open-source and closed-source models, on the\n2024 Portuguese National Exam for medical specialty access (PNA), a\nstandardized medical knowledge assessment. Our results highlight considerable\nvariation in accuracy and cost-effectiveness, with several models demonstrating\nperformance exceeding human benchmarks for medical students on this specific\ntask. We identify leading models based on a combined score of accuracy and\ncost, discuss the implications of reasoning methodologies like\nChain-of-Thought, and underscore the potential for LLMs to function as valuable\ncomplementary tools aiding medical professionals in complex clinical\ndecision-making.\n","authors":["Diogo Sousa","Guilherme Barbosa","Catarina Rocha","Dulce Oliveira"],"pdf_url":"https://arxiv.org/pdf/2504.10405v1.pdf","comment":"21 pages, 6 figures, 4 tables. Acknowledgements: The authors\n  acknowledge the support of the AITriage4SU Project (2024.07400.IACDC/2024),\n  funded by the FCT (Foundation for Science and Technology), Portugal"},{"id":"http://arxiv.org/abs/2504.10391v1","updated":"2025-04-14T16:38:28Z","published":"2025-04-14T16:38:28Z","title":"LLM-driven Constrained Copy Generation through Iterative Refinement","summary":"  Crafting a marketing message (copy), or copywriting is a challenging\ngeneration task, as the copy must adhere to various constraints. Copy creation\nis inherently iterative for humans, starting with an initial draft followed by\nsuccessive refinements. However, manual copy creation is time-consuming and\nexpensive, resulting in only a few copies for each use case. This limitation\nrestricts our ability to personalize content to customers. Contrary to the\nmanual approach, LLMs can generate copies quickly, but the generated content\ndoes not consistently meet all the constraints on the first attempt (similar to\nhumans). While recent studies have shown promise in improving constrained\ngeneration through iterative refinement, they have primarily addressed tasks\nwith only a few simple constraints. Consequently, the effectiveness of\niterative refinement for tasks such as copy generation, which involves many\nintricate constraints, remains unclear. To address this gap, we propose an\nLLM-based end-to-end framework for scalable copy generation using iterative\nrefinement. To the best of our knowledge, this is the first study to address\nmultiple challenging constraints simultaneously in copy generation. Examples of\nthese constraints include length, topics, keywords, preferred lexical ordering,\nand tone of voice. We demonstrate the performance of our framework by creating\ncopies for e-commerce banners for three different use cases of varying\ncomplexity. Our results show that iterative refinement increases the copy\nsuccess rate by $16.25-35.91$% across use cases. Furthermore, the copies\ngenerated using our approach outperformed manually created content in multiple\npilot studies using a multi-armed bandit framework. The winning copy improved\nthe click-through rate by $38.5-45.21$%.\n","authors":["Varun Vasudevan","Faezeh Akhavizadegan","Abhinav Prakash","Yokila Arora","Jason Cho","Tanya Mendiratta","Sushant Kumar","Kannan Achan"],"pdf_url":"https://arxiv.org/pdf/2504.10391v1.pdf","comment":"10 pages, 2 figures, 7 Tables"},{"id":"http://arxiv.org/abs/2403.15449v3","updated":"2025-04-14T16:35:50Z","published":"2024-03-18T07:20:35Z","title":"Hatred Stems from Ignorance! Distillation of the Persuasion Modes in\n  Countering Conversational Hate Speech","summary":"  Examining the factors that the counterspeech uses are at the core of\nunderstanding the optimal methods for confronting hate speech online. Various\nstudies have assessed the emotional base factors used in counter speech, such\nas emotional empathy, offensiveness, and hostility. To better understand the\ncounterspeech used in conversations, this study distills persuasion modes into\nreason, emotion, and credibility and evaluates their use in two types of\nconversation interactions: closed (multi-turn) and open (single-turn)\nconcerning racism, sexism, and religious bigotry. The evaluation covers the\ndistinct behaviors seen with human-sourced as opposed to machine-generated\ncounterspeech. It also assesses the interplay between the stance taken and the\nmode of persuasion seen in the counterspeech.\n  Notably, we observe nuanced differences in the counterspeech persuasion modes\nused in open and closed interactions, especially in terms of the topic, with a\ngeneral tendency to use reason as a persuasion mode to express the counterpoint\nto hate comments. The machine-generated counterspeech tends to exhibit an\nemotional persuasion mode, while human counters lean toward reason.\nFurthermore, our study shows that reason tends to obtain more supportive\nreplies than other persuasion modes. The findings highlight the potential for\nincorporating persuasion modes into studies about countering hate speech, as\nthey can serve as an optimal means of explainability and pave the way for the\nfurther adoption of the reply's stance and the role it plays in assessing what\ncomprises the optimal counterspeech.\n","authors":["Ghadi Alyahya","Abeer Aldayel"],"pdf_url":"https://arxiv.org/pdf/2403.15449v3.pdf","comment":"Accepted to appear @ ICWSM 2025. The link to the camera-ready paper\n  will be added soon"},{"id":"http://arxiv.org/abs/2504.10384v1","updated":"2025-04-14T16:28:14Z","published":"2025-04-14T16:28:14Z","title":"A 10.8mW Mixed-Signal Simulated Bifurcation Ising Solver using SRAM\n  Compute-In-Memory with 0.6us Time-to-Solution","summary":"  Combinatorial optimization problems are funda- mental for various fields\nranging from finance to wireless net- works. This work presents a simulated\nbifurcation (SB) Ising solver in CMOS for NP-hard optimization problems. Analog\ndomain computing led to a superior implementation of this algorithm as inherent\nand injected noise is required in SB Ising solvers. The architecture novelties\ninclude the use of SRAM compute-in-memory (CIM) to accelerate bifurcation as\nwell as the generation and injection of optimal decaying noise in the analog\ndomain. We propose a novel 10-T SRAM cell capable of performing ternary\nmultiplication. When measured with 60- node, 50% density, random, binary MAXCUT\ngraphs, this all- to-all connected Ising solver reliably achieves above 93% of\nthe ground state solution in 0.6us with 10.8mW average power in TSMC 180nm\nCMOS. Our chip achieves an order of magnitude improvement in time-to-solution\nand power compared to previously proposed Ising solvers in CMOS and other\nplatforms.\n","authors":["Alana Marie Dee","Sajjad Moazeni"],"pdf_url":"https://arxiv.org/pdf/2504.10384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07282v2","updated":"2025-04-14T16:23:29Z","published":"2025-04-09T21:17:52Z","title":"RAISE: Reinforenced Adaptive Instruction Selection For Large Language\n  Models","summary":"  In the instruction fine-tuning of large language models (LLMs), it has become\na consensus that a few high-quality instructions are superior to a large number\nof low-quality instructions. At present, many instruction selection methods\nhave been proposed, but most of these methods select instruction based on\nheuristic quality metrics, and only consider data selection before training.\nThese designs lead to insufficient optimization of instruction fine-tuning, and\nfixed heuristic indicators are often difficult to optimize for specific tasks.\nSo we designed a dynamic, task-objective-driven instruction selection framework\nRAISE(Reinforenced Adaptive Instruction SElection), which incorporates the\nentire instruction fine-tuning process into optimization, selecting instruction\nat each step based on the expected impact of instruction on model performance\nimprovement. Our approach is well interpretable and has strong task-specific\noptimization capabilities. By modeling dynamic instruction selection as a\nsequential decision-making process, we use RL to train our selection strategy.\nExtensive experiments and result analysis prove the superiority of our method\ncompared with other instruction selection methods. Notably, RAISE achieves\nsuperior performance by updating only 1\\% of the training steps compared to\nfull-data training, demonstrating its efficiency and effectiveness.\n","authors":["Lv Qingsong","Yangning Li","Zihua Lan","Zishan Xu","Jiwei Tang","Yinghui Li","Wenhao Jiang","Hai-Tao Zheng","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2504.07282v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10368v1","updated":"2025-04-14T16:13:23Z","published":"2025-04-14T16:13:23Z","title":"S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models","summary":"  We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning\nModels' (LRMs) performance on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their reliance on deep analytical thinking may limit their system 1\nthinking capabilities. Moreover, a lack of benchmark currently exists to\nevaluate LRMs' performance in tasks that require such capabilities. To fill\nthis gap, S1-Bench presents a set of simple, diverse, and naturally clear\nquestions across multiple domains and languages, specifically designed to\nassess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs\nreveals significant lower efficiency tendencies, with outputs averaging 15.5\ntimes longer than those of traditional small LLMs. Additionally, LRMs often\nidentify correct answers early but continue unnecessary deliberation, with some\nmodels even producing numerous errors. These findings highlight the rigid\nreasoning patterns of current LRMs and underscore the substantial development\nneeded to achieve balanced dual-system thinking capabilities that can adapt\nappropriately to task complexity.\n","authors":["Wenyuan Zhang","Shuaiyi Nie","Xinghua Zhang","Zefeng Zhang","Tingwen Liu"],"pdf_url":"https://arxiv.org/pdf/2504.10368v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2504.10359v1","updated":"2025-04-14T16:08:13Z","published":"2025-04-14T16:08:13Z","title":"DICE: A Framework for Dimensional and Contextual Evaluation of Language\n  Models","summary":"  Language models (LMs) are increasingly being integrated into a wide range of\napplications, yet the modern evaluation paradigm does not sufficiently reflect\nhow they are actually being used. Current evaluations rely on benchmarks that\noften lack direct applicability to the real-world contexts in which LMs are\nbeing deployed. To address this gap, we propose Dimensional and Contextual\nEvaluation (DICE), an approach that evaluates LMs on granular,\ncontext-dependent dimensions. In this position paper, we begin by examining the\ninsufficiency of existing LM benchmarks, highlighting their limited\napplicability to real-world use cases. Next, we propose a set of granular\nevaluation parameters that capture dimensions of LM behavior that are more\nmeaningful to stakeholders across a variety of application domains.\nSpecifically, we introduce the concept of context-agnostic parameters - such as\nrobustness, coherence, and epistemic honesty - and context-specific parameters\nthat must be tailored to the specific contextual constraints and demands of\nstakeholders choosing to deploy LMs into a particular setting. We then discuss\npotential approaches to operationalize this evaluation framework, finishing\nwith the opportunities and challenges DICE presents to the LM evaluation\nlandscape. Ultimately, this work serves as a practical and approachable\nstarting point for context-specific and stakeholder-relevant evaluation of LMs.\n","authors":["Aryan Shrivastava","Paula Akemi Aoyagui"],"pdf_url":"https://arxiv.org/pdf/2504.10359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01677v4","updated":"2025-04-14T16:08:09Z","published":"2024-01-20T08:44:34Z","title":"Embedding Ontologies via Incorporating Extensional and Intensional\n  Knowledge","summary":"  Ontologies contain rich knowledge within domain, which can be divided into\ntwo categories, namely extensional knowledge and intensional knowledge.\nExtensional knowledge provides information about the concrete instances that\nbelong to specific concepts in the ontology, while intensional knowledge\ndetails inherent properties, characteristics, and semantic associations among\nconcepts. However, existing ontology embedding approaches fail to take both\nextensional knowledge and intensional knowledge into fine consideration\nsimultaneously. In this paper, we propose a novel ontology embedding approach\nnamed EIKE (Extensional and Intensional Knowledge Embedding) by representing\nontologies in two spaces, called extensional space and intensional space. EIKE\npresents a unified framework for embedding instances, concepts and their\nrelations in an ontology, applying a geometry-based method to model extensional\nknowledge and a pretrained language model to model intensional knowledge, which\ncan capture both structure information and textual information. Experimental\nresults show that EIKE significantly outperforms state-of-the-art methods in\nthree datasets for both triple classification and link prediction, indicating\nthat EIKE provides a more comprehensive and representative perspective of the\ndomain.\n","authors":["Keyu Wang","Guilin Qi","Jiaoyan Chen","Yi Huang","Tianxing Wu"],"pdf_url":"https://arxiv.org/pdf/2402.01677v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10356v1","updated":"2025-04-14T16:05:59Z","published":"2025-04-14T16:05:59Z","title":"MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31\n  languages","summary":"  We present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs\ncovering 31 languages. MultiLoKo consists of three partitions: a main partition\nconsisting of 500 questions per language, separately sourced to be locally\nrelevant to the specific language, and two translated partitions, containing\nhuman-authored translations from 30 non-English languages to English and vice\nversa. For comparison, we also release corresponding machine-authored\ntranslations. The data is equally distributed over two splits: a dev split and\na blind, out-of-distribution test split. MultiLoKo can be used to study a\nvariety of questions regarding the multilinguality of LLMs as well as\nmeta-questions about multilingual benchmark creation. We compute MultiLoKo\nscores for 11 base and chat models marketed to be multilingual and study their\naverage performance, their performance parity across languages, how much their\nability to answer questions depends on the question language, and which\nlanguages are most difficult. None of the models we studied performs well on\nMultiLoKo, as indicated by low average scores as well as large differences\nbetween the best and worst scoring languages. Furthermore, we find a\nsubstantial effect of the question language, indicating sub-optimal knowledge\ntransfer between languages. Lastly, we find that using local vs\nEnglish-translated data can result in differences more than 20 points for the\nbest performing models, drastically change the estimated difficulty of some\nlanguages. For using machines instead of human translations, we find a weaker\neffect on ordering of language difficulty, a larger difference in model\nrankings, and a substantial drop in estimated performance for all models.\n","authors":["Dieuwke Hupkes","Nikolay Bogoychev"],"pdf_url":"https://arxiv.org/pdf/2504.10356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10352v1","updated":"2025-04-14T16:03:21Z","published":"2025-04-14T16:03:21Z","title":"Pseudo-Autoregressive Neural Codec Language Models for Efficient\n  Zero-Shot Text-to-Speech Synthesis","summary":"  Recent zero-shot text-to-speech (TTS) systems face a common dilemma:\nautoregressive (AR) models suffer from slow generation and lack duration\ncontrollability, while non-autoregressive (NAR) models lack temporal modeling\nand typically require complex designs. In this paper, we introduce a novel\npseudo-autoregressive (PAR) codec language modeling approach that unifies AR\nand NAR modeling. Combining explicit temporal modeling from AR with parallel\ngeneration from NAR, PAR generates dynamic-length spans at fixed time steps.\nBuilding on PAR, we propose PALLE, a two-stage TTS system that leverages PAR\nfor initial generation followed by NAR refinement. In the first stage, PAR\nprogressively generates speech tokens along the time dimension, with each step\npredicting all positions in parallel but only retaining the left-most span. In\nthe second stage, low-confidence tokens are iteratively refined in parallel,\nleveraging the global contextual information. Experiments demonstrate that\nPALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on\nlarge-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech\ntest-clean set in terms of speech quality, speaker similarity, and\nintelligibility, while achieving up to ten times faster inference speed. Audio\nsamples are available at https://anonymous-palle.github.io.\n","authors":["Yifan Yang","Shujie Liu","Jinyu Li","Yuxuan Hu","Haibin Wu","Hui Wang","Jianwei Yu","Lingwei Meng","Haiyang Sun","Yanqing Liu","Yan Lu","Kai Yu","Xie Chen"],"pdf_url":"https://arxiv.org/pdf/2504.10352v1.pdf","comment":"Submitted to ACM MM 2025"},{"id":"http://arxiv.org/abs/2504.10342v1","updated":"2025-04-14T15:50:39Z","published":"2025-04-14T15:50:39Z","title":"VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain\n  Knowledge","summary":"  Current multimodal benchmarks often conflate reasoning with domain-specific\nknowledge, making it difficult to isolate and evaluate general reasoning\nabilities in non-expert settings. To address this, we introduce VisualPuzzles,\na benchmark that targets visual reasoning while deliberately minimizing\nreliance on specialized knowledge. VisualPuzzles consists of diverse questions\nspanning five categories: algorithmic, analogical, deductive, inductive, and\nspatial reasoning. One major source of our questions is manually translated\nlogical reasoning questions from the Chinese Civil Service Examination.\nExperiments show that VisualPuzzles requires significantly less intensive\ndomain-specific knowledge and more complex reasoning compared to benchmarks\nlike MMMU, enabling us to better evaluate genuine multimodal reasoning.\nEvaluations show that state-of-the-art multimodal large language models\nconsistently lag behind human performance on VisualPuzzles, and that strong\nperformance on knowledge-intensive benchmarks does not necessarily translate to\nsuccess on reasoning-focused, knowledge-light tasks. Additionally, reasoning\nenhancements such as scaling up inference compute (with \"thinking\" modes) yield\ninconsistent gains across models and task types, and we observe no clear\ncorrelation between model size and performance. We also found that models\nexhibit different reasoning and answering patterns on VisualPuzzles compared to\nbenchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer\nlens through which to evaluate reasoning capabilities beyond factual recall and\ndomain knowledge.\n","authors":["Yueqi Song","Tianyue Ou","Yibo Kong","Zecheng Li","Graham Neubig","Xiang Yue"],"pdf_url":"https://arxiv.org/pdf/2504.10342v1.pdf","comment":"56 pages, 43 figures"},{"id":"http://arxiv.org/abs/2504.10340v1","updated":"2025-04-14T15:48:56Z","published":"2025-04-14T15:48:56Z","title":"Forecasting from Clinical Textual Time Series: Adaptations of the\n  Encoder and Decoder Language Model Families","summary":"  Clinical case reports encode rich, temporal patient trajectories that are\noften underexploited by traditional machine learning methods relying on\nstructured data. In this work, we introduce the forecasting problem from\ntextual time series, where timestamped clinical findings--extracted via an\nLLM-assisted annotation pipeline--serve as the primary input for prediction. We\nsystematically evaluate a diverse suite of models, including fine-tuned\ndecoder-based large language models and encoder-based transformers, on tasks of\nevent occurrence prediction, temporal ordering, and survival analysis. Our\nexperiments reveal that encoder-based models consistently achieve higher F1\nscores and superior temporal concordance for short- and long-horizon event\nforecasting, while fine-tuned masking approaches enhance ranking performance.\nIn contrast, instruction-tuned decoder models demonstrate a relative advantage\nin survival analysis, especially in early prognosis settings. Our sensitivity\nanalyses further demonstrate the importance of time ordering, which requires\nclinical time series construction, as compared to text ordering, the format of\nthe text inputs that LLMs are classically trained on. This highlights the\nadditional benefit that can be ascertained from time-ordered corpora, with\nimplications for temporal tasks in the era of widespread LLM use.\n","authors":["Shahriar Noroozizadeh","Sayantan Kumar","Jeremy C. Weiss"],"pdf_url":"https://arxiv.org/pdf/2504.10340v1.pdf","comment":"Machine Learning for Healthcare (MLHC 2025)"},{"id":"http://arxiv.org/abs/2307.03667v4","updated":"2025-04-14T15:45:41Z","published":"2023-07-07T15:37:50Z","title":"Testing the Predictions of Surprisal Theory in 11 Languages","summary":"  A fundamental result in psycholinguistics is that less predictable words take\na longer time to process. One theoretical explanation for this finding is\nSurprisal Theory (Hale, 2001; Levy, 2008), which quantifies a word's\npredictability as its surprisal, i.e. its negative log-probability given a\ncontext. While evidence supporting the predictions of Surprisal Theory have\nbeen replicated widely, most have focused on a very narrow slice of data:\nnative English speakers reading English texts. Indeed, no comprehensive\nmultilingual analysis exists. We address this gap in the current literature by\ninvestigating the relationship between surprisal and reading times in eleven\ndifferent languages, distributed across five language families. Deriving\nestimates from language models trained on monolingual and multilingual corpora,\nwe test three predictions associated with surprisal theory: (i) whether\nsurprisal is predictive of reading times; (ii) whether expected surprisal, i.e.\ncontextual entropy, is predictive of reading times; (iii) and whether the\nlinking function between surprisal and reading times is linear. We find that\nall three predictions are borne out crosslinguistically. By focusing on a more\ndiverse set of languages, we argue that these results offer the most robust\nlink to-date between information theory and incremental language processing\nacross languages.\n","authors":["Ethan Gotlieb Wilcox","Tiago Pimentel","Clara Meister","Ryan Cotterell","Roger P. Levy"],"pdf_url":"https://arxiv.org/pdf/2307.03667v4.pdf","comment":"This is a revised version of the paper: The original version of the\n  paper used raw frequencies instead of unigram surprisals in our regression\n  models, despite stating otherwise in the text. This has been amended, and\n  several typos have been fixed"},{"id":"http://arxiv.org/abs/2504.10335v1","updated":"2025-04-14T15:44:45Z","published":"2025-04-14T15:44:45Z","title":"MorphTok: Morphologically Grounded Tokenization for Indian Languages","summary":"  Tokenization is a crucial step in NLP, especially with the rise of large\nlanguage models (LLMs), impacting downstream performance, computational cost,\nand efficiency. Existing LLMs rely on the classical Byte-pair Encoding (BPE)\nalgorithm for subword tokenization that greedily merges frequent character\nbigrams. This often leads to segmentation that does not align with\nlinguistically meaningful units. To address this, we propose morphology-aware\nsegmentation as a pre-tokenization step prior to applying BPE. To facilitate\nmorphology-aware segmentation, we create a novel dataset for Hindi and Marathi,\nincorporating sandhi splitting to enhance the subword tokenization. Experiments\non downstream tasks show that morphologically grounded tokenization improves\nperformance for machine translation and language modeling. Additionally, to\nhandle the ambiguity in the Unicode characters for diacritics, particularly\ndependent vowels in syllable-based writing systems, we introduce Constrained\nBPE (CBPE), an extension to the traditional BPE algorithm that incorporates\nscript-specific constraints. Specifically, CBPE handles dependent vowels. Our\nresults show that CBPE achieves a 1.68\\% reduction in fertility scores while\nmaintaining comparable or improved downstream performance in machine\ntranslation, offering a computationally efficient alternative to standard BPE.\nMoreover, to evaluate segmentation across different tokenization algorithms, we\nintroduce a new human evaluation metric, \\textit{EvalTok}, enabling more\nhuman-grounded assessment.\n","authors":["Maharaj Brahma","N J Karthika","Atul Singh","Devaraj Adiga","Smruti Bhate","Ganesh Ramakrishnan","Rohit Saluja","Maunendra Sankar Desarkar"],"pdf_url":"https://arxiv.org/pdf/2504.10335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.02637v2","updated":"2025-04-14T15:38:57Z","published":"2024-12-03T18:10:28Z","title":"Words and Action: Modeling Linguistic Leadership in #BlackLivesMatter\n  Communities","summary":"  In this project, we describe a method of modeling semantic leadership across\na set of communities associated with the #BlackLivesMatter movement, which has\nbeen informed by qualitative research on the structure of social media and\nBlack Twitter in particular. We describe our bespoke approaches to\ntime-binning, community clustering, and connecting communities over time, as\nwell as our adaptation of state-of-the-art approaches to semantic change\ndetection and semantic leadership induction. We find substantial evidence of\nthe leadership role of BLM activists and progressives, as well as Black\ncelebrities. We also find evidence of the sustained engagement of the\nconservative community with this discourse, suggesting an alternative\nexplanation for how we arrived at the present moment, in which \"anti-woke\" and\n\"anti-CRT\" bills are being enacted nationwide.\n","authors":["Dani Roytburg","Deborah Olorunisola","Sandeep Soni","Lauren Klein"],"pdf_url":"https://arxiv.org/pdf/2412.02637v2.pdf","comment":"Accepted at ICWSM 2025"},{"id":"http://arxiv.org/abs/2502.12110v4","updated":"2025-04-14T15:21:49Z","published":"2025-02-17T18:36:14Z","title":"A-MEM: Agentic Memory for LLM Agents","summary":"  While large language model (LLM) agents can effectively use external tools\nfor complex real-world tasks, they require memory systems to leverage\nhistorical experiences. Current memory systems enable basic storage and\nretrieval but lack sophisticated memory organization, despite recent attempts\nto incorporate graph databases. Moreover, these systems' fixed operations and\nstructures limit their adaptability across diverse tasks. To address this\nlimitation, this paper proposes a novel agentic memory system for LLM agents\nthat can dynamically organize memories in an agentic way. Following the basic\nprinciples of the Zettelkasten method, we designed our memory system to create\ninterconnected knowledge networks through dynamic indexing and linking. When a\nnew memory is added, we generate a comprehensive note containing multiple\nstructured attributes, including contextual descriptions, keywords, and tags.\nThe system then analyzes historical memories to identify relevant connections,\nestablishing links where meaningful similarities exist. Additionally, this\nprocess enables memory evolution - as new memories are integrated, they can\ntrigger updates to the contextual representations and attributes of existing\nhistorical memories, allowing the memory network to continuously refine its\nunderstanding. Our approach combines the structured organization principles of\nZettelkasten with the flexibility of agent-driven decision making, allowing for\nmore adaptive and context-aware memory management. Empirical experiments on six\nfoundation models show superior improvement against existing SOTA baselines.\nThe source code for evaluating performance is available at\nhttps://github.com/WujiangXu/AgenticMemory, while the source code of agentic\nmemory system is available at https://github.com/agiresearch/A-mem.\n","authors":["Wujiang Xu","Zujie Liang","Kai Mei","Hang Gao","Juntao Tan","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12110v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.03036v2","updated":"2025-04-14T15:18:43Z","published":"2025-04-03T21:22:19Z","title":"IPA-CHILDES & G2P+: Feature-Rich Resources for Cross-Lingual Phonology\n  and Phonemic Language Modeling","summary":"  In this paper, we introduce two resources: (i) G2P+, a tool for converting\northographic datasets to a consistent phonemic representation; and (ii) IPA\nCHILDES, a phonemic dataset of child-centered speech across 31 languages. Prior\ntools for grapheme-to-phoneme conversion result in phonemic vocabularies that\nare inconsistent with established phonemic inventories, an issue which G2P+\naddresses by leveraging the inventories in the Phoible database. Using this\ntool, we augment CHILDES with phonemic transcriptions to produce IPA CHILDES.\nThis new resource fills several gaps in existing phonemic datasets, which often\nlack multilingual coverage, spontaneous speech, and a focus on child-directed\nlanguage. We demonstrate the utility of this dataset for phonological research\nby training phoneme language models on 11 languages and probing them for\ndistinctive features, finding that the distributional properties of phonemes\nare sufficient to learn major class and place features cross-lingually.\n","authors":["Zébulon Goriely","Paula Buttery"],"pdf_url":"https://arxiv.org/pdf/2504.03036v2.pdf","comment":"19 pages, 7 figures. Submitted to CoNLL 2025"},{"id":"http://arxiv.org/abs/2504.07615v2","updated":"2025-04-14T15:15:54Z","published":"2025-04-10T10:05:15Z","title":"VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model","summary":"  Recently DeepSeek R1 has shown that reinforcement learning (RL) can\nsubstantially improve the reasoning capabilities of Large Language Models\n(LLMs) through a simple yet effective design. The core of R1 lies in its\nrule-based reward formulation, which leverages tasks with deterministic\nground-truth answers to enable precise and stable reward computation. In the\nvisual domain, we similarly observe that a wide range of visual understanding\ntasks are inherently equipped with well-defined ground-truth annotations. This\nproperty makes them naturally compatible with rule-based reward mechanisms.\nMotivated by this observation, we investigate the extension of R1-style\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\ndedicated framework designed to harness RL for improving VLMs' performance on\ngeneral vision-language tasks. Using this framework, we further explore the\nfeasibility of applying RL to visual domain. Experimental results indicate that\nthe RL-based model not only delivers competitive performance on visual\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\nthat uncover a series of noteworthy insights, including the presence of reward\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\nof training data quality, and the scaling behavior of RL across different model\nsizes. Through these analyses, we aim to deepen the understanding of how\nreinforcement learning enhances the capabilities of vision-language models, and\nwe hope our findings and open-source contributions will support continued\nprogress in the vision-language RL community. Our code and model are available\nat https://github.com/om-ai-lab/VLM-R1\n","authors":["Haozhan Shen","Peng Liu","Jingcheng Li","Chunxin Fang","Yibo Ma","Jiajia Liao","Qiaoli Shen","Zilun Zhang","Kangjia Zhao","Qianqian Zhang","Ruochen Xu","Tiancheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.07615v2.pdf","comment":"11 pages, fix some minor typos in the previous version"},{"id":"http://arxiv.org/abs/2504.03338v2","updated":"2025-04-14T15:12:17Z","published":"2025-04-04T10:42:56Z","title":"BabyLM's First Words: Word Segmentation as a Phonological Probing Task","summary":"  Language models provide a key framework for studying linguistic theories\nbased on prediction, but phonological analysis using large language models\n(LLMs) is difficult; there are few phonological benchmarks beyond English and\nthe standard input representation used in LLMs (subwords of graphemes) is not\nsuitable for analyzing the representation of phonemes. In this work, we\ndemonstrate how word segmentation can be used as a phonological probing task,\nallowing us to study the representations learned by phoneme-based language\nmodels trained on child-directed speech across 31 languages. Following\ncomputational models of word segmentation, we present unsupervised methods for\nextracting word boundaries from a trained model using the observation that\nprediction-error peaks at the start of words. We also use linear probes to\nidentify that these models implicitly track word boundaries, even when they do\nnot appear in training. This cross-lingual work corroborates statistical\nlearning theories of acquisition and empirically motivates new methods for\ntraining subword tokenizers.\n","authors":["Zébulon Goriely","Paula Buttery"],"pdf_url":"https://arxiv.org/pdf/2504.03338v2.pdf","comment":"17 pages, 10 figures, submitted to CoNLL 2025"},{"id":"http://arxiv.org/abs/2410.05168v4","updated":"2025-04-14T14:58:01Z","published":"2024-10-07T16:25:39Z","title":"ReasoningRank: Teaching Student Models to Rank through Reasoning-Based\n  Knowledge Distillation","summary":"  Reranking documents based on their relevance to a given query is a critical\ntask in information retrieval. Traditional reranking methods often lack\ntransparency and rely on proprietary models, hindering reproducibility and\ninterpretability. We propose Reason-to-Rank (R2R), a novel open-source\nreranking approach that enhances transparency by generating two types of\nreasoning: direct relevance reasoning, which explains how a document addresses\nthe query, and comparison reasoning, which justifies the relevance of one\ndocument over another. We leverage large language models (LLMs) as teacher\nmodels to generate these explanations and distill this knowledge into smaller,\nopenly available student models. Our student models are trained to generate\nmeaningful reasoning and rerank documents, achieving competitive performance\nacross multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate\nthat R2R not only improves reranking accuracy but also provides valuable\ninsights into the decision-making process. By offering a structured and\ninterpretable solution with openly accessible resources, R2R aims to bridge the\ngap between effectiveness and transparency in information retrieval, fostering\nreproducibility and further research in the field.\n","authors":["Yuelyu Ji","Zhuochun Li","Rui Meng","Daqing He"],"pdf_url":"https://arxiv.org/pdf/2410.05168v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10284v1","updated":"2025-04-14T14:52:28Z","published":"2025-04-14T14:52:28Z","title":"Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the\n  Evaluation Protocol","summary":"  Literature review tables are essential for summarizing and comparing\ncollections of scientific papers. We explore the task of generating tables that\nbest fulfill a user's informational needs given a collection of scientific\npapers. Building on recent work (Newman et al., 2024), we extend prior\napproaches to address real-world complexities through a combination of\nLLM-based methods and human annotations. Our contributions focus on three key\nchallenges encountered in real-world use: (i) User prompts are often\nunder-specified; (ii) Retrieved candidate papers frequently contain irrelevant\ncontent; and (iii) Task evaluation should move beyond shallow text similarity\ntechniques and instead assess the utility of inferred tables for\ninformation-seeking tasks (e.g., comparing papers). To support reproducible\nevaluation, we introduce ARXIV2TABLE, a more realistic and challenging\nbenchmark for this task, along with a novel approach to improve literature\nreview table generation in real-world scenarios. Our extensive experiments on\nthis benchmark show that both open-weight and proprietary LLMs struggle with\nthe task, highlighting its difficulty and the need for further advancements.\nOur dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table.\n","authors":["Weiqi Wang","Jiefu Ou","Yangqiu Song","Benjamin Van Durme","Daniel Khashabi"],"pdf_url":"https://arxiv.org/pdf/2504.10284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10277v1","updated":"2025-04-14T14:44:41Z","published":"2025-04-14T14:44:41Z","title":"RealHarm: A Collection of Real-World Language Model Application Failures","summary":"  Language model deployments in consumer-facing applications introduce numerous\nrisks. While existing research on harms and hazards of such applications\nfollows top-down approaches derived from regulatory frameworks and theoretical\nanalyses, empirical evidence of real-world failure modes remains underexplored.\nIn this work, we introduce RealHarm, a dataset of annotated problematic\ninteractions with AI agents built from a systematic review of publicly reported\nincidents. Analyzing harms, causes, and hazards specifically from the\ndeployer's perspective, we find that reputational damage constitutes the\npredominant organizational harm, while misinformation emerges as the most\ncommon hazard category. We empirically evaluate state-of-the-art guardrails and\ncontent moderation systems to probe whether such systems would have prevented\nthe incidents, revealing a significant gap in the protection of AI\napplications.\n","authors":["Pierre Le Jeune","Jiaen Liu","Luca Rossi","Matteo Dora"],"pdf_url":"https://arxiv.org/pdf/2504.10277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10250v1","updated":"2025-04-14T14:13:03Z","published":"2025-04-14T14:13:03Z","title":"MURR: Model Updating with Regularized Replay for Searching a Document\n  Stream","summary":"  The Internet produces a continuous stream of new documents and user-generated\nqueries. These naturally change over time based on events in the world and the\nevolution of language. Neural retrieval models that were trained once on a\nfixed set of query-document pairs will quickly start misrepresenting\nnewly-created content and queries, leading to less effective retrieval.\nTraditional statistical sparse retrieval can update collection statistics to\nreflect these changes in the use of language in documents and queries. In\ncontrast, continued fine-tuning of the language model underlying neural\nretrieval approaches such as DPR and ColBERT creates incompatibility with\npreviously-encoded documents. Re-encoding and re-indexing all\npreviously-processed documents can be costly. In this work, we explore updating\na neural dual encoder retrieval model without reprocessing past documents in\nthe stream. We propose MURR, a model updating strategy with regularized replay,\nto ensure the model can still faithfully search existing documents without\nreprocessing, while continuing to update the model for the latest topics. In\nour simulated streaming environments, we show that fine-tuning models using\nMURR leads to more effective and more consistent retrieval results than other\nstrategies as the stream of documents and queries progresses.\n","authors":["Eugene Yang","Nicola Tonellotto","Dawn Lawrie","Sean MacAvaney","James Mayfield","Douglas W. Oard","Scott Miller"],"pdf_url":"https://arxiv.org/pdf/2504.10250v1.pdf","comment":"Published at ECIR 2025. 16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.12899v2","updated":"2025-04-14T13:57:28Z","published":"2025-03-17T07:59:42Z","title":"A Semantic-based Optimization Approach for Repairing LLMs: Case Study on\n  Code Generation","summary":"  Language Models (LMs) are widely used in software engineering for code\ngeneration, but they may produce code with errors. Rather than repairing the\ngenerated code, an alternative way is to address the underlying failures of\nmodels. LM repair offers a lightweight solution to this challenge: it requires\nminimal data, reduces computational costs, and reduces the side effects. Unlike\nretraining, LM repair focuses on applying tailored updates to targeted neurons,\nmaking it ideal for scenarios with limited resources, high-performance demands,\nor strict safety requirements. In this paper, we propose \\ul{S}emantic\n\\ul{T}argeting for \\ul{A}nalytical \\ul{R}epair (\\textsc{STAR}), a pioneering\nand novel semantic-based optimization approach for repairing LLMs.\n\\textsc{STAR} realizes main operations in LM repair methods in an optimization\nprocess, including locating ``buggy neurons'', solving ``neuron patches'', and\npatching ``buggy neurons''. Correspondingly, it computes the deltas of weight\nmatrix as the prior information to guide optimization; and attributes the\ntargeted layers and neurons leveraging statistical insights. The neuron patches\nare computed with a solid semantic-based analytical formula, which directly\nbridges the changes to logits with the deltas of neurons, by steering latent\nrepresentations. Compared to the prior work of LM repair (\\textsc{MINT}) and\noptimization methods (\\textsc{SGD}), \\textsc{STAR} integrates their strengths\nwhile mitigating their limitations. \\textsc{STAR} supports solving multiple\nfailures together, significantly improving the usefulness. Evaluated on three\ncode generation tasks using popular code LMs, \\textsc{STAR} demonstrates\nsuperior effectiveness. Additionally, \\textsc{STAR} exhibits better efficiency.\nIn terms of side effects, namely the balance between generalization and\nspecificity, \\textsc{STAR} outperforms prior work by a significant margin.\n","authors":["Jian Gu","Aldeida Aleti","Chunyang Chen","Hongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.12899v2.pdf","comment":"12 pages, 6 figure, 6 tables, under peer-review"},{"id":"http://arxiv.org/abs/2504.10227v1","updated":"2025-04-14T13:46:35Z","published":"2025-04-14T13:46:35Z","title":"Probing then Editing Response Personality of Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated promising capabilities to\ngenerate responses that exhibit consistent personality traits. Despite the\nmajor attempts to analyze personality expression through output-based\nevaluations, little is known about how such traits are internally encoded\nwithin LLM parameters. In this paper, we introduce a layer-wise probing\nframework to systematically investigate the layer-wise capability of LLMs in\nencoding personality for responding. We conduct probing experiments on 11\nopen-source LLMs over the PersonalityEdit benchmark and find that LLMs\npredominantly encode personality for responding in their middle and upper\nlayers, with instruction-tuned models demonstrating a slightly clearer\nseparation of personality traits. Furthermore, by interpreting the trained\nprobing hyperplane as a layer-wise boundary for each personality category, we\npropose a layer-wise perturbation method to edit the personality expressed by\nLLMs during inference. Our results show that even when the prompt explicitly\nspecifies a particular personality, our method can still successfully alter the\nresponse personality of LLMs. Interestingly, the difficulty of converting\nbetween certain personality traits varies substantially, which aligns with the\nrepresentational distances in our probing experiments. Finally, we conduct a\ncomprehensive MMLU benchmark evaluation and time overhead analysis,\ndemonstrating that our proposed personality editing method incurs only minimal\ndegradation in general capabilities while maintaining low training costs and\nacceptable inference latency. Our code is publicly available at\nhttps://github.com/universe-sky/probing-then-editing-personality.\n","authors":["Tianjie Ju","Zhenyu Shao","Bowen Wang","Yujia Chen","Zhuosheng Zhang","Hao Fei","Mong-Li Lee","Wynne Hsu","Sufeng Duan","Gongshen Liu"],"pdf_url":"https://arxiv.org/pdf/2504.10227v1.pdf","comment":"Working in Progress"},{"id":"http://arxiv.org/abs/2504.08619v2","updated":"2025-04-14T13:45:49Z","published":"2025-04-11T15:24:23Z","title":"Analyzing 16,193 LLM Papers for Fun and Profits","summary":"  Large Language Models (LLMs) are reshaping the landscape of computer science\nresearch, driving significant shifts in research priorities across diverse\nconferences and fields. This study provides a comprehensive analysis of the\npublication trend of LLM-related papers in 77 top-tier computer science\nconferences over the past six years (2019-2024). We approach this analysis from\nfour distinct perspectives: (1) We investigate how LLM research is driving\ntopic shifts within major conferences. (2) We adopt a topic modeling approach\nto identify various areas of LLM-related topic growth and reveal the topics of\nconcern at different conferences. (3) We explore distinct contribution patterns\nof academic and industrial institutions. (4) We study the influence of national\norigins on LLM development trajectories. Synthesizing the findings from these\ndiverse analytical angles, we derive ten key insights that illuminate the\ndynamics and evolution of the LLM research ecosystem.\n","authors":["Zhiqiu Xia","Lang Zhu","Bingzhe Li","Feng Chen","Qiannan Li","Hang Liu"],"pdf_url":"https://arxiv.org/pdf/2504.08619v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21106v4","updated":"2025-04-14T13:44:55Z","published":"2025-03-27T02:59:01Z","title":"Function Alignment: A New Theory of Mind and Intelligence, Part I:\n  Foundations","summary":"  This paper introduces function alignment, a novel theory of mind and\nintelligence that is both intuitively compelling and structurally grounded. It\nexplicitly models how meaning, interpretation, and analogy emerge from\ninteractions among layered representations, forming a coherent framework\ncapable not only of modeling minds but also of serving as a blueprint for\nbuilding them. One of the key theoretical insights derived from function\nalignment is bounded interpretability, which provides a unified explanation for\npreviously fragmented ideas in cognitive science, such as bounded rationality,\nsymbol grounding, and analogy-making. Beyond modeling, the function alignment\nframework bridges disciplines often kept apart, linking computational\narchitecture, psychological theory, and even contemplative traditions such as\nZen. Rather than building on any philosophical systems, it offers a structural\nfoundation upon which multiple ways of understanding the mind may be\nreconstructed.\n","authors":["Gus G. Xia"],"pdf_url":"https://arxiv.org/pdf/2503.21106v4.pdf","comment":"12 pages, 2 figures. Part I of a multi-part position paper on a new\n  theory of mind"},{"id":"http://arxiv.org/abs/2504.10198v1","updated":"2025-04-14T13:02:53Z","published":"2025-04-14T13:02:53Z","title":"DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization\n  for Dynamic Retrieval-Augmented Generation","summary":"  Dynamic Retrieval-augmented Generation (RAG) has shown great success in\nmitigating hallucinations in large language models (LLMs) during generation.\nHowever, existing dynamic RAG methods face significant limitations in two key\naspects: 1) Lack of an effective mechanism to control retrieval triggers, and\n2) Lack of effective scrutiny of retrieval content. To address these\nlimitations, we propose an innovative dynamic RAG method, DioR (Adaptive\nCognitive Detection and Contextual Retrieval Optimization), which consists of\ntwo main components: adaptive cognitive detection and contextual retrieval\noptimization, specifically designed to determine when retrieval is needed and\nwhat to retrieve for LLMs is useful. Experimental results demonstrate that DioR\nachieves superior performance on all tasks, demonstrating the effectiveness of\nour work.\n","authors":["Hanghui Guo","Jia Zhu","Shimin Di","Weijie Shi","Zhangze Chen","Jiajie Xu"],"pdf_url":"https://arxiv.org/pdf/2504.10198v1.pdf","comment":"24 pages, 9 figures"},{"id":"http://arxiv.org/abs/2504.10191v1","updated":"2025-04-14T12:53:58Z","published":"2025-04-14T12:53:58Z","title":"Localized Cultural Knowledge is Conserved and Controllable in Large\n  Language Models","summary":"  Just as humans display language patterns influenced by their native tongue\nwhen speaking new languages, LLMs often default to English-centric responses\neven when generating in other languages. Nevertheless, we observe that local\ncultural information persists within the models and can be readily activated\nfor cultural customization. We first demonstrate that explicitly providing\ncultural context in prompts significantly improves the models' ability to\ngenerate culturally localized responses. We term the disparity in model\nperformance with versus without explicit cultural context the explicit-implicit\nlocalization gap, indicating that while cultural knowledge exists within LLMs,\nit may not naturally surface in multilingual interactions if cultural context\nis not explicitly provided. Despite the explicit prompting benefit, however,\nthe answers reduce in diversity and tend toward stereotypes. Second, we\nidentify an explicit cultural customization vector, conserved across all\nnon-English languages we explore, which enables LLMs to be steered from the\nsynthetic English cultural world-model toward each non-English cultural world.\nSteered responses retain the diversity of implicit prompting and reduce\nstereotypes to dramatically improve the potential for customization. We discuss\nthe implications of explicit cultural customization for understanding the\nconservation of alternative cultural world models within LLMs, and their\ncontrollable utility for translation, cultural customization, and the\npossibility of making the explicit implicit through soft control for expanded\nLLM function and appeal.\n","authors":["Veniamin Veselovsky","Berke Argin","Benedikt Stroebl","Chris Wendler","Robert West","James Evans","Thomas L. Griffiths","Arvind Narayanan"],"pdf_url":"https://arxiv.org/pdf/2504.10191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10423v2","updated":"2025-04-14T12:52:24Z","published":"2024-12-10T12:42:33Z","title":"Look Before You Leap: Enhancing Attention and Vigilance Regarding\n  Harmful Content with GuidelineLLM","summary":"  Despite being empowered with alignment mechanisms, large language models\n(LLMs) are increasingly vulnerable to emerging jailbreak attacks that can\ncompromise their alignment mechanisms. This vulnerability poses significant\nrisks to real-world applications. Existing work faces challenges in both\ntraining efficiency and generalization capabilities (i.e., Reinforcement\nLearning from Human Feedback and Red-Teaming). Developing effective strategies\nto enable LLMs to resist continuously evolving jailbreak attempts represents a\nsignificant challenge. To address this challenge, we propose a novel defensive\nparadigm called GuidelineLLM, which assists LLMs in recognizing queries that\nmay have harmful content. Before LLMs respond to a query, GuidelineLLM first\nidentifies potential risks associated with the query, summarizes these risks\ninto guideline suggestions, and then feeds these guidelines to the responding\nLLMs. Importantly, our approach eliminates the necessity for additional safety\nfine-tuning of the LLMs themselves; only the GuidelineLLM requires fine-tuning.\nThis characteristic enhances the general applicability of GuidelineLLM across\nvarious LLMs. Experimental results demonstrate that GuidelineLLM can\nsignificantly reduce the attack success rate (ASR) against LLM (an average\nreduction of 34.17\\% ASR) while maintaining the usefulness of LLM in handling\nbenign queries. The code is available at\nhttps://github.com/sqzhang-lazy/GuidelineLLM.\n","authors":["Shaoqing Zhang","Zhuosheng Zhang","Kehai Chen","Rongxiang Weng","Muyun Yang","Tiejun Zhao","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10423v2.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2504.10187v1","updated":"2025-04-14T12:40:39Z","published":"2025-04-14T12:40:39Z","title":"Deep Reasoning Translation via Reinforcement Learning","summary":"  Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown\npromising performance in various complex tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation and taking cultural differences into account.\nThis task is still under-explored in deep reasoning LLMs. In this paper, we\nintroduce DeepTrans, a deep reasoning translation model that learns free\ntranslation via reinforcement learning. Specifically, we carefully build a\nreward model with pre-defined scoring criteria on both the translation results\nand the thought process. Given the source sentences, the reward model teaches\nthe deep translation model how to think and free-translate them during\nreinforcement learning. In this way, training DeepTrans does not need any\nlabeled translations, avoiding the human-intensive annotation or\nresource-intensive data synthesis. Experimental results show the effectiveness\nof DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance\nby 16.3% in literature translation, and outperforms strong deep reasoning\nbaselines as well as baselines that are fine-tuned with synthesized data.\nMoreover, we summarize the failures and interesting findings during our RL\nexploration. We hope this work could inspire other researchers in free\ntranslation.\n","authors":["Jiaan Wang","Fandong Meng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.10187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10185v1","updated":"2025-04-14T12:38:37Z","published":"2025-04-14T12:38:37Z","title":"LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in\n  Current Benchmarks","summary":"  Large language model unlearning has become a critical challenge in ensuring\nsafety and controlled model behavior by removing undesired data-model\ninfluences from the pretrained model while preserving general utility.\nSignificant recent efforts have been dedicated to developing LLM unlearning\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\nUnlearning Six-way Evaluation), facilitating standardized unlearning\nperformance assessment and method comparison. Despite their usefulness, we\nuncover for the first time a novel coreset effect within these benchmarks.\nSpecifically, we find that LLM unlearning achieved with the original (full)\nforget set can be effectively maintained using a significantly smaller subset\n(functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\ncan be performed surprisingly easily, even in an extremely low-data regime. We\ndemonstrate that this coreset effect remains strong, regardless of the LLM\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\nThe surprisingly strong coreset effect is also robust across various data\nselection methods, ranging from random selection to more sophisticated\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\nkeyword-based perspective, showing that keywords extracted from the forget set\nalone contribute significantly to unlearning effectiveness and indicating that\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\nmodels along additional dimensions, such as mode connectivity and robustness to\njailbreaking attacks. Codes are available at\nhttps://github.com/OPTML-Group/MU-Coreset.\n","authors":["Soumyadeep Pal","Changsheng Wang","James Diffenderfer","Bhavya Kailkhura","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2504.10185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.00698v2","updated":"2025-04-14T12:37:51Z","published":"2025-04-01T12:08:07Z","title":"Command A: An Enterprise-Ready Large Language Model","summary":"  In this report we describe the development of Command A, a powerful large\nlanguage model purpose-built to excel at real-world enterprise use cases.\nCommand A is an agent-optimised and multilingual-capable model, with support\nfor 23 languages of global business, and a novel hybrid architecture balancing\nefficiency with top of the range performance. It offers best-in-class Retrieval\nAugmented Generation (RAG) capabilities with grounding and tool use to automate\nsophisticated business processes. These abilities are achieved through a\ndecentralised training approach, including self-refinement algorithms and model\nmerging techniques. We also include results for Command R7B which shares\ncapability and architectural similarities to Command A. Weights for both models\nhave been released for research purposes. This technical report details our\noriginal training pipeline and presents an extensive evaluation of our models\nacross a suite of enterprise-relevant tasks and public benchmarks,\ndemonstrating excellent performance and efficiency.\n","authors":["Team Cohere"," :"," Aakanksha","Arash Ahmadian","Marwan Ahmed","Jay Alammar","Milad Alizadeh","Yazeed Alnumay","Sophia Althammer","Arkady Arkhangorodsky","Viraat Aryabumi","Dennis Aumiller","Raphaël Avalos","Zahara Aviv","Sammie Bae","Saurabh Baji","Alexandre Barbet","Max Bartolo","Björn Bebensee","Neeral Beladia","Walter Beller-Morales","Alexandre Bérard","Andrew Berneshawi","Anna Bialas","Phil Blunsom","Matt Bobkin","Adi Bongale","Sam Braun","Maxime Brunet","Samuel Cahyawijaya","David Cairuz","Jon Ander Campos","Cassie Cao","Kris Cao","Roman Castagné","Julián Cendrero","Leila Chan Currie","Yash Chandak","Diane Chang","Giannis Chatziveroglou","Hongyu Chen","Claire Cheng","Alexis Chevalier","Justin T. Chiu","Eugene Cho","Eugene Choi","Eujeong Choi","Tim Chung","Volkan Cirik","Ana Cismaru","Pierre Clavier","Henry Conklin","Lucas Crawhall-Stein","Devon Crouse","Andres Felipe Cruz-Salinas","Ben Cyrus","Daniel D'souza","Hugo Dalla-Torre","John Dang","William Darling","Omar Darwiche Domingues","Saurabh Dash","Antoine Debugne","Théo Dehaze","Shaan Desai","Joan Devassy","Rishit Dholakia","Kyle Duffy","Ali Edalati","Ace Eldeib","Abdullah Elkady","Sarah Elsharkawy","Irem Ergün","Beyza Ermis","Marzieh Fadaee","Boyu Fan","Lucas Fayoux","Yannis Flet-Berliac","Nick Frosst","Matthias Gallé","Wojciech Galuba","Utsav Garg","Matthieu Geist","Mohammad Gheshlaghi Azar","Ellen Gilsenan-McMahon","Seraphina Goldfarb-Tarrant","Tomas Goldsack","Aidan Gomez","Victor Machado Gonzaga","Nithya Govindarajan","Manoj Govindassamy","Nathan Grinsztajn","Nikolas Gritsch","Patrick Gu","Shangmin Guo","Kilian Haefeli","Rod Hajjar","Tim Hawes","Jingyi He","Sebastian Hofstätter","Sungjin Hong","Sara Hooker","Tom Hosking","Stephanie Howe","Eric Hu","Renjie Huang","Hemant Jain","Ritika Jain","Nick Jakobi","Madeline Jenkins","JJ Jordan","Dhruti Joshi","Jason Jung","Trushant Kalyanpur","Siddhartha Rao Kamalakara","Julia Kedrzycki","Gokce Keskin","Edward Kim","Joon Kim","Wei-Yin Ko","Tom Kocmi","Michael Kozakov","Wojciech Kryściński","Arnav Kumar Jain","Komal Kumar Teru","Sander Land","Michael Lasby","Olivia Lasche","Justin Lee","Patrick Lewis","Jeffrey Li","Jonathan Li","Hangyu Lin","Acyr Locatelli","Kevin Luong","Raymond Ma","Lukáš Mach","Marina Machado","Joanne Magbitang","Brenda Malacara Lopez","Aryan Mann","Kelly Marchisio","Olivia Markham","Alexandre Matton","Alex McKinney","Dominic McLoughlin","Jozef Mokry","Adrien Morisot","Autumn Moulder","Harry Moynehan","Maximilian Mozes","Vivek Muppalla","Lidiya Murakhovska","Hemangani Nagarajan","Alekhya Nandula","Hisham Nasir","Shauna Nehra","Josh Netto-Rosen","Daniel Ohashi","James Owers-Bardsley","Jason Ozuzu","Dennis Padilla","Gloria Park","Sam Passaglia","Jeremy Pekmez","Laura Penstone","Aleksandra Piktus","Case Ploeg","Andrew Poulton","Youran Qi","Shubha Raghvendra","Miguel Ramos","Ekagra Ranjan","Pierre Richemond","Cécile Robert-Michon","Aurélien Rodriguez","Sudip Roy","Sebastian Ruder","Laura Ruis","Louise Rust","Anubhav Sachan","Alejandro Salamanca","Kailash Karthik Saravanakumar","Isha Satyakam","Alice Schoenauer Sebag","Priyanka Sen","Sholeh Sepehri","Preethi Seshadri","Ye Shen","Tom Sherborne","Sylvie Shang Shi","Sanal Shivaprasad","Vladyslav Shmyhlo","Anirudh Shrinivason","Inna Shteinbuk","Amir Shukayev","Mathieu Simard","Ella Snyder","Ava Spataru","Victoria Spooner","Trisha Starostina","Florian Strub","Yixuan Su","Jimin Sun","Dwarak Talupuru","Eugene Tarassov","Elena Tommasone","Jennifer Tracey","Billy Trend","Evren Tumer","Ahmet Üstün","Bharat Venkitesh","David Venuto","Pat Verga","Maxime Voisin","Alex Wang","Donglu Wang","Shijian Wang","Edmond Wen","Naomi White","Jesse Willman","Marysia Winkels","Chen Xia","Jessica Xie","Minjie Xu","Bowen Yang","Tan Yi-Chern","Ivan Zhang","Zhenyu Zhao","Zhoujie Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.00698v2.pdf","comment":"55 pages"},{"id":"http://arxiv.org/abs/2503.23174v3","updated":"2025-04-14T12:34:14Z","published":"2025-03-29T18:06:28Z","title":"TRA: Better Length Generalisation with Threshold Relative Attention","summary":"  Transformers struggle with length generalisation, displaying poor performance\neven on basic tasks. We test whether these limitations can be explained through\ntwo key failures of the self-attention mechanism. The first is the inability to\nfully remove irrelevant information. The second is tied to position, even if\nthe dot product between a key and query is highly negative (i.e. an irrelevant\nkey) learned positional biases may unintentionally up-weight such information -\ndangerous when distances become out of distribution. Put together, these two\nfailure cases lead to compounding generalisation difficulties. We test whether\nthey can be mitigated through the combination of a) selective sparsity -\ncompletely removing irrelevant keys from the attention softmax and b)\ncontextualised relative distance - distance is only considered as between the\nquery and the keys that matter. We show how refactoring the attention mechanism\nwith these two mitigations in place can substantially improve generalisation\ncapabilities of decoder only transformers.\n","authors":["Mattia Opper","Roland Fernandez","Paul Smolensky","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2503.23174v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10179v1","updated":"2025-04-14T12:31:39Z","published":"2025-04-14T12:31:39Z","title":"The Future of MLLM Prompting is Adaptive: A Comprehensive Experimental\n  Evaluation of Prompt Engineering Methods for Robust Multimodal Performance","summary":"  Multimodal Large Language Models (MLLMs) are set to transform how machines\nprocess and generate human-like responses by integrating diverse modalities\nsuch as text, images, and code. Yet, effectively harnessing their capabilities\nhinges on optimal prompt engineering. We present a comprehensive experimental\nevaluation of seven prompt engineering methods applied to 13 open-source MLLMs\nover 24 tasks spanning Reasoning and Compositionality, Multimodal Understanding\nand Alignment, Complex Code Generation and Execution, and Knowledge Retrieval\nand Integration. Our approach stratifies models by parameter count into Small\n(<4B), Medium (4B-10B), and Large (>10B) categories and compares prompting\ntechniques including Zero-Shot, One-Shot, Few-Shot, Chain-of-Thought,\nAnalogical, Generated Knowledge, and Tree-of-Thought. While Large MLLMs excel\nin structured tasks such as code generation, achieving accuracies up to 96.88%\nunder Few-Shot prompting, all models struggle with complex reasoning and\nabstract understanding, often yielding accuracies below 60% and high\nhallucination rates. Structured reasoning prompts frequently increased\nhallucination up to 75% in small models and led to longer response times (over\n20 seconds in Large MLLMs), while simpler prompting methods provided more\nconcise and efficient outputs. No single prompting method uniformly optimises\nall task types. Instead, adaptive strategies combining example-based guidance\nwith selective structured reasoning are essential to enhance robustness,\nefficiency, and factual accuracy. Our findings offer practical recommendations\nfor prompt engineering and support more reliable deployment of MLLMs across\napplications including AI-assisted coding, knowledge retrieval, and multimodal\ncontent understanding.\n","authors":["Anwesha Mohanty","Venkatesh Balavadhani Parthasarathy","Arsalan Shahid"],"pdf_url":"https://arxiv.org/pdf/2504.10179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13929v6","updated":"2025-04-14T12:23:27Z","published":"2024-05-22T18:58:58Z","title":"Vikhr: The Family of Open-Source Instruction-Tuned Large Language Models\n  for Russian","summary":"  There has been a surge in the development of various Large Language Models\n(LLMs). However, text generation for languages other than English often faces\nsignificant challenges, including poor generation quality and reduced\ncomputational performance due to the disproportionate representation of tokens\nin the model's vocabulary. In this work, we address these issues by developing\na pipeline for the adaptation of English-oriented pre-trained models to other\nlanguages and constructing efficient bilingual LLMs. Using this pipeline, we\nconstruct Vikhr, a series of bilingual open-source instruction-following LLMs\ndesigned specifically for the Russian language. ``Vikhr'' refers to the name of\nthe Mistral LLM series and means a ``strong gust of wind.'' Unlike previous\nRussian-language models that typically rely on LoRA adapters on top of\nEnglish-oriented models, sacrificing performance for lower training costs,\nVikhr features an adapted tokenizer vocabulary and undergoes the continued\npre-training and instruction tuning of all weights. This not only enhances the\nmodel's performance but also significantly improves its computational and\ncontextual efficiency. We also expanded the instruction datasets and corpora\nfor continued pre-training. The model weights, instruction sets, and code are\npublicly available.\n","authors":["Aleksandr Nikolich","Konstantin Korolev","Sergei Bratchikov","Igor Kiselev","Artem Shelmanov"],"pdf_url":"https://arxiv.org/pdf/2405.13929v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10168v1","updated":"2025-04-14T12:22:30Z","published":"2025-04-14T12:22:30Z","title":"HalluSearch at SemEval-2025 Task 3: A Search-Enhanced RAG Pipeline for\n  Hallucination Detection","summary":"  In this paper, we present HalluSearch, a multilingual pipeline designed to\ndetect fabricated text spans in Large Language Model (LLM) outputs. Developed\nas part of Mu-SHROOM, the Multilingual Shared-task on Hallucinations and\nRelated Observable Overgeneration Mistakes, HalluSearch couples\nretrieval-augmented verification with fine-grained factual splitting to\nidentify and localize hallucinations in fourteen different languages. Empirical\nevaluations show that HalluSearch performs competitively, placing fourth in\nboth English (within the top ten percent) and Czech. While the system's\nretrieval-based strategy generally proves robust, it faces challenges in\nlanguages with limited online coverage, underscoring the need for further\nresearch to ensure consistent hallucination detection across diverse linguistic\ncontexts.\n","authors":["Mohamed A. Abdallah","Samhaa R. El-Beltagy"],"pdf_url":"https://arxiv.org/pdf/2504.10168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10167v1","updated":"2025-04-14T12:21:55Z","published":"2025-04-14T12:21:55Z","title":"C-FAITH: A Chinese Fine-Grained Benchmark for Automated Hallucination\n  Evaluation","summary":"  Despite the rapid advancement of large language models, they remain highly\nsusceptible to generating hallucinations, which significantly hinders their\nwidespread application. Hallucination research requires dynamic and\nfine-grained evaluation. However, most existing hallucination benchmarks\n(especially in Chinese language) rely on human annotations, making automatical\nand cost-effective hallucination evaluation challenging. To address this, we\nintroduce HaluAgent, an agentic framework that automatically constructs\nfine-grained QA dataset based on some knowledge documents. Our experiments\ndemonstrate that the manually designed rules and prompt optimization can\nimprove the quality of generated data. Using HaluAgent, we construct C-FAITH, a\nChinese QA hallucination benchmark created from 1,399 knowledge documents\nobtained from web scraping, totaling 60,702 entries. We comprehensively\nevaluate 16 mainstream LLMs with our proposed C-FAITH, providing detailed\nexperimental results and analysis.\n","authors":["Xu Zhang","Zhifei Liu","Jiahao Wang","Huixuan Zhang","Fan Xu","Junzhe Zhang","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2504.10167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10160v1","updated":"2025-04-14T12:14:18Z","published":"2025-04-14T12:14:18Z","title":"MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like\n  Reinforcement Learning","summary":"  Large-scale reinforcement learning (RL) methods have proven highly effective\nin enhancing the reasoning abilities of large language models (LLMs),\nparticularly for tasks with verifiable solutions such as mathematics and\ncoding. However, applying this idea to machine translation (MT), where outputs\nare flexibly formatted and difficult to automatically evaluate with explicit\nrules, remains underexplored. In this work, we introduce MT-R1-Zero, the first\nopen-source adaptation of the R1-Zero RL framework for MT without supervised\nfine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to\nguide LLMs towards improved translation quality via emergent reasoning. On the\nWMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive\nperformance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points.\nMeanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across\nall metrics, placing it on par with advanced proprietary models such as GPT-4o\nand Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves\nstate-of-the-art scores on semantic metrics. Moreover, our work exhibits strong\ngeneralization capabilities on out-of-distribution MT tasks, robustly\nsupporting multilingual and low-resource settings. Extensive analysis of model\nbehavior across different initializations and reward metrics offers pioneering\ninsight into the critical role of reward design, LLM adaptability, training\ndynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT.\nOur code is available at https://github.com/fzp0424/MT-R1-Zero.\n","authors":["Zhaopeng Feng","Shaosheng Cao","Jiahan Ren","Jiayuan Su","Ruizhe Chen","Yan Zhang","Zhe Xu","Yao Hu","Jian Wu","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2504.10160v1.pdf","comment":"Work in progress. Our code is available at\n  https://github.com/fzp0424/MT-R1-Zero"},{"id":"http://arxiv.org/abs/2504.10157v1","updated":"2025-04-14T12:12:52Z","published":"2025-04-14T12:12:52Z","title":"SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users","summary":"  Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments.\n","authors":["Xinnong Zhang","Jiayu Lin","Xinyi Mou","Shiyue Yang","Xiawei Liu","Libo Sun","Hanjia Lyu","Yihang Yang","Weihong Qi","Yue Chen","Guanying Li","Ling Yan","Yao Hu","Siming Chen","Yu Wang","Jingxuan Huang","Jiebo Luo","Shiping Tang","Libo Wu","Baohua Zhou","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2504.10157v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2504.10127v1","updated":"2025-04-14T11:35:02Z","published":"2025-04-14T11:35:02Z","title":"Breaking the Data Barrier -- Building GUI Agents Through Task\n  Generalization","summary":"  Graphical User Interface (GUI) agents offer cross-platform solutions for\nautomating complex digital tasks, with significant potential to transform\nproductivity workflows. However, their performance is often constrained by the\nscarcity of high-quality trajectory data. To address this limitation, we\npropose training Vision Language Models (VLMs) on data-rich,\nreasoning-intensive tasks during a dedicated mid-training stage, and then\nexamine how incorporating these tasks facilitates generalization to GUI\nplanning scenarios. Specifically, we explore a range of tasks with readily\navailable instruction-tuning data, including GUI perception, multimodal\nreasoning, and textual reasoning. Through extensive experiments across 11\nmid-training tasks, we demonstrate that: (1) Task generalization proves highly\neffective, yielding substantial improvements across most settings. For\ninstance, multimodal mathematical reasoning enhances performance on\nAndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data\nsignificantly boosts GUI web agent performance, achieving a 5.6% improvement on\nWebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal\ngeneralization from text-based to visual domains; (2) Contrary to prior\nassumptions, GUI perception data - previously considered closely aligned with\nGUI agent tasks and widely utilized for training - has a comparatively limited\nimpact on final performance; (3) Building on these insights, we identify the\nmost effective mid-training tasks and curate optimized mixture datasets,\nresulting in absolute performance gains of 8.0% on WebArena and 12.2% on\nAndroidWorld. Our work provides valuable insights into cross-domain knowledge\ntransfer for GUI agents and offers a practical approach to addressing data\nscarcity challenges in this emerging field. The code, data and models will be\navailable at https://github.com/hkust-nlp/GUIMid.\n","authors":["Junlei Zhang","Zichen Ding","Chang Ma","Zijie Chen","Qiushi Sun","Zhenzhong Lan","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2504.10127v1.pdf","comment":"24 pages, 11 figures"},{"id":"http://arxiv.org/abs/2503.08980v2","updated":"2025-04-14T11:00:31Z","published":"2025-03-12T01:21:17Z","title":"I Predict Therefore I Am: Is Next Token Prediction Enough to Learn\n  Human-Interpretable Concepts from Data?","summary":"  The remarkable achievements of large language models (LLMs) have led many to\nconclude that they exhibit a form of intelligence. This is as opposed to\nexplanations of their capabilities based on their ability to perform relatively\nsimple manipulations of vast volumes of data. To illuminate the distinction\nbetween these explanations, we introduce a novel generative model that\ngenerates tokens on the basis of human interpretable concepts represented as\nlatent discrete variables. Under mild conditions, even when the mapping from\nthe latent space to the observed space is non-invertible, we establish an\nidentifiability result: the representations learned by LLMs through next-token\nprediction can be approximately modeled as the logarithm of the posterior\nprobabilities of these latent discrete concepts, up to an invertible linear\ntransformation. This theoretical finding not only provides evidence that LLMs\ncapture underlying generative factors, but also strongly reinforces the linear\nrepresentation hypothesis, which posits that LLMs learn linear representations\nof human-interpretable concepts. Empirically, we validate our theoretical\nresults through evaluations on both simulation data and the Pythia, Llama, and\nDeepSeek model families.\n","authors":["Yuhang Liu","Dong Gong","Erdun Gao","Zhen Zhang","Biwei Huang","Mingming Gong","Anton van den Hengel","Javen Qinfeng Shi"],"pdf_url":"https://arxiv.org/pdf/2503.08980v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10090v1","updated":"2025-04-14T10:53:44Z","published":"2025-04-14T10:53:44Z","title":"CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography","summary":"  Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning.\n","authors":["I-Sheng Fang","Jun-Cheng Chen"],"pdf_url":"https://arxiv.org/pdf/2504.10090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15004v3","updated":"2025-04-14T10:36:33Z","published":"2024-12-19T16:20:22Z","title":"From Vulnerabilities to Remediation: A Systematic Literature Review of\n  LLMs in Code Security","summary":"  Large Language Models (LLMs) have emerged as powerful tools for automating\nvarious programming tasks, including security-related ones, such as detecting\nand fixing vulnerabilities. Despite their promising capabilities, when required\nto produce or modify pre-existing code, LLMs could introduce vulnerabilities\nunbeknown to the programmer. When analyzing code, they could miss clear\nvulnerabilities or signal nonexistent ones. In this Systematic Literature\nReview (SLR), we aim to investigate both the security benefits and potential\ndrawbacks of using LLMs for a variety of code-related tasks. In particular,\nfirst we focus on the types of vulnerabilities that could be introduced by\nLLMs, when used for producing code. Second, we analyze the capabilities of LLMs\nto detect and fix vulnerabilities, in any given code, and how the prompting\nstrategy of choice impacts their performance in these two tasks. Last, we\nprovide an in-depth analysis on how data poisoning attacks on LLMs can impact\nperformance in the aforementioned tasks.\n","authors":["Enna Basic","Alberto Giaretta"],"pdf_url":"https://arxiv.org/pdf/2412.15004v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10081v1","updated":"2025-04-14T10:26:37Z","published":"2025-04-14T10:26:37Z","title":"RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning\n  Capability","summary":"  Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have been\nrapidly progressing and achieving breakthrough performance on complex reasoning\ntasks such as mathematics and coding. However, the open-source R1 models have\nraised safety concerns in wide applications, such as the tendency to comply\nwith malicious queries, which greatly impacts the utility of these powerful\nmodels in their applications. In this paper, we introduce RealSafe-R1 as\nsafety-aligned versions of DeepSeek-R1 distilled models. To train these models,\nwe construct a dataset of 15k safety-aware reasoning trajectories generated by\nDeepSeek-R1, under explicit instructions for expected refusal behavior. Both\nquantitative experiments and qualitative case studies demonstrate the models'\nimprovements, which are shown in their safety guardrails against both harmful\nqueries and jailbreak attacks. Importantly, unlike prior safety alignment\nefforts that often compromise reasoning performance, our method preserves the\nmodels' reasoning capabilities by maintaining the training data within the\noriginal distribution of generation. Model weights of RealSafe-R1 are\nopen-source at https://huggingface.co/RealSafe.\n","authors":["Yichi Zhang","Zihao Zeng","Dongbai Li","Yao Huang","Zhijie Deng","Yinpeng Dong"],"pdf_url":"https://arxiv.org/pdf/2504.10081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10077v1","updated":"2025-04-14T10:21:59Z","published":"2025-04-14T10:21:59Z","title":"Towards Quantifying Commonsense Reasoning with Mechanistic Insights","summary":"  Commonsense reasoning deals with the implicit knowledge that is well\nunderstood by humans and typically acquired via interactions with the world. In\nrecent times, commonsense reasoning and understanding of various LLMs have been\nevaluated using text-based tasks. In this work, we argue that a proxy of this\nunderstanding can be maintained as a graphical structure that can further help\nto perform a rigorous evaluation of commonsense reasoning abilities about\nvarious real-world activities. We create an annotation scheme for capturing\nthis implicit knowledge in the form of a graphical structure for 37 daily human\nactivities. We find that the created resource can be used to frame an enormous\nnumber of commonsense queries (~ 10^{17}), facilitating rigorous evaluation of\ncommonsense reasoning in LLMs. Moreover, recently, the remarkable performance\nof LLMs has raised questions about whether these models are truly capable of\nreasoning in the wild and, in general, how reasoning occurs inside these\nmodels. In this resource paper, we bridge this gap by proposing design\nmechanisms that facilitate research in a similar direction. Our findings\nsuggest that the reasoning components are localized in LLMs that play a\nprominent role in decision-making when prompted with a commonsense query.\n","authors":["Abhinav Joshi","Areeb Ahmad","Divyaksh Shukla","Ashutosh Modi"],"pdf_url":"https://arxiv.org/pdf/2504.10077v1.pdf","comment":"Accepted at NAACL 2025; 28 pages (9 pages + 7 pages references + 12\n  pages appendix)"},{"id":"http://arxiv.org/abs/2504.10068v1","updated":"2025-04-14T10:14:44Z","published":"2025-04-14T10:14:44Z","title":"Mavors: Multi-granularity Video Representation for Multimodal Large\n  Language Model","summary":"  Long-context video understanding in multimodal large language models (MLLMs)\nfaces a critical challenge: balancing computational efficiency with the\nretention of fine-grained spatio-temporal patterns. Existing approaches (e.g.,\nsparse sampling, dense sampling with low resolution, and token compression)\nsuffer from significant information loss in temporal dynamics, spatial details,\nor subtle interactions, particularly in videos with complex motion or varying\nresolutions. To address this, we propose $\\mathbf{Mavors}$, a novel framework\nthat introduces $\\mathbf{M}$ulti-gr$\\mathbf{a}$nularity\n$\\mathbf{v}$ide$\\mathbf{o}$ $\\mathbf{r}$epre$\\mathbf{s}$entation for holistic\nlong-video modeling. Specifically, Mavors directly encodes raw video content\ninto latent representations through two core components: 1) an Intra-chunk\nVision Encoder (IVE) that preserves high-resolution spatial features via 3D\nconvolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator\n(IFA) that establishes temporal coherence across chunks using transformer-based\ndependency modeling with chunk-level rotary position encodings. Moreover, the\nframework unifies image and video understanding by treating images as\nsingle-frame videos via sub-image decomposition. Experiments across diverse\nbenchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity\nand temporal continuity, significantly outperforming existing methods in tasks\nrequiring fine-grained spatio-temporal reasoning.\n","authors":["Yang Shi","Jiaheng Liu","Yushuo Guan","Zhenhua Wu","Yuanxing Zhang","Zihao Wang","Weihong Lin","Jingyun Hua","Zekun Wang","Xinlong Chen","Bohan Zeng","Wentao Zhang","Fuzheng Zhang","Wenjing Yang","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.10068v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2401.00448v3","updated":"2025-04-14T10:11:13Z","published":"2023-12-31T10:53:58Z","title":"Beyond Chinchilla-Optimal: Accounting for Inference in Language Model\n  Scaling Laws","summary":"  Large language model (LLM) scaling laws are empirical formulas that estimate\nchanges in model quality as a result of increasing parameter count and training\ndata. However, these formulas, including the popular Deepmind Chinchilla\nscaling laws, neglect to include the cost of inference. We modify the\nChinchilla scaling laws to calculate the optimal LLM parameter count and\npre-training data size to train and deploy a model of a given quality and\ninference demand. We conduct our analysis both in terms of a compute budget and\nreal-world costs and find that LLM researchers expecting reasonably large\ninference demand (~1B requests) should train models smaller and longer than\nChinchilla-optimal. Furthermore, we train 47 models of varying sizes and\nparameter counts to validate our formula and find that model quality continues\nto improve as we scale tokens per parameter to extreme ranges (up to 10,000).\nFinally, we ablate the procedure used to fit the Chinchilla scaling law\ncoefficients and find that developing scaling laws only from data collected at\ntypical token/parameter ratios overestimates the impact of additional tokens at\nthese extreme ranges.\n","authors":["Nikhil Sardana","Jacob Portes","Sasha Doubov","Jonathan Frankle"],"pdf_url":"https://arxiv.org/pdf/2401.00448v3.pdf","comment":"16 pages, 7 figures, In the 41st International Conference on Machine\n  Learning, 2024"},{"id":"http://arxiv.org/abs/2504.10065v1","updated":"2025-04-14T10:08:28Z","published":"2025-04-14T10:08:28Z","title":"A Computational Cognitive Model for Processing Repetitions of\n  Hierarchical Relations","summary":"  Patterns are fundamental to human cognition, enabling the recognition of\nstructure and regularity across diverse domains. In this work, we focus on\nstructural repeats, patterns that arise from the repetition of hierarchical\nrelations within sequential data, and develop a candidate computational model\nof how humans detect and understand such structural repeats. Based on a\nweighted deduction system, our model infers the minimal generative process of a\ngiven sequence in the form of a Template program, a formalism that enriches the\ncontext-free grammar with repetition combinators. Such representation\nefficiently encodes the repetition of sub-computations in a recursive manner.\nAs a proof of concept, we demonstrate the expressiveness of our model on short\nsequences from music and action planning. The proposed model offers broader\ninsights into the mental representations and cognitive mechanisms underlying\nhuman pattern recognition.\n","authors":["Zeng Ren","Xinyi Guan","Martin Rohrmeier"],"pdf_url":"https://arxiv.org/pdf/2504.10065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10063v1","updated":"2025-04-14T10:06:27Z","published":"2025-04-14T10:06:27Z","title":"Hallucination Detection in LLMs via Topological Divergence on Attention\n  Graphs","summary":"  Hallucination, i.e., generating factually incorrect content, remains a\ncritical challenge for large language models (LLMs). We introduce TOHA, a\nTOpology-based HAllucination detector in the RAG setting, which leverages a\ntopological divergence metric to quantify the structural properties of graphs\ninduced by attention matrices. Examining the topological divergence between\nprompt and response subgraphs reveals consistent patterns: higher divergence\nvalues in specific attention heads correlate with hallucinated outputs,\nindependent of the dataset. Extensive experiments, including evaluation on\nquestion answering and data-to-text tasks, show that our approach achieves\nstate-of-the-art or competitive results on several benchmarks, two of which\nwere annotated by us and are being publicly released to facilitate further\nresearch. Beyond its strong in-domain performance, TOHA maintains remarkable\ndomain transferability across multiple open-source LLMs. Our findings suggest\nthat analyzing the topological structure of attention matrices can serve as an\nefficient and robust indicator of factual reliability in LLMs.\n","authors":["Alexandra Bazarova","Aleksandr Yugay","Andrey Shulga","Alina Ermilova","Andrei Volodichev","Konstantin Polev","Julia Belikova","Rauf Parchiev","Dmitry Simakov","Maxim Savchenko","Andrey Savchenko","Serguei Barannikov","Alexey Zaytsev"],"pdf_url":"https://arxiv.org/pdf/2504.10063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10055v1","updated":"2025-04-14T09:57:37Z","published":"2025-04-14T09:57:37Z","title":"Joint Action Language Modelling for Transparent Policy Execution","summary":"  An agent's intention often remains hidden behind the black-box nature of\nembodied policies. Communication using natural language statements that\ndescribe the next action can provide transparency towards the agent's behavior.\nWe aim to insert transparent behavior directly into the learning process, by\ntransforming the problem of policy learning into a language generation problem\nand combining it with traditional autoregressive modelling. The resulting model\nproduces transparent natural language statements followed by tokens\nrepresenting the specific actions to solve long-horizon tasks in the\nLanguage-Table environment. Following previous work, the model is able to learn\nto produce a policy represented by special discretized tokens in an\nautoregressive manner. We place special emphasis on investigating the\nrelationship between predicting actions and producing high-quality language for\na transparent agent. We find that in many cases both the quality of the action\ntrajectory and the transparent statement increase when they are generated\nsimultaneously.\n","authors":["Theodor Wulff","Rahul Singh Maharjan","Xinyun Chi","Angelo Cangelosi"],"pdf_url":"https://arxiv.org/pdf/2504.10055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10049v1","updated":"2025-04-14T09:55:01Z","published":"2025-04-14T09:55:01Z","title":"Summarization of Multimodal Presentations with Vision-Language Models:\n  Study of the Effect of Modalities and Structure","summary":"  Vision-Language Models (VLMs) can process visual and textual information in\nmultiple formats: texts, images, interleaved texts and images, or even\nhour-long videos. In this work, we conduct fine-grained quantitative and\nqualitative analyses of automatic summarization of multimodal presentations\nusing VLMs with various representations as input. From these experiments, we\nsuggest cost-effective strategies for generating summaries from text-heavy\nmultimodal documents under different input-length budgets using VLMs. We show\nthat slides extracted from the video stream can be beneficially used as input\nagainst the raw video, and that a structured representation from interleaved\nslides and transcript provides the best performance. Finally, we reflect and\ncomment on the nature of cross-modal interactions in multimodal presentations\nand share suggestions to improve the capabilities of VLMs to understand\ndocuments of this nature.\n","authors":["Théo Gigant","Camille Guinaudeau","Frédéric Dufaux"],"pdf_url":"https://arxiv.org/pdf/2504.10049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10036v1","updated":"2025-04-14T09:38:23Z","published":"2025-04-14T09:38:23Z","title":"DataMosaic: Explainable and Verifiable Multi-Modal Data Analytics\n  through Extract-Reason-Verify","summary":"  Large Language Models (LLMs) are transforming data analytics, but their\nwidespread adoption is hindered by two critical limitations: they are not\nexplainable (opaque reasoning processes) and not verifiable (prone to\nhallucinations and unchecked errors). While retrieval-augmented generation\n(RAG) improves accuracy by grounding LLMs in external data, it fails to address\nthe core challenges of trustworthy analytics - especially when processing\nnoisy, inconsistent, or multi-modal data (for example, text, tables, images).\nWe propose DataMosaic, a framework designed to make LLM-powered analytics both\nexplainable and verifiable. By dynamically extracting task-specific structures\n(for example, tables, graphs, trees) from raw data, DataMosaic provides\ntransparent, step-by-step reasoning traces and enables validation of\nintermediate results. Built on a multi-agent framework, DataMosaic orchestrates\nself-adaptive agents that align with downstream task requirements, enhancing\nconsistency, completeness, and privacy. Through this approach, DataMosaic not\nonly tackles the limitations of current LLM-powered analytics systems but also\nlays the groundwork for a new paradigm of grounded, accurate, and explainable\nmulti-modal data analytics.\n","authors":["Zhengxuan Zhang","Zhuowen Liang","Yin Wu","Teng Lin","Yuyu Luo","Nan Tang"],"pdf_url":"https://arxiv.org/pdf/2504.10036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08525v2","updated":"2025-04-14T09:38:19Z","published":"2025-04-11T13:38:36Z","title":"Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware\n  Extensions for Multi-Step LLM Agent Tasks","summary":"  Large Language Models (LLMs) are increasingly used as autonomous agents for\nmulti-step tasks. However, most existing frameworks fail to maintain a\nstructured understanding of the task state, often relying on linear prompt\nconcatenation or shallow memory buffers. This leads to brittle performance,\nfrequent hallucinations, and poor long-range coherence. In this work, we\npropose the Task Memory Engine (TME), a lightweight and structured memory\nmodule that tracks task execution using a hierarchical Task Memory Tree (TMT).\nEach node in the tree corresponds to a task step, storing relevant input,\noutput, status, and sub-task relationships. We introduce a prompt synthesis\nmethod that dynamically generates LLM prompts based on the active node path,\nsignificantly improving execution consistency and contextual grounding. Through\ncase studies and comparative experiments on multi-step agent tasks, we\ndemonstrate that TME leads to better task completion accuracy and more\ninterpretable behavior with minimal implementation overhead. A reference\nimplementation of the core TME components is available at\nhttps://github.com/biubiutomato/TME-Agent, including basic examples and\nstructured memory integration. While the current implementation uses a\ntree-based structure, TME is designed to be graph-aware, supporting reusable\nsubsteps, converging task paths, and shared dependencies. This lays the\ngroundwork for future DAG-based memory architectures.\n","authors":["Ye Ye"],"pdf_url":"https://arxiv.org/pdf/2504.08525v2.pdf","comment":"14 pages, 5 figures. Preprint prepared for future submission.\n  Includes implementation and token-efficiency analysis. Code at\n  https://github.com/biubiutomato/TME-Agent"},{"id":"http://arxiv.org/abs/2504.10020v1","updated":"2025-04-14T09:25:37Z","published":"2025-04-14T09:25:37Z","title":"The Mirage of Performance Gains: Why Contrastive Decoding Fails to\n  Address Multimodal Hallucination","summary":"  Contrastive decoding strategies are widely used to reduce hallucinations in\nmultimodal large language models (MLLMs). These methods work by constructing\ncontrastive samples to induce hallucinations and then suppressing them in the\noutput distribution. However, this paper demonstrates that such approaches fail\nto effectively mitigate the hallucination problem. The performance improvements\nobserved on POPE Benchmark are largely driven by two misleading factors: (1)\ncrude, unidirectional adjustments to the model's output distribution and (2)\nthe adaptive plausibility constraint, which reduces the sampling strategy to\ngreedy search. To further illustrate these issues, we introduce a series of\nspurious improvement methods and evaluate their performance against contrastive\ndecoding techniques. Experimental results reveal that the observed performance\ngains in contrastive decoding are entirely unrelated to its intended goal of\nmitigating hallucinations. Our findings challenge common assumptions about the\neffectiveness of contrastive decoding strategies and pave the way for\ndeveloping genuinely effective solutions to hallucinations in MLLMs.\n","authors":["Hao Yin","Gunagzong Si","Zilei Wang"],"pdf_url":"https://arxiv.org/pdf/2504.10020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12486v4","updated":"2025-04-14T09:24:54Z","published":"2025-02-18T03:15:55Z","title":"EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via\n  Reinforcement Learning","summary":"  Large Language Models (LLMs) have shown impressive reasoning capabilities in\nwell-defined problems with clear solutions, such as mathematics and coding.\nHowever, they still struggle with complex real-world scenarios like business\nnegotiations, which require strategic reasoning-an ability to navigate dynamic\nenvironments and align long-term goals amidst uncertainty. Existing methods for\nstrategic reasoning face challenges in adaptability, scalability, and\ntransferring strategies to new contexts. To address these issues, we propose\nexplicit policy optimization (EPO) for strategic reasoning, featuring an LLM\nthat provides strategies in open-ended action space and can be plugged into\narbitrary LLM agents to motivate goal-directed behavior. To improve\nadaptability and policy transferability, we train the strategic reasoning model\nvia multi-turn reinforcement learning (RL) using process rewards and iterative\nself-play, without supervised fine-tuning (SFT) as a preliminary step.\nExperiments across social and physical domains demonstrate EPO's ability of\nlong-term goal alignment through enhanced strategic reasoning, achieving\nstate-of-the-art performance on social dialogue and web navigation tasks. Our\nfindings reveal various collaborative reasoning mechanisms emergent in EPO and\nits effectiveness in generating novel strategies, underscoring its potential\nfor strategic reasoning in real-world applications.\n","authors":["Xiaoqian Liu","Ke Wang","Yongbin Li","Yuchuan Wu","Wentao Ma","Aobo Kong","Fei Huang","Jianbin Jiao","Junge Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12486v4.pdf","comment":"22 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.12877v2","updated":"2025-04-14T09:04:45Z","published":"2024-10-15T08:38:20Z","title":"Improving Instruction-Following in Language Models through Activation\n  Steering","summary":"  The ability to follow instructions is crucial for numerous real-world\napplications of language models. In pursuit of deeper insights and more\npowerful capabilities, we derive instruction-specific vector representations\nfrom language models and use them to steer models accordingly. These vectors\nare computed as the difference in activations between inputs with and without\ninstructions, enabling a modular approach to activation steering. We\ndemonstrate how this method can enhance model adherence to constraints such as\noutput format, length, and word inclusion, providing inference-time control\nover instruction following. Our experiments across four models demonstrate how\nwe can use the activation vectors to guide models to follow constraints even\nwithout explicit instructions and to enhance performance when instructions are\npresent. Additionally, we explore the compositionality of activation steering,\nsuccessfully applying multiple instructions simultaneously. Finally, we\ndemonstrate that steering vectors computed on instruction-tuned models can\ntransfer to improve base models. Our findings demonstrate that activation\nsteering offers a practical and scalable approach for fine-grained control in\nlanguage generation. Our code and data are available at\nhttps://github.com/microsoft/llm-steer-instruct.\n","authors":["Alessandro Stolfo","Vidhisha Balachandran","Safoora Yousefi","Eric Horvitz","Besmira Nushi"],"pdf_url":"https://arxiv.org/pdf/2410.12877v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2504.10000v1","updated":"2025-04-14T09:03:51Z","published":"2025-04-14T09:03:51Z","title":"Do We Really Need Curated Malicious Data for Safety Alignment in\n  Multi-modal Large Language Models?","summary":"  Multi-modal large language models (MLLMs) have made significant progress, yet\ntheir safety alignment remains limited. Typically, current open-source MLLMs\nrely on the alignment inherited from their language module to avoid harmful\ngenerations. However, the lack of safety measures specifically designed for\nmulti-modal inputs creates an alignment gap, leaving MLLMs vulnerable to\nvision-domain attacks such as typographic manipulation. Current methods utilize\na carefully designed safety dataset to enhance model defense capability, while\nthe specific knowledge or patterns acquired from the high-quality dataset\nremain unclear. Through comparison experiments, we find that the alignment gap\nprimarily arises from data distribution biases, while image content, response\nquality, or the contrastive behavior of the dataset makes little contribution\nto boosting multi-modal safety. To further investigate this and identify the\nkey factors in improving MLLM safety, we propose finetuning MLLMs on a small\nset of benign instruct-following data with responses replaced by simple, clear\nrejection sentences. Experiments show that, without the need for\nlabor-intensive collection of high-quality malicious data, model safety can\nstill be significantly improved, as long as a specific fraction of rejection\ndata exists in the finetuning set, indicating the security alignment is not\nlost but rather obscured during multi-modal pretraining or instruction\nfinetuning. Simply correcting the underlying data bias could narrow the safety\ngap in the vision domain.\n","authors":["Yanbo Wang","Jiyang Guan","Jian Liang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2504.10000v1.pdf","comment":"Accepted to CVPR 2025, codes in process"},{"id":"http://arxiv.org/abs/2504.09980v1","updated":"2025-04-14T08:45:04Z","published":"2025-04-14T08:45:04Z","title":"Turn-taking annotation for quantitative and qualitative analyses of\n  conversation","summary":"  This paper has two goals. First, we present the turn-taking annotation layers\ncreated for 95 minutes of conversational speech of the Graz Corpus of Read and\nSpontaneous Speech (GRASS), available to the scientific community. Second, we\ndescribe the annotation system and the annotation process in more detail, so\nother researchers may use it for their own conversational data. The annotation\nsystem was developed with an interdisciplinary application in mind. It should\nbe based on sequential criteria according to Conversation Analysis, suitable\nfor subsequent phonetic analysis, thus time-aligned annotations were made\nPraat, and it should be suitable for automatic classification, which required\nthe continuous annotation of speech and a label inventory that is not too large\nand results in a high inter-rater agreement. Turn-taking was annotated on two\nlayers, Inter-Pausal Units (IPU) and points of potential completion (PCOMP;\nsimilar to transition relevance places). We provide a detailed description of\nthe annotation process and of segmentation and labelling criteria. A detailed\nanalysis of inter-rater agreement and common confusions shows that agreement\nfor IPU annotation is near-perfect, that agreement for PCOMP annotations is\nsubstantial, and that disagreements often are either partial or can be\nexplained by a different analysis of a sequence which also has merit. The\nannotation system can be applied to a variety of conversational data for\nlinguistic studies and technological applications, and we hope that the\nannotations, as well as the annotation system will contribute to a stronger\ncross-fertilization between these disciplines.\n","authors":["Anneliese Kelterer","Barbara Schuppler"],"pdf_url":"https://arxiv.org/pdf/2504.09980v1.pdf","comment":"41 pages"},{"id":"http://arxiv.org/abs/2503.15166v2","updated":"2025-04-14T08:38:46Z","published":"2025-03-19T12:47:37Z","title":"Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive\n  Learning: Adapting Alignment Calibration to MERU","summary":"  Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC\n","authors":["Àlex Pujol Vidal","Sergio Escalera","Kamal Nasrollahi","Thomas B. Moeslund"],"pdf_url":"https://arxiv.org/pdf/2503.15166v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2402.02379v2","updated":"2025-04-14T08:30:35Z","published":"2024-02-04T07:33:45Z","title":"Unveiling the Deficiencies of Pre-trained Text-and-Layout Models in\n  Real-world Visually-rich Document Information Extraction","summary":"  Recently developed pre-trained text-and-layout models (PTLMs) have shown\nremarkable success in multiple information extraction tasks on visually-rich\ndocuments (VrDs). However, despite achieving extremely high performance on\nbenchmarks, their real-world performance falls short of expectations. Owing to\nthis issue, we investigate the prevailing evaluation pipeline to reveal that:\n(1) The inadequate annotations within benchmark datasets introduce spurious\ncorrelations between task inputs and labels, which would lead to\noverly-optimistic estimation of model performance. (2) The evaluation solely\nrelies on the performance on benchmarks and is insufficient to comprehensively\nexplore the capabilities of methods in real-world scenarios. These problems\nimpede the prevailing evaluation pipeline from reflecting the real-world\nperformance of methods, misleading the design choices of method optimization.\nIn this work, we introduce EC-FUNSD, an entity-centric dataset crafted for\nbenchmarking information extraction from visually-rich documents. This dataset\ncontains diverse layouts and high-quality annotations. Additionally, this\ndataset disentangles the falsely-coupled segment and entity annotations that\narises from the block-level annotation of FUNSD. Using the proposed dataset, we\nevaluate the real-world information extraction capabilities of PTLMs from\nmultiple aspects, including their absolute performance, as well as\ngeneralization, robustness and fairness. The results indicate that prevalent\nPTLMs do not perform as well as anticipated in real-world information\nextraction scenarios. We hope that our study can inspire reflection on the\ndirections of PTLM development.\n","authors":["Chong Zhang","Yixi Zhao","Yulu Xie","Chenshu Yuan","Yi Tu","Ya Guo","Mingxu Chai","Ziyu Shen","Yue Zhang","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.02379v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17039v2","updated":"2025-04-14T08:25:42Z","published":"2025-03-21T10:52:20Z","title":"Summarization Metrics for Spanish and Basque: Do Automatic Scores and\n  LLM-Judges Correlate with Humans?","summary":"  Studies on evaluation metrics and LLM-as-a-Judge models for automatic text\nsummarization have largely been focused on English, limiting our understanding\nof their effectiveness in other languages. Through our new dataset BASSE\n(BAsque and Spanish Summarization Evaluation), we address this situation by\ncollecting human judgments on 2,040 abstractive summaries in Basque and\nSpanish, generated either manually or by five LLMs with four different prompts.\nFor each summary, annotators evaluated five criteria on a 5-point Likert scale:\ncoherence, consistency, fluency, relevance, and 5W1H. We use these data to\nreevaluate traditional automatic metrics used for evaluating summaries, as well\nas several LLM-as-a-Judge models that show strong performance on this task in\nEnglish. Our results show that currently proprietary judge LLMs have the\nhighest correlation with human judgments, followed by criteria-specific\nautomatic metrics, while open-sourced judge LLMs perform poorly. We release\nBASSE and our code publicly, along with the first large-scale Basque\nsummarization dataset containing 22,525 news articles with their subheads.\n","authors":["Jeremy Barnes","Naiara Perez","Alba Bonet-Jover","Begoña Altuna"],"pdf_url":"https://arxiv.org/pdf/2503.17039v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09958v1","updated":"2025-04-14T07:55:47Z","published":"2025-04-14T07:55:47Z","title":"C-MTCSD: A Chinese Multi-Turn Conversational Stance Detection Dataset","summary":"  Stance detection has become an essential tool for analyzing public\ndiscussions on social media. Current methods face significant challenges,\nparticularly in Chinese language processing and multi-turn conversational\nanalysis. To address these limitations, we introduce C-MTCSD, the largest\nChinese multi-turn conversational stance detection dataset, comprising 24,264\ncarefully annotated instances from Sina Weibo, which is 4.2 times larger than\nthe only prior Chinese conversational stance detection dataset. Our\ncomprehensive evaluation using both traditional approaches and large language\nmodels reveals the complexity of C-MTCSD: even state-of-the-art models achieve\nonly 64.07% F1 score in the challenging zero-shot setting, while performance\nconsistently degrades with increasing conversation depth. Traditional models\nparticularly struggle with implicit stance detection, achieving below 50% F1\nscore. This work establishes a challenging new benchmark for Chinese stance\ndetection research, highlighting significant opportunities for future\nimprovements.\n","authors":["Fuqiang Niu","Yi Yang","Xianghua Fu","Genan Dai","Bowen Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.09958v1.pdf","comment":"WWW2025"},{"id":"http://arxiv.org/abs/2504.09946v1","updated":"2025-04-14T07:14:27Z","published":"2025-04-14T07:14:27Z","title":"Assessing Judging Bias in Large Reasoning Models: An Empirical Study","summary":"  Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have\ndemonstrated remarkable reasoning capabilities, raising important questions\nabout their biases in LLM-as-a-judge settings. We present a comprehensive\nbenchmark comparing judging biases between LLMs and LRMs across both subjective\npreference-alignment datasets and objective fact-based datasets. Through\ninvestigation of bandwagon, authority, position, and distraction biases, we\nuncover four key findings: (1) despite their advanced reasoning capabilities,\nLRMs remain susceptible to the above biases; (2) LRMs demonstrate better\nrobustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit\nnotable position bias, preferring options in later positions; and (4) we\nidentify a novel \"superficial reflection bias\" where phrases mimicking\nreasoning (e.g., \"wait, let me think...\") significantly influence model\njudgments. To address these biases, we design and evaluate three mitigation\nstrategies: specialized system prompts that reduce judging biases by up to 19\\%\nin preference alignment datasets and 14\\% in fact-related datasets, in-context\nlearning that provides up to 27\\% improvement on preference tasks but shows\ninconsistent results on factual tasks, and a self-reflection mechanism that\nreduces biases by up to 10\\% in preference datasets and 16\\% in fact-related\ndatasets, with self-reflection proving particularly effective for LRMs. Our\nwork provides crucial insights for developing more reliable LLM-as-a-Judge\nframeworks, especially as LRMs become increasingly deployed as automated\njudges.\n","authors":["Qian Wang","Zhanzhi Lou","Zhenheng Tang","Nuo Chen","Xuandong Zhao","Wenxuan Zhang","Dawn Song","Bingsheng He"],"pdf_url":"https://arxiv.org/pdf/2504.09946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09936v1","updated":"2025-04-14T06:58:00Z","published":"2025-04-14T06:58:00Z","title":"KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference","summary":"  Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets.\n","authors":["Yuxuan Tian","Zihan Wang","Yebo Peng","Aomufei Yuan","Zhiming Wang","Bairen Yi","Xin Liu","Yong Cui","Tong Yang"],"pdf_url":"https://arxiv.org/pdf/2504.09936v1.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2211.13613v3","updated":"2025-04-14T06:52:30Z","published":"2022-11-24T13:59:32Z","title":"Ham2Pose: Animating Sign Language Notation into Pose Sequences","summary":"  Translating spoken languages into Sign languages is necessary for open\ncommunication between the hearing and hearing-impaired communities. To achieve\nthis goal, we propose the first method for animating a text written in\nHamNoSys, a lexical Sign language notation, into signed pose sequences. As\nHamNoSys is universal, our proposed method offers a generic solution invariant\nto the target Sign language. Our method gradually generates pose predictions\nusing transformer encoders that create meaningful representations of the text\nand poses while considering their spatial and temporal information. We use weak\nsupervision for the training process and show that our method succeeds in\nlearning from partial and inaccurate data. Additionally, we offer a new\ndistance measurement for pose sequences, normalized Dynamic Time Warping\n(nDTW), based on DTW over normalized keypoints trajectories, and validate its\ncorrectness using AUTSL, a large-scale Sign language dataset. We show that it\nmeasures the distance between pose sequences more accurately than existing\nmeasurements and use it to assess the quality of our generated pose sequences.\nCode for the data pre-processing, the model, and the distance measurement is\npublicly released for future research.\n","authors":["Rotem Shalev-Arkushin","Amit Moryossef","Ohad Fried"],"pdf_url":"https://arxiv.org/pdf/2211.13613v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09923v1","updated":"2025-04-14T06:32:45Z","published":"2025-04-14T06:32:45Z","title":"Guiding Reasoning in Small Language Models with LLM Assistance","summary":"  The limited reasoning capabilities of small language models (SLMs) cast doubt\non their suitability for tasks demanding deep, multi-step logical deduction.\nThis paper introduces a framework called Small Reasons, Large Hints (SMART),\nwhich selectively augments SLM reasoning with targeted guidance from large\nlanguage models (LLMs). Inspired by the concept of cognitive scaffolding, SMART\nemploys a score-based evaluation to identify uncertain reasoning steps and\ninjects corrective LLM-generated reasoning only when necessary. By framing\nstructured reasoning as an optimal policy search, our approach steers the\nreasoning trajectory toward correct solutions without exhaustive sampling. Our\nexperiments on mathematical reasoning datasets demonstrate that targeted\nexternal scaffolding significantly improves performance, paving the way for\ncollaborative use of both SLM and LLM to tackle complex reasoning tasks that\nare currently unsolvable by SLMs alone.\n","authors":["Yujin Kim","Euiin Yi","Minu Kim","Se-Young Yun","Taehyeon Kim"],"pdf_url":"https://arxiv.org/pdf/2504.09923v1.pdf","comment":"20 pages, 10 figures, 11 tables"},{"id":"http://arxiv.org/abs/2504.09910v1","updated":"2025-04-14T06:10:31Z","published":"2025-04-14T06:10:31Z","title":"Learning to Erase Private Knowledge from Multi-Documents for\n  Retrieval-Augmented Large Language Models","summary":"  Retrieval-Augmented Generation (RAG) is a promising technique for applying\nLLMs to proprietary domains. However, retrieved documents may contain sensitive\nknowledge, posing risks of privacy leakage in generative results. Thus,\neffectively erasing private information from retrieved documents is a key\nchallenge for RAG. Unlike traditional text anonymization, RAG should consider:\n(1) the inherent multi-document reasoning may face de-anonymization attacks;\n(2) private knowledge varies by scenarios, so users should be allowed to\ncustomize which information to erase; (3) preserving sufficient publicly\navailable knowledge for generation tasks. This paper introduces the privacy\nerasure task for RAG and proposes Eraser4RAG, a private knowledge eraser which\neffectively removes user-defined private knowledge from documents while\npreserving sufficient public knowledge for generation. Specifically, we first\nconstruct a global knowledge graph to identify potential knowledge across\ndocuments, aiming to defend against de-anonymization attacks. Then we randomly\nsplit it into private and public sub-graphs, and fine-tune Flan-T5 to rewrite\nthe retrieved documents excluding private triples. Finally, PPO algorithm\noptimizes the rewriting model to minimize private triples and maximize public\ntriples retention. Experiments on four QA datasets demonstrate that Eraser4RAG\nachieves superior erase performance than GPT-4o.\n","authors":["Yujing Wang","Hainan Zhang","Liang Pang","Yongxin Tong","Binghui Guo","Hongwei Zheng","Zhiming Zheng"],"pdf_url":"https://arxiv.org/pdf/2504.09910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09909v1","updated":"2025-04-14T06:09:26Z","published":"2025-04-14T06:09:26Z","title":"Quantum Natural Language Processing: A Comprehensive Review of Models,\n  Methods, and Applications","summary":"  In recent developments, deep learning methodologies applied to Natural\nLanguage Processing (NLP) have revealed a paradox: They improve performance but\ndemand considerable data and resources for their training. Alternatively,\nquantum computing exploits the principles of quantum mechanics to overcome the\ncomputational limitations of current methodologies, thereby establishing an\nemerging field known as quantum natural language processing (QNLP). This domain\nholds the potential to attain a quantum advantage in the processing of\nlinguistic structures, surpassing classical models in both efficiency and\naccuracy. In this paper, it is proposed to categorise QNLP models based on\nquantum computing principles, architecture, and computational approaches. This\npaper attempts to provide a survey on how quantum meets language by mapping\nstate-of-the-art in this area, embracing quantum encoding techniques for\nclassical data, QNLP models for prevalent NLP tasks, and quantum optimisation\ntechniques for hyper parameter tuning. The landscape of quantum computing\napproaches applied to various NLP tasks is summarised by showcasing the\nspecific QNLP methods used, and the popularity of these methods is indicated by\ntheir count. From the findings, it is observed that QNLP approaches are still\nlimited to small data sets, with only a few models explored extensively, and\nthere is increasing interest in the application of quantum computing to natural\nlanguage processing tasks.\n","authors":["Farha Nausheen","Khandakar Ahmed","M Imad Khan"],"pdf_url":"https://arxiv.org/pdf/2504.09909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09903v1","updated":"2025-04-14T05:51:31Z","published":"2025-04-14T05:51:31Z","title":"Refining Financial Consumer Complaints through Multi-Scale Model\n  Interaction","summary":"  Legal writing demands clarity, formality, and domain-specific\nprecision-qualities often lacking in documents authored by individuals without\nlegal training. To bridge this gap, this paper explores the task of legal text\nrefinement that transforms informal, conversational inputs into persuasive\nlegal arguments. We introduce FinDR, a Chinese dataset of financial dispute\nrecords, annotated with official judgments on claim reasonableness. Our\nproposed method, Multi-Scale Model Interaction (MSMI), leverages a lightweight\nclassifier to evaluate outputs and guide iterative refinement by Large Language\nModels (LLMs). Experimental results demonstrate that MSMI significantly\noutperforms single-pass prompting strategies. Additionally, we validate the\ngeneralizability of MSMI on several short-text benchmarks, showing improved\nadversarial robustness. Our findings reveal the potential of multi-model\ncollaboration for enhancing legal document generation and broader text\nrefinement tasks.\n","authors":["Bo-Wei Chen","An-Zi Yen","Chung-Chi Chen"],"pdf_url":"https://arxiv.org/pdf/2504.09903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09896v1","updated":"2025-04-14T05:44:11Z","published":"2025-04-14T05:44:11Z","title":"TWSSenti: A Novel Hybrid Framework for Topic-Wise Sentiment Analysis on\n  Social Media Using Transformer Models","summary":"  Sentiment analysis is a crucial task in natural language processing (NLP)\nthat enables the extraction of meaningful insights from textual data,\nparticularly from dynamic platforms like Twitter and IMDB. This study explores\na hybrid framework combining transformer-based models, specifically BERT,\nGPT-2, RoBERTa, XLNet, and DistilBERT, to improve sentiment classification\naccuracy and robustness. The framework addresses challenges such as noisy data,\ncontextual ambiguity, and generalization across diverse datasets by leveraging\nthe unique strengths of these models. BERT captures bidirectional context,\nGPT-2 enhances generative capabilities, RoBERTa optimizes contextual\nunderstanding with larger corpora and dynamic masking, XLNet models dependency\nthrough permutation-based learning, and DistilBERT offers efficiency with\nreduced computational overhead while maintaining high accuracy. We demonstrate\ntext cleaning, tokenization, and feature extraction using Term Frequency\nInverse Document Frequency (TF-IDF) and Bag of Words (BoW), ensure high-quality\ninput data for the models. The hybrid approach was evaluated on benchmark\ndatasets Sentiment140 and IMDB, achieving superior accuracy rates of 94\\% and\n95\\%, respectively, outperforming standalone models. The results validate the\neffectiveness of combining multiple transformer models in ensemble-like setups\nto address the limitations of individual architectures. This research\nhighlights its applicability to real-world tasks such as social media\nmonitoring, customer sentiment analysis, and public opinion tracking which\noffers a pathway for future advancements in hybrid NLP frameworks.\n","authors":["Aish Albladi","Md Kaosar Uddin","Minarul Islam","Cheryl Seals"],"pdf_url":"https://arxiv.org/pdf/2504.09896v1.pdf","comment":"41 pages, 12 figures, includes algorithm and comparative tables"},{"id":"http://arxiv.org/abs/2504.09895v1","updated":"2025-04-14T05:43:21Z","published":"2025-04-14T05:43:21Z","title":"Learning from Reference Answers: Versatile Language Model Alignment\n  without Binary Human Preference Data","summary":"  Large language models~(LLMs) are expected to be helpful, harmless, and\nhonest. In various alignment scenarios, such as general human preference,\nsafety, and confidence alignment, binary preference data collection and reward\nmodeling are resource-intensive but necessary for human preference\ntransferring. In this work, we explore using the similarity between sampled\ngenerations and high-quality reference answers as an alternative reward\nfunction for LLM alignment. Using similarity as a reward circumvents training\nreward models, and collecting a single reference answer potentially costs less\ntime than constructing binary preference pairs when multiple candidates are\navailable. Specifically, we develop \\textit{RefAlign}, a versatile\nREINFORCE-style alignment algorithm, which is free of reference and reward\nmodels. Instead, RefAlign utilizes BERTScore between sampled generations and\nhigh-quality reference answers as the surrogate reward. Beyond general human\npreference optimization, RefAlign can be readily extended to diverse scenarios,\nsuch as safety and confidence alignment, by incorporating the similarity reward\nwith task-related objectives. In various scenarios, {RefAlign} demonstrates\ncomparable performance to previous alignment methods while offering high\nefficiency.\n","authors":["Shuai Zhao","Linchao Zhu","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2504.09895v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2504.09886v1","updated":"2025-04-14T05:19:23Z","published":"2025-04-14T05:19:23Z","title":"Investigating Syntactic Biases in Multilingual Transformers with RC\n  Attachment Ambiguities in Italian and English","summary":"  This paper leverages past sentence processing studies to investigate whether\nmonolingual and multilingual LLMs show human-like preferences when presented\nwith examples of relative clause attachment ambiguities in Italian and English.\nFurthermore, we test whether these preferences can be modulated by lexical\nfactors (the type of verb/noun in the matrix clause) which have been shown to\nbe tied to subtle constraints on syntactic and semantic relations. Our results\noverall showcase how LLM behavior varies interestingly across models, but also\ngeneral failings of these models in correctly capturing human-like preferences.\nIn light of these results, we argue that RC attachment is the ideal benchmark\nfor cross-linguistic investigations of LLMs' linguistic knowledge and biases.\n","authors":["Michael Kamerath","Aniello De Santo"],"pdf_url":"https://arxiv.org/pdf/2504.09886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09866v1","updated":"2025-04-14T04:24:37Z","published":"2025-04-14T04:24:37Z","title":"PASS-FC: Progressive and Adaptive Search Scheme for Fact Checking of\n  Comprehensive Claims","summary":"  Automated fact-checking faces challenges in handling complex real-world\nclaims. We present PASS-FC, a novel framework that addresses these issues\nthrough claim augmentation, adaptive question generation, and iterative\nverification. PASS-FC enhances atomic claims with temporal and entity context,\nemploys advanced search techniques, and utilizes a reflection mechanism. We\nevaluate PASS-FC on six diverse datasets, demonstrating superior performance\nacross general knowledge, scientific, real-world, and multilingual\nfact-checking tasks. Our framework often surpasses stronger baseline models.\nHyperparameter analysis reveals optimal settings for evidence quantity and\nreflection label triggers, while ablation studies highlight the importance of\nclaim augmentation and language-specific adaptations. PASS-FC's performance\nunderscores its effectiveness in improving fact-checking accuracy and\nadaptability across various domains. We will open-source our code and\nexperimental results to facilitate further research in this area.\n","authors":["Ziyu Zhuang"],"pdf_url":"https://arxiv.org/pdf/2504.09866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09858v1","updated":"2025-04-14T04:08:16Z","published":"2025-04-14T04:08:16Z","title":"Reasoning Models Can Be Effective Without Thinking","summary":"  Recent LLMs have significantly improved reasoning capabilities, primarily by\nincluding an explicit, lengthy Thinking process as part of generation. In this\npaper, we question whether this explicit thinking is necessary. Using the\nstate-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking\nprocess via simple prompting, denoted as NoThinking, can be surprisingly\neffective. When controlling for the number of tokens, NoThinking outperforms\nThinking across a diverse set of seven challenging reasoning\ndatasets--including mathematical problem solving, formal theorem proving, and\ncoding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with\n700 tokens. Notably, the performance of NoThinking becomes more competitive\nwith pass@k as k increases. Building on this observation, we demonstrate that a\nparallel scaling approach that uses NoThinking to generate N outputs\nindependently and aggregates them is highly effective. For aggregation, we use\ntask-specific verifiers when available, or we apply simple best-of-N strategies\nsuch as confidence-based selection. Our method outperforms a range of baselines\nwith similar latency using Thinking, and is comparable to Thinking with\nsignificantly longer latency (up to 9x). Together, our research encourages a\nreconsideration of the necessity of lengthy thinking processes, while also\nestablishing a competitive reference for achieving strong reasoning performance\nin low-budget settings or at low latency using parallel scaling.\n","authors":["Wenjie Ma","Jingxuan He","Charlie Snell","Tyler Griggs","Sewon Min","Matei Zaharia"],"pdf_url":"https://arxiv.org/pdf/2504.09858v1.pdf","comment":"33 pages, 7 main figures, 2 tables"},{"id":"http://arxiv.org/abs/2504.09848v1","updated":"2025-04-14T03:38:31Z","published":"2025-04-14T03:38:31Z","title":"A Survey of Large Language Model-Powered Spatial Intelligence Across\n  Scales: Advances in Embodied Agents, Smart Cities, and Earth Science","summary":"  Over the past year, the development of large language models (LLMs) has\nbrought spatial intelligence into focus, with much attention on vision-based\nembodied intelligence. However, spatial intelligence spans a broader range of\ndisciplines and scales, from navigation and urban planning to remote sensing\nand earth science. What are the differences and connections between spatial\nintelligence across these fields? In this paper, we first review human spatial\ncognition and its implications for spatial intelligence in LLMs. We then\nexamine spatial memory, knowledge representations, and abstract reasoning in\nLLMs, highlighting their roles and connections. Finally, we analyze spatial\nintelligence across scales -- from embodied to urban and global levels --\nfollowing a framework that progresses from spatial memory and understanding to\nspatial reasoning and intelligence. Through this survey, we aim to provide\ninsights into interdisciplinary spatial intelligence research and inspire\nfuture studies.\n","authors":["Jie Feng","Jinwei Zeng","Qingyue Long","Hongyi Chen","Jie Zhao","Yanxin Xi","Zhilun Zhou","Yuan Yuan","Shengyuan Wang","Qingbin Zeng","Songwei Li","Yunke Zhang","Yuming Lin","Tong Li","Jingtao Ding","Chen Gao","Fengli Xu","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2504.09848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09824v1","updated":"2025-04-14T02:49:54Z","published":"2025-04-14T02:49:54Z","title":"Abacus-SQL: A Text-to-SQL System Empowering Cross-Domain and Open-Domain\n  Database Retrieval","summary":"  The existing text-to-SQL systems have made significant progress in SQL query\ngeneration, but they still face numerous challenges. Existing systems often\nlack retrieval capabilities for open-domain databases, requiring users to\nmanually filter relevant databases. Additionally, their cross-domain\ntransferability is limited, making it challenging to accommodate diverse query\nrequirements. To address these issues, we propose Abacus-SQL. Abacus-SQL\nutilizes database retrieval technology to accurately locate the required\ndatabases in an open-domain database environment. It also enhances the system\ncross-domain transfer ability through data augmentation methods. Moreover,\nAbacus-SQL employs Pre-SQL and Self-debug methods, thereby enhancing the\naccuracy of SQL queries. Experimental results demonstrate that Abacus-SQL\nperforms excellently in multi-turn text-to-SQL tasks, effectively validating\nthe approach's effectiveness. Abacus-SQL is publicly accessible at\nhttps://huozi.8wss.com/abacus-sql/.\n","authors":["Keyan Xu","Dingzirui Wang","Xuanliang Zhang","Qingfu Zhu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2504.09824v1.pdf","comment":"11 pages, 3figures"},{"id":"http://arxiv.org/abs/2504.09818v1","updated":"2025-04-14T02:39:26Z","published":"2025-04-14T02:39:26Z","title":"Transferable text data distillation by trajectory matching","summary":"  In the realm of large language model (LLM), as the size of large models\nincreases, it also brings higher training costs. There is a urgent need to\nminimize the data size in LLM training. Compared with data selection method,\nthe data distillation method aims to synthesize a small number of data samples\nto achieve the training effect of the full data set and has better flexibility.\nDespite its successes in computer vision, the discreteness of text data has\nhitherto stymied its exploration in natural language processing (NLP). In this\nwork, we proposed a method that involves learning pseudo prompt data based on\ntrajectory matching and finding its nearest neighbor ID to achieve\ncross-architecture transfer. During the distillation process, we introduce a\nregularization loss to improve the robustness of our distilled data. To our\nbest knowledge, this is the first data distillation work suitable for text\ngeneration tasks such as instruction tuning. Evaluations on two benchmarks,\nincluding ARC-Easy and MMLU instruction tuning datasets, established the\nsuperiority of our distillation approach over the SOTA data selection method\nLESS. Furthermore, our method demonstrates a good transferability over LLM\nstructures (i.e., OPT to Llama).\n","authors":["Rong Yao","Hailin Hu","Yifei Fu","Hanting Chen","Wenyi Fang","Fanyi Du","Kai Han","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2504.09818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09816v1","updated":"2025-04-14T02:35:00Z","published":"2025-04-14T02:35:00Z","title":"Augmented Relevance Datasets with Fine-Tuned Small LLMs","summary":"  Building high-quality datasets and labeling query-document relevance are\nessential yet resource-intensive tasks, requiring detailed guidelines and\nsubstantial effort from human annotators. This paper explores the use of small,\nfine-tuned large language models (LLMs) to automate relevance assessment, with\na focus on improving ranking models' performance by augmenting their training\ndataset. We fine-tuned small LLMs to enhance relevance assessments, thereby\nimproving dataset creation quality for downstream ranking model training. Our\nexperiments demonstrate that these fine-tuned small LLMs not only outperform\ncertain closed source models on our dataset but also lead to substantial\nimprovements in ranking model performance. These results highlight the\npotential of leveraging small LLMs for efficient and scalable dataset\naugmentation, providing a practical solution for search engine optimization.\n","authors":["Quentin Fitte-Rey","Matyas Amrouche","Romain Deveaud"],"pdf_url":"https://arxiv.org/pdf/2504.09816v1.pdf","comment":"10 pages, 3 figures, and 6 tables. Accepted and presented to LLM4EVAL\n  at WSDM '25"},{"id":"http://arxiv.org/abs/2504.08300v2","updated":"2025-04-14T02:27:13Z","published":"2025-04-11T07:04:44Z","title":"Large language models could be rote learners","summary":"  Multiple-choice question (MCQ) benchmarks are widely used for evaluating\nLarge Language Models (LLMs), yet their reliability is undermined by benchmark\ncontamination. In this study, we reframe contamination as an inherent aspect of\nlearning and seek to disentangle genuine capability acquisition from\nsuperficial memorization in LLM evaluation. First, by analyzing model\nperformance under different memorization conditions, we uncover a\ncounterintuitive trend: LLMs perform worse on memorized MCQs than on\nnon-memorized ones, indicating the coexistence of two distinct learning\nphenomena, i.e., rote memorization and genuine capability learning. To\ndisentangle them, we propose TrinEval, a novel evaluation framework that\nreformulates MCQs into an alternative trinity format, reducing memorization\nwhile preserving knowledge assessment. Experiments validate TrinEval's\neffectiveness in reformulation, and its evaluation reveals that common LLMs may\nmemorize by rote 20.5% of knowledge points (in MMLU on average).\n","authors":["Yuyang Xu","Renjun Hu","Haochao Ying","Jian Wu","Xing Shi","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2504.08300v2.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2504.09802v1","updated":"2025-04-14T02:03:54Z","published":"2025-04-14T02:03:54Z","title":"Training Small Reasoning LLMs with Cognitive Preference Alignment","summary":"  The reasoning capabilities of large language models (LLMs), such as OpenAI's\no1 and DeepSeek-R1, have seen substantial advancements through deep thinking.\nHowever, these enhancements come with significant resource demands,\nunderscoring the need to explore strategies to train effective reasoning LLMs\nwith far fewer parameters. A critical challenge is that smaller models have\ndifferent capacities and cognitive trajectories than their larger counterparts.\nHence, direct distillation of chain-of-thought (CoT) results from large LLMs to\nsmaller ones can be sometimes ineffective and requires a huge amount of\nannotated data. In this paper, we introduce a novel framework called\nCritique-Rethink-Verify (CRV), designed for training smaller yet powerful\nreasoning LLMs. Our CRV framework consists of multiple LLM agents, each\nspecializing in unique abilities: (i) critiquing the CoTs according to the\ncognitive capabilities of smaller models, (ii) rethinking and refining these\nCoTs based on the critiques, and (iii) verifying the correctness of the refined\nresults. We further propose the cognitive preference optimization (CogPO)\nalgorithm to enhance the reasoning abilities of smaller models by aligning\nthoughts of these models with their cognitive capacities. Comprehensive\nevaluations on challenging reasoning benchmarks demonstrate the efficacy of CRV\nand CogPO, which outperforms other training methods by a large margin.\n","authors":["Wenrui Cai","Chengyu Wang","Junbing Yan","Jun Huang","Xiangzhong Fang"],"pdf_url":"https://arxiv.org/pdf/2504.09802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09795v1","updated":"2025-04-14T01:50:33Z","published":"2025-04-14T01:50:33Z","title":"VDocRAG: Retrieval-Augmented Generation over Visually-Rich Documents","summary":"  We aim to develop a retrieval-augmented generation (RAG) framework that\nanswers questions over a corpus of visually-rich documents presented in mixed\nmodalities (e.g., charts, tables) and diverse formats (e.g., PDF, PPTX). In\nthis paper, we introduce a new RAG framework, VDocRAG, which can directly\nunderstand varied documents and modalities in a unified image format to prevent\nmissing information that occurs by parsing documents to obtain text. To improve\nthe performance, we propose novel self-supervised pre-training tasks that adapt\nlarge vision-language models for retrieval by compressing visual information\ninto dense token representations while aligning them with textual content in\ndocuments. Furthermore, we introduce OpenDocVQA, the first unified collection\nof open-domain document visual question answering datasets, encompassing\ndiverse document types and formats. OpenDocVQA provides a comprehensive\nresource for training and evaluating retrieval and question answering models on\nvisually-rich documents in an open-domain setting. Experiments show that\nVDocRAG substantially outperforms conventional text-based RAG and has strong\ngeneralization capability, highlighting the potential of an effective RAG\nparadigm for real-world documents.\n","authors":["Ryota Tanaka","Taichi Iki","Taku Hasegawa","Kyosuke Nishida","Kuniko Saito","Jun Suzuki"],"pdf_url":"https://arxiv.org/pdf/2504.09795v1.pdf","comment":"Accepted by CVPR 2025; project page: https://vdocrag.github.io"},{"id":"http://arxiv.org/abs/2504.07983v2","updated":"2025-04-14T01:47:33Z","published":"2025-04-05T22:57:22Z","title":"Psychological Health Knowledge-Enhanced LLM-based Social Network Crisis\n  Intervention Text Transfer Recognition Method","summary":"  As the prevalence of mental health crises increases on social media\nplatforms, identifying and preventing potential harm has become an urgent\nchallenge. This study introduces a large language model (LLM)-based text\ntransfer recognition method for social network crisis intervention, enhanced\nwith domain-specific mental health knowledge. We propose a multi-level\nframework that incorporates transfer learning using BERT, and integrates mental\nhealth knowledge, sentiment analysis, and behavior prediction techniques. The\nframework includes a crisis annotation tool trained on social media datasets\nfrom real-world events, enabling the model to detect nuanced emotional cues and\nidentify psychological crises. Experimental results show that the proposed\nmethod outperforms traditional models in crisis detection accuracy and exhibits\ngreater sensitivity to subtle emotional and contextual variations.\n","authors":["Shurui Wu","Xinyi Huang","Dingxin Lu"],"pdf_url":"https://arxiv.org/pdf/2504.07983v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09781v1","updated":"2025-04-14T00:56:08Z","published":"2025-04-14T00:56:08Z","title":"Reasoning Court: Combining Reasoning, Action, and Judgment for Multi-Hop\n  Reasoning","summary":"  While large language models (LLMs) have demonstrated strong capabilities in\ntasks like question answering and fact verification, they continue to suffer\nfrom hallucinations and reasoning errors, especially in multi-hop tasks that\nrequire integration of multiple information sources. Current methods address\nthese issues through retrieval-based techniques (grounding reasoning in\nexternal evidence), reasoning-based approaches (enhancing coherence via\nimproved prompting), or hybrid strategies combining both elements. One\nprominent hybrid method, ReAct, has outperformed purely retrieval-based or\nreasoning-based approaches; however, it lacks internal verification of\nintermediate reasoning steps, allowing potential errors to propagate through\ncomplex reasoning tasks. In this paper, we introduce Reasoning Court (RC), a\nnovel framework that extends iterative reasoning-and-retrieval methods, such as\nReAct, with a dedicated LLM judge. Unlike ReAct, RC employs this judge to\nindependently evaluate multiple candidate answers and their associated\nreasoning generated by separate LLM agents. The judge is asked to select the\nanswer that it considers the most factually grounded and logically coherent\nbased on the presented reasoning and evidence, or synthesizes a new answer\nusing available evidence and its pre-trained knowledge if all candidates are\ninadequate, flawed, or invalid. Evaluations on multi-hop benchmarks (HotpotQA,\nMuSiQue) and fact-verification (FEVER) demonstrate that RC consistently\noutperforms state-of-the-art few-shot prompting methods without task-specific\nfine-tuning.\n","authors":["Jingtian Wu","Claire Cardie"],"pdf_url":"https://arxiv.org/pdf/2504.09781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09763v1","updated":"2025-04-14T00:06:48Z","published":"2025-04-14T00:06:48Z","title":"Executable Functional Abstractions: Inferring Generative Programs for\n  Advanced Math Problems","summary":"  Scientists often infer abstract procedures from specific instances of\nproblems and use the abstractions to generate new, related instances. For\nexample, programs encoding the formal rules and properties of a system have\nbeen useful in fields ranging from RL (procedural environments) to physics\n(simulation engines). These programs can be seen as functions which execute to\ndifferent outputs based on their parameterizations (e.g., gridworld\nconfiguration or initial physical conditions). We introduce the term EFA\n(Executable Functional Abstraction) to denote such programs for math problems.\nEFA-like constructs have been shown to be useful for math reasoning as problem\ngenerators for stress-testing models. However, prior work has been limited to\nabstractions for grade-school math (whose simple rules are easy to encode in\nprograms), while generating EFAs for advanced math has thus far required human\nengineering. We explore the automatic construction of EFAs for advanced math\nproblems. We operationalize the task of automatically constructing EFAs as a\nprogram synthesis task, and develop EFAGen, which conditions an LLM on a seed\nmath problem and its step-by-step solution to generate candidate EFA programs\nthat are faithful to the generalized problem and solution class underlying the\nseed problem. Furthermore, we formalize properties any valid EFA must possess\nin terms of executable unit tests, and show how the tests can be used as\nverifiable rewards to train LLMs to become better writers of EFAs. We\ndemonstrate that EFAs constructed by EFAGen behave rationally by remaining\nfaithful to seed problems, produce learnable problem variations, and that\nEFAGen can infer EFAs across multiple diverse sources of competition-level math\nproblems. Finally, we show downstream uses of model-written EFAs e.g. finding\nproblem variations that are harder or easier for a learner to solve, as well as\ndata generation.\n","authors":["Zaid Khan","Elias Stengel-Eskin","Archiki Prasad","Jaemin Cho","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2504.09763v1.pdf","comment":"Project Page: https://zaidkhan.me/EFAGen/"},{"id":"http://arxiv.org/abs/2504.10766v1","updated":"2025-04-14T23:53:47Z","published":"2025-04-14T23:53:47Z","title":"How Instruction and Reasoning Data shape Post-Training: Data Quality\n  through the Lens of Layer-wise Gradients","summary":"  As the post-training of large language models (LLMs) advances from\ninstruction-following to complex reasoning tasks, understanding how different\ndata affect finetuning dynamics remains largely unexplored. In this paper, we\npresent a spectral analysis of layer-wise gradients induced by low/high-quality\ninstruction and reasoning data for LLM post-training. Our analysis reveals that\nwidely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and\nReward, can be explained and unified by spectral properties computed from\ngradients' singular value decomposition (SVD). Specifically, higher-quality\ndata are usually associated with lower nuclear norms and higher effective\nranks. Notably, effective rank exhibits better robustness and resolution than\nnuclear norm in capturing subtle quality differences. For example, reasoning\ndata achieves substantially higher effective ranks than instruction data,\nimplying richer gradient structures on more complex tasks. Our experiments also\nhighlight that models within the same family share similar gradient patterns\nregardless of their sizes, whereas different model families diverge\nsignificantly. Providing a unified view on the effects of data quality across\ninstruction and reasoning data, this work illuminates the interplay between\ndata quality and training stability, shedding novel insights into developing\nbetter data exploration strategies for post-training.\n","authors":["Ming Li","Yanhong Li","Ziyue Li","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.10766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15124v5","updated":"2025-04-14T22:39:09Z","published":"2024-11-22T18:44:04Z","title":"Tulu 3: Pushing Frontiers in Open Language Model Post-Training","summary":"  Language model post-training is applied to refine behaviors and unlock new\nskills across a wide range of recent language models, but open recipes for\napplying these techniques lag behind proprietary ones. The underlying training\ndata and recipes for post-training are simultaneously the most important pieces\nof the puzzle and the portion with the least transparency. To bridge this gap,\nwe introduce Tulu 3, a family of fully-open state-of-the-art post-trained\nmodels, alongside its data, code, and training recipes, serving as a\ncomprehensive guide for modern post-training techniques. Tulu 3, which builds\non Llama 3.1 base models, achieves results surpassing the instruct versions of\nLlama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and\nClaude 3.5-Haiku. The training algorithms for our models include supervised\nfinetuning (SFT), Direct Preference Optimization (DPO), and a novel method we\ncall Reinforcement Learning with Verifiable Rewards (RLVR). With Tulu 3, we\nintroduce a multi-task evaluation scheme for post-training recipes with\ndevelopment and unseen evaluations, standard benchmark implementations, and\nsubstantial decontamination of existing open datasets on said benchmarks. We\nconclude with analysis and discussion of training methods that did not reliably\nimprove performance.\n  In addition to the Tulu 3 model weights and demo, we release the complete\nrecipe -- including datasets for diverse core skills, a robust toolkit for data\ncuration and evaluation, the training code and infrastructure, and, most\nimportantly, a detailed report for reproducing and further adapting the Tulu 3\napproach to more domains.\n","authors":["Nathan Lambert","Jacob Morrison","Valentina Pyatkin","Shengyi Huang","Hamish Ivison","Faeze Brahman","Lester James V. Miranda","Alisa Liu","Nouha Dziri","Shane Lyu","Yuling Gu","Saumya Malik","Victoria Graf","Jena D. Hwang","Jiangjiang Yang","Ronan Le Bras","Oyvind Tafjord","Chris Wilhelm","Luca Soldaini","Noah A. Smith","Yizhong Wang","Pradeep Dasigi","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2411.15124v5.pdf","comment":"Added Tulu 3 405B results and additional analyses"},{"id":"http://arxiv.org/abs/2504.10738v1","updated":"2025-04-14T22:16:10Z","published":"2025-04-14T22:16:10Z","title":"CleanMAP: Distilling Multimodal LLMs for Confidence-Driven Crowdsourced\n  HD Map Updates","summary":"  The rapid growth of intelligent connected vehicles (ICVs) and integrated\nvehicle-road-cloud systems has increased the demand for accurate, real-time HD\nmap updates. However, ensuring map reliability remains challenging due to\ninconsistencies in crowdsourced data, which suffer from motion blur, lighting\nvariations, adverse weather, and lane marking degradation. This paper\nintroduces CleanMAP, a Multimodal Large Language Model (MLLM)-based\ndistillation framework designed to filter and refine crowdsourced data for\nhigh-confidence HD map updates. CleanMAP leverages an MLLM-driven lane\nvisibility scoring model that systematically quantifies key visual parameters,\nassigning confidence scores (0-10) based on their impact on lane detection. A\nnovel dynamic piecewise confidence-scoring function adapts scores based on lane\nvisibility, ensuring strong alignment with human evaluations while effectively\nfiltering unreliable data. To further optimize map accuracy, a\nconfidence-driven local map fusion strategy ranks and selects the top-k\nhighest-scoring local maps within an optimal confidence range (best score minus\n10%), striking a balance between data quality and quantity. Experimental\nevaluations on a real-world autonomous vehicle dataset validate CleanMAP's\neffectiveness, demonstrating that fusing the top three local maps achieves the\nlowest mean map update error of 0.28m, outperforming the baseline (0.37m) and\nmeeting stringent accuracy thresholds (<= 0.32m). Further validation with\nreal-vehicle data confirms 84.88% alignment with human evaluators, reinforcing\nthe model's robustness and reliability. This work establishes CleanMAP as a\nscalable and deployable solution for crowdsourced HD map updates, ensuring more\nprecise and reliable autonomous navigation. The code will be available at\nhttps://Ankit-Zefan.github.io/CleanMap/\n","authors":["Ankit Kumar Shaw","Kun Jiang","Tuopu Wen","Chandan Kumar Sah","Yining Shi","Mengmeng Yang","Diange Yang","Xiaoli Lian"],"pdf_url":"https://arxiv.org/pdf/2504.10738v1.pdf","comment":"Kun Jiang, Mengmeng Yang and Diange Yang are Corresponding Author.\n  The main paper and supplementary material are both included here, total 23\n  pages (main paper is 10 pages and supplementary material is 13 pages), total\n  17 figures (6 figures in main paper and 11 figures in supplementary\n  material), this paper is Accepted to CVPR WDFM-AD Workshop 2025, The code\n  will be available at https://Ankit-Zefan.github.io/CleanMap/"},{"id":"http://arxiv.org/abs/2504.10724v1","updated":"2025-04-14T21:30:43Z","published":"2025-04-14T21:30:43Z","title":"HELIOS: Adaptive Model And Early-Exit Selection for Efficient LLM\n  Inference Serving","summary":"  Deploying large language models (LLMs) presents critical challenges due to\nthe inherent trade-offs associated with key performance metrics, such as\nlatency, accuracy, and throughput. Typically, gains in one metric is\naccompanied with degradation in others. Early-Exit LLMs (EE-LLMs) efficiently\nnavigate this trade-off space by skipping some of the later model layers when\nit confidently finds an output token early, thus reducing latency without\nimpacting accuracy. However, as the early exits taken depend on the task and\nare unknown apriori to request processing, EE-LLMs conservatively load the\nentire model, limiting resource savings and throughput. Also, current\nframeworks statically select a model for a user task, limiting our ability to\nadapt to changing nature of the input queries.\n  We propose HELIOS to address these challenges. First, HELIOS shortlists a set\nof candidate LLMs, evaluates them using a subset of prompts, gathering\ntelemetry data in real-time. Second, HELIOS uses the early exit data from these\nevaluations to greedily load the selected model only up to a limited number of\nlayers. This approach yields memory savings which enables us to process more\nrequests at the same time, thereby improving throughput. Third, HELIOS monitors\nand periodically reassesses the performance of the candidate LLMs and if\nneeded, switches to another model that can service incoming queries more\nefficiently (such as using fewer layers without lowering accuracy). Our\nevaluations show that HELIOS achieves 1.48$\\times$ throughput, 1.10$\\times$\nenergy-efficiency, 1.39$\\times$ lower response time, and 3.7$\\times$\nimprovements in inference batch sizes compared to the baseline, when optimizing\nfor the respective service level objectives.\n","authors":["Avinash Kumar","Shashank Nag","Jason Clemons","Lizy John","Poulami Das"],"pdf_url":"https://arxiv.org/pdf/2504.10724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10681v1","updated":"2025-04-14T20:04:54Z","published":"2025-04-14T20:04:54Z","title":"EMAFusion: A Self-Optimizing System for Seamless LLM Selection and\n  Integration","summary":"  While recent advances in large language models (LLMs) have significantly\nenhanced performance across diverse natural language tasks, the high\ncomputational and financial costs associated with their deployment remain\nsubstantial barriers. Existing routing strategies partially alleviate this\nchallenge by assigning queries to cheaper or specialized models, but they\nfrequently rely on extensive labeled data or fragile task-specific heuristics.\nConversely, fusion techniques aggregate multiple LLM outputs to boost accuracy\nand robustness, yet they often exacerbate cost and may reinforce shared biases.\n  We introduce EMAFusion, a new framework that self-optimizes for seamless LLM\nselection and reliable execution for a given query. Specifically, EMAFusion\nintegrates a taxonomy-based router for familiar query types, a learned router\nfor ambiguous inputs, and a cascading approach that progressively escalates\nfrom cheaper to more expensive models based on multi-judge confidence\nevaluations. Through extensive evaluations, we find EMAFusion outperforms the\nbest individual models by over 2.6 percentage points (94.3% vs. 91.7%), while\nbeing 4X cheaper than the average cost. EMAFusion further achieves a remarkable\n17.1 percentage point improvement over models like GPT-4 at less than 1/20th\nthe cost. Our combined routing approach delivers 94.3% accuracy compared to\ntaxonomy-based (88.1%) and learned model predictor-based (91.7%) methods alone,\ndemonstrating the effectiveness of our unified strategy. Finally, EMAFusion\nsupports flexible cost-accuracy trade-offs, allowing users to balance their\nbudgetary constraints and performance needs.\n","authors":["Soham Shah","Kumar Shridhar","Surojit Chatterjee","Souvik Sen"],"pdf_url":"https://arxiv.org/pdf/2504.10681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10679v1","updated":"2025-04-14T20:01:34Z","published":"2025-04-14T20:01:34Z","title":"Keyword Extraction, and Aspect Classification in Sinhala, English, and\n  Code-Mixed Content","summary":"  Brand reputation in the banking sector is maintained through insightful\nanalysis of customer opinion on code-mixed and multilingual content.\nConventional NLP models misclassify or ignore code-mixed text, when mix with\nlow resource languages such as Sinhala-English and fail to capture\ndomain-specific knowledge. This study introduces a hybrid NLP method to improve\nkeyword extraction, content filtering, and aspect-based classification of\nbanking content. Keyword extraction in English is performed with a hybrid\napproach comprising a fine-tuned SpaCy NER model, FinBERT-based KeyBERT\nembeddings, YAKE, and EmbedRank, which results in a combined accuracy of 91.2%.\nCode-mixed and Sinhala keywords are extracted using a fine-tuned XLM-RoBERTa\nmodel integrated with a domain-specific Sinhala financial vocabulary, and it\nresults in an accuracy of 87.4%. To ensure data quality, irrelevant comment\nfiltering was performed using several models, with the BERT-base-uncased model\nachieving 85.2% for English and XLM-RoBERTa 88.1% for Sinhala, which was better\nthan GPT-4o, SVM, and keyword-based filtering. Aspect classification followed\nthe same pattern, with the BERT-base-uncased model achieving 87.4% for English\nand XLM-RoBERTa 85.9% for Sinhala, both exceeding GPT-4 and keyword-based\napproaches. These findings confirm that fine-tuned transformer models\noutperform traditional methods in multilingual financial text analysis. The\npresent framework offers an accurate and scalable solution for brand reputation\nmonitoring in code-mixed and low-resource banking environments.\n","authors":["F. A. Rizvi","T. Navojith","A. M. N. H. Adhikari","W. P. U. Senevirathna","Dharshana Kasthurirathna","Lakmini Abeywardhana"],"pdf_url":"https://arxiv.org/pdf/2504.10679v1.pdf","comment":"6 Pages, 2 figures, 7 Tables"},{"id":"http://arxiv.org/abs/2504.10663v1","updated":"2025-04-14T19:30:30Z","published":"2025-04-14T19:30:30Z","title":"Characterizing Knowledge Manipulation in a Russian Wikipedia Fork","summary":"  Wikipedia is powered by MediaWiki, a free and open-source software that is\nalso the infrastructure for many other wiki-based online encyclopedias. These\ninclude the recently launched website Ruwiki, which has copied and modified the\noriginal Russian Wikipedia content to conform to Russian law. To identify\npractices and narratives that could be associated with different forms of\nknowledge manipulation, this article presents an in-depth analysis of this\nRussian Wikipedia fork. We propose a methodology to characterize the main\nchanges with respect to the original version. The foundation of this study is a\ncomprehensive comparative analysis of more than 1.9M articles from Russian\nWikipedia and its fork. Using meta-information and geographical, temporal,\ncategorical, and textual features, we explore the changes made by Ruwiki\neditors. Furthermore, we present a classification of the main topics of\nknowledge manipulation in this fork, including a numerical estimation of their\nscope. This research not only sheds light on significant changes within Ruwiki,\nbut also provides a methodology that could be applied to analyze other\nWikipedia forks and similar collaborative projects.\n","authors":["Mykola Trokhymovych","Oleksandr Kosovan","Nathan Forrester","Pablo Aragón","Diego Saez-Trumper","Ricardo Baeza-Yates"],"pdf_url":"https://arxiv.org/pdf/2504.10663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10660v1","updated":"2025-04-14T19:21:20Z","published":"2025-04-14T19:21:20Z","title":"LITERA: An LLM Based Approach to Latin-to-English Translation","summary":"  This paper introduces an LLM-based Latin-to-English translation platform\ndesigned to address the challenges of translating Latin texts. We named the\nmodel LITERA, which stands for Latin Interpretation and Translations into\nEnglish for Research Assistance. Through a multi-layered translation process\nutilizing a fine-tuned version of GPT-4o-mini and GPT-4o, LITERA offers an\nunprecedented level of accuracy, showcased by greatly improved BLEU scores,\nparticularly in classical Latin, along with improved BLEURT scores. The\ndevelopment of LITERA involved close collaboration with Duke University's\nClassical Studies Department, which was instrumental in creating a small,\nhigh-quality parallel Latin-English dataset. This paper details the\narchitecture, fine-tuning methodology, and prompting strategies used in LITERA,\nemphasizing its ability to produce literal translations.\n","authors":["Paul Rosu"],"pdf_url":"https://arxiv.org/pdf/2504.10660v1.pdf","comment":"NAACL Findings"},{"id":"http://arxiv.org/abs/2504.10650v1","updated":"2025-04-14T19:04:32Z","published":"2025-04-14T19:04:32Z","title":"Will AI shape the way we speak? The emerging sociolinguistic influence\n  of synthetic voices","summary":"  The growing prevalence of conversational voice interfaces, powered by\ndevelopments in both speech and language technologies, raises important\nquestions about their influence on human communication. While written\ncommunication can signal identity through lexical and stylistic choices,\nvoice-based interactions inherently amplify socioindexical elements - such as\naccent, intonation, and speech style - which more prominently convey social\nidentity and group affiliation. There is evidence that even passive media such\nas television is likely to influence the audience's linguistic patterns. Unlike\npassive media, conversational AI is interactive, creating a more immersive and\nreciprocal dynamic that holds a greater potential to impact how individuals\nspeak in everyday interactions. Such heightened influence can be expected to\narise from phenomena such as acoustic-prosodic entrainment and linguistic\naccommodation, which occur naturally during interaction and enable users to\nadapt their speech patterns in response to the system. While this phenomenon is\nstill emerging, its potential societal impact could provide organisations,\nmovements, and brands with a subtle yet powerful avenue for shaping and\ncontrolling public perception and social identity. We argue that the\nsocioindexical influence of AI-generated speech warrants attention and should\nbecome a focus of interdisciplinary research, leveraging new and existing\nmethodologies and technologies to better understand its implications.\n","authors":["Éva Székely","Jūra Miniota"," Míša"," Hejná"],"pdf_url":"https://arxiv.org/pdf/2504.10650v1.pdf","comment":"5 pages, 0 figures, International Workshop on Spoken Dialogue Systems\n  Technology (IWSDS) 2025"},{"id":"http://arxiv.org/abs/2503.10650v2","updated":"2025-04-14T19:01:43Z","published":"2025-03-04T05:11:42Z","title":"AI Enabled User-Specific Cyberbullying Severity Detection with\n  Explainability","summary":"  The rise of social media has significantly increased the prevalence of\ncyberbullying (CB), posing serious risks to both mental and physical\nwell-being. Effective detection systems are essential for mitigating its\nimpact. While several machine learning (ML) models have been developed, few\nincorporate victims' psychological, demographic, and behavioral factors\nalongside bullying comments to assess severity. In this study, we propose an AI\nmodel intregrating user-specific attributes, including psychological factors\n(self-esteem, anxiety, depression), online behavior (internet usage,\ndisciplinary history), and demographic attributes (race, gender, ethnicity),\nalong with social media comments. Additionally, we introduce a re-labeling\ntechnique that categorizes social media comments into three severity levels:\nNot Bullying, Mild Bullying, and Severe Bullying, considering user-specific\nfactors.Our LSTM model is trained using 146 features, incorporating emotional,\ntopical, and word2vec representations of social media comments as well as\nuser-level attributes and it outperforms existing baseline models, achieving\nthe highest accuracy of 98\\% and an F1-score of 0.97. To identify key factors\ninfluencing the severity of cyberbullying, we employ explainable AI techniques\n(SHAP and LIME) to interpret the model's decision-making process. Our findings\nreveal that, beyond hate comments, victims belonging to specific racial and\ngender groups are more frequently targeted and exhibit higher incidences of\ndepression, disciplinary issues, and low self-esteem. Additionally, individuals\nwith a prior history of bullying are at a greater risk of becoming victims of\ncyberbullying.\n","authors":["Tabia Tanzin Prama","Jannatul Ferdaws Amrin","Md. Mushfique Anwar","Iqbal H. Sarker"],"pdf_url":"https://arxiv.org/pdf/2503.10650v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10647v1","updated":"2025-04-14T18:59:10Z","published":"2025-04-14T18:59:10Z","title":"Improving In-Context Learning with Reasoning Distillation","summary":"  Language models rely on semantic priors to perform in-context learning, which\nleads to poor performance on tasks involving inductive reasoning.\nInstruction-tuning methods based on imitation learning can superficially\nenhance the in-context learning performance of language models, but they often\nfail to improve the model's understanding of the underlying rules that connect\ninputs and outputs in few-shot demonstrations. We propose ReDis, a reasoning\ndistillation technique designed to improve the inductive reasoning capabilities\nof language models. Through a careful combination of data augmentation,\nfiltering, supervised fine-tuning, and alignment, ReDis achieves significant\nperformance improvements across a diverse range of tasks, including 1D-ARC,\nList Function, ACRE, and MiniSCAN. Experiments on three language model\nbackbones show that ReDis outperforms equivalent few-shot prompting baselines\nacross all tasks and even surpasses the teacher model, GPT-4o, in some cases.\nReDis, based on the LLaMA-3 backbone, achieves relative improvements of 23.2%,\n2.8%, and 66.6% over GPT-4o on 1D-ARC, ACRE, and MiniSCAN, respectively, within\na similar hypothesis search space. The code, dataset, and model checkpoints\nwill be made available at\nhttps://github.com/NafisSadeq/reasoning-distillation.git.\n","authors":["Nafis Sadeq","Xin Xu","Zhouhang Xie","Julian McAuley","Byungkyu Kang","Prarit Lamba","Xiang Gao"],"pdf_url":"https://arxiv.org/pdf/2504.10647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10646v1","updated":"2025-04-14T18:56:29Z","published":"2025-04-14T18:56:29Z","title":"Weight-of-Thought Reasoning: Exploring Neural Network Weights for\n  Enhanced LLM Reasoning","summary":"  Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities when prompted with strategies such as Chain-of-Thought (CoT).\nHowever, these approaches focus on token-level output without considering\ninternal weight dynamics. We introduce Weight-of-Thought (WoT) reasoning, a\nnovel approach that examines neural network weights before inference to\nidentify reasoning pathways. Unlike existing methods, WoT explores the weight\nspace through graph-based message passing, multi-step reasoning processes, and\nattention mechanisms. Our implementation creates an interconnected graph of\nreasoning nodes. Experiments on diverse reasoning tasks (syllogistic,\nmathematical, algebraic, combinatorial, and geometric) demonstrate that WoT\nachieves superior performance compared to traditional methods, particularly for\ncomplex problems. This approach leads to both improved performance and greater\ninterpretability of the reasoning process, offering a promising direction for\nenhancing LLM reasoning capabilities.\n","authors":["Saif Punjwani","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2504.10646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10637v1","updated":"2025-04-14T18:40:02Z","published":"2025-04-14T18:40:02Z","title":"Better Estimation of the KL Divergence Between Language Models","summary":"  Estimating the Kullback--Leibler (KL) divergence between language models has\nmany applications, e.g., reinforcement learning from human feedback (RLHF),\ninterpretability, and knowledge distillation. However, computing the exact KL\ndivergence between two arbitrary language models is intractable. Thus,\npractitioners often resort to the use of sampling-based estimators. While it is\neasy to fashion a simple Monte Carlo (MC) estimator that provides an unbiased\nestimate of the KL divergence between language models, this estimator\nnotoriously suffers from high variance, and can even result in a negative\nestimate of the KL divergence, a non-negative quantity. In this paper, we\nintroduce a Rao--Blackwellized estimator that is also unbiased and provably has\nvariance less than or equal to that of the standard Monte Carlo estimator. In\nan empirical study on sentiment-controlled fine-tuning, we show that our\nestimator provides more stable KL estimates and reduces variance substantially\nin practice. Additionally, we derive an analogous Rao--Blackwellized estimator\nof the gradient of the KL divergence, which leads to more stable training and\nproduces models that more frequently appear on the Pareto frontier of reward\nvs. KL compared to the ones trained with the MC estimator of the gradient.\n","authors":["Afra Amini","Tim Vieira","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2504.10637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14740v2","updated":"2025-04-14T18:30:57Z","published":"2024-09-23T06:36:57Z","title":"ToxiCraft: A Novel Framework for Synthetic Generation of Harmful\n  Information","summary":"  In different NLP tasks, detecting harmful content is crucial for online\nenvironments, especially with the growing influence of social media. However,\nprevious research has two main issues: 1) a lack of data in low-resource\nsettings, and 2) inconsistent definitions and criteria for judging harmful\ncontent, requiring classification models to be robust to spurious features and\ndiverse. We propose Toxicraft, a novel framework for synthesizing datasets of\nharmful information to address these weaknesses. With only a small amount of\nseed data, our framework can generate a wide variety of synthetic, yet\nremarkably realistic, examples of toxic information. Experimentation across\nvarious datasets showcases a notable enhancement in detection model robustness\nand adaptability, surpassing or close to the gold labels.\n","authors":["Zheng Hui","Zhaoxiao Guo","Hang Zhao","Juanyong Duan","Congrui Huang"],"pdf_url":"https://arxiv.org/pdf/2409.14740v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2504.10615v1","updated":"2025-04-14T18:15:27Z","published":"2025-04-14T18:15:27Z","title":"Beyond Chains of Thought: Benchmarking Latent-Space Reasoning Abilities\n  in Large Language Models","summary":"  Large language models (LLMs) can perform reasoning computations both\ninternally within their latent space and externally by generating explicit\ntoken sequences like chains of thought. Significant progress in enhancing\nreasoning abilities has been made by scaling test-time compute. However,\nunderstanding and quantifying model-internal reasoning abilities - the\ninferential \"leaps\" models make between individual token predictions - remains\ncrucial. This study introduces a benchmark (n = 4,000 items) designed to\nquantify model-internal reasoning in different domains. We achieve this by\nhaving LLMs indicate the correct solution to reasoning problems not through\ndescriptive text, but by selecting a specific language of their initial\nresponse token that is different from English, the benchmark language. This not\nonly requires models to reason beyond their context window, but also to\noverrise their default tendency to respond in the same language as the prompt,\nthereby posing an additional cognitive strain. We evaluate a set of 18 LLMs,\nshowing significant performance variations, with GPT-4.5 achieving the highest\naccuracy (74.7%), outperforming models like Grok-2 (67.2%), and Llama 3.1 405B\n(65.6%). Control experiments and difficulty scaling analyses suggest that while\nLLMs engage in internal reasoning, we cannot rule out heuristic exploitations\nunder certain conditions, marking an area for future investigation. Our\nexperiments demonstrate that LLMs can \"think\" via latent-space computations,\nrevealing model-internal inference strategies that need further understanding,\nespecially regarding safety-related concerns such as covert planning,\ngoal-seeking, or deception emerging without explicit token traces.\n","authors":["Thilo Hagendorff","Sarah Fabi"],"pdf_url":"https://arxiv.org/pdf/2504.10615v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2504.10487v1","updated":"2025-04-14T17:59:59Z","published":"2025-04-14T17:59:59Z","title":"FLOSS: Free Lunch in Open-vocabulary Semantic Segmentation","summary":"  Recent Open-Vocabulary Semantic Segmentation (OVSS) models extend the CLIP\nmodel to segmentation while maintaining the use of multiple templates (e.g., a\nphoto of <class>, a sketch of a <class>, etc.) for constructing class-wise\naveraged text embeddings, acting as a classifier. In this paper, we challenge\nthis status quo and investigate the impact of templates for OVSS. Empirically,\nwe observe that for each class, there exist single-template classifiers\nsignificantly outperforming the conventional averaged classifier. We refer to\nthem as class-experts. Given access to unlabeled images and without any\ntraining involved, we estimate these experts by leveraging the class-wise\nprediction entropy of single-template classifiers, selecting as class-wise\nexperts those which yield the lowest entropy. All experts, each specializing in\na specific class, collaborate in a newly proposed fusion method to generate\nmore accurate OVSS predictions. Our plug-and-play method, coined FLOSS, is\northogonal and complementary to existing OVSS methods, offering a ''free\nlunch'' to systematically improve OVSS without labels and additional training.\nExtensive experiments demonstrate that FLOSS consistently boosts\nstate-of-the-art methods on various OVSS benchmarks. Moreover, the selected\nexpert templates can generalize well from one dataset to others sharing the\nsame semantic categories, yet exhibiting distribution shifts. Additionally, we\nobtain satisfactory improvements under a low-data regime, where only a few\nunlabeled images are available. Our code is available at\nhttps://github.com/yasserben/FLOSS .\n","authors":["Yasser Benigmim","Mohammad Fahes","Tuan-Hung Vu","Andrei Bursuc","Raoul de Charette"],"pdf_url":"https://arxiv.org/pdf/2504.10487v1.pdf","comment":"Project Page: https://yasserben.github.io/FLOSS/"},{"id":"http://arxiv.org/abs/2504.10486v1","updated":"2025-04-14T17:59:58Z","published":"2025-04-14T17:59:58Z","title":"DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar\n  Relighting","summary":"  Creating relightable and animatable human avatars from monocular videos is a\nrising research topic with a range of applications, e.g. virtual reality,\nsports, and video games. Previous works utilize neural fields together with\nphysically based rendering (PBR), to estimate geometry and disentangle\nappearance properties of human avatars. However, one drawback of these methods\nis the slow rendering speed due to the expensive Monte Carlo ray tracing. To\ntackle this problem, we proposed to distill the knowledge from implicit neural\nfields (teacher) to explicit 2D Gaussian splatting (student) representation to\ntake advantage of the fast rasterization property of Gaussian splatting. To\navoid ray-tracing, we employ the split-sum approximation for PBR appearance. We\nalso propose novel part-wise ambient occlusion probes for shadow computation.\nShadow prediction is achieved by querying these probes only once per pixel,\nwhich paves the way for real-time relighting of avatars. These techniques\ncombined give high-quality relighting results with realistic shadow effects.\nOur experiments demonstrate that the proposed student model achieves comparable\nor even better relighting results with our teacher model while being 370 times\nfaster at inference time, achieving a 67 FPS rendering speed.\n","authors":["Zeren Jiang","Shaofei Wang","Siyu Tang"],"pdf_url":"https://arxiv.org/pdf/2504.10486v1.pdf","comment":"16 pages, 8 figures, Project pages:\n  https://jzr99.github.io/DNF-Avatar/"},{"id":"http://arxiv.org/abs/2504.10485v1","updated":"2025-04-14T17:59:57Z","published":"2025-04-14T17:59:57Z","title":"Decoupled Diffusion Sparks Adaptive Scene Generation","summary":"  Controllable scene generation could reduce the cost of diverse data\ncollection substantially for autonomous driving. Prior works formulate the\ntraffic layout generation as predictive progress, either by denoising entire\nsequences at once or by iteratively predicting the next frame. However, full\nsequence denoising hinders online reaction, while the latter's short-sighted\nnext-frame prediction lacks precise goal-state guidance. Further, the learned\nmodel struggles to generate complex or challenging scenarios due to a large\nnumber of safe and ordinal driving behaviors from open datasets. To overcome\nthese, we introduce Nexus, a decoupled scene generation framework that improves\nreactivity and goal conditioning by simulating both ordinal and challenging\nscenarios from fine-grained tokens with independent noise states. At the core\nof the decoupled pipeline is the integration of a partial noise-masking\ntraining strategy and a noise-aware schedule that ensures timely environmental\nupdates throughout the denoising process. To complement challenging scenario\ngeneration, we collect a dataset consisting of complex corner cases. It covers\n540 hours of simulated data, including high-risk interactions such as cut-in,\nsudden braking, and collision. Nexus achieves superior generation realism while\npreserving reactivity and goal orientation, with a 40% reduction in\ndisplacement error. We further demonstrate that Nexus improves closed-loop\nplanning by 20% through data augmentation and showcase its capability in\nsafety-critical data generation.\n","authors":["Yunsong Zhou","Naisheng Ye","William Ljungbergh","Tianyu Li","Jiazhi Yang","Zetong Yang","Hongzi Zhu","Christoffer Petersson","Hongyang Li"],"pdf_url":"https://arxiv.org/pdf/2504.10485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10483v1","updated":"2025-04-14T17:59:53Z","published":"2025-04-14T17:59:53Z","title":"REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion\n  Transformers","summary":"  In this paper we tackle a fundamental question: \"Can we train latent\ndiffusion models together with the variational auto-encoder (VAE) tokenizer in\nan end-to-end manner?\" Traditional deep-learning wisdom dictates that\nend-to-end training is often preferable when possible. However, for latent\ndiffusion transformers, it is observed that end-to-end training both VAE and\ndiffusion-model using standard diffusion-loss is ineffective, even causing a\ndegradation in final performance. We show that while diffusion loss is\nineffective, end-to-end training can be unlocked through the\nrepresentation-alignment (REPA) loss -- allowing both VAE and diffusion model\nto be jointly tuned during the training process. Despite its simplicity, the\nproposed training recipe (REPA-E) shows remarkable performance; speeding up\ndiffusion model training by over 17x and 45x over REPA and vanilla training\nrecipes, respectively. Interestingly, we observe that end-to-end tuning with\nREPA-E also improves the VAE itself; leading to improved latent space structure\nand downstream generation performance. In terms of final performance, our\napproach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and\nwithout classifier-free guidance on ImageNet 256 x 256. Code is available at\nhttps://end2end-diffusion.github.io.\n","authors":["Xingjian Leng","Jaskirat Singh","Yunzhong Hou","Zhenchang Xing","Saining Xie","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2504.10483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10479v1","updated":"2025-04-14T17:59:25Z","published":"2025-04-14T17:59:25Z","title":"InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models","summary":"  We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.\n","authors":["Jinguo Zhu","Weiyun Wang","Zhe Chen","Zhaoyang Liu","Shenglong Ye","Lixin Gu","Yuchen Duan","Hao Tian","Weijie Su","Jie Shao","Zhangwei Gao","Erfei Cui","Yue Cao","Yangzhou Liu","Weiye Xu","Hao Li","Jiahao Wang","Han Lv","Dengnian Chen","Songze Li","Yinan He","Tan Jiang","Jiapeng Luo","Yi Wang","Conghui He","Botian Shi","Xingcheng Zhang","Wenqi Shao","Junjun He","Yingtong Xiong","Wenwen Qu","Peng Sun","Penglong Jiao","Lijun Wu","Kaipeng Zhang","Huipeng Deng","Jiaye Ge","Kai Chen","Limin Wang","Min Dou","Lewei Lu","Xizhou Zhu","Tong Lu","Dahua Lin","Yu Qiao","Jifeng Dai","Wenhai Wang"],"pdf_url":"https://arxiv.org/pdf/2504.10479v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2504.10471v1","updated":"2025-04-14T17:54:28Z","published":"2025-04-14T17:54:28Z","title":"MIEB: Massive Image Embedding Benchmark","summary":"  Image representations are often evaluated through disjointed, task-specific\nprotocols, leading to a fragmented understanding of model capabilities. For\ninstance, it is unclear whether an image embedding model adept at clustering\nimages is equally good at retrieving relevant images given a piece of text. We\nintroduce the Massive Image Embedding Benchmark (MIEB) to evaluate the\nperformance of image and image-text embedding models across the broadest\nspectrum to date. MIEB spans 38 languages across 130 individual tasks, which we\ngroup into 8 high-level categories. We benchmark 50 models across our\nbenchmark, finding that no single method dominates across all task categories.\nWe reveal hidden capabilities in advanced vision models such as their accurate\nvisual representation of texts, and their yet limited capabilities in\ninterleaved encodings and matching images and texts in the presence of\nconfounders. We also show that the performance of vision encoders on MIEB\ncorrelates highly with their performance when used in multimodal large language\nmodels. Our code, dataset, and leaderboard are publicly available at\nhttps://github.com/embeddings-benchmark/mteb.\n","authors":["Chenghao Xiao","Isaac Chung","Imene Kerboua","Jamie Stirling","Xin Zhang","Márton Kardos","Roman Solomatin","Noura Al Moubayed","Kenneth Enevoldsen","Niklas Muennighoff"],"pdf_url":"https://arxiv.org/pdf/2504.10471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10466v1","updated":"2025-04-14T17:53:10Z","published":"2025-04-14T17:53:10Z","title":"Art3D: Training-Free 3D Generation from Flat-Colored Illustration","summary":"  Large-scale pre-trained image-to-3D generative models have exhibited\nremarkable capabilities in diverse shape generations. However, most of them\nstruggle to synthesize plausible 3D assets when the reference image is\nflat-colored like hand drawings due to the lack of 3D illusion, which are often\nthe most user-friendly input modalities in art content creation. To this end,\nwe propose Art3D, a training-free method that can lift flat-colored 2D designs\ninto 3D. By leveraging structural and semantic features with pre- trained 2D\nimage generation models and a VLM-based realism evaluation, Art3D successfully\nenhances the three-dimensional illusion in reference images, thus simplifying\nthe process of generating 3D from 2D, and proves adaptable to a wide range of\npainting styles. To benchmark the generalization performance of existing\nimage-to-3D models on flat-colored images without 3D feeling, we collect a new\ndataset, Flat-2D, with over 100 samples. Experimental results demonstrate the\nperformance and robustness of Art3D, exhibiting superior generalizable capacity\nand promising practical applicability. Our source code and dataset will be\npublicly available on our project page: https://joy-jy11.github.io/ .\n","authors":["Xiaoyan Cong","Jiayi Shen","Zekun Li","Rao Fu","Tao Lu","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2504.10466v1.pdf","comment":"Technical Report. Course Project of Brown CSCI 1430 Computer Vision.\n  Project Page: https://joy-jy11.github.io/"},{"id":"http://arxiv.org/abs/2312.15856v3","updated":"2025-04-14T17:53:02Z","published":"2023-12-26T02:50:42Z","title":"SplatMesh: Interactive 3D Segmentation and Editing Using Mesh-Based\n  Gaussian Splatting","summary":"  A key challenge in fine-grained 3D-based interactive editing is the absence\nof an efficient representation that balances diverse modifications with\nhigh-quality view synthesis under a given memory constraint. While 3D meshes\nprovide robustness for various modifications, they often yield lower-quality\nview synthesis compared to 3D Gaussian Splatting, which, in turn, suffers from\ninstability during extensive editing. A straightforward combination of these\ntwo representations results in suboptimal performance and fails to meet memory\nconstraints. In this paper, we introduce SplatMesh, a novel fine-grained\ninteractive 3D segmentation and editing algorithm that integrates 3D Gaussian\nSplat with a precomputed mesh and could adjust the memory request based on the\nrequirement. Specifically, given a mesh, \\method simplifies it while\nconsidering both color and shape, ensuring it meets memory constraints. Then,\nSplatMesh aligns Gaussian splats with the simplified mesh by treating each\ntriangle as a new reference point. By segmenting and editing the simplified\nmesh, we can effectively edit the Gaussian splats as well, which will lead to\nextensive experiments on real and synthetic datasets, coupled with illustrative\nvisual examples, highlighting the superiority of our approach in terms of\nrepresentation quality and editing performance. Code of our paper can be found\nhere: https://github.com/kaichen-z/SplatMesh.\n","authors":["Kaichen Zhou","Lanqing Hong","Xinhai Chang","Yingji Zhong","Enze Xie","Hao Dong","Zhihao Li","Yongxin Yang","Zhenguo Li","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.15856v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10465v1","updated":"2025-04-14T17:52:22Z","published":"2025-04-14T17:52:22Z","title":"Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding","summary":"  Multimodal Large Language Models (MLLMs) achieve remarkable performance for\nfine-grained pixel-level understanding tasks. However, all the works rely\nheavily on extra components, such as vision encoder (CLIP), segmentation\nexperts, leading to high system complexity and limiting model scaling. In this\nwork, our goal is to explore a highly simplified MLLM without introducing extra\ncomponents. Our work is motivated by the recent works on Single trAnsformer as\na unified vIsion-Language Model (SAIL) design, where these works jointly learn\nvision tokens and text tokens in transformers. We present Pixel-SAIL, a single\ntransformer for pixel-wise MLLM tasks. In particular, we present three\ntechnical improvements on the plain baseline. First, we design a learnable\nupsampling module to refine visual token features. Secondly, we propose a novel\nvisual prompt injection strategy to enable the single transformer to understand\nvisual prompt inputs and benefit from the early fusion of visual prompt\nembeddings and vision tokens. Thirdly, we introduce a vision expert\ndistillation strategy to efficiently enhance the single transformer's\nfine-grained feature extraction capability. In addition, we have collected a\ncomprehensive pixel understanding benchmark (PerBench), using a manual check.\nIt includes three tasks: detailed object description, visual prompt-based\nquestion answering, and visual-text referring segmentation. Extensive\nexperiments on four referring segmentation benchmarks, one visual prompt\nbenchmark, and our PerBench show that our Pixel-SAIL achieves comparable or\neven better results with a much simpler pipeline. Code and model will be\nreleased at https://github.com/magic-research/Sa2VA.\n","authors":["Tao Zhang","Xiangtai Li","Zilong Huang","Yanwei Li","Weixian Lei","Xueqing Deng","Shihao Chen","Shunping Ji","Jiashi Feng"],"pdf_url":"https://arxiv.org/pdf/2504.10465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10462v1","updated":"2025-04-14T17:50:20Z","published":"2025-04-14T17:50:20Z","title":"The Scalability of Simplicity: Empirical Analysis of Vision-Language\n  Learning with a Single Transformer","summary":"  This paper introduces SAIL, a single transformer unified multimodal large\nlanguage model (MLLM) that integrates raw pixel encoding and language decoding\nwithin a singular architecture. Unlike existing modular MLLMs, which rely on a\npre-trained vision transformer (ViT), SAIL eliminates the need for a separate\nvision encoder, presenting a more minimalist architecture design. Instead of\nintroducing novel architectural components, SAIL adapts mix-attention\nmechanisms and multimodal positional encodings to better align with the\ndistinct characteristics of visual and textual modalities. We systematically\ncompare SAIL's properties-including scalability, cross-modal information flow\npatterns, and visual representation capabilities-with those of modular MLLMs.\nBy scaling both training data and model size, SAIL achieves performance\ncomparable to modular MLLMs. Notably, the removal of pretrained ViT components\nenhances SAIL's scalability and results in significantly different cross-modal\ninformation flow patterns. Moreover, SAIL demonstrates strong visual\nrepresentation capabilities, achieving results on par with ViT-22B in vision\ntasks such as semantic segmentation. Code and models are available at\nhttps://github.com/bytedance/SAIL.\n","authors":["Weixian Lei","Jiacong Wang","Haochen Wang","Xiangtai Li","Jun Hao Liew","Jiashi Feng","Zilong Huang"],"pdf_url":"https://arxiv.org/pdf/2504.10462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04486v2","updated":"2025-04-14T17:45:58Z","published":"2025-01-08T13:13:52Z","title":"MB-TaylorFormer V2: Improved Multi-branch Linear Transformer Expanded by\n  Taylor Formula for Image Restoration","summary":"  Recently, Transformer networks have demonstrated outstanding performance in\nthe field of image restoration due to the global receptive field and\nadaptability to input. However, the quadratic computational complexity of\nSoftmax-attention poses a significant limitation on its extensive application\nin image restoration tasks, particularly for high-resolution images. To tackle\nthis challenge, we propose a novel variant of the Transformer. This variant\nleverages the Taylor expansion to approximate the Softmax-attention and\nutilizes the concept of norm-preserving mapping to approximate the remainder of\nthe first-order Taylor expansion, resulting in a linear computational\ncomplexity. Moreover, we introduce a multi-branch architecture featuring\nmulti-scale patch embedding into the proposed Transformer, which has four\ndistinct advantages: 1) various sizes of the receptive field; 2) multi-level\nsemantic information; 3) flexible shapes of the receptive field; 4) accelerated\ntraining and inference speed. Hence, the proposed model, named the second\nversion of Taylor formula expansion-based Transformer (for short\nMB-TaylorFormer V2) has the capability to concurrently process coarse-to-fine\nfeatures, capture long-distance pixel interactions with limited computational\ncost, and improve the approximation of the Taylor expansion remainder.\nExperimental results across diverse image restoration benchmarks demonstrate\nthat MB-TaylorFormer V2 achieves state-of-the-art performance in multiple image\nrestoration tasks, such as image dehazing, deraining, desnowing, motion\ndeblurring, and denoising, with very little computational overhead. The source\ncode is available at https://github.com/FVL2020/MB-TaylorFormerV2.\n","authors":["Zhi Jin","Yuwei Qiu","Kaihao Zhang","Hongdong Li","Wenhan Luo"],"pdf_url":"https://arxiv.org/pdf/2501.04486v2.pdf","comment":"accepted by IEEE TPAMI"},{"id":"http://arxiv.org/abs/2504.10458v1","updated":"2025-04-14T17:45:54Z","published":"2025-04-14T17:45:54Z","title":"GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI\n  Agents","summary":"  Existing efforts in building Graphical User Interface (GUI) agents largely\nrely on the training paradigm of supervised fine-tuning on Large\nVision-Language Models (LVLMs). However, this approach not only demands\nextensive amounts of training data but also struggles to effectively understand\nGUI screenshots and generalize to unseen interfaces. The issue significantly\nlimits its application in real-world scenarios, especially for high-level\ntasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models\n(e.g., DeepSeek-R1), which efficiently enhances the problem-solving\ncapabilities of large language models in real-world settings, we propose \\name,\nthe first reinforcement learning framework designed to enhance the GUI\ncapabilities of LVLMs in high-level real-world task scenarios, through unified\naction space rule modeling. By leveraging a small amount of carefully curated\nhigh-quality data across multiple platforms (including Windows, Linux, MacOS,\nAndroid, and Web) and employing policy optimization algorithms such as Group\nRelative Policy Optimization (GRPO) to update the model, \\name achieves\nsuperior performance using only 0.02\\% of the data (3K vs. 13M) compared to\nprevious state-of-the-art methods like OS-Atlas across eight benchmarks\nspanning three different platforms (mobile, desktop, and web). These results\ndemonstrate the immense potential of reinforcement learning based on unified\naction space rule modeling in improving the execution capabilities of LVLMs for\nreal-world GUI agent tasks.\n","authors":["Xiaobo Xia","Run Luo"],"pdf_url":"https://arxiv.org/pdf/2504.10458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10452v1","updated":"2025-04-14T17:39:18Z","published":"2025-04-14T17:39:18Z","title":"Integrating Vision and Location with Transformers: A Multimodal Deep\n  Learning Framework for Medical Wound Analysis","summary":"  Effective recognition of acute and difficult-to-heal wounds is a necessary\nstep in wound diagnosis. An efficient classification model can help wound\nspecialists classify wound types with less financial and time costs and also\nhelp in deciding on the optimal treatment method. Traditional machine learning\nmodels suffer from feature selection and are usually cumbersome models for\naccurate recognition. Recently, deep learning (DL) has emerged as a powerful\ntool in wound diagnosis. Although DL seems promising for wound type\nrecognition, there is still a large scope for improving the efficiency and\naccuracy of the model. In this study, a DL-based multimodal classifier was\ndeveloped using wound images and their corresponding locations to classify them\ninto multiple classes, including diabetic, pressure, surgical, and venous\nulcers. A body map was also created to provide location data, which can help\nwound specialists label wound locations more effectively. The model uses a\nVision Transformer to extract hierarchical features from input images, a\nDiscrete Wavelet Transform (DWT) layer to capture low and high frequency\ncomponents, and a Transformer to extract spatial features. The number of\nneurons and weight vector optimization were performed using three swarm-based\noptimization techniques (Monster Gorilla Toner (MGTO), Improved Gray Wolf\nOptimization (IGWO), and Fox Optimization Algorithm). The evaluation results\nshow that weight vector optimization using optimization algorithms can increase\ndiagnostic accuracy and make it a very effective approach for wound detection.\nIn the classification using the original body map, the proposed model was able\nto achieve an accuracy of 0.8123 using image data and an accuracy of 0.8007\nusing a combination of image data and wound location. Also, the accuracy of the\nmodel in combination with the optimization models varied from 0.7801 to 0.8342.\n","authors":["Ramin Mousa","Hadis Taherinia","Khabiba Abdiyeva","Amir Ali Bengari","Mohammadmahdi Vahediahmar"],"pdf_url":"https://arxiv.org/pdf/2504.10452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10445v1","updated":"2025-04-14T17:36:46Z","published":"2025-04-14T17:36:46Z","title":"RealWebAssist: A Benchmark for Long-Horizon Web Assistance with\n  Real-World Users","summary":"  To achieve successful assistance with long-horizon web-based tasks, AI agents\nmust be able to sequentially follow real-world user instructions over a long\nperiod. Unlike existing web-based agent benchmarks, sequential instruction\nfollowing in the real world poses significant challenges beyond performing a\nsingle, clearly defined task. For instance, real-world human instructions can\nbe ambiguous, require different levels of AI assistance, and may evolve over\ntime, reflecting changes in the user's mental state. To address this gap, we\nintroduce RealWebAssist, a novel benchmark designed to evaluate sequential\ninstruction-following in realistic scenarios involving long-horizon\ninteractions with the web, visual GUI grounding, and understanding ambiguous\nreal-world user instructions. RealWebAssist includes a dataset of sequential\ninstructions collected from real-world human users. Each user instructs a\nweb-based assistant to perform a series of tasks on multiple websites. A\nsuccessful agent must reason about the true intent behind each instruction,\nkeep track of the mental state of the user, understand user-specific routines,\nand ground the intended tasks to actions on the correct GUI elements. Our\nexperimental results show that state-of-the-art models struggle to understand\nand ground user instructions, posing critical challenges in following\nreal-world user instructions for long-horizon web assistance.\n","authors":["Suyu Ye","Haojun Shi","Darren Shih","Hyokun Yun","Tanya Roosta","Tianmin Shu"],"pdf_url":"https://arxiv.org/pdf/2504.10445v1.pdf","comment":"Project Website: https://scai.cs.jhu.edu/projects/RealWebAssist/\n  Code: https://github.com/SCAI-JHU/RealWebAssist"},{"id":"http://arxiv.org/abs/2501.17391v2","updated":"2025-04-14T17:34:19Z","published":"2025-01-29T02:52:32Z","title":"Learning Free Token Reduction for Multi-Modal Large Language Models","summary":"  Vision-Language Models (VLMs) have achieved remarkable success across a range\nof multimodal tasks; however, their practical deployment is often constrained\nby high computational costs and prolonged inference times. Since the vision\nmodality typically carries more information than the text modality, compressing\nvisual prompts offers a promising solution to alleviate these challenges.\nExisting approaches predominantly focus on refining model architectures or\ndirectly reducing the number of visual tokens. However, these methods often\ncompromise inference performance due to a lack of consideration for the unique\nspatial and temporal characteristics of visual data. In this work, we propose a\ntoken compression paradigm that operates on both spatial and temporal\ndimensions. Our approach includes a learning-free, plug-and-play compression\npipeline that can be seamlessly integrated into most Multimodal Large Language\nModel (MLLM) frameworks. By leveraging this method, we enhance the model\ninference capability while simultaneously reducing its computational cost.\nExperimental results on the Video-QA task demonstrate the effectiveness of the\nproposed approach, showcasing significant improvements in efficiency without\nsacrificing performance.\n","authors":["Zihui Zhao","Yingxin Li","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2501.17391v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10443v1","updated":"2025-04-14T17:34:06Z","published":"2025-04-14T17:34:06Z","title":"Multimodal Long Video Modeling Based on Temporal Dynamic Context","summary":"  Recent advances in Large Language Models (LLMs) have led to significant\nbreakthroughs in video understanding. However, existing models still struggle\nwith long video processing due to the context length constraint of LLMs and the\nvast amount of information within the video. Although some recent methods are\ndesigned for long video understanding, they often lose crucial information\nduring token compression and struggle with additional modality like audio. In\nthis work, we propose a dynamic long video encoding method utilizing the\ntemporal relationship between frames, named Temporal Dynamic Context (TDC).\nFirstly, we segment the video into semantically consistent scenes based on\ninter-frame similarities, then encode each frame into tokens using visual-audio\nencoders. Secondly, we propose a novel temporal context compressor to reduce\nthe number of tokens within each segment. Specifically, we employ a query-based\nTransformer to aggregate video, audio, and instruction text tokens into a\nlimited set of temporal context tokens. Finally, we feed the static frame\ntokens and the temporal context tokens into the LLM for video understanding.\nFurthermore, to handle extremely long videos, we propose a training-free\nchain-of-thought strategy that progressively extracts answers from multiple\nvideo segments. These intermediate answers serve as part of the reasoning\nprocess and contribute to the final answer. We conduct extensive experiments on\ngeneral video understanding and audio-video understanding benchmarks, where our\nmethod demonstrates strong performance. The code and models are available at\nhttps://github.com/Hoar012/TDC-Video.\n","authors":["Haoran Hao","Jiaming Han","Yiyuan Zhang","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2504.10443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08727v2","updated":"2025-04-14T17:30:56Z","published":"2025-04-11T17:55:45Z","title":"Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images","summary":"  We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles.\n","authors":["Boyang Deng","Songyou Peng","Kyle Genova","Gordon Wetzstein","Noah Snavely","Leonidas Guibas","Thomas Funkhouser"],"pdf_url":"https://arxiv.org/pdf/2504.08727v2.pdf","comment":"Project page: https://boyangdeng.com/visual-chronicles , second and\n  third listed authors have equal contributions"},{"id":"http://arxiv.org/abs/2504.07210v2","updated":"2025-04-14T17:25:41Z","published":"2025-04-09T18:37:24Z","title":"MESA: Text-Driven Terrain Generation Using Latent Diffusion and Global\n  Copernicus Data","summary":"  Terrain modeling has traditionally relied on procedural techniques, which\noften require extensive domain expertise and handcrafted rules. In this paper,\nwe present MESA - a novel data-centric alternative by training a diffusion\nmodel on global remote sensing data. This approach leverages large-scale\ngeospatial information to generate high-quality terrain samples from text\ndescriptions, showcasing a flexible and scalable solution for terrain\ngeneration. The model's capabilities are demonstrated through extensive\nexperiments, highlighting its ability to generate realistic and diverse terrain\nlandscapes. The dataset produced to support this work, the Major TOM Core-DEM\nextension dataset, is released openly as a comprehensive resource for global\nterrain data. The results suggest that data-driven models, trained on remote\nsensing data, can provide a powerful tool for realistic terrain modeling and\ngeneration.\n","authors":["Paul Borne--Pons","Mikolaj Czerkawski","Rosalie Martin","Romain Rouffet"],"pdf_url":"https://arxiv.org/pdf/2504.07210v2.pdf","comment":"Accepted at CVPR 2025 Workshop MORSE"},{"id":"http://arxiv.org/abs/2504.10434v1","updated":"2025-04-14T17:25:19Z","published":"2025-04-14T17:25:19Z","title":"Anchor Token Matching: Implicit Structure Locking for Training-free AR\n  Image Editing","summary":"  Text-to-image generation has seen groundbreaking advancements with diffusion\nmodels, enabling high-fidelity synthesis and precise image editing through\ncross-attention manipulation. Recently, autoregressive (AR) models have\nre-emerged as powerful alternatives, leveraging next-token generation to match\ndiffusion models. However, existing editing techniques designed for diffusion\nmodels fail to translate directly to AR models due to fundamental differences\nin structural control. Specifically, AR models suffer from spatial poverty of\nattention maps and sequential accumulation of structural errors during image\nediting, which disrupt object layouts and global consistency. In this work, we\nintroduce Implicit Structure Locking (ISLock), the first training-free editing\nstrategy for AR visual models. Rather than relying on explicit attention\nmanipulation or fine-tuning, ISLock preserves structural blueprints by\ndynamically aligning self-attention patterns with reference images through the\nAnchor Token Matching (ATM) protocol. By implicitly enforcing structural\nconsistency in latent space, our method ISLock enables structure-aware editing\nwhile maintaining generative autonomy. Extensive experiments demonstrate that\nISLock achieves high-quality, structure-consistent edits without additional\ntraining and is superior or comparable to conventional editing techniques. Our\nfindings pioneer the way for efficient and flexible AR-based image editing,\nfurther bridging the performance gap between diffusion and autoregressive\ngenerative models. The code will be publicly available at\nhttps://github.com/hutaiHang/ATM\n","authors":["Taihang Hu","Linxuan Li","Kai Wang","Yaxing Wang","Jian Yang","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2504.10434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10433v1","updated":"2025-04-14T17:21:10Z","published":"2025-04-14T17:21:10Z","title":"MonoDiff9D: Monocular Category-Level 9D Object Pose Estimation via\n  Diffusion Model","summary":"  Object pose estimation is a core means for robots to understand and interact\nwith their environment. For this task, monocular category-level methods are\nattractive as they require only a single RGB camera. However, current methods\nrely on shape priors or CAD models of the intra-class known objects. We propose\na diffusion-based monocular category-level 9D object pose generation method,\nMonoDiff9D. Our motivation is to leverage the probabilistic nature of diffusion\nmodels to alleviate the need for shape priors, CAD models, or depth sensors for\nintra-class unknown object pose estimation. We first estimate coarse depth via\nDINOv2 from the monocular image in a zero-shot manner and convert it into a\npoint cloud. We then fuse the global features of the point cloud with the input\nimage and use the fused features along with the encoded time step to condition\nMonoDiff9D. Finally, we design a transformer-based denoiser to recover the\nobject pose from Gaussian noise. Extensive experiments on two popular benchmark\ndatasets show that MonoDiff9D achieves state-of-the-art monocular\ncategory-level 9D object pose estimation accuracy without the need for shape\npriors or CAD models at any stage. Our code will be made public at\nhttps://github.com/CNJianLiu/MonoDiff9D.\n","authors":["Jian Liu","Wei Sun","Hui Yang","Jin Zheng","Zichen Geng","Hossein Rahmani","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2504.10433v1.pdf","comment":"Accepted by ICRA'25"},{"id":"http://arxiv.org/abs/2503.07901v2","updated":"2025-04-14T17:02:26Z","published":"2025-03-10T22:43:07Z","title":"Intelligent Framework for Human-Robot Collaboration: Dynamic Ergonomics\n  and Adaptive Decision-Making","summary":"  The integration of collaborative robots into industrial environments has\nimproved productivity, but has also highlighted significant challenges related\nto operator safety and ergonomics. This paper proposes an innovative framework\nthat integrates advanced visual perception, continuous ergonomic monitoring,\nand adaptive Behaviour Tree decision-making to overcome the limitations of\ntraditional methods that typically operate as isolated components. Our approach\nsynthesizes deep learning models, advanced tracking algorithms, and dynamic\nergonomic assessments into a modular, scalable, and adaptive system.\nExperimental validation demonstrates the framework's superiority over existing\nsolutions across multiple dimensions: the visual perception module outperformed\nprevious detection models with 72.4% mAP@50:95; the system achieved high\naccuracy in recognizing operator intentions (92.5%); it promptly classified\nergonomic risks with minimal latency (0.57 seconds); and it dynamically managed\nrobotic interventions with exceptionally responsive decision-making\ncapabilities (0.07 seconds), representing a 56% improvement over benchmark\nsystems. This comprehensive solution provides a robust platform for enhancing\nhuman-robot collaboration in industrial environments by prioritizing ergonomic\nsafety, operational efficiency, and real-time adaptability.\n","authors":["Francesco Iodice","Elena De Momi","Arash Ajoudani"],"pdf_url":"https://arxiv.org/pdf/2503.07901v2.pdf","comment":"15 pagine, 8figure, 3 tabelle, formato conferenza IEEE"},{"id":"http://arxiv.org/abs/2504.10414v1","updated":"2025-04-14T16:59:29Z","published":"2025-04-14T16:59:29Z","title":"HUMOTO: A 4D Dataset of Mocap Human Object Interactions","summary":"  We present Human Motions with Objects (HUMOTO), a high-fidelity dataset of\nhuman-object interactions for motion generation, computer vision, and robotics\napplications. Featuring 736 sequences (7,875 seconds at 30 fps), HUMOTO\ncaptures interactions with 63 precisely modeled objects and 72 articulated\nparts. Our innovations include a scene-driven LLM scripting pipeline creating\ncomplete, purposeful tasks with natural progression, and a mocap-and-camera\nrecording setup to effectively handle occlusions. Spanning diverse activities\nfrom cooking to outdoor picnics, HUMOTO preserves both physical accuracy and\nlogical task flow. Professional artists rigorously clean and verify each\nsequence, minimizing foot sliding and object penetrations. We also provide\nbenchmarks compared to other datasets. HUMOTO's comprehensive full-body motion\nand simultaneous multi-object interactions address key data-capturing\nchallenges and provide opportunities to advance realistic human-object\ninteraction modeling across research domains with practical applications in\nanimation, robotics, and embodied AI systems. Project:\nhttps://jiaxin-lu.github.io/humoto/ .\n","authors":["Jiaxin Lu","Chun-Hao Paul Huang","Uttaran Bhattacharya","Qixing Huang","Yi Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.10414v1.pdf","comment":"19 pages, 15 figures"},{"id":"http://arxiv.org/abs/2504.10409v1","updated":"2025-04-14T16:58:02Z","published":"2025-04-14T16:58:02Z","title":"GPS: Distilling Compact Memories via Grid-based Patch Sampling for\n  Efficient Online Class-Incremental Learning","summary":"  Online class-incremental learning aims to enable models to continuously adapt\nto new classes with limited access to past data, while mitigating catastrophic\nforgetting. Replay-based methods address this by maintaining a small memory\nbuffer of previous samples, achieving competitive performance. For effective\nreplay under constrained storage, recent approaches leverage distilled data to\nenhance the informativeness of memory. However, such approaches often involve\nsignificant computational overhead due to the use of bi-level optimization.\nMotivated by these limitations, we introduce Grid-based Patch Sampling (GPS), a\nlightweight and effective strategy for distilling informative memory samples\nwithout relying on a trainable model. GPS generates informative samples by\nsampling a subset of pixels from the original image, yielding compact\nlow-resolution representations that preserve both semantic content and\nstructural information. During replay, these representations are reassembled to\nsupport training and evaluation. Experiments on extensive benchmarks\ndemonstrate that GRS can be seamlessly integrated into existing replay\nframeworks, leading to 3%-4% improvements in average end accuracy under\nmemory-constrained settings, with limited computational overhead.\n","authors":["Mingchuan Ma","Yuhao Zhou","Jindi Lv","Yuxin Tian","Dan Si","Shujian Li","Qing Ye","Jiancheng Lv"],"pdf_url":"https://arxiv.org/pdf/2504.10409v1.pdf","comment":"10 pages, 10 figures"},{"id":"http://arxiv.org/abs/2504.10400v1","updated":"2025-04-14T16:51:10Z","published":"2025-04-14T16:51:10Z","title":"Towards Low-Latency Event-based Obstacle Avoidance on a FPGA-Drone","summary":"  This work quantitatively evaluates the performance of event-based vision\nsystems (EVS) against conventional RGB-based models for action prediction in\ncollision avoidance on an FPGA accelerator. Our experiments demonstrate that\nthe EVS model achieves a significantly higher effective frame rate (1 kHz) and\nlower temporal (-20 ms) and spatial prediction errors (-20 mm) compared to the\nRGB-based model, particularly when tested on out-of-distribution data. The EVS\nmodel also exhibits superior robustness in selecting optimal evasion maneuvers.\nIn particular, in distinguishing between movement and stationary states, it\nachieves a 59 percentage point advantage in precision (78% vs. 19%) and a\nsubstantially higher F1 score (0.73 vs. 0.06), highlighting the susceptibility\nof the RGB model to overfitting. Further analysis in different combinations of\nspatial classes confirms the consistent performance of the EVS model in both\ntest data sets. Finally, we evaluated the system end-to-end and achieved a\nlatency of approximately 2.14 ms, with event aggregation (1 ms) and inference\non the processing unit (0.94 ms) accounting for the largest components. These\nresults underscore the advantages of event-based vision for real-time collision\navoidance and demonstrate its potential for deployment in resource-constrained\nenvironments.\n","authors":["Pietro Bonazzi","Christian Vogt","Michael Jost","Lyes Khacef","Federico Paredes-Vallés","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2504.10400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10395v1","updated":"2025-04-14T16:44:08Z","published":"2025-04-14T16:44:08Z","title":"Better Coherence, Better Height: Fusing Physical Models and Deep\n  Learning for Forest Height Estimation from Interferometric SAR Data","summary":"  Estimating forest height from Synthetic Aperture Radar (SAR) images often\nrelies on traditional physical models, which, while interpretable and\ndata-efficient, can struggle with generalization. In contrast, Deep Learning\n(DL) approaches lack physical insight. To address this, we propose CoHNet - an\nend-to-end framework that combines the best of both worlds: DL optimized with\nphysics-informed constraints. We leverage a pre-trained neural surrogate model\nto enforce physical plausibility through a unique training loss. Our\nexperiments show that this approach not only improves forest height estimation\naccuracy but also produces meaningful features that enhance the reliability of\npredictions.\n","authors":["Ragini Bal Mahesh","Ronny Hänsch"],"pdf_url":"https://arxiv.org/pdf/2504.10395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19656v2","updated":"2025-04-14T16:26:03Z","published":"2024-04-30T15:52:49Z","title":"Towards Scenario- and Capability-Driven Dataset Development and\n  Evaluation: An Approach in the Context of Mapless Automated Driving","summary":"  The foundational role of datasets in defining the capabilities of deep\nlearning models has led to their rapid proliferation. At the same time,\npublished research focusing on the process of dataset development for\nenvironment perception in automated driving has been scarce, thereby reducing\nthe applicability of openly available datasets and impeding the development of\neffective environment perception systems. Sensor-based, mapless automated\ndriving is one of the contexts where this limitation is evident. While\nleveraging real-time sensor data, instead of pre-defined HD maps promises\nenhanced adaptability and safety by effectively navigating unexpected\nenvironmental changes, it also increases the demands on the scope and\ncomplexity of the information provided by the perception system.\n  To address these challenges, we propose a scenario- and capability-based\napproach to dataset development. Grounded in the principles of ISO 21448\n(safety of the intended functionality, SOTIF), extended by ISO/TR 4804, our\napproach facilitates the structured derivation of dataset requirements. This\nnot only aids in the development of meaningful new datasets but also enables\nthe effective comparison of existing ones. Applying this methodology to a broad\nrange of existing lane detection datasets, we identify significant limitations\nin current datasets, particularly in terms of real-world applicability, a lack\nof labeling of critical features, and an absence of comprehensive information\nfor complex driving maneuvers.\n","authors":["Felix Grün","Marcus Nolte","Markus Maurer"],"pdf_url":"https://arxiv.org/pdf/2404.19656v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10375v1","updated":"2025-04-14T16:23:15Z","published":"2025-04-14T16:23:15Z","title":"PG-DPIR: An efficient plug-and-play method for high-count\n  Poisson-Gaussian inverse problems","summary":"  Poisson-Gaussian noise describes the noise of various imaging systems thus\nthe need of efficient algorithms for Poisson-Gaussian image restoration. Deep\nlearning methods offer state-of-the-art performance but often require\nsensor-specific training when used in a supervised setting. A promising\nalternative is given by plug-and-play (PnP) methods, which consist in learning\nonly a regularization through a denoiser, allowing to restore images from\nseveral sources with the same network. This paper introduces PG-DPIR, an\nefficient PnP method for high-count Poisson-Gaussian inverse problems, adapted\nfrom DPIR. While DPIR is designed for white Gaussian noise, a naive adaptation\nto Poisson-Gaussian noise leads to prohibitively slow algorithms due to the\nabsence of a closed-form proximal operator. To address this, we adapt DPIR for\nthe specificities of Poisson-Gaussian noise and propose in particular an\nefficient initialization of the gradient descent required for the proximal step\nthat accelerates convergence by several orders of magnitude. Experiments are\nconducted on satellite image restoration and super-resolution problems.\nHigh-resolution realistic Pleiades images are simulated for the experiments,\nwhich demonstrate that PG-DPIR achieves state-of-the-art performance with\nimproved efficiency, which seems promising for on-ground satellite processing\nchains.\n","authors":["Maud Biquard","Marie Chabert","Florence Genin","Christophe Latry","Thomas Oberlin"],"pdf_url":"https://arxiv.org/pdf/2504.10375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16003v4","updated":"2025-04-14T16:10:57Z","published":"2025-01-27T12:42:20Z","title":"Improving Tropical Cyclone Forecasting With Video Diffusion Models","summary":"  Tropical cyclone (TC) forecasting is crucial for disaster preparedness and\nmitigation. While recent deep learning approaches have shown promise, existing\nmethods often treat TC evolution as a series of independent frame-to-frame\npredictions, limiting their ability to capture long-term dynamics. We present a\nnovel application of video diffusion models for TC forecasting that explicitly\nmodels temporal dependencies through additional temporal layers. Our approach\nenables the model to generate multiple frames simultaneously, better capturing\ncyclone evolution patterns. We introduce a two-stage training strategy that\nsignificantly improves individual-frame quality and performance in low-data\nregimes. Experimental results show our method outperforms the previous approach\nof Nath et al. by 19.3% in MAE, 16.2% in PSNR, and 36.1% in SSIM. Most notably,\nwe extend the reliable forecasting horizon from 36 to 50 hours. Through\ncomprehensive evaluation using both traditional metrics and Fr\\'echet Video\nDistance (FVD), we demonstrate that our approach produces more temporally\ncoherent forecasts while maintaining competitive single-frame quality. Code\naccessible at https://github.com/Ren-creater/forecast-video-diffmodels.\n","authors":["Zhibo Ren","Pritthijit Nath","Pancham Shukla"],"pdf_url":"https://arxiv.org/pdf/2501.16003v4.pdf","comment":"Accepted for spotlight presentation at the ICLR 2025 workshop on\n  Tackling Climate Change with Machine Learning. 7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.10358v1","updated":"2025-04-14T16:07:16Z","published":"2025-04-14T16:07:16Z","title":"FingER: Content Aware Fine-grained Evaluation with Reasoning for\n  AI-Generated Videos","summary":"  Recent advances in video generation have posed great challenges in the\nassessment of AI-generated content, particularly with the emergence of\nincreasingly sophisticated models. The various inconsistencies and defects\nobserved in such videos are inherently complex, making overall scoring\nnotoriously difficult. In this paper, we emphasize the critical importance of\nintegrating fine-grained reasoning into video evaluation, and we propose\n$\\textbf{F}$ing$\\textbf{ER}$, a novel entity-level reasoning evaluation\nframework that first automatically generates $\\textbf{F}$ine-grained\n$\\textbf{E}$ntity-level questions, and then answers those questions by a\n$\\textbf{R}$easoning model with scores, which can be subsequently weighted\nsummed to an overall score for different applications. Specifically, we\nleverage LLMs to derive entity-level questions across five distinct\nperspectives, which (i) often focus on some specific entities of the content,\nthereby making answering or scoring much easier by MLLMs, and (ii) are more\ninterpretable. Then we construct a FingER dataset, consisting of approximately\n3.3k videos and corresponding 60k fine-grained QA annotations, each with\ndetailed reasons. Based on that, we further investigate various training\nprotocols to best incentivize the reasoning capability of MLLMs for correct\nanswer prediction. Extensive experiments demonstrate that a reasoning model\ntrained using Group Relative Policy Optimization (GRPO) with a cold-start\nstrategy achieves the best performance. Notably, our model surpasses existing\nmethods by a relative margin of $11.8\\%$ on GenAI-Bench and $5.5\\%$ on\nMonetBench with only 3.3k training videos, which is at most one-tenth of the\ntraining samples utilized by other methods. Our code and dataset will be\nreleased soon.\n","authors":["Rui Chen","Lei Sun","Jing Tang","Geng Li","Xiangxiang Chu"],"pdf_url":"https://arxiv.org/pdf/2504.10358v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.10353v1","updated":"2025-04-14T16:03:21Z","published":"2025-04-14T16:03:21Z","title":"Patch and Shuffle: A Preprocessing Technique for Texture Classification\n  in Autonomous Cementitious Fabrication","summary":"  Autonomous fabrication systems are transforming construction and\nmanufacturing, yet they remain vulnerable to print errors. Texture\nclassification is a key component of computer vision systems that enable\nreal-time monitoring and adjustment during cementitious fabrication.\nTraditional classification methods often rely on global image features, which\ncan bias the model toward semantic content rather than low-level textures. In\nthis paper, we introduce a novel preprocessing technique called \"patch and\nshuffle,\" which segments input images into smaller patches, shuffles them, and\nreconstructs a jumbled image before classification. This transformation removes\nsemantic context, forcing the classifier to rely on local texture features.\n  We evaluate this approach on a dataset of extruded cement images, using a\nResNet-18-based architecture. Our experiments compare the patch and shuffle\nmethod to a standard pipeline, holding all other factors constant. Results show\na significant improvement in accuracy: the patch and shuffle model achieved\n90.64% test accuracy versus 72.46% for the baseline. These findings suggest\nthat disrupting global structure enhances performance in texture-based\nclassification tasks.\n  This method has implications for broader vision tasks where low-level\nfeatures matter more than high-level semantics. The technique may improve\nclassification in applications ranging from fabrication monitoring to medical\nimaging.\n","authors":["Jeremiah Giordani"],"pdf_url":"https://arxiv.org/pdf/2504.10353v1.pdf","comment":"Originally completed as a final project for CEE 374 at Princeton\n  University"},{"id":"http://arxiv.org/abs/2411.12044v2","updated":"2025-04-14T16:02:15Z","published":"2024-11-18T20:31:38Z","title":"ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text,\n  and Architectural Enhancements","summary":"  Recent advances in foundational Vision Language Models (VLMs) have reshaped\nthe evaluation paradigm in computer vision tasks. These foundational models,\nespecially CLIP, have accelerated research in open-vocabulary computer vision\ntasks, including Open-Vocabulary Semantic Segmentation (OVSS). Although the\ninitial results are promising, the dense prediction capabilities of VLMs still\nrequire further improvement. In this study, we enhance the semantic\nsegmentation performance of CLIP by introducing new modules and modifications:\n1) architectural changes in the last layer of ViT and the incorporation of\nattention maps from the middle layers with the last layer, 2) Image\nEngineering: applying data augmentations to enrich input image representations,\nand 3) using Large Language Models (LLMs) to generate definitions and synonyms\nfor each class name to leverage CLIP's open-vocabulary capabilities. Our\ntraining-free method, ITACLIP, outperforms current state-of-the-art approaches\non segmentation benchmarks such as COCO-Stuff, COCO-Object, Pascal Context, and\nPascal VOC. Our code is available at https://github.com/m-arda-aydn/ITACLIP.\n","authors":["M. Arda Aydın","Efe Mert Çırpar","Elvin Abdinli","Gozde Unal","Yusuf H. Sahin"],"pdf_url":"https://arxiv.org/pdf/2411.12044v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10209v2","updated":"2025-04-14T16:02:03Z","published":"2024-12-13T15:31:22Z","title":"GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view\n  Diffusion","summary":"  We propose a novel approach for reconstructing animatable 3D Gaussian avatars\nfrom monocular videos captured by commodity devices like smartphones.\nPhotorealistic 3D head avatar reconstruction from such recordings is\nchallenging due to limited observations, which leaves unobserved regions\nunder-constrained and can lead to artifacts in novel views. To address this\nproblem, we introduce a multi-view head diffusion model, leveraging its priors\nto fill in missing regions and ensure view consistency in Gaussian splatting\nrenderings. To enable precise viewpoint control, we use normal maps rendered\nfrom FLAME-based head reconstruction, which provides pixel-aligned inductive\nbiases. We also condition the diffusion model on VAE features extracted from\nthe input image to preserve facial identity and appearance details. For\nGaussian avatar reconstruction, we distill multi-view diffusion priors by using\niteratively denoised images as pseudo-ground truths, effectively mitigating\nover-saturation issues. To further improve photorealism, we apply latent\nupsampling priors to refine the denoised latent before decoding it into an\nimage. We evaluate our method on the NeRSemble dataset, showing that GAF\noutperforms previous state-of-the-art methods in novel view synthesis.\nFurthermore, we demonstrate higher-fidelity avatar reconstructions from\nmonocular videos captured on commodity devices.\n","authors":["Jiapeng Tang","Davide Davoli","Tobias Kirschstein","Liam Schoneveld","Matthias Niessner"],"pdf_url":"https://arxiv.org/pdf/2412.10209v2.pdf","comment":"Paper Video: https://youtu.be/QuIYTljvhyg Project Page:\n  https://tangjiapeng.github.io/projects/GAF"},{"id":"http://arxiv.org/abs/2504.10351v1","updated":"2025-04-14T16:00:57Z","published":"2025-04-14T16:00:57Z","title":"Multimodal Representation Learning Techniques for Comprehensive Facial\n  State Analysis","summary":"  Multimodal foundation models have significantly improved feature\nrepresentation by integrating information from multiple modalities, making them\nhighly suitable for a broader set of applications. However, the exploration of\nmultimodal facial representation for understanding perception has been limited.\nUnderstanding and analyzing facial states, such as Action Units (AUs) and\nemotions, require a comprehensive and robust framework that bridges visual and\nlinguistic modalities. In this paper, we present a comprehensive pipeline for\nmultimodal facial state analysis. First, we compile a new Multimodal Face\nDataset (MFA) by generating detailed multilevel language descriptions of face,\nincorporating Action Unit (AU) and emotion descriptions, by leveraging GPT-4o.\nSecond, we introduce a novel Multilevel Multimodal Face Foundation model (MF^2)\ntailored for Action Unit (AU) and emotion recognition. Our model incorporates\ncomprehensive visual feature modeling at both local and global levels of face\nimage, enhancing its ability to represent detailed facial appearances. This\ndesign aligns visual representations with structured AU and emotion\ndescriptions, ensuring effective cross-modal integration. Third, we develop a\nDecoupled Fine-Tuning Network (DFN) that efficiently adapts MF^2 across various\ntasks and datasets. This approach not only reduces computational overhead but\nalso broadens the applicability of the foundation model to diverse scenarios.\nExperimentation show superior performance for AU and emotion detection tasks.\n","authors":["Kaiwen Zheng","Xuri Ge","Junchen Fu","Jun Peng","Joemon M. Jose"],"pdf_url":"https://arxiv.org/pdf/2504.10351v1.pdf","comment":"Accepted by ICME2025"},{"id":"http://arxiv.org/abs/2504.10350v1","updated":"2025-04-14T16:00:25Z","published":"2025-04-14T16:00:25Z","title":"Benchmarking 3D Human Pose Estimation Models Under Occlusions","summary":"  This paper addresses critical challenges in 3D Human Pose Estimation (HPE) by\nanalyzing the robustness and sensitivity of existing models to occlusions,\ncamera position, and action variability. Using a novel synthetic dataset,\nBlendMimic3D, which includes diverse scenarios with multi-camera setups and\nseveral occlusion types, we conduct specific tests on several state-of-the-art\nmodels. Our study focuses on the discrepancy in keypoint formats between common\ndatasets such as Human3.6M, and 2D datasets such as COCO, commonly used for 2D\ndetection models and frequently input of 3D HPE models. Our work explores the\nimpact of occlusions on model performance and the generality of models trained\nexclusively under standard conditions. The findings suggest significant\nsensitivity to occlusions and camera settings, revealing a need for models that\nbetter adapt to real-world variability and occlusion scenarios. This research\ncontributed to ongoing efforts to improve the fidelity and applicability of 3D\nHPE systems in complex environments.\n","authors":["Filipa Lino","Carlos Santiago","Manuel Marques"],"pdf_url":"https://arxiv.org/pdf/2504.10350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12047v2","updated":"2025-04-14T15:46:04Z","published":"2025-03-15T08:38:47Z","title":"PSGait: Gait Recognition using Parsing Skeleton","summary":"  Gait recognition has emerged as a robust biometric modality due to its\nnon-intrusive nature and resilience to occlusion. Conventional gait recognition\nmethods typically rely on silhouettes or skeletons. Despite their success in\ngait recognition for controlled laboratory environments, they usually fail in\nreal-world scenarios due to their limited information entropy for gait\nrepresentations. To achieve accurate gait recognition in the wild, we propose a\nnovel gait representation, named Parsing Skeleton. This representation\ninnovatively introduces the skeleton-guided human parsing method to capture\nfine-grained body dynamics, so they have much higher information entropy to\nencode the shapes and dynamics of fine-grained human parts during walking.\nMoreover, to effectively explore the capability of the Parsing Skeleton\nrepresentation, we propose a novel Parsing Skeleton-based gait recognition\nframework, named PSGait, which takes Parsing Skeletons and silhouettes as\ninput. By fusing these two modalities, the resulting image sequences are fed\ninto gait recognition models for enhanced individual differentiation. We\nconduct comprehensive benchmarks on various datasets to evaluate our model.\nPSGait outperforms existing state-of-the-art multimodal methods that utilize\nboth skeleton and silhouette inputs while significantly reducing computational\nresources. Furthermore, as a plug-and-play method, PSGait leads to a maximum\nimprovement of 10.9% in Rank-1 accuracy across various gait recognition models.\nThese results demonstrate that Parsing Skeleton offers a lightweight,\neffective, and highly generalizable representation for gait recognition in the\nwild.\n","authors":["Hangrui Xu","Chuanrui Zhang","Zhengxian Wu","Peng Jiao","Haoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2503.12047v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10331v1","updated":"2025-04-14T15:39:31Z","published":"2025-04-14T15:39:31Z","title":"LL-Gaussian: Low-Light Scene Reconstruction and Enhancement via Gaussian\n  Splatting for Novel View Synthesis","summary":"  Novel view synthesis (NVS) in low-light scenes remains a significant\nchallenge due to degraded inputs characterized by severe noise, low dynamic\nrange (LDR) and unreliable initialization. While recent NeRF-based approaches\nhave shown promising results, most suffer from high computational costs, and\nsome rely on carefully captured or pre-processed data--such as RAW sensor\ninputs or multi-exposure sequences--which severely limits their practicality.\nIn contrast, 3D Gaussian Splatting (3DGS) enables real-time rendering with\ncompetitive visual fidelity; however, existing 3DGS-based methods struggle with\nlow-light sRGB inputs, resulting in unstable Gaussian initialization and\nineffective noise suppression. To address these challenges, we propose\nLL-Gaussian, a novel framework for 3D reconstruction and enhancement from\nlow-light sRGB images, enabling pseudo normal-light novel view synthesis. Our\nmethod introduces three key innovations: 1) an end-to-end Low-Light Gaussian\nInitialization Module (LLGIM) that leverages dense priors from learning-based\nMVS approach to generate high-quality initial point clouds; 2) a dual-branch\nGaussian decomposition model that disentangles intrinsic scene properties\n(reflectance and illumination) from transient interference, enabling stable and\ninterpretable optimization; 3) an unsupervised optimization strategy guided by\nboth physical constrains and diffusion prior to jointly steer decomposition and\nenhancement. Additionally, we contribute a challenging dataset collected in\nextreme low-light environments and demonstrate the effectiveness of\nLL-Gaussian. Compared to state-of-the-art NeRF-based methods, LL-Gaussian\nachieves up to 2,000 times faster inference and reduces training time to just\n2%, while delivering superior reconstruction and rendering quality.\n","authors":["Hao Sun","Fenggen Yu","Huiyao Xu","Tao Zhang","Changqing Zou"],"pdf_url":"https://arxiv.org/pdf/2504.10331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17636v3","updated":"2025-04-14T15:39:19Z","published":"2025-01-29T13:12:06Z","title":"HOMER: Homography-Based Efficient Multi-view 3D Object Removal","summary":"  3D object removal is an important sub-task in 3D scene editing, with broad\napplications in scene understanding, augmented reality, and robotics. However,\nexisting methods struggle to achieve a desirable balance among consistency,\nusability, and computational efficiency in multi-view settings. These\nlimitations are primarily due to unintuitive user interaction in the source\nview, inefficient multi-view object mask generation, computationally expensive\ninpainting procedures, and a lack of applicability across different radiance\nfield representations. To address these challenges, we propose a novel pipeline\nthat improves the quality and efficiency of multi-view object mask generation\nand inpainting. Our method introduces an intuitive region-based interaction\nmechanism in the source view and eliminates the need for camera poses or extra\nmodel training. Our lightweight HoMM module is employed to achieve high-quality\nmulti-view mask propagation with enhanced efficiency. In the inpainting stage,\nwe further reduce computational costs by performing inpainting only on selected\nkey views and propagating the results to other views via homography-based\nmapping. Our pipeline is compatible with a variety of radiance field\nframeworks, including NeRF and 3D Gaussian Splatting, demonstrating improved\ngeneralizability and practicality in real-world scenarios. Additionally, we\npresent a new 3D multi-object removal dataset with greater object diversity and\nviewpoint variation than existing datasets. Experiments on public benchmarks\nand our proposed dataset show that our method achieves state-of-the-art\nperformance while reducing runtime to one-fifth of that required by leading\nbaselines.\n","authors":["Jingcheng Ni","Weiguang Zhao","Daniel Wang","Ziyao Zeng","Chenyu You","Alex Wong","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2501.17636v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10329v1","updated":"2025-04-14T15:36:28Z","published":"2025-04-14T15:36:28Z","title":"InstructEngine: Instruction-driven Text-to-Image Alignment","summary":"  Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF) has been\nextensively utilized for preference alignment of text-to-image models. Existing\nmethods face certain limitations in terms of both data and algorithm. For\ntraining data, most approaches rely on manual annotated preference data, either\nby directly fine-tuning the generators or by training reward models to provide\ntraining signals. However, the high annotation cost makes them difficult to\nscale up, the reward model consumes extra computation and cannot guarantee\naccuracy. From an algorithmic perspective, most methods neglect the value of\ntext and only take the image feedback as a comparative signal, which is\ninefficient and sparse. To alleviate these drawbacks, we propose the\nInstructEngine framework. Regarding annotation cost, we first construct a\ntaxonomy for text-to-image generation, then develop an automated data\nconstruction pipeline based on it. Leveraging advanced large multimodal models\nand human-defined rules, we generate 25K text-image preference pairs. Finally,\nwe introduce cross-validation alignment method, which refines data efficiency\nby organizing semantically analogous samples into mutually comparable pairs.\nEvaluations on DrawBench demonstrate that InstructEngine improves SD v1.5 and\nSDXL's performance by 10.53% and 5.30%, outperforming state-of-the-art\nbaselines, with ablation study confirming the benefits of InstructEngine's all\ncomponents. A win rate of over 50% in human reviews also proves that\nInstructEngine better aligns with human preferences.\n","authors":["Xingyu Lu","Yuhang Hu","YiFan Zhang","Kaiyu Jiang","Changyi Liu","Tianke Zhang","Jinpeng Wang","Bin Wen","Chun Yuan","Fan Yang","Tingting Gao","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.10329v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.10320v1","updated":"2025-04-14T15:30:03Z","published":"2025-04-14T15:30:03Z","title":"SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and\n  RAG-Enhanced Vision-Language Model","summary":"  Video anomaly detection (VAD) aims to identify unexpected events in videos\nand has wide applications in safety-critical domains. While semi-supervised\nmethods trained on only normal samples have gained traction, they often suffer\nfrom high false alarm rates and poor interpretability. Recently,\nvision-language models (VLMs) have demonstrated strong multimodal reasoning\ncapabilities, offering new opportunities for explainable anomaly detection.\nHowever, their high computational cost and lack of domain adaptation hinder\nreal-time deployment and reliability. Inspired by dual complementary pathways\nin human visual perception, we propose SlowFastVAD, a hybrid framework that\nintegrates a fast anomaly detector with a slow anomaly detector (namely a\nretrieval augmented generation (RAG) enhanced VLM), to address these\nlimitations. Specifically, the fast detector first provides coarse anomaly\nconfidence scores, and only a small subset of ambiguous segments, rather than\nthe entire video, is further analyzed by the slower yet more interpretable VLM\nfor elaborate detection and reasoning. Furthermore, to adapt VLMs to\ndomain-specific VAD scenarios, we construct a knowledge base including normal\npatterns based on few normal samples and abnormal patterns inferred by VLMs.\nDuring inference, relevant patterns are retrieved and used to augment prompts\nfor anomaly reasoning. Finally, we smoothly fuse the anomaly confidence of fast\nand slow detectors to enhance robustness of anomaly detection. Extensive\nexperiments on four benchmarks demonstrate that SlowFastVAD effectively\ncombines the strengths of both fast and slow detectors, and achieves remarkable\ndetection accuracy and interpretability with significantly reduced\ncomputational overhead, making it well-suited for real-world VAD applications\nwith high reliability requirements.\n","authors":["Zongcan Ding","Haodong Zhang","Peng Wu","Guansong Pang","Zhiwei Yang","Peng Wang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.10320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10317v1","updated":"2025-04-14T15:25:37Z","published":"2025-04-14T15:25:37Z","title":"Analysis of Attention in Video Diffusion Transformers","summary":"  We conduct an in-depth analysis of attention in video diffusion transformers\n(VDiTs) and report a number of novel findings. We identify three key properties\nof attention in VDiTs: Structure, Sparsity, and Sinks. Structure: We observe\nthat attention patterns across different VDiTs exhibit similar structure across\ndifferent prompts, and that we can make use of the similarity of attention\npatterns to unlock video editing via self-attention map transfer. Sparse: We\nstudy attention sparsity in VDiTs, finding that proposed sparsity methods do\nnot work for all VDiTs, because some layers that are seemingly sparse cannot be\nsparsified. Sinks: We make the first study of attention sinks in VDiTs,\ncomparing and contrasting them to attention sinks in language models. We\npropose a number of future directions that can make use of our insights to\nimprove the efficiency-quality Pareto frontier for VDiTs.\n","authors":["Yuxin Wen","Jim Wu","Ajay Jain","Tom Goldstein","Ashwinee Panda"],"pdf_url":"https://arxiv.org/pdf/2504.10317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10316v1","updated":"2025-04-14T15:25:21Z","published":"2025-04-14T15:25:21Z","title":"ESCT3D: Efficient and Selectively Controllable Text-Driven 3D Content\n  Generation with Gaussian Splatting","summary":"  In recent years, significant advancements have been made in text-driven 3D\ncontent generation. However, several challenges remain. In practical\napplications, users often provide extremely simple text inputs while expecting\nhigh-quality 3D content. Generating optimal results from such minimal text is a\ndifficult task due to the strong dependency of text-to-3D models on the quality\nof input prompts. Moreover, the generation process exhibits high variability,\nmaking it difficult to control. Consequently, multiple iterations are typically\nrequired to produce content that meets user expectations, reducing generation\nefficiency. To address this issue, we propose GPT-4V for self-optimization,\nwhich significantly enhances the efficiency of generating satisfactory content\nin a single attempt. Furthermore, the controllability of text-to-3D generation\nmethods has not been fully explored. Our approach enables users to not only\nprovide textual descriptions but also specify additional conditions, such as\nstyle, edges, scribbles, poses, or combinations of multiple conditions,\nallowing for more precise control over the generated 3D content. Additionally,\nduring training, we effectively integrate multi-view information, including\nmulti-view depth, masks, features, and images, to address the common Janus\nproblem in 3D content generation. Extensive experiments demonstrate that our\nmethod achieves robust generalization, facilitating the efficient and\ncontrollable generation of high-quality 3D content.\n","authors":["Huiqi Wu","Jianbo Mei","Yingjie Huang","Yining Xu","Jingjiao You","Yilong Liu","Li Yao"],"pdf_url":"https://arxiv.org/pdf/2504.10316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07615v2","updated":"2025-04-14T15:15:54Z","published":"2025-04-10T10:05:15Z","title":"VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model","summary":"  Recently DeepSeek R1 has shown that reinforcement learning (RL) can\nsubstantially improve the reasoning capabilities of Large Language Models\n(LLMs) through a simple yet effective design. The core of R1 lies in its\nrule-based reward formulation, which leverages tasks with deterministic\nground-truth answers to enable precise and stable reward computation. In the\nvisual domain, we similarly observe that a wide range of visual understanding\ntasks are inherently equipped with well-defined ground-truth annotations. This\nproperty makes them naturally compatible with rule-based reward mechanisms.\nMotivated by this observation, we investigate the extension of R1-style\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\ndedicated framework designed to harness RL for improving VLMs' performance on\ngeneral vision-language tasks. Using this framework, we further explore the\nfeasibility of applying RL to visual domain. Experimental results indicate that\nthe RL-based model not only delivers competitive performance on visual\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\nthat uncover a series of noteworthy insights, including the presence of reward\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\nof training data quality, and the scaling behavior of RL across different model\nsizes. Through these analyses, we aim to deepen the understanding of how\nreinforcement learning enhances the capabilities of vision-language models, and\nwe hope our findings and open-source contributions will support continued\nprogress in the vision-language RL community. Our code and model are available\nat https://github.com/om-ai-lab/VLM-R1\n","authors":["Haozhan Shen","Peng Liu","Jingcheng Li","Chunxin Fang","Yibo Ma","Jiajia Liao","Qiaoli Shen","Zilun Zhang","Kangjia Zhao","Qianqian Zhang","Ruochen Xu","Tiancheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.07615v2.pdf","comment":"11 pages, fix some minor typos in the previous version"},{"id":"http://arxiv.org/abs/2504.10288v1","updated":"2025-04-14T14:58:19Z","published":"2025-04-14T14:58:19Z","title":"Noise2Ghost: Self-supervised deep convolutional reconstruction for ghost\n  imaging","summary":"  We present a new self-supervised deep-learning-based Ghost Imaging (GI)\nreconstruction method, which provides unparalleled reconstruction performance\nfor noisy acquisitions among unsupervised methods. We present the supporting\nmathematical framework and results from theoretical and real data use cases.\nSelf-supervision removes the need for clean reference data while offering\nstrong noise reduction. This provides the necessary tools for addressing\nsignal-to-noise ratio concerns for GI acquisitions in emerging and cutting-edge\nlow-light GI scenarios. Notable examples include micro- and nano-scale x-ray\nemission imaging, e.g., x-ray fluorescence imaging of dose-sensitive samples.\nTheir applications include in-vivo and in-operando case studies for biological\nsamples and batteries.\n","authors":["Mathieu Manni","Dmitry Karpov","K. Joost Batenburg","Sharon Shwartz","Nicola Viganò"],"pdf_url":"https://arxiv.org/pdf/2504.10288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14819v3","updated":"2025-04-14T14:54:46Z","published":"2024-12-19T13:10:38Z","title":"Multi-Level Embedding and Alignment Network with Consistency and\n  Invariance Learning for Cross-View Geo-Localization","summary":"  Cross-View Geo-Localization (CVGL) involves determining the localization of\ndrone images by retrieving the most similar GPS-tagged satellite images.\nHowever, the imaging gaps between platforms are often significant and the\nvariations in viewpoints are substantial, which limits the ability of existing\nmethods to effectively associate cross-view features and extract consistent and\ninvariant characteristics. Moreover, existing methods often overlook the\nproblem of increased computational and storage requirements when improving\nmodel performance. To handle these limitations, we propose a lightweight\nenhanced alignment network, called the Multi-Level Embedding and Alignment\nNetwork (MEAN). The MEAN network uses a progressive multi-level enhancement\nstrategy, global-to-local associations, and cross-domain alignment, enabling\nfeature communication across levels. This allows MEAN to effectively connect\nfeatures at different levels and learn robust cross-view consistent mappings\nand modality-invariant features. Moreover, MEAN adopts a shallow backbone\nnetwork combined with a lightweight branch design, effectively reducing\nparameter count and computational complexity. Experimental results on the\nUniversity-1652 and SUES-200 datasets demonstrate that MEAN reduces parameter\ncount by 62.17% and computational complexity by 70.99% compared to\nstate-of-the-art models, while maintaining competitive or even superior\nperformance. Our code and models will be released on\nhttps://github.com/ISChenawei/MEAN.\n","authors":["Zhongwei Chen","Zhao-Xu Yang","Hai-Jun Rong"],"pdf_url":"https://arxiv.org/pdf/2412.14819v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10281v1","updated":"2025-04-14T14:49:45Z","published":"2025-04-14T14:49:45Z","title":"Zero-shot Autonomous Microscopy for Scalable and Intelligent\n  Characterization of 2D Materials","summary":"  Characterization of atomic-scale materials traditionally requires human\nexperts with months to years of specialized training. Even for trained human\noperators, accurate and reliable characterization remains challenging when\nexamining newly discovered materials such as two-dimensional (2D) structures.\nThis bottleneck drives demand for fully autonomous experimentation systems\ncapable of comprehending research objectives without requiring large training\ndatasets. In this work, we present ATOMIC (Autonomous Technology for Optical\nMicroscopy & Intelligent Characterization), an end-to-end framework that\nintegrates foundation models to enable fully autonomous, zero-shot\ncharacterization of 2D materials. Our system integrates the vision foundation\nmodel (i.e., Segment Anything Model), large language models (i.e., ChatGPT),\nunsupervised clustering, and topological analysis to automate microscope\ncontrol, sample scanning, image segmentation, and intelligent analysis through\nprompt engineering, eliminating the need for additional training. When\nanalyzing typical MoS2 samples, our approach achieves 99.7% segmentation\naccuracy for single layer identification, which is equivalent to that of human\nexperts. In addition, the integrated model is able to detect grain boundary\nslits that are challenging to identify with human eyes. Furthermore, the system\nretains robust accuracy despite variable conditions including defocus, color\ntemperature fluctuations, and exposure variations. It is applicable to a broad\nspectrum of common 2D materials-including graphene, MoS2, WSe2, SnSe-regardless\nof whether they were fabricated via chemical vapor deposition or mechanical\nexfoliation. This work represents the implementation of foundation models to\nachieve autonomous analysis, establishing a scalable and data-efficient\ncharacterization paradigm that fundamentally transforms the approach to\nnanoscale materials research.\n","authors":["Jingyun Yang","Ruoyan Avery Yin","Chi Jiang","Yuepeng Hu","Xiaokai Zhu","Xingjian Hu","Sutharsika Kumar","Xiao Wang","Xiaohua Zhai","Keran Rong","Yunyue Zhu","Tianyi Zhang","Zongyou Yin","Jing Kong","Neil Zhenqiang Gong","Zhichu Ren","Haozhe Wang"],"pdf_url":"https://arxiv.org/pdf/2504.10281v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.07029v2","updated":"2025-04-14T14:47:35Z","published":"2025-04-09T16:44:19Z","title":"Distilling Textual Priors from LLM to Efficient Image Fusion","summary":"  Multi-modality image fusion aims to synthesize a single, comprehensive image\nfrom multiple source inputs. Traditional approaches, such as CNNs and GANs,\noffer efficiency but struggle to handle low-quality or complex inputs. Recent\nadvances in text-guided methods leverage large model priors to overcome these\nlimitations, but at the cost of significant computational overhead, both in\nmemory and inference time. To address this challenge, we propose a novel\nframework for distilling large model priors, eliminating the need for text\nguidance during inference while dramatically reducing model size. Our framework\nutilizes a teacher-student architecture, where the teacher network incorporates\nlarge model priors and transfers this knowledge to a smaller student network\nvia a tailored distillation process. Additionally, we introduce spatial-channel\ncross-fusion module to enhance the model's ability to leverage textual priors\nacross both spatial and channel dimensions. Our method achieves a favorable\ntrade-off between computational efficiency and fusion quality. The distilled\nnetwork, requiring only 10% of the parameters and inference time of the teacher\nnetwork, retains 90% of its performance and outperforms existing SOTA methods.\nExtensive experiments demonstrate the effectiveness of our approach. The\nimplementation will be made publicly available as an open-source resource.\n","authors":["Ran Zhang","Xuanhua He","Ke Cao","Liu Liu","Li Zhang","Man Zhou","Jie Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.07029v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10278v1","updated":"2025-04-14T14:44:52Z","published":"2025-04-14T14:44:52Z","title":"DiffMOD: Progressive Diffusion Point Denoising for Moving Object\n  Detection in Remote Sensing","summary":"  Moving object detection (MOD) in remote sensing is significantly challenged\nby low resolution, extremely small object sizes, and complex noise\ninterference. Current deep learning-based MOD methods rely on probability\ndensity estimation, which restricts flexible information interaction between\nobjects and across temporal frames. To flexibly capture high-order inter-object\nand temporal relationships, we propose a point-based MOD in remote sensing.\nInspired by diffusion models, the network optimization is formulated as a\nprogressive denoising process that iteratively recovers moving object centers\nfrom sparse noisy points. Specifically, we sample scattered features from the\nbackbone outputs as atomic units for subsequent processing, while global\nfeature embeddings are aggregated to compensate for the limited coverage of\nsparse point features. By modeling spatial relative positions and semantic\naffinities, Spatial Relation Aggregation Attention is designed to enable\nhigh-order interactions among point-level features for enhanced object\nrepresentation. To enhance temporal consistency, the Temporal Propagation and\nGlobal Fusion module is designed, which leverages an implicit memory reasoning\nmechanism for robust cross-frame feature integration. To align with the\nprogressive denoising process, we propose a progressive MinK optimal transport\nassignment strategy that establishes specialized learning objectives at each\ndenoising level. Additionally, we introduce a missing loss function to\ncounteract the clustering tendency of denoised points around salient objects.\nExperiments on the RsData remote sensing MOD dataset show that our MOD method\nbased on scattered point denoising can more effectively explore potential\nrelationships between sparse moving objects and improve the detection\ncapability and temporal consistency.\n","authors":["Jinyue Zhang","Xiangrong Zhang","Zhongjian Huang","Tianyang Zhang","Yifei Jiang","Licheng Jiao"],"pdf_url":"https://arxiv.org/pdf/2504.10278v1.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.10275v1","updated":"2025-04-14T14:43:46Z","published":"2025-04-14T14:43:46Z","title":"LMFormer: Lane based Motion Prediction Transformer","summary":"  Motion prediction plays an important role in autonomous driving. This study\npresents LMFormer, a lane-aware transformer network for trajectory prediction\ntasks. In contrast to previous studies, our work provides a simple mechanism to\ndynamically prioritize the lanes and shows that such a mechanism introduces\nexplainability into the learning behavior of the network. Additionally,\nLMFormer uses the lane connection information at intersections, lane merges,\nand lane splits, in order to learn long-range dependency in lane structure.\nMoreover, we also address the issue of refining the predicted trajectories and\npropose an efficient method for iterative refinement through stacked\ntransformer layers. For benchmarking, we evaluate LMFormer on the nuScenes\ndataset and demonstrate that it achieves SOTA performance across multiple\nmetrics. Furthermore, the Deep Scenario dataset is used to not only illustrate\ncross-dataset network performance but also the unification capabilities of\nLMFormer to train on multiple datasets and achieve better performance.\n","authors":["Harsh Yadav","Maximilian Schaefer","Kun Zhao","Tobias Meisen"],"pdf_url":"https://arxiv.org/pdf/2504.10275v1.pdf","comment":"Accepted: Autonomous Driving Workshop, CVPR 2025"},{"id":"http://arxiv.org/abs/2412.13183v2","updated":"2025-04-14T14:38:34Z","published":"2024-12-17T18:57:38Z","title":"Real-time Free-view Human Rendering from Sparse-view RGB Videos using\n  Double Unprojected Textures","summary":"  Real-time free-view human rendering from sparse-view RGB inputs is a\nchallenging task due to the sensor scarcity and the tight time budget. To\nensure efficiency, recent methods leverage 2D CNNs operating in texture space\nto learn rendering primitives. However, they either jointly learn geometry and\nappearance, or completely ignore sparse image information for geometry\nestimation, significantly harming visual quality and robustness to unseen body\nposes. To address these issues, we present Double Unprojected Textures, which\nat the core disentangles coarse geometric deformation estimation from\nappearance synthesis, enabling robust and photorealistic 4K rendering in\nreal-time. Specifically, we first introduce a novel image-conditioned template\ndeformation network, which estimates the coarse deformation of the human\ntemplate from a first unprojected texture. This updated geometry is then used\nto apply a second and more accurate texture unprojection. The resulting texture\nmap has fewer artifacts and better alignment with input views, which benefits\nour learning of finer-level geometry and appearance represented by Gaussian\nsplats. We validate the effectiveness and efficiency of the proposed method in\nquantitative and qualitative experiments, which significantly surpasses other\nstate-of-the-art methods. Project page:\nhttps://vcai.mpi-inf.mpg.de/projects/DUT/\n","authors":["Guoxing Sun","Rishabh Dabral","Heming Zhu","Pascal Fua","Christian Theobalt","Marc Habermann"],"pdf_url":"https://arxiv.org/pdf/2412.13183v2.pdf","comment":"Accepted at CVPR 2025, Project page:\n  https://vcai.mpi-inf.mpg.de/projects/DUT/"},{"id":"http://arxiv.org/abs/2504.10267v1","updated":"2025-04-14T14:29:38Z","published":"2025-04-14T14:29:38Z","title":"Trade-offs in Privacy-Preserving Eye Tracking through Iris Obfuscation:\n  A Benchmarking Study","summary":"  Recent developments in hardware, computer graphics, and AI may soon enable\nAR/VR head-mounted displays (HMDs) to become everyday devices like smartphones\nand tablets. Eye trackers within HMDs provide a special opportunity for such\nsetups as it is possible to facilitate gaze-based research and interaction.\nHowever, estimating users' gaze information often requires raw eye images and\nvideos that contain iris textures, which are considered a gold standard\nbiometric for user authentication, and this raises privacy concerns. Previous\nresearch in the eye-tracking community focused on obfuscating iris textures\nwhile keeping utility tasks such as gaze estimation accurate. Despite these\nattempts, there is no comprehensive benchmark that evaluates state-of-the-art\napproaches. Considering all, in this paper, we benchmark blurring, noising,\ndownsampling, rubber sheet model, and iris style transfer to obfuscate user\nidentity, and compare their impact on image quality, privacy, utility, and risk\nof imposter attack on two datasets. We use eye segmentation and gaze estimation\nas utility tasks, and reduction in iris recognition accuracy as a measure of\nprivacy protection, and false acceptance rate to estimate risk of attack. Our\nexperiments show that canonical image processing methods like blurring and\nnoising cause a marginal impact on deep learning-based tasks. While\ndownsampling, rubber sheet model, and iris style transfer are effective in\nhiding user identifiers, iris style transfer, with higher computation cost,\noutperforms others in both utility tasks, and is more resilient against spoof\nattacks. Our analyses indicate that there is no universal optimal approach to\nbalance privacy, utility, and computation burden. Therefore, we recommend\npractitioners consider the strengths and weaknesses of each approach, and\npossible combinations of those to reach an optimal privacy-utility trade-off.\n","authors":["Mengdi Wang","Efe Bozkir","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2504.10267v1.pdf","comment":"The 25th International Conference on Digital Signal Processing (DSP\n  2025)"},{"id":"http://arxiv.org/abs/2503.04707v2","updated":"2025-04-14T14:29:04Z","published":"2025-03-06T18:55:21Z","title":"Iris Style Transfer: Enhancing Iris Recognition with Style Features and\n  Privacy Preservation through Neural Style Transfer","summary":"  Iris texture is widely regarded as a gold standard biometric modality for\nauthentication and identification. The demand for robust iris recognition\nmethods, coupled with growing security and privacy concerns regarding iris\nattacks, has escalated recently. Inspired by neural style transfer, an advanced\ntechnique that leverages neural networks to separate content and style\nfeatures, we hypothesize that iris texture's style features provide a reliable\nfoundation for recognition and are more resilient to variations like rotation\nand perspective shifts than traditional approaches. Our experimental results\nsupport this hypothesis, showing a significantly higher classification accuracy\ncompared to conventional features. Further, we propose using neural style\ntransfer to obfuscate the identifiable iris style features, ensuring the\nprotection of sensitive biometric information while maintaining the utility of\neye images for tasks like eye segmentation and gaze estimation. This work opens\nnew avenues for iris-oriented, secure, and privacy-aware biometric systems.\n","authors":["Mengdi Wang","Efe Bozkir","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2503.04707v2.pdf","comment":"The 2025 ACM Symposium on Eye Tracking Research & Applications (ETRA)"},{"id":"http://arxiv.org/abs/2504.10258v1","updated":"2025-04-14T14:19:57Z","published":"2025-04-14T14:19:57Z","title":"XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a\n  Novel Benchmark","summary":"  Document Reading Order Recovery is a fundamental task in document image\nunderstanding, playing a pivotal role in enhancing Retrieval-Augmented\nGeneration (RAG) and serving as a critical preprocessing step for large\nlanguage models (LLMs). Existing methods often struggle with complex\nlayouts(e.g., multi-column newspapers), high-overhead interactions between\ncross-modal elements (visual regions and textual semantics), and a lack of\nrobust evaluation benchmarks. We introduce XY-Cut++, an advanced layout\nordering method that integrates pre-mask processing, multi-granularity\nsegmentation, and cross-modal matching to address these challenges. Our method\nsignificantly enhances layout ordering accuracy compared to traditional XY-Cut\ntechniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8\nBLEU overall) while maintaining simplicity and efficiency. It outperforms\nexisting baselines by up to 24\\% and demonstrates consistent accuracy across\nsimple and complex layouts on the newly introduced DocBench-100 dataset. This\nadvancement establishes a reliable foundation for document structure recovery,\nsetting a new standard for layout ordering tasks and facilitating more\neffective RAG and LLM preprocessing.\n","authors":["Shuai Liu","Youmeng Li","Jizeng Wei"],"pdf_url":"https://arxiv.org/pdf/2504.10258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10254v1","updated":"2025-04-14T14:15:46Z","published":"2025-04-14T14:15:46Z","title":"MASSeg : 2nd Technical Report for 4th PVUW MOSE Track","summary":"  Complex video object segmentation continues to face significant challenges in\nsmall object recognition, occlusion handling, and dynamic scene modeling. This\nreport presents our solution, which ranked second in the MOSE track of CVPR\n2025 PVUW Challenge. Based on an existing segmentation framework, we propose an\nimproved model named MASSeg for complex video object segmentation, and\nconstruct an enhanced dataset, MOSE+, which includes typical scenarios with\nocclusions, cluttered backgrounds, and small target instances. During training,\nwe incorporate a combination of inter-frame consistent and inconsistent data\naugmentation strategies to improve robustness and generalization. During\ninference, we design a mask output scaling strategy to better adapt to varying\nobject sizes and occlusion levels. As a result, MASSeg achieves a J score of\n0.8250, F score of 0.9007, and a J&F score of 0.8628 on the MOSE test set.\n","authors":["Xuqiang Cao","Linnan Zhao","Jiaxuan Zhao","Fang Liu","Puhua Chen","Wenping Ma"],"pdf_url":"https://arxiv.org/pdf/2504.10254v1.pdf","comment":"5 pages,4 figures,Technical report on Complex Video Object\n  Segmentation"},{"id":"http://arxiv.org/abs/2504.10244v1","updated":"2025-04-14T14:08:26Z","published":"2025-04-14T14:08:26Z","title":"Towards contrast- and pathology-agnostic clinical fetal brain MRI\n  segmentation using SynthSeg","summary":"  Magnetic resonance imaging (MRI) has played a crucial role in fetal\nneurodevelopmental research. Structural annotations of MR images are an\nimportant step for quantitative analysis of the developing human brain, with\nDeep learning providing an automated alternative for this otherwise tedious\nmanual process. However, segmentation performances of Convolutional Neural\nNetworks often suffer from domain shift, where the network fails when applied\nto subjects that deviate from the distribution with which it is trained on. In\nthis work, we aim to train networks capable of automatically segmenting fetal\nbrain MRIs with a wide range of domain shifts pertaining to differences in\nsubject physiology and acquisition environments, in particular shape-based\ndifferences commonly observed in pathological cases. We introduce a novel\ndata-driven train-time sampling strategy that seeks to fully exploit the\ndiversity of a given training dataset to enhance the domain generalizability of\nthe trained networks. We adapted our sampler, together with other existing data\naugmentation techniques, to the SynthSeg framework, a generator that utilizes\ndomain randomization to generate diverse training data, and ran thorough\nexperimentations and ablation studies on a wide range of training/testing data\nto test the validity of the approaches. Our networks achieved notable\nimprovements in the segmentation quality on testing subjects with intense\nanatomical abnormalities (p < 1e-4), though at the cost of a slighter decrease\nin performance in cases with fewer abnormalities. Our work also lays the\nfoundation for future works on creating and adapting data-driven sampling\nstrategies for other training pipelines.\n","authors":["Ziyao Shang","Misha Kaandorp","Kelly Payette","Marina Fernandez Garcia","Roxane Licandro","Georg Langs","Jordina Aviles Verdera","Jana Hutter","Bjoern Menze","Gregor Kasprian","Meritxell Bach Cuadra","Andras Jakab"],"pdf_url":"https://arxiv.org/pdf/2504.10244v1.pdf","comment":"21 pages, 16 figures"},{"id":"http://arxiv.org/abs/2504.10242v1","updated":"2025-04-14T14:04:55Z","published":"2025-04-14T14:04:55Z","title":"CAT: A Conditional Adaptation Tailor for Efficient and Effective\n  Instance-Specific Pansharpening on Real-World Data","summary":"  Pansharpening is a crucial remote sensing technique that fuses low-resolution\nmultispectral (LRMS) images with high-resolution panchromatic (PAN) images to\ngenerate high-resolution multispectral (HRMS) imagery. Although deep learning\ntechniques have significantly advanced pansharpening, many existing methods\nsuffer from limited cross-sensor generalization and high computational\noverhead, restricting their real-time applications. To address these\nchallenges, we propose an efficient framework that quickly adapts to a specific\ninput instance, completing both training and inference in a short time. Our\nframework splits the input image into multiple patches, selects a subset for\nunsupervised CAT training, and then performs inference on all patches,\nstitching them into the final output. The CAT module, integrated between the\nfeature extraction and channel transformation stages of a pre-trained network,\ntailors the fused features and fixes the parameters for efficient inference,\ngenerating improved results. Our approach offers two key advantages: (1)\n$\\textit{Improved Generalization Ability}$: by mitigating cross-sensor\ndegradation, our model--although pre-trained on a specific dataset--achieves\nsuperior performance on datasets captured by other sensors; (2)\n$\\textit{Enhanced Computational Efficiency}$: the CAT-enhanced network can\nswiftly adapt to the test sample using the single LRMS-PAN pair input, without\nrequiring extensive large-scale data retraining. Experiments on the real-world\ndata from WorldView-3 and WorldView-2 datasets demonstrate that our method\nachieves state-of-the-art performance on cross-sensor real-world data, while\nachieving both training and inference of $512\\times512$ image within\n$\\textit{0.4 seconds}$ and $4000\\times4000$ image within $\\textit{3 seconds}$\nat the fastest setting on a commonly used RTX 3090 GPU.\n","authors":["Tianyu Xin","Jin-Liang Xiao","Zeyu Xia","Shan Yin","Liang-Jian Deng"],"pdf_url":"https://arxiv.org/pdf/2504.10242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13984v2","updated":"2025-04-14T14:00:32Z","published":"2024-04-22T08:44:34Z","title":"RHanDS: Refining Malformed Hands for Generated Images with Decoupled\n  Structure and Style Guidance","summary":"  Although diffusion models can generate high-quality human images, their\napplications are limited by the instability in generating hands with correct\nstructures. In this paper, we introduce RHanDS, a conditional diffusion-based\nframework designed to refine malformed hands by utilizing decoupled structure\nand style guidance. The hand mesh reconstructed from the malformed hand offers\nstructure guidance for correcting the structure of the hand, while the\nmalformed hand itself provides style guidance for preserving the style of the\nhand. To alleviate the mutual interference between style and structure\nguidance, we introduce a two-stage training strategy and build a series of\nmulti-style hand datasets. In the first stage, we use paired hand images for\ntraining to ensure stylistic consistency in hand refining. In the second stage,\nvarious hand images generated based on human meshes are used for training,\nenabling the model to gain control over the hand structure. Experimental\nresults demonstrate that RHanDS can effectively refine hand structure while\npreserving consistency in hand style.\n","authors":["Chengrui Wang","Pengfei Liu","Min Zhou","Ming Zeng","Xubin Li","Tiezheng Ge","Bo zheng"],"pdf_url":"https://arxiv.org/pdf/2404.13984v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00246v2","updated":"2025-04-14T13:51:08Z","published":"2024-10-31T22:51:45Z","title":"ResiDual Transformer Alignment with Spectral Decomposition","summary":"  When examined through the lens of their residual streams, a puzzling property\nemerges in transformer networks: residual contributions (e.g., attention heads)\nsometimes specialize in specific tasks or input attributes. In this paper, we\nanalyze this phenomenon in vision transformers, focusing on the spectral\ngeometry of residuals, and explore its implications for modality alignment in\nvision-language models. First, we link it to the intrinsically low-dimensional\nstructure of visual head representations, zooming into their principal\ncomponents and showing that they encode specialized roles across a wide variety\nof input data distributions. Then, we analyze the effect of head specialization\nin multimodal models, focusing on how improved alignment between text and\nspecialized heads impacts zero-shot classification performance. This\nspecialization-performance link consistently holds across diverse pre-training\ndata, network sizes, and objectives, demonstrating a powerful new mechanism for\nboosting zero-shot classification through targeted alignment. Ultimately, we\ntranslate these insights into actionable terms by introducing ResiDual, a\ntechnique for spectral alignment of the residual stream. Much like panning for\ngold, it lets the noise from irrelevant unit principal components (i.e.,\nattributes) wash away to amplify task-relevant ones. Remarkably, this dual\nperspective on modality alignment yields fine-tuning level performance on\ndifferent data distributions while modelling an extremely interpretable and\nparameter-efficient transformation, as we extensively show on 70 pre-trained\nnetwork-dataset combinations (7 models, 10 datasets).\n","authors":["Lorenzo Basile","Valentino Maiorca","Luca Bortolussi","Emanuele Rodolà","Francesco Locatello"],"pdf_url":"https://arxiv.org/pdf/2411.00246v2.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2504.10214v1","updated":"2025-04-14T13:31:35Z","published":"2025-04-14T13:31:35Z","title":"Balancing Stability and Plasticity in Pretrained Detector: A Dual-Path\n  Framework for Incremental Object Detection","summary":"  The balance between stability and plasticity remains a fundamental challenge\nin pretrained model-based incremental object detection (PTMIOD). While existing\nPTMIOD methods demonstrate strong performance on in-domain tasks aligned with\npretraining data, their plasticity to cross-domain scenarios remains\nunderexplored. Through systematic component-wise analysis of pretrained\ndetectors, we reveal a fundamental discrepancy: the localization modules\ndemonstrate inherent cross-domain stability-preserving precise bounding box\nestimation across distribution shifts-while the classification components\nrequire enhanced plasticity to mitigate discriminability degradation in\ncross-domain scenarios. Motivated by these findings, we propose a dual-path\nframework built upon pretrained DETR-based detectors which decouples\nlocalization stability and classification plasticity: the localization path\nmaintains stability to preserve pretrained localization knowledge, while the\nclassification path facilitates plasticity via parameter-efficient fine-tuning\nand resists forgetting with pseudo-feature replay. Extensive evaluations on\nboth in-domain (MS COCO and PASCAL VOC) and cross-domain (TT100K) benchmarks\nshow state-of-the-art performance, demonstrating our method's ability to\neffectively balance stability and plasticity in PTMIOD, achieving robust\ncross-domain adaptation and strong retention of anti-forgetting capabilities.\n","authors":["Songze Li","Qixing Xu","Tonghua Su","Xu-Yao Zhang","Zhongjie Wang"],"pdf_url":"https://arxiv.org/pdf/2504.10214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00664v2","updated":"2025-04-14T13:25:03Z","published":"2024-12-01T03:57:21Z","title":"Improving Decoupled Posterior Sampling for Inverse Problems using Data\n  Consistency Constraint","summary":"  Diffusion models have shown strong performances in solving inverse problems\nthrough posterior sampling while they suffer from errors during earlier steps.\nTo mitigate this issue, several Decoupled Posterior Sampling methods have been\nrecently proposed. However, the reverse process in these methods ignores\nmeasurement information, leading to errors that impede effective optimization\nin subsequent steps. To solve this problem, we propose Guided Decoupled\nPosterior Sampling (GDPS) by integrating a data consistency constraint in the\nreverse process. The constraint performs a smoother transition within the\noptimization process, facilitating a more effective convergence toward the\ntarget distribution. Furthermore, we extend our method to latent diffusion\nmodels and Tweedie's formula, demonstrating its scalability. We evaluate GDPS\non the FFHQ and ImageNet datasets across various linear and nonlinear tasks\nunder both standard and challenging conditions. Experimental results\ndemonstrate that GDPS achieves state-of-the-art performance, improving accuracy\nover existing methods.\n","authors":["Zhi Qi","Shihong Yuan","Yulin Yuan","Linling Kuang","Yoshiyuki Kabashima","Xiangming Meng"],"pdf_url":"https://arxiv.org/pdf/2412.00664v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.06521v2","updated":"2025-04-14T13:22:13Z","published":"2025-04-09T01:40:38Z","title":"DUKAE: DUal-level Knowledge Accumulation and Ensemble for Pre-Trained\n  Model-Based Continual Learning","summary":"  Pre-trained model-based continual learning (PTMCL) has garnered growing\nattention, as it enables more rapid acquisition of new knowledge by leveraging\nthe extensive foundational understanding inherent in pre-trained model (PTM).\nMost existing PTMCL methods use Parameter-Efficient Fine-Tuning (PEFT) to learn\nnew knowledge while consolidating existing memory. However, they often face\nsome challenges. A major challenge lies in the misalignment of classification\nheads, as the classification head of each task is trained within a distinct\nfeature space, leading to inconsistent decision boundaries across tasks and,\nconsequently, increased forgetting. Another critical limitation stems from the\nrestricted feature-level knowledge accumulation, with feature learning\ntypically restricted to the initial task only, which constrains the model's\nrepresentation capabilities. To address these issues, we propose a method named\nDUal-level Knowledge Accumulation and Ensemble (DUKAE) that leverages both\nfeature-level and decision-level knowledge accumulation by aligning\nclassification heads into a unified feature space through Gaussian distribution\nsampling and introducing an adaptive expertise ensemble to fuse knowledge\nacross feature subspaces. Extensive experiments on CIFAR-100, ImageNet-R,\nCUB-200, and Cars-196 datasets demonstrate the superior performance of our\napproach.\n","authors":["Songze Li","Tonghua Su","Xu-Yao Zhang","Qixing Xu","Zhongjie Wang"],"pdf_url":"https://arxiv.org/pdf/2504.06521v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10201v1","updated":"2025-04-14T13:09:49Z","published":"2025-04-14T13:09:49Z","title":"VibrantLeaves: A principled parametric image generator for training deep\n  restoration models","summary":"  Even though Deep Neural Networks are extremely powerful for image restoration\ntasks, they have several limitations. They are poorly understood and suffer\nfrom strong biases inherited from the training sets. One way to address these\nshortcomings is to have a better control over the training sets, in particular\nby using synthetic sets. In this paper, we propose a synthetic image generator\nrelying on a few simple principles. In particular, we focus on geometric\nmodeling, textures, and a simple modeling of image acquisition. These\nproperties, integrated in a classical Dead Leaves model, enable the creation of\nefficient training sets. Standard image denoising and super-resolution networks\ncan be trained on such datasets, reaching performance almost on par with\ntraining on natural image datasets. As a first step towards explainability, we\nprovide a careful analysis of the considered principles, identifying which\nimage properties are necessary to obtain good performances. Besides, such\ntraining also yields better robustness to various geometric and radiometric\nperturbations of the test sets.\n","authors":["Raphael Achddou","Yann Gousseau","Saïd Ladjal","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2504.10201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16148v2","updated":"2025-04-14T12:55:19Z","published":"2024-12-20T18:51:41Z","title":"Frequency Is What You Need: Word-frequency Masking Benefits\n  Vision-Language Model Pre-training","summary":"  Vision Language Models (VLMs) can be trained more efficiently if training\nsets can be reduced in size. Recent work has shown the benefits of masking text\nduring VLM training using a variety of approaches: truncation, random masking,\nblock masking and syntax masking. In this paper, we show that the best masking\nstrategy changes over training epochs and that, given sufficient training\nepochs. We analyze existing text masking approaches including syntax masking,\nwhich is currently the state of the art, and identify the word frequency\ndistribution as important in determining their success. Experiments on a large\nrange of data sets demonstrate that syntax masking is outperformed by other\napproaches, given sufficient epochs, and that our proposed frequency-based\napproach, called Contrastive Language-Image Pre-training with Word Frequency\nMasking (CLIPF) has numerous advantages. The benefits are particularly evident\nas the number of input tokens decreases.\n","authors":["Mingliang Liang","Martha Larson"],"pdf_url":"https://arxiv.org/pdf/2412.16148v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10190v1","updated":"2025-04-14T12:50:37Z","published":"2025-04-14T12:50:37Z","title":"Differentially Private 2D Human Pose Estimation","summary":"  Human pose estimation (HPE) has become essential in numerous applications\nincluding healthcare, activity recognition, and human-computer interaction.\nHowever, the privacy implications of processing sensitive visual data present\nsignificant deployment barriers in critical domains. While traditional\nanonymization techniques offer limited protection and often compromise data\nutility for broader motion analysis, Differential Privacy (DP) provides formal\nprivacy guarantees but typically degrades model performance when applied\nnaively. In this work, we present the first differentially private 2D human\npose estimation (2D-HPE) by applying Differentially Private Stochastic Gradient\nDescent (DP-SGD) to this task. To effectively balance privacy with performance,\nwe adopt Projected DP-SGD (PDP-SGD), which projects the noisy gradients to a\nlow-dimensional subspace. Additionally, we adapt TinyViT, a compact and\nefficient vision transformer for coordinate classification in HPE, providing a\nlightweight yet powerful backbone that enhances privacy-preserving deployment\nfeasibility on resource-limited devices. Our approach is particularly valuable\nfor multimedia interpretation tasks, enabling privacy-safe analysis and\nunderstanding of human motion across diverse visual media while preserving the\nsemantic meaning required for downstream applications. Comprehensive\nexperiments on the MPII Human Pose Dataset demonstrate significant performance\nenhancement with PDP-SGD achieving 78.48% PCKh@0.5 at a strict privacy budget\n($\\epsilon=0.2$), compared to 63.85% for standard DP-SGD. This work lays\nfoundation for privacy-preserving human pose estimation in real-world,\nsensitive applications.\n","authors":["Kaushik Bhargav Sivangi","Idris Zakariyya","Paul Henderson","Fani Deligianni"],"pdf_url":"https://arxiv.org/pdf/2504.10190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05089v2","updated":"2025-04-14T12:47:23Z","published":"2025-04-07T13:58:55Z","title":"Climplicit: Climatic Implicit Embeddings for Global Ecological Tasks","summary":"  Deep learning on climatic data holds potential for macroecological\napplications. However, its adoption remains limited among scientists outside\nthe deep learning community due to storage, compute, and technical expertise\nbarriers. To address this, we introduce Climplicit, a spatio-temporal\ngeolocation encoder pretrained to generate implicit climatic representations\nanywhere on Earth. By bypassing the need to download raw climatic rasters and\ntrain feature extractors, our model uses x3500 less disk space and\nsignificantly reduces computational needs for downstream tasks. We evaluate our\nClimplicit embeddings on biomes classification, species distribution modeling,\nand plant trait regression. We find that single-layer probing our Climplicit\nembeddings consistently performs better or on par with training a model from\nscratch on downstream tasks and overall better than alternative geolocation\nencoding models.\n","authors":["Johannes Dollinger","Damien Robert","Elena Plekhanova","Lukas Drees","Jan Dirk Wegner"],"pdf_url":"https://arxiv.org/pdf/2504.05089v2.pdf","comment":"Published as a workshop paper at \"Tackling Climate Change with\n  Machine Learning\", ICLR 2025"},{"id":"http://arxiv.org/abs/2504.10174v1","updated":"2025-04-14T12:26:31Z","published":"2025-04-14T12:26:31Z","title":"LLaVA-ReID: Selective Multi-image Questioner for Interactive Person\n  Re-Identification","summary":"  Traditional text-based person ReID assumes that person descriptions from\nwitnesses are complete and provided at once. However, in real-world scenarios,\nsuch descriptions are often partial or vague. To address this limitation, we\nintroduce a new task called interactive person re-identification (Inter-ReID).\nInter-ReID is a dialogue-based retrieval task that iteratively refines initial\ndescriptions through ongoing interactions with the witnesses. To facilitate the\nstudy of this new task, we construct a dialogue dataset that incorporates\nmultiple types of questions by decomposing fine-grained attributes of\nindividuals. We further propose LLaVA-ReID, a question model that generates\ntargeted questions based on visual and textual contexts to elicit additional\ndetails about the target person. Leveraging a looking-forward strategy, we\nprioritize the most informative questions as supervision during training.\nExperimental results on both Inter-ReID and text-based ReID benchmarks\ndemonstrate that LLaVA-ReID significantly outperforms baselines.\n","authors":["Yiding Lu","Mouxing Yang","Dezhong Peng","Peng Hu","Yijie Lin","Xi Peng"],"pdf_url":"https://arxiv.org/pdf/2504.10174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10165v1","updated":"2025-04-14T12:21:16Z","published":"2025-04-14T12:21:16Z","title":"WildLive: Near Real-time Visual Wildlife Tracking onboard UAVs","summary":"  Live tracking of wildlife via high-resolution video processing directly\nonboard drones is widely unexplored and most existing solutions rely on\nstreaming video to ground stations to support navigation. Yet, both autonomous\nanimal-reactive flight control beyond visual line of sight and/or\nmission-specific individual and behaviour recognition tasks rely to some degree\non this capability. In response, we introduce WildLive -- a near real-time\nanimal detection and tracking framework for high-resolution imagery running\ndirectly onboard uncrewed aerial vehicles (UAVs). The system performs\nmulti-animal detection and tracking at 17fps+ for HD and 7fps+ on 4K video\nstreams suitable for operation during higher altitude flights to minimise\nanimal disturbance. Our system is optimised for Jetson Orin AGX onboard\nhardware. It integrates the efficiency of sparse optical flow tracking and\nmission-specific sampling with device-optimised and proven YOLO-driven object\ndetection and segmentation techniques. Essentially, computational resource is\nfocused onto spatio-temporal regions of high uncertainty to significantly\nimprove UAV processing speeds without domain-specific loss of accuracy.\nAlongside, we introduce our WildLive dataset, which comprises 200k+ annotated\nanimal instances across 19k+ frames from 4K UAV videos collected at the Ol\nPejeta Conservancy in Kenya. All frames contain ground truth bounding boxes,\nsegmentation masks, as well as individual tracklets and tracking point\ntrajectories. We compare our system against current object tracking approaches\nincluding OC-SORT, ByteTrack, and SORT. Our multi-animal tracking experiments\nwith onboard hardware confirm that near real-time high-resolution wildlife\ntracking is possible on UAVs whilst maintaining high accuracy levels as needed\nfor future navigational and mission-specific animal-centric operational\nautonomy.\n","authors":["Nguyen Ngoc Dat","Tom Richardson","Matthew Watson","Kilian Meier","Jenna Kline","Sid Reid","Guy Maalouf","Duncan Hine","Majid Mirmehdi","Tilo Burghardt"],"pdf_url":"https://arxiv.org/pdf/2504.10165v1.pdf","comment":"Submitted in CV4Animals 2025"},{"id":"http://arxiv.org/abs/2504.05579v2","updated":"2025-04-14T12:17:03Z","published":"2025-04-08T00:28:42Z","title":"TAPNext: Tracking Any Point (TAP) as Next Token Prediction","summary":"  Tracking Any Point (TAP) in a video is a challenging computer vision problem\nwith many demonstrated applications in robotics, video editing, and 3D\nreconstruction. Existing methods for TAP rely heavily on complex\ntracking-specific inductive biases and heuristics, limiting their generality\nand potential for scaling. To address these challenges, we present TAPNext, a\nnew approach that casts TAP as sequential masked token decoding. Our model is\ncausal, tracks in a purely online fashion, and removes tracking-specific\ninductive biases. This enables TAPNext to run with minimal latency, and removes\nthe temporal windowing required by many existing state of art trackers. Despite\nits simplicity, TAPNext achieves a new state-of-the-art tracking performance\namong both online and offline trackers. Finally, we present evidence that many\nwidely used tracking heuristics emerge naturally in TAPNext through end-to-end\ntraining. The TAPNext model and code can be found at\nhttps://tap-next.github.io/.\n","authors":["Artem Zholus","Carl Doersch","Yi Yang","Skanda Koppula","Viorica Patraucean","Xu Owen He","Ignacio Rocco","Mehdi S. M. Sajjadi","Sarath Chandar","Ross Goroshin"],"pdf_url":"https://arxiv.org/pdf/2504.05579v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10158v1","updated":"2025-04-14T12:13:33Z","published":"2025-04-14T12:13:33Z","title":"COUNTS: Benchmarking Object Detectors and Multimodal Large Language\n  Models under Distribution Shifts","summary":"  Current object detectors often suffer significant perfor-mance degradation in\nreal-world applications when encountering distributional shifts. Consequently,\nthe out-of-distribution (OOD) generalization capability of object detectors has\ngarnered increasing attention from researchers. Despite this growing interest,\nthere remains a lack of a large-scale, comprehensive dataset and evaluation\nbenchmark with fine-grained annotations tailored to assess the OOD\ngeneralization on more intricate tasks like object detection and grounding. To\naddress this gap, we introduce COUNTS, a large-scale OOD dataset with\nobject-level annotations. COUNTS encompasses 14 natural distributional shifts,\nover 222K samples, and more than 1,196K labeled bounding boxes. Leveraging\nCOUNTS, we introduce two novel benchmarks: O(OD)2 and OODG. O(OD)2 is designed\nto comprehensively evaluate the OOD generalization capabilities of object\ndetectors by utilizing controlled distribution shifts between training and\ntesting data. OODG, on the other hand, aims to assess the OOD generalization of\ngrounding abilities in multimodal large language models (MLLMs). Our findings\nreveal that, while large models and extensive pre-training data substantially\nen hance performance in in-distribution (IID) scenarios, significant\nlimitations and opportunities for improvement persist in OOD contexts for both\nobject detectors and MLLMs. In visual grounding tasks, even the advanced GPT-4o\nand Gemini-1.5 only achieve 56.7% and 28.0% accuracy, respectively. We hope\nCOUNTS facilitates advancements in the development and assessment of robust\nobject detectors and MLLMs capable of maintaining high performance under\ndistributional shifts.\n","authors":["Jiansheng Li","Xingxuan Zhang","Hao Zou","Yige Guo","Renzhe Xu","Yilong Liu","Chuzhao Zhu","Yue He","Peng Cui"],"pdf_url":"https://arxiv.org/pdf/2504.10158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11116v4","updated":"2025-04-14T12:11:51Z","published":"2024-03-17T06:53:44Z","title":"PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset","summary":"  Multimodal Large Language Models (MLLMs) hallucinate, resulting in an\nemerging topic of visual hallucination evaluation (VHE). This paper contributes\na ChatGPT-Prompted visual hallucination evaluation Dataset (PhD) for objective\nVHE at a large scale. The essence of VHE is to ask an MLLM questions about\nspecific images to assess its susceptibility to hallucination. Depending on\nwhat to ask (objects, attributes, sentiment, etc.) and how the questions are\nasked, we structure PhD along two dimensions, i.e. task and mode. Five visual\nrecognition tasks, ranging from low-level (object / attribute recognition) to\nmiddle-level (sentiment / position recognition and counting), are considered.\nBesides a normal visual QA mode, which we term PhD-base, PhD also asks\nquestions with specious context (PhD-sec) or with incorrect context ({PhD-icc),\nor with AI-generated counter common sense images (PhD-ccs). We construct PhD by\na ChatGPT-assisted semi-automated pipeline, encompassing four pivotal modules:\ntask-specific hallucinatory item (hitem) selection, hitem-embedded question\ngeneration, specious / incorrect context generation, and counter-common-sense\n(CCS) image generation. With over 14k daily images, 750 CCS images and 102k VQA\ntriplets in total, PhD reveals considerable variability in MLLMs' performance\nacross various modes and tasks, offering valuable insights into the nature of\nhallucination. As such, PhD stands as a potent tool not only for VHE but may\nalso play a significant role in the refinement of MLLMs.\n","authors":["Jiazhen Liu","Yuhan Fu","Ruobing Xie","Runquan Xie","Xingwu Sun","Fengzong Lian","Zhanhui Kang","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2403.11116v4.pdf","comment":"Accepted by CVPR 2025, Highlight"},{"id":"http://arxiv.org/abs/2504.10148v1","updated":"2025-04-14T11:59:58Z","published":"2025-04-14T11:59:58Z","title":"Hierarchical and Step-Layer-Wise Tuning of Attention Specialty for\n  Multi-Instance Synthesis in Diffusion Transformers","summary":"  Text-to-image (T2I) generation models often struggle with multi-instance\nsynthesis (MIS), where they must accurately depict multiple distinct instances\nin a single image based on complex prompts detailing individual features.\nTraditional MIS control methods for UNet architectures like SD v1.5/SDXL fail\nto adapt to DiT-based models like FLUX and SD v3.5, which rely on integrated\nattention between image and text tokens rather than text-image cross-attention.\nTo enhance MIS in DiT, we first analyze the mixed attention mechanism in DiT.\nOur token-wise and layer-wise analysis of attention maps reveals a hierarchical\nresponse structure: instance tokens dominate early layers, background tokens in\nmiddle layers, and attribute tokens in later layers. Building on this\nobservation, we propose a training-free approach for enhancing MIS in DiT-based\nmodels with hierarchical and step-layer-wise attention specialty tuning (AST).\nAST amplifies key regions while suppressing irrelevant areas in distinct\nattention maps across layers and steps, guided by the hierarchical structure.\nThis optimizes multimodal interactions by hierarchically decoupling the complex\nprompts with instance-based sketches. We evaluate our approach using upgraded\nsketch-based layouts for the T2I-CompBench and customized complex scenes. Both\nquantitative and qualitative results confirm our method enhances complex layout\ngeneration, ensuring precise instance placement and attribute representation in\nMIS.\n","authors":["Chunyang Zhang","Zhenhong Sun","Zhicheng Zhang","Junyan Wang","Yu Zhang","Dong Gong","Huadong Mo","Daoyi Dong"],"pdf_url":"https://arxiv.org/pdf/2504.10148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10143v1","updated":"2025-04-14T11:54:19Z","published":"2025-04-14T11:54:19Z","title":"Negate or Embrace: On How Misalignment Shapes Multimodal Representation\n  Learning","summary":"  Multimodal representation learning, exemplified by multimodal contrastive\nlearning (MMCL) using image-text pairs, aims to learn powerful representations\nby aligning cues across modalities. This approach relies on the core assumption\nthat the exemplar image-text pairs constitute two representations of an\nidentical concept. However, recent research has revealed that real-world\ndatasets often exhibit misalignment. There are two distinct viewpoints on how\nto address this issue: one suggests mitigating the misalignment, and the other\nleveraging it. We seek here to reconcile these seemingly opposing perspectives,\nand to provide a practical guide for practitioners. Using latent variable\nmodels we thus formalize misalignment by introducing two specific mechanisms:\nselection bias, where some semantic variables are missing, and perturbation\nbias, where semantic variables are distorted -- both affecting latent variables\nshared across modalities. Our theoretical analysis demonstrates that, under\nmild assumptions, the representations learned by MMCL capture exactly the\ninformation related to the subset of the semantic variables invariant to\nselection and perturbation biases. This provides a unified perspective for\nunderstanding misalignment. Based on this, we further offer actionable insights\ninto how misalignment should inform the design of real-world ML systems. We\nvalidate our theoretical findings through extensive empirical studies on both\nsynthetic data and real image-text datasets, shedding light on the nuanced\nimpact of misalignment on multimodal representation learning.\n","authors":["Yichao Cai","Yuhang Liu","Erdun Gao","Tianjiao Jiang","Zhen Zhang","Anton van den Hengel","Javen Qinfeng Shi"],"pdf_url":"https://arxiv.org/pdf/2504.10143v1.pdf","comment":"38 pages"},{"id":"http://arxiv.org/abs/2504.10127v1","updated":"2025-04-14T11:35:02Z","published":"2025-04-14T11:35:02Z","title":"Breaking the Data Barrier -- Building GUI Agents Through Task\n  Generalization","summary":"  Graphical User Interface (GUI) agents offer cross-platform solutions for\nautomating complex digital tasks, with significant potential to transform\nproductivity workflows. However, their performance is often constrained by the\nscarcity of high-quality trajectory data. To address this limitation, we\npropose training Vision Language Models (VLMs) on data-rich,\nreasoning-intensive tasks during a dedicated mid-training stage, and then\nexamine how incorporating these tasks facilitates generalization to GUI\nplanning scenarios. Specifically, we explore a range of tasks with readily\navailable instruction-tuning data, including GUI perception, multimodal\nreasoning, and textual reasoning. Through extensive experiments across 11\nmid-training tasks, we demonstrate that: (1) Task generalization proves highly\neffective, yielding substantial improvements across most settings. For\ninstance, multimodal mathematical reasoning enhances performance on\nAndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data\nsignificantly boosts GUI web agent performance, achieving a 5.6% improvement on\nWebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal\ngeneralization from text-based to visual domains; (2) Contrary to prior\nassumptions, GUI perception data - previously considered closely aligned with\nGUI agent tasks and widely utilized for training - has a comparatively limited\nimpact on final performance; (3) Building on these insights, we identify the\nmost effective mid-training tasks and curate optimized mixture datasets,\nresulting in absolute performance gains of 8.0% on WebArena and 12.2% on\nAndroidWorld. Our work provides valuable insights into cross-domain knowledge\ntransfer for GUI agents and offers a practical approach to addressing data\nscarcity challenges in this emerging field. The code, data and models will be\navailable at https://github.com/hkust-nlp/GUIMid.\n","authors":["Junlei Zhang","Zichen Ding","Chang Ma","Zijie Chen","Qiushi Sun","Zhenzhong Lan","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2504.10127v1.pdf","comment":"24 pages, 11 figures"},{"id":"http://arxiv.org/abs/2504.10123v1","updated":"2025-04-14T11:32:01Z","published":"2025-04-14T11:32:01Z","title":"M2S-RoAD: Multi-Modal Semantic Segmentation for Road Damage Using Camera\n  and LiDAR Data","summary":"  Road damage can create safety and comfort challenges for both human drivers\nand autonomous vehicles (AVs). This damage is particularly prevalent in rural\nareas due to less frequent surveying and maintenance of roads. Automated\ndetection of pavement deterioration can be used as an input to AVs and driver\nassistance systems to improve road safety. Current research in this field has\npredominantly focused on urban environments driven largely by public datasets,\nwhile rural areas have received significantly less attention. This paper\nintroduces M2S-RoAD, a dataset for the semantic segmentation of different\nclasses of road damage. M2S-RoAD was collected in various towns across New\nSouth Wales, Australia, and labelled for semantic segmentation to identify nine\ndistinct types of road damage. This dataset will be released upon the\nacceptance of the paper.\n","authors":["Tzu-Yun Tseng","Hongyu Lyu","Josephine Li","Julie Stephany Berrio","Mao Shan","Stewart Worrall"],"pdf_url":"https://arxiv.org/pdf/2504.10123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10117v1","updated":"2025-04-14T11:26:20Z","published":"2025-04-14T11:26:20Z","title":"AGO: Adaptive Grounding for Open World 3D Occupancy Prediction","summary":"  Open-world 3D semantic occupancy prediction aims to generate a voxelized 3D\nrepresentation from sensor inputs while recognizing both known and unknown\nobjects. Transferring open-vocabulary knowledge from vision-language models\n(VLMs) offers a promising direction but remains challenging. However, methods\nbased on VLM-derived 2D pseudo-labels with traditional supervision are limited\nby a predefined label space and lack general prediction capabilities. Direct\nalignment with pretrained image embeddings, on the other hand, fails to achieve\nreliable performance due to often inconsistent image and text representations\nin VLMs. To address these challenges, we propose AGO, a novel 3D occupancy\nprediction framework with adaptive grounding to handle diverse open-world\nscenarios. AGO first encodes surrounding images and class prompts into 3D and\ntext embeddings, respectively, leveraging similarity-based grounding training\nwith 3D pseudo-labels. Additionally, a modality adapter maps 3D embeddings into\na space aligned with VLM-derived image embeddings, reducing modality gaps.\nExperiments on Occ3D-nuScenes show that AGO improves unknown object prediction\nin zero-shot and few-shot transfer while achieving state-of-the-art\nclosed-world self-supervised performance, surpassing prior methods by 4.09\nmIoU.\n","authors":["Peizheng Li","Shuxiao Ding","You Zhou","Qingwen Zhang","Onat Inak","Larissa Triess","Niklas Hanselmann","Marius Cordts","Andreas Zell"],"pdf_url":"https://arxiv.org/pdf/2504.10117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10106v1","updated":"2025-04-14T11:15:13Z","published":"2025-04-14T11:15:13Z","title":"SoccerNet-v3D: Leveraging Sports Broadcast Replays for 3D Scene\n  Understanding","summary":"  Sports video analysis is a key domain in computer vision, enabling detailed\nspatial understanding through multi-view correspondences. In this work, we\nintroduce SoccerNet-v3D and ISSIA-3D, two enhanced and scalable datasets\ndesigned for 3D scene understanding in soccer broadcast analysis. These\ndatasets extend SoccerNet-v3 and ISSIA by incorporating field-line-based camera\ncalibration and multi-view synchronization, enabling 3D object localization\nthrough triangulation. We propose a monocular 3D ball localization task built\nupon the triangulation of ground-truth 2D ball annotations, along with several\ncalibration and reprojection metrics to assess annotation quality on demand.\nAdditionally, we present a single-image 3D ball localization method as a\nbaseline, leveraging camera calibration and ball size priors to estimate the\nball's position from a monocular viewpoint. To further refine 2D annotations,\nwe introduce a bounding box optimization technique that ensures alignment with\nthe 3D scene representation. Our proposed datasets establish new benchmarks for\n3D soccer scene understanding, enhancing both spatial and temporal analysis in\nsports analytics. Finally, we provide code to facilitate access to our\nannotations and the generation pipelines for the datasets.\n","authors":["Marc Gutiérrez-Pérez","Antonio Agudo"],"pdf_url":"https://arxiv.org/pdf/2504.10106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10105v1","updated":"2025-04-14T11:14:24Z","published":"2025-04-14T11:14:24Z","title":"Global and Local Mamba Network for Multi-Modality Medical Image\n  Super-Resolution","summary":"  Convolutional neural networks and Transformer have made significant\nprogresses in multi-modality medical image super-resolution. However, these\nmethods either have a fixed receptive field for local learning or significant\ncomputational burdens for global learning, limiting the super-resolution\nperformance. To solve this problem, State Space Models, notably Mamba, is\nintroduced to efficiently model long-range dependencies in images with linear\ncomputational complexity. Relying on the Mamba and the fact that low-resolution\nimages rely on global information to compensate for missing details, while\nhigh-resolution reference images need to provide more local details for\naccurate super-resolution, we propose a global and local Mamba network\n(GLMamba) for multi-modality medical image super-resolution. To be specific,\nour GLMamba is a two-branch network equipped with a global Mamba branch and a\nlocal Mamba branch. The global Mamba branch captures long-range relationships\nin low-resolution inputs, and the local Mamba branch focuses more on\nshort-range details in high-resolution reference images. We also use the deform\nblock to adaptively extract features of both branches to enhance the\nrepresentation ability. A modulator is designed to further enhance deformable\nfeatures in both global and local Mamba blocks. To fully integrate the\nreference image for low-resolution image super-resolution, we further develop a\nmulti-modality feature fusion block to adaptively fuse features by considering\nsimilarities, differences, and complementary aspects between modalities. In\naddition, a contrastive edge loss (CELoss) is developed for sufficient\nenhancement of edge textures and contrast in medical images.\n","authors":["Zexin Ji","Beiji Zou","Xiaoyan Kui","Sebastien Thureau","Su Ruan"],"pdf_url":"https://arxiv.org/pdf/2504.10105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10090v1","updated":"2025-04-14T10:53:44Z","published":"2025-04-14T10:53:44Z","title":"CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography","summary":"  Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning.\n","authors":["I-Sheng Fang","Jun-Cheng Chen"],"pdf_url":"https://arxiv.org/pdf/2504.10090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05336v3","updated":"2025-04-14T10:51:41Z","published":"2024-05-08T18:10:59Z","title":"Joint semi-supervised and contrastive learning enables domain\n  generalization and multi-domain segmentation","summary":"  Despite their effectiveness, current deep learning models face challenges\nwith images coming from different domains with varying appearance and content.\nWe introduce SegCLR, a versatile framework designed to segment images across\ndifferent domains, employing supervised and contrastive learning simultaneously\nto effectively learn from both labeled and unlabeled data. We demonstrate the\nsuperior performance of SegCLR through a comprehensive evaluation involving\nthree diverse clinical datasets of 3D retinal Optical Coherence Tomography\n(OCT) images, for the slice-wise segmentation of fluids with various network\nconfigurations and verification across 10 different network initializations. In\nan unsupervised domain adaptation context, SegCLR achieves results on par with\na supervised upper-bound model trained on the intended target domain. Notably,\nwe discover that the segmentation performance of SegCLR framework is marginally\nimpacted by the abundance of unlabeled data from the target domain, thereby we\nalso propose an effective domain generalization extension of SegCLR, known also\nas zero-shot domain adaptation, which eliminates the need for any target domain\ninformation. This shows that our proposed addition of contrastive loss in\nstandard supervised training for segmentation leads to superior models,\ninherently more generalizable to both in- and out-of-domain test data. We\nadditionally propose a pragmatic solution for SegCLR deployment in realistic\nscenarios with multiple domains containing labeled data. Accordingly, our\nframework pushes the boundaries of deep-learning based segmentation in\nmulti-domain applications, regardless of data availability - labeled,\nunlabeled, or nonexistent.\n","authors":["Alvaro Gomariz","Yusuke Kikuchi","Yun Yvonna Li","Thomas Albrecht","Andreas Maunz","Daniela Ferrara","Huanxiang Lu","Orcun Goksel"],"pdf_url":"https://arxiv.org/pdf/2405.05336v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16555v2","updated":"2025-04-14T10:44:13Z","published":"2024-05-26T12:58:04Z","title":"Building Vision Models upon Heat Conduction","summary":"  Visual representation models leveraging attention mechanisms are challenged\nby significant computational overhead, particularly when pursuing large\nreceptive fields. In this study, we aim to mitigate this challenge by\nintroducing the Heat Conduction Operator (HCO) built upon the physical heat\nconduction principle. HCO conceptualizes image patches as heat sources and\nmodels their correlations through adaptive thermal energy diffusion, enabling\nrobust visual representations. HCO enjoys a computational complexity of\nO(N^1.5), as it can be implemented using discrete cosine transformation (DCT)\noperations. HCO is plug-and-play, combining with deep learning backbones\nproduces visual representation models (termed vHeat) with global receptive\nfields. Experiments across vision tasks demonstrate that, beyond the stronger\nperformance, vHeat achieves up to a 3x throughput, 80% less GPU memory\nallocation, and 35% fewer computational FLOPs compared to the Swin-Transformer.\nCode is available at https://github.com/MzeroMiko/vHeat.\n","authors":["Zhaozhi Wang","Yue Liu","Yunjie Tian","Yunfan Liu","Yaowei Wang","Qixiang Ye"],"pdf_url":"https://arxiv.org/pdf/2405.16555v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05260v3","updated":"2025-04-14T10:43:50Z","published":"2024-10-07T17:58:22Z","title":"DartControl: A Diffusion-Based Autoregressive Motion Model for Real-Time\n  Text-Driven Motion Control","summary":"  Text-conditioned human motion generation, which allows for user interaction\nthrough natural language, has become increasingly popular. Existing methods\ntypically generate short, isolated motions based on a single input sentence.\nHowever, human motions are continuous and can extend over long periods,\ncarrying rich semantics. Creating long, complex motions that precisely respond\nto streams of text descriptions, particularly in an online and real-time\nsetting, remains a significant challenge. Furthermore, incorporating spatial\nconstraints into text-conditioned motion generation presents additional\nchallenges, as it requires aligning the motion semantics specified by text\ndescriptions with geometric information, such as goal locations and 3D scene\ngeometry. To address these limitations, we propose DartControl, in short DART,\na Diffusion-based Autoregressive motion primitive model for Real-time\nText-driven motion control. Our model effectively learns a compact motion\nprimitive space jointly conditioned on motion history and text inputs using\nlatent diffusion models. By autoregressively generating motion primitives based\non the preceding history and current text input, DART enables real-time,\nsequential motion generation driven by natural language descriptions.\nAdditionally, the learned motion primitive space allows for precise spatial\nmotion control, which we formulate either as a latent noise optimization\nproblem or as a Markov decision process addressed through reinforcement\nlearning. We present effective algorithms for both approaches, demonstrating\nour model's versatility and superior performance in various motion synthesis\ntasks. Experiments show our method outperforms existing baselines in motion\nrealism, efficiency, and controllability. Video results are available on the\nproject page: https://zkf1997.github.io/DART/.\n","authors":["Kaifeng Zhao","Gen Li","Siyu Tang"],"pdf_url":"https://arxiv.org/pdf/2410.05260v3.pdf","comment":"Updated ICLR camera ready version"},{"id":"http://arxiv.org/abs/2504.10084v1","updated":"2025-04-14T10:40:54Z","published":"2025-04-14T10:40:54Z","title":"UP-Person: Unified Parameter-Efficient Transfer Learning for Text-based\n  Person Retrieval","summary":"  Text-based Person Retrieval (TPR) as a multi-modal task, which aims to\nretrieve the target person from a pool of candidate images given a text\ndescription, has recently garnered considerable attention due to the progress\nof contrastive visual-language pre-trained model. Prior works leverage\npre-trained CLIP to extract person visual and textual features and fully\nfine-tune the entire network, which have shown notable performance improvements\ncompared to uni-modal pre-training models. However, full-tuning a large model\nis prone to overfitting and hinders the generalization ability. In this paper,\nwe propose a novel Unified Parameter-Efficient Transfer Learning (PETL) method\nfor Text-based Person Retrieval (UP-Person) to thoroughly transfer the\nmulti-modal knowledge from CLIP. Specifically, UP-Person simultaneously\nintegrates three lightweight PETL components including Prefix, LoRA and\nAdapter, where Prefix and LoRA are devised together to mine local information\nwith task-specific information prompts, and Adapter is designed to adjust\nglobal feature representations. Additionally, two vanilla submodules are\noptimized to adapt to the unified architecture of TPR. For one thing, S-Prefix\nis proposed to boost attention of prefix and enhance the gradient propagation\nof prefix tokens, which improves the flexibility and performance of the vanilla\nprefix. For another thing, L-Adapter is designed in parallel with layer\nnormalization to adjust the overall distribution, which can resolve conflicts\ncaused by overlap and interaction among multiple submodules. Extensive\nexperimental results demonstrate that our UP-Person achieves state-of-the-art\nresults across various person retrieval datasets, including CUHK-PEDES,\nICFG-PEDES and RSTPReid while merely fine-tuning 4.7\\% parameters. Code is\navailable at https://github.com/Liu-Yating/UP-Person.\n","authors":["Yating Liu","Yaowei Li","Xiangyuan Lan","Wenming Yang","Zimo Liu","Qingmin Liao"],"pdf_url":"https://arxiv.org/pdf/2504.10084v1.pdf","comment":"16 pages, 7 figures, first submited to IEEE TCSVT on 2024 May. Under\n  review"},{"id":"http://arxiv.org/abs/2407.06165v2","updated":"2025-04-14T10:28:26Z","published":"2024-06-04T12:05:20Z","title":"Tumor likelihood estimation on MRI prostate data by utilizing k-Space\n  information","summary":"  We present a novel preprocessing and prediction pipeline for the\nclassification of magnetic resonance imaging (MRI) that takes advantage of the\ninformation rich complex valued k-Space. Using a publicly available MRI raw\ndataset with 312 subject and a total of 9508 slices, we show the advantage of\nutilizing the k-Space for better prostate cancer likelihood estimation in\ncomparison to just using the magnitudinal information in the image domain, with\nan AUROC of $86.1\\%\\pm1.8\\%$. Additionally, by using high undersampling rates\nand a simple principal component analysis (PCA) for coil compression, we reduce\nthe time needed for reconstruction by avoiding the time intensive GRAPPA\nreconstruction algorithm. By using digital undersampling for our experiments,\nwe show that scanning and reconstruction time could be reduced. Even with an\nundersampling factor of 16, our approach achieves meaningful results, with an\nAUROC of $71.4\\%\\pm2.9\\%$, using the PCA coil combination and taking into\naccount the k-Space information. With this study, we were able to show the\nfeasibility of preserving phase and k-Space information, with consistent\nresults. Besides preserving valuable information for further diagnostics, this\napproach can work without the time intensive ADC and reconstruction\ncalculations, greatly reducing the post processing, as well as potential\nscanning time, increasing patient comfort and allowing a close to real-time\nprediction.\n","authors":["M. Rempe","F. Hörst","C. Seibold","B. Hadaschik","M. Schlimbach","J. Egger","K. Kröninger","F. Breuer","M. Blaimer","J. Kleesiek"],"pdf_url":"https://arxiv.org/pdf/2407.06165v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10080v1","updated":"2025-04-14T10:24:57Z","published":"2025-04-14T10:24:57Z","title":"Learning to Harmonize Cross-vendor X-ray Images by Non-linear Image\n  Dynamics Correction","summary":"  In this paper, we explore how conventional image enhancement can improve\nmodel robustness in medical image analysis. By applying commonly used\nnormalization methods to images from various vendors and studying their\ninfluence on model generalization in transfer learning, we show that the\nnonlinear characteristics of domain-specific image dynamics cannot be addressed\nby simple linear transforms. To tackle this issue, we reformulate the image\nharmonization task as an exposure correction problem and propose a method\ntermed Global Deep Curve Estimation (GDCE) to reduce domain-specific exposure\nmismatch. GDCE performs enhancement via a pre-defined polynomial function and\nis trained with the help of a ``domain discriminator'', aiming to improve model\ntransparency in downstream tasks compared to existing black-box methods.\n","authors":["Yucheng Lu","Shunxin Wang","Dovile Juodelyte","Veronika Cheplygina"],"pdf_url":"https://arxiv.org/pdf/2504.10080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10079v1","updated":"2025-04-14T10:23:22Z","published":"2025-04-14T10:23:22Z","title":"Hierarchical Relation-augmented Representation Generalization for\n  Few-shot Action Recognition","summary":"  Few-shot action recognition (FSAR) aims to recognize novel action categories\nwith few exemplars. Existing methods typically learn frame-level\nrepresentations independently for each video by designing various inter-frame\ntemporal modeling strategies. However, they neglect explicit relation modeling\nbetween videos and tasks, thus failing to capture shared temporal patterns\nacross videos and reuse temporal knowledge from historical tasks. In light of\nthis, we propose HR2G-shot, a Hierarchical Relation-augmented Representation\nGeneralization framework for FSAR, which unifies three types of relation\nmodeling (inter-frame, inter-video, and inter-task) to learn task-specific\ntemporal patterns from a holistic view. In addition to conducting inter-frame\ntemporal interactions, we further devise two components to respectively explore\ninter-video and inter-task relationships: i) Inter-video Semantic Correlation\n(ISC) performs cross-video frame-level interactions in a fine-grained manner,\nthereby capturing task-specific query features and learning intra- and\ninter-class temporal correlations among support features; ii) Inter-task\nKnowledge Transfer (IKT) retrieves and aggregates relevant temporal knowledge\nfrom the bank, which stores diverse temporal patterns from historical tasks.\nExtensive experiments on five benchmarks show that HR2G-shot outperforms\ncurrent top-leading FSAR methods.\n","authors":["Hongyu Qu","Ling Xing","Rui Yan","Yazhou Yao","Guo-Sen Xie","Xiangbo Shu"],"pdf_url":"https://arxiv.org/pdf/2504.10079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10070v1","updated":"2025-04-14T10:17:25Z","published":"2025-04-14T10:17:25Z","title":"DTFSal: Audio-Visual Dynamic Token Fusion for Video Saliency Prediction","summary":"  Audio-visual saliency prediction aims to mimic human visual attention by\nidentifying salient regions in videos through the integration of both visual\nand auditory information. Although visual-only approaches have significantly\nadvanced, effectively incorporating auditory cues remains challenging due to\ncomplex spatio-temporal interactions and high computational demands. To address\nthese challenges, we propose Dynamic Token Fusion Saliency (DFTSal), a novel\naudio-visual saliency prediction framework designed to balance accuracy with\ncomputational efficiency. Our approach features a multi-scale visual encoder\nequipped with two novel modules: the Learnable Token Enhancement Block (LTEB),\nwhich adaptively weights tokens to emphasize crucial saliency cues, and the\nDynamic Learnable Token Fusion Block (DLTFB), which employs a shifting\noperation to reorganize and merge features, effectively capturing long-range\ndependencies and detailed spatial information. In parallel, an audio branch\nprocesses raw audio signals to extract meaningful auditory features. Both\nvisual and audio features are integrated using our Adaptive Multimodal Fusion\nBlock (AMFB), which employs local, global, and adaptive fusion streams for\nprecise cross-modal fusion. The resulting fused features are processed by a\nhierarchical multi-decoder structure, producing accurate saliency maps.\nExtensive evaluations on six audio-visual benchmarks demonstrate that DFTSal\nachieves SOTA performance while maintaining computational efficiency.\n","authors":["Kiana Hoshanfar","Alireza Hosseini","Ahmad Kalhor","Babak Nadjar Araabi"],"pdf_url":"https://arxiv.org/pdf/2504.10070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10068v1","updated":"2025-04-14T10:14:44Z","published":"2025-04-14T10:14:44Z","title":"Mavors: Multi-granularity Video Representation for Multimodal Large\n  Language Model","summary":"  Long-context video understanding in multimodal large language models (MLLMs)\nfaces a critical challenge: balancing computational efficiency with the\nretention of fine-grained spatio-temporal patterns. Existing approaches (e.g.,\nsparse sampling, dense sampling with low resolution, and token compression)\nsuffer from significant information loss in temporal dynamics, spatial details,\nor subtle interactions, particularly in videos with complex motion or varying\nresolutions. To address this, we propose $\\mathbf{Mavors}$, a novel framework\nthat introduces $\\mathbf{M}$ulti-gr$\\mathbf{a}$nularity\n$\\mathbf{v}$ide$\\mathbf{o}$ $\\mathbf{r}$epre$\\mathbf{s}$entation for holistic\nlong-video modeling. Specifically, Mavors directly encodes raw video content\ninto latent representations through two core components: 1) an Intra-chunk\nVision Encoder (IVE) that preserves high-resolution spatial features via 3D\nconvolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator\n(IFA) that establishes temporal coherence across chunks using transformer-based\ndependency modeling with chunk-level rotary position encodings. Moreover, the\nframework unifies image and video understanding by treating images as\nsingle-frame videos via sub-image decomposition. Experiments across diverse\nbenchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity\nand temporal continuity, significantly outperforming existing methods in tasks\nrequiring fine-grained spatio-temporal reasoning.\n","authors":["Yang Shi","Jiaheng Liu","Yushuo Guan","Zhenhua Wu","Yuanxing Zhang","Zihao Wang","Weihong Lin","Jingyun Hua","Zekun Wang","Xinlong Chen","Bohan Zeng","Wentao Zhang","Fuzheng Zhang","Wenjing Yang","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.10068v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2310.05241v2","updated":"2025-04-14T10:07:02Z","published":"2023-10-08T17:19:58Z","title":"SCANet: Scene Complexity Aware Network for Weakly-Supervised Video\n  Moment Retrieval","summary":"  Video moment retrieval aims to localize moments in video corresponding to a\ngiven language query. To avoid the expensive cost of annotating the temporal\nmoments, weakly-supervised VMR (wsVMR) systems have been studied. For such\nsystems, generating a number of proposals as moment candidates and then\nselecting the most appropriate proposal has been a popular approach. These\nproposals are assumed to contain many distinguishable scenes in a video as\ncandidates. However, existing proposals of wsVMR systems do not respect the\nvarying numbers of scenes in each video, where the proposals are heuristically\ndetermined irrespective of the video. We argue that the retrieval system should\nbe able to counter the complexities caused by varying numbers of scenes in each\nvideo. To this end, we present a novel concept of a retrieval system referred\nto as Scene Complexity Aware Network (SCANet), which measures the `scene\ncomplexity' of multiple scenes in each video and generates adaptive proposals\nresponding to variable complexities of scenes in each video. Experimental\nresults on three retrieval benchmarks (i.e., Charades-STA, ActivityNet, TVR)\nachieve state-of-the-art performances and demonstrate the effectiveness of\nincorporating the scene complexity.\n","authors":["Sunjae Yoon","Gwanhyeong Koo","Dahyun Kim","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2310.05241v2.pdf","comment":"11 pages, Accepted in ICCV 2023"},{"id":"http://arxiv.org/abs/2406.01708v2","updated":"2025-04-14T09:56:26Z","published":"2024-06-03T18:04:37Z","title":"SnatchML: Hijacking ML models without Training Access","summary":"  Model hijacking can cause significant accountability and security risks since\nthe owner of a hijacked model can be framed for having their model offer\nillegal or unethical services. Prior works consider model hijacking as a\ntraining time attack, whereby an adversary requires full access to the ML model\ntraining. In this paper, we consider a stronger threat model for an\ninference-time hijacking attack, where the adversary has no access to the\ntraining phase of the victim model. Our intuition is that ML models, which are\ntypically over-parameterized, might have the capacity to (unintentionally)\nlearn more than the intended task they are trained for. We propose SnatchML, a\nnew training-free model hijacking attack, that leverages the extra capacity\nlearnt by the victim model to infer different tasks that can be semantically\nrelated or unrelated to the original one. Our results on models deployed on AWS\nSagemaker showed that SnatchML can deliver high accuracy on hijacking tasks.\nInterestingly, while all previous approaches are limited by the number of\nclasses in the benign task, SnatchML can hijack models for tasks that contain\nmore classes than the original. We explore different methods to mitigate this\nrisk; We propose meta-unlearning, which is designed to help the model unlearn a\npotentially malicious task while training for the original task. We also\nprovide insights on over-parametrization as a possible inherent factor that\nfacilitates model hijacking, and accordingly, we propose a compression-based\ncountermeasure to counteract this attack. We believe this work offers a\npreviously overlooked perspective on model hijacking attacks, presenting a\nstronger threat model and higher applicability in real-world contexts.\n","authors":["Mahmoud Ghorbel","Halima Bouzidi","Ioan Marius Bilasco","Ihsen Alouani"],"pdf_url":"https://arxiv.org/pdf/2406.01708v2.pdf","comment":"17 pages, 14 figures, 7 tables"},{"id":"http://arxiv.org/abs/2407.18449v3","updated":"2025-04-14T09:55:21Z","published":"2024-07-26T01:12:54Z","title":"Towards A Generalizable Pathology Foundation Model via Unified Knowledge\n  Distillation","summary":"  Foundation models pretrained on large-scale datasets are revolutionizing the\nfield of computational pathology (CPath). The generalization ability of\nfoundation models is crucial for the success in various downstream clinical\ntasks. However, current foundation models have only been evaluated on a limited\ntype and number of tasks, leaving their generalization ability and overall\nperformance unclear. To address this gap, we established a most comprehensive\nbenchmark to evaluate the performance of off-the-shelf foundation models across\nsix distinct clinical task types, encompassing a total of 72 specific tasks,\nincluding slide-level classification, survival prediction, ROI-tissue\nclassification, ROI retrieval, visual question answering, and report\ngeneration. Our findings reveal that existing foundation models excel at\ncertain task types but struggle to effectively handle the full breadth of\nclinical tasks. To improve the generalization of pathology foundation models,\nwe propose a unified knowledge distillation framework consisting of both expert\nand self-knowledge distillation, where the former allows the model to learn\nfrom the knowledge of multiple expert models, while the latter leverages\nself-distillation to enable image representation learning via local-global\nalignment. Based on this framework, we curated a dataset of 96,000 whole slide\nimages (WSIs) and developed a Generalizable Pathology Foundation Model (GPFM).\nThis advanced model was trained on a substantial dataset comprising 190 million\nimages extracted from approximately 72,000 publicly available slides,\nencompassing 34 major tissue types. Evaluated on the established benchmark,\nGPFM achieves an impressive average rank of 1.6, with 42 tasks ranked 1st,\nwhile the second-best model, UNI, attains an average rank of 3.7, with only 6\ntasks ranked 1st.\n","authors":["Jiabo Ma","Zhengrui Guo","Fengtao Zhou","Yihui Wang","Yingxue Xu","Jinbang Li","Fang Yan","Yu Cai","Zhengjie Zhu","Cheng Jin","Yi Lin","Xinrui Jiang","Chenglong Zhao","Danyi Li","Anjia Han","Zhenhui Li","Ronald Cheong Kin Chan","Jiguang Wang","Peng Fei","Kwang-Ting Cheng","Shaoting Zhang","Li Liang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2407.18449v3.pdf","comment":"update"},{"id":"http://arxiv.org/abs/2504.10049v1","updated":"2025-04-14T09:55:01Z","published":"2025-04-14T09:55:01Z","title":"Summarization of Multimodal Presentations with Vision-Language Models:\n  Study of the Effect of Modalities and Structure","summary":"  Vision-Language Models (VLMs) can process visual and textual information in\nmultiple formats: texts, images, interleaved texts and images, or even\nhour-long videos. In this work, we conduct fine-grained quantitative and\nqualitative analyses of automatic summarization of multimodal presentations\nusing VLMs with various representations as input. From these experiments, we\nsuggest cost-effective strategies for generating summaries from text-heavy\nmultimodal documents under different input-length budgets using VLMs. We show\nthat slides extracted from the video stream can be beneficially used as input\nagainst the raw video, and that a structured representation from interleaved\nslides and transcript provides the best performance. Finally, we reflect and\ncomment on the nature of cross-modal interactions in multimodal presentations\nand share suggestions to improve the capabilities of VLMs to understand\ndocuments of this nature.\n","authors":["Théo Gigant","Camille Guinaudeau","Frédéric Dufaux"],"pdf_url":"https://arxiv.org/pdf/2504.10049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10048v1","updated":"2025-04-14T09:53:48Z","published":"2025-04-14T09:53:48Z","title":"Multi-Object Grounding via Hierarchical Contrastive Siamese Transformers","summary":"  Multi-object grounding in 3D scenes involves localizing multiple objects\nbased on natural language input. While previous work has primarily focused on\nsingle-object grounding, real-world scenarios often demand the localization of\nseveral objects. To tackle this challenge, we propose Hierarchical Contrastive\nSiamese Transformers (H-COST), which employs a Hierarchical Processing strategy\nto progressively refine object localization, enhancing the understanding of\ncomplex language instructions. Additionally, we introduce a Contrastive Siamese\nTransformer framework, where two networks with the identical structure are\nused: one auxiliary network processes robust object relations from ground-truth\nlabels to guide and enhance the second network, the reference network, which\noperates on segmented point-cloud data. This contrastive mechanism strengthens\nthe model' s semantic understanding and significantly enhances its ability to\nprocess complex point-cloud data. Our approach outperforms previous\nstate-of-the-art methods by 9.5% on challenging multi-object grounding\nbenchmarks.\n","authors":["Chengyi Du","Keyan Jin"],"pdf_url":"https://arxiv.org/pdf/2504.10048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10044v1","updated":"2025-04-14T09:49:34Z","published":"2025-04-14T09:49:34Z","title":"Aligning Anime Video Generation with Human Feedback","summary":"  Anime video generation faces significant challenges due to the scarcity of\nanime data and unusual motion patterns, leading to issues such as motion\ndistortion and flickering artifacts, which result in misalignment with human\npreferences. Existing reward models, designed primarily for real-world videos,\nfail to capture the unique appearance and consistency requirements of anime. In\nthis work, we propose a pipeline to enhance anime video generation by\nleveraging human feedback for better alignment. Specifically, we construct the\nfirst multi-dimensional reward dataset for anime videos, comprising 30k\nhuman-annotated samples that incorporating human preferences for both visual\nappearance and visual consistency. Based on this, we develop AnimeReward, a\npowerful reward model that employs specialized vision-language models for\ndifferent evaluation dimensions to guide preference alignment. Furthermore, we\nintroduce Gap-Aware Preference Optimization (GAPO), a novel training method\nthat explicitly incorporates preference gaps into the optimization process,\nenhancing alignment performance and efficiency. Extensive experiment results\nshow that AnimeReward outperforms existing reward models, and the inclusion of\nGAPO leads to superior alignment in both quantitative benchmarks and human\nevaluations, demonstrating the effectiveness of our pipeline in enhancing anime\nvideo quality. Our dataset and code will be publicly available.\n","authors":["Bingwen Zhu","Yudong Jiang","Baohan Xu","Siqian Yang","Mingyu Yin","Yidi Wu","Huyang Sun","Zuxuan Wu"],"pdf_url":"https://arxiv.org/pdf/2504.10044v1.pdf","comment":"10 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2503.08915v2","updated":"2025-04-14T09:44:57Z","published":"2025-03-11T21:53:58Z","title":"Reconstruct Anything Model: a lightweight foundation model for\n  computational imaging","summary":"  Most existing learning-based methods for solving imaging inverse problems can\nbe roughly divided into two classes: iterative algorithms, such as\nplug-and-play and diffusion methods, that leverage pretrained denoisers, and\nunrolled architectures that are trained end-to-end for specific imaging\nproblems. Iterative methods in the first class are computationally costly and\noften provide suboptimal reconstruction performance, whereas unrolled\narchitectures are generally specific to a single inverse problem and require\nexpensive training. In this work, we propose a novel non-iterative, lightweight\narchitecture that incorporates knowledge about the forward operator\n(acquisition physics and noise parameters) without relying on unrolling. Our\nmodel is trained to solve a wide range of inverse problems beyond denoising,\nincluding deblurring, magnetic resonance imaging, computed tomography,\ninpainting, and super-resolution. The proposed model can be easily adapted to\nunseen inverse problems or datasets with a few fine-tuning steps (up to a few\nimages) in a self-supervised way, without ground-truth references. Throughout a\nseries of experiments, we demonstrate state-of-the-art performance from medical\nimaging to low-photon imaging and microscopy.\n","authors":["Matthieu Terris","Samuel Hurault","Maxime Song","Julian Tachella"],"pdf_url":"https://arxiv.org/pdf/2503.08915v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10041v1","updated":"2025-04-14T09:42:35Z","published":"2025-04-14T09:42:35Z","title":"Prior Does Matter: Visual Navigation via Denoising Diffusion Bridge\n  Models","summary":"  Recent advancements in diffusion-based imitation learning, which show\nimpressive performance in modeling multimodal distributions and training\nstability, have led to substantial progress in various robot learning tasks. In\nvisual navigation, previous diffusion-based policies typically generate action\nsequences by initiating from denoising Gaussian noise. However, the target\naction distribution often diverges significantly from Gaussian noise, leading\nto redundant denoising steps and increased learning complexity. Additionally,\nthe sparsity of effective action distributions makes it challenging for the\npolicy to generate accurate actions without guidance. To address these issues,\nwe propose a novel, unified visual navigation framework leveraging the\ndenoising diffusion bridge models named NaviBridger. This approach enables\naction generation by initiating from any informative prior actions, enhancing\nguidance and efficiency in the denoising process. We explore how diffusion\nbridges can enhance imitation learning in visual navigation tasks and further\nexamine three source policies for generating prior actions. Extensive\nexperiments in both simulated and real-world indoor and outdoor scenarios\ndemonstrate that NaviBridger accelerates policy inference and outperforms the\nbaselines in generating target action sequences. Code is available at\nhttps://github.com/hren20/NaiviBridger.\n","authors":["Hao Ren","Yiming Zeng","Zetong Bi","Zhaoliang Wan","Junlong Huang","Hui Cheng"],"pdf_url":"https://arxiv.org/pdf/2504.10041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10039v1","updated":"2025-04-14T09:41:47Z","published":"2025-04-14T09:41:47Z","title":"Investigating the Role of Bilateral Symmetry for Inpainting Brain MRI","summary":"  Inpainting has recently emerged as a valuable and interesting technology to\nemploy in the analysis of medical imaging data, in particular brain MRI. A wide\nvariety of methodologies for inpainting MRI have been proposed and demonstrated\non tasks including anomaly detection. In this work we investigate the\nstatistical relationship between inpainted brain structures and the amount of\nsubject-specific conditioning information, i.e. the other areas of the image\nthat are masked. In particular, we analyse the distribution of inpainting\nresults when masking additional regions of the image, specifically the\ncontra-lateral structure. This allows us to elucidate where in the brain the\nmodel is drawing information from, and in particular, what is the importance of\nhemispherical symmetry? Our experiments interrogate a diffusion inpainting\nmodel through analysing the inpainting of subcortical brain structures based on\nintensity and estimated area change. We demonstrate that some structures show a\nstrong influence of symmetry in the conditioning of the inpainting process.\n","authors":["Sergey Kuznetsov","Sanduni Pinnawala","Peter A. Wijeratne","Ivor J. A. Simpson"],"pdf_url":"https://arxiv.org/pdf/2504.10039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10035v1","updated":"2025-04-14T09:37:47Z","published":"2025-04-14T09:37:47Z","title":"TT3D: Table Tennis 3D Reconstruction","summary":"  Sports analysis requires processing large amounts of data, which is\ntime-consuming and costly. Advancements in neural networks have significantly\nalleviated this burden, enabling highly accurate ball tracking in sports\nbroadcasts. However, relying solely on 2D ball tracking is limiting, as it\ndepends on the camera's viewpoint and falls short of supporting comprehensive\ngame analysis. To address this limitation, we propose a novel approach for\nreconstructing precise 3D ball trajectories from online table tennis match\nrecordings. Our method leverages the underlying physics of the ball's motion to\nidentify the bounce state that minimizes the reprojection error of the ball's\nflying trajectory, hence ensuring an accurate and reliable 3D reconstruction. A\nkey advantage of our approach is its ability to infer ball spin without relying\non human pose estimation or racket tracking, which are often unreliable or\nunavailable in broadcast footage. We developed an automated camera calibration\nmethod capable of reliably tracking camera movements. Additionally, we adapted\nan existing 3D pose estimation model, which lacks depth motion capture, to\naccurately track player movements. Together, these contributions enable the\nfull 3D reconstruction of a table tennis rally.\n","authors":["Thomas Gossard","Andreas Ziegler","Andreas Zell"],"pdf_url":"https://arxiv.org/pdf/2504.10035v1.pdf","comment":"Accepted to CVSport 2025"},{"id":"http://arxiv.org/abs/2412.12496v4","updated":"2025-04-14T09:37:17Z","published":"2024-12-17T02:56:35Z","title":"Faster Vision Mamba is Rebuilt in Minutes via Merged Token Re-training","summary":"  Vision Mamba has shown close to state of the art performance on computer\nvision tasks, drawing much interest in increasing it's efficiency. A promising\napproach is token reduction (that has been successfully implemented in ViTs).\nPruning informative tokens in Mamba leads to a high loss of key knowledge and\ndegraded performance. An alternative, of merging tokens preserves more\ninformation than pruning, also suffers for large compression ratios. Our key\ninsight is that a quick round of retraining after token merging yeilds robust\nresults across various compression ratios. Empirically, pruned Vims only drop\nup to 0.9% accuracy on ImageNet-1K, recovered by our proposed framework R-MeeTo\nin our main evaluation. We show how simple and effective the fast recovery can\nbe achieved at minute-level, in particular, a 35.9% accuracy spike over 3\nepochs of training on Vim-Ti. Moreover, Vim-Ti/S/B are re-trained within 5/7/17\nminutes, and Vim-S only drops 1.3% with 1.2x (up to 1.5x) speed up in\ninference.\n","authors":["Mingjia Shi","Yuhao Zhou","Ruiji Yu","Zekai Li","Zhiyuan Liang","Xuanlei Zhao","Xiaojiang Peng","Shanmukha Ramakrishna Vedantam","Wangbo Zhao","Kai Wang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2412.12496v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10025v1","updated":"2025-04-14T09:28:10Z","published":"2025-04-14T09:28:10Z","title":"Progressive Transfer Learning for Multi-Pass Fundus Image Restoration","summary":"  Diabetic retinopathy is a leading cause of vision impairment, making its\nearly diagnosis through fundus imaging critical for effective treatment\nplanning. However, the presence of poor quality fundus images caused by factors\nsuch as inadequate illumination, noise, blurring and other motion artifacts\nyields a significant challenge for accurate DR screening. In this study, we\npropose progressive transfer learning for multi pass restoration to iteratively\nenhance the quality of degraded fundus images, ensuring more reliable DR\nscreening. Unlike previous methods that often focus on a single pass\nrestoration, multi pass restoration via PTL can achieve a superior blind\nrestoration performance that can even improve most of the good quality fundus\nimages in the dataset. Initially, a Cycle GAN model is trained to restore low\nquality images, followed by PTL induced restoration passes over the latest\nrestored outputs to improve overall quality in each pass. The proposed method\ncan learn blind restoration without requiring any paired data while surpassing\nits limitations by leveraging progressive learning and fine tuning strategies\nto minimize distortions and preserve critical retinal features. To evaluate\nPTL's effectiveness on multi pass restoration, we conducted experiments on\nDeepDRiD, a large scale fundus imaging dataset specifically curated for\ndiabetic retinopathy detection. Our result demonstrates state of the art\nperformance, showcasing PTL's potential as a superior approach to iterative\nimage quality restoration.\n","authors":["Uyen Phan","Ozer Can Devecioglu","Serkan Kiranyaz","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2504.10025v1.pdf","comment":"13 pages, 12 figures including appendix"},{"id":"http://arxiv.org/abs/2504.10024v1","updated":"2025-04-14T09:28:04Z","published":"2025-04-14T09:28:04Z","title":"Relative Illumination Fields: Learning Medium and Light Independent\n  Underwater Scenes","summary":"  We address the challenge of constructing a consistent and photorealistic\nNeural Radiance Field in inhomogeneously illuminated, scattering environments\nwith unknown, co-moving light sources. While most existing works on underwater\nscene representation focus on a static homogeneous illumination, limited\nattention has been paid to scenarios such as when a robot explores water deeper\nthan a few tens of meters, where sunlight becomes insufficient. To address\nthis, we propose a novel illumination field locally attached to the camera,\nenabling the capture of uneven lighting effects within the viewing frustum. We\ncombine this with a volumetric medium representation to an overall method that\neffectively handles interaction between dynamic illumination field and static\nscattering medium. Evaluation results demonstrate the effectiveness and\nflexibility of our approach.\n","authors":["Mengkun She","Felix Seegräber","David Nakath","Patricia Schöntag","Kevin Köser"],"pdf_url":"https://arxiv.org/pdf/2504.10024v1.pdf","comment":"10 pages, 6 figures. First two authors contributed equally to this\n  work"},{"id":"http://arxiv.org/abs/2504.10021v1","updated":"2025-04-14T09:25:50Z","published":"2025-04-14T09:25:50Z","title":"Masked Autoencoder Self Pre-Training for Defect Detection in\n  Microelectronics","summary":"  Whereas in general computer vision, transformer-based architectures have\nquickly become the gold standard, microelectronics defect detection still\nheavily relies on convolutional neural networks (CNNs). We hypothesize that\nthis is due to the fact that a) transformers have an increased need for data\nand b) labelled image generation procedures for microelectronics are costly,\nand labelled data is therefore sparse. Whereas in other domains, pre-training\non large natural image datasets can mitigate this problem, in microelectronics\ntransfer learning is hindered due to the dissimilarity of domain data and\nnatural images. Therefore, we evaluate self pre-training, where models are\npre-trained on the target dataset, rather than another dataset. We propose a\nvision transformer (ViT) pre-training framework for defect detection in\nmicroelectronics based on masked autoencoders (MAE). In MAE, a large share of\nimage patches is masked and reconstructed by the model during pre-training. We\nperform pre-training and defect detection using a dataset of less than 10.000\nscanning acoustic microscopy (SAM) images labelled using transient thermal\nanalysis (TTA). Our experimental results show that our approach leads to\nsubstantial performance gains compared to a) supervised ViT, b) ViT pre-trained\non natural image datasets, and c) state-of-the-art CNN-based defect detection\nmodels used in the literature. Additionally, interpretability analysis reveals\nthat our self pre-trained models, in comparison to ViT baselines, correctly\nfocus on defect-relevant features such as cracks in the solder material. This\ndemonstrates that our approach yields fault-specific feature representations,\nmaking our self pre-trained models viable for real-world defect detection in\nmicroelectronics.\n","authors":["Nikolai Röhrich","Alwin Hoffmann","Richard Nordsieck","Emilio Zarbali","Alireza Javanmardi"],"pdf_url":"https://arxiv.org/pdf/2504.10021v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.10020v1","updated":"2025-04-14T09:25:37Z","published":"2025-04-14T09:25:37Z","title":"The Mirage of Performance Gains: Why Contrastive Decoding Fails to\n  Address Multimodal Hallucination","summary":"  Contrastive decoding strategies are widely used to reduce hallucinations in\nmultimodal large language models (MLLMs). These methods work by constructing\ncontrastive samples to induce hallucinations and then suppressing them in the\noutput distribution. However, this paper demonstrates that such approaches fail\nto effectively mitigate the hallucination problem. The performance improvements\nobserved on POPE Benchmark are largely driven by two misleading factors: (1)\ncrude, unidirectional adjustments to the model's output distribution and (2)\nthe adaptive plausibility constraint, which reduces the sampling strategy to\ngreedy search. To further illustrate these issues, we introduce a series of\nspurious improvement methods and evaluate their performance against contrastive\ndecoding techniques. Experimental results reveal that the observed performance\ngains in contrastive decoding are entirely unrelated to its intended goal of\nmitigating hallucinations. Our findings challenge common assumptions about the\neffectiveness of contrastive decoding strategies and pave the way for\ndeveloping genuinely effective solutions to hallucinations in MLLMs.\n","authors":["Hao Yin","Gunagzong Si","Zilei Wang"],"pdf_url":"https://arxiv.org/pdf/2504.10020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10018v1","updated":"2025-04-14T09:22:16Z","published":"2025-04-14T09:22:16Z","title":"RGB-Event based Pedestrian Attribute Recognition: A Benchmark Dataset\n  and An Asymmetric RWKV Fusion Framework","summary":"  Existing pedestrian attribute recognition methods are generally developed\nbased on RGB frame cameras. However, these approaches are constrained by the\nlimitations of RGB cameras, such as sensitivity to lighting conditions and\nmotion blur, which hinder their performance. Furthermore, current attribute\nrecognition primarily focuses on analyzing pedestrians' external appearance and\nclothing, lacking an exploration of emotional dimensions. In this paper, we\nrevisit these issues and propose a novel multi-modal RGB-Event attribute\nrecognition task by drawing inspiration from the advantages of event cameras in\nlow-light, high-speed, and low-power consumption. Specifically, we introduce\nthe first large-scale multi-modal pedestrian attribute recognition dataset,\ntermed EventPAR, comprising 100K paired RGB-Event samples that cover 50\nattributes related to both appearance and six human emotions, diverse scenes,\nand various seasons. By retraining and evaluating mainstream PAR models on this\ndataset, we establish a comprehensive benchmark and provide a solid foundation\nfor future research in terms of data and algorithmic baselines. In addition, we\npropose a novel RWKV-based multi-modal pedestrian attribute recognition\nframework, featuring an RWKV visual encoder and an asymmetric RWKV fusion\nmodule. Extensive experiments are conducted on our proposed dataset as well as\ntwo simulated datasets (MARS-Attribute and DukeMTMC-VID-Attribute), achieving\nstate-of-the-art results. The source code and dataset will be released on\nhttps://github.com/Event-AHU/OpenPAR\n","authors":["Xiao Wang","Haiyang Wang","Shiao Wang","Qiang Chen","Jiandong Jin","Haoyu Song","Bo Jiang","Chenglong Li"],"pdf_url":"https://arxiv.org/pdf/2504.10018v1.pdf","comment":"The First Benchmark Dataset for RGB-Event Multimodal Pedestrian\n  Attribute Recognition Task"},{"id":"http://arxiv.org/abs/2504.10014v1","updated":"2025-04-14T09:18:11Z","published":"2025-04-14T09:18:11Z","title":"Air Quality Prediction with A Meteorology-Guided Modality-Decoupled\n  Spatio-Temporal Network","summary":"  Air quality prediction plays a crucial role in public health and\nenvironmental protection. Accurate air quality prediction is a complex\nmultivariate spatiotemporal problem, that involves interactions across temporal\npatterns, pollutant correlations, spatial station dependencies, and\nparticularly meteorological influences that govern pollutant dispersion and\nchemical transformations. Existing works underestimate the critical role of\natmospheric conditions in air quality prediction and neglect comprehensive\nmeteorological data utilization, thereby impairing the modeling of dynamic\ninterdependencies between air quality and meteorological data. To overcome\nthis, we propose MDSTNet, an encoder-decoder framework that explicitly models\nair quality observations and atmospheric conditions as distinct modalities,\nintegrating multi-pressure-level meteorological data and weather forecasts to\ncapture atmosphere-pollution dependencies for prediction. Meantime, we\nconstruct ChinaAirNet, the first nationwide dataset combining air quality\nrecords with multi-pressure-level meteorological observations. Experimental\nresults on ChinaAirNet demonstrate MDSTNet's superiority, substantially\nreducing 48-hour prediction errors by 17.54\\% compared to the state-of-the-art\nmodel. The source code and dataset will be available on github.\n","authors":["Hang Yin","Yan-Ming Zhang","Jian Xu","Jian-Long Chang","Yin Li","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2504.10014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10012v1","updated":"2025-04-14T09:17:00Z","published":"2025-04-14T09:17:00Z","title":"EBAD-Gaussian: Event-driven Bundle Adjusted Deblur Gaussian Splatting","summary":"  While 3D Gaussian Splatting (3D-GS) achieves photorealistic novel view\nsynthesis, its performance degrades with motion blur. In scenarios with rapid\nmotion or low-light conditions, existing RGB-based deblurring methods struggle\nto model camera pose and radiance changes during exposure, reducing\nreconstruction accuracy. Event cameras, capturing continuous brightness changes\nduring exposure, can effectively assist in modeling motion blur and improving\nreconstruction quality. Therefore, we propose Event-driven Bundle Adjusted\nDeblur Gaussian Splatting (EBAD-Gaussian), which reconstructs sharp 3D\nGaussians from event streams and severely blurred images. This method jointly\nlearns the parameters of these Gaussians while recovering camera motion\ntrajectories during exposure time. Specifically, we first construct a blur loss\nfunction by synthesizing multiple latent sharp images during the exposure time,\nminimizing the difference between real and synthesized blurred images. Then we\nuse event stream to supervise the light intensity changes between latent sharp\nimages at any time within the exposure period, supplementing the light\nintensity dynamic changes lost in RGB images. Furthermore, we optimize the\nlatent sharp images at intermediate exposure times based on the event-based\ndouble integral (EDI) prior, applying consistency constraints to enhance the\ndetails and texture information of the reconstructed images. Extensive\nexperiments on synthetic and real-world datasets show that EBAD-Gaussian can\nachieve high-quality 3D scene reconstruction under the condition of blurred\nimages and event stream inputs.\n","authors":["Yufei Deng","Yuanjian Wang","Rong Xiao","Chenwei Tang","Jizhe Zhou","Jiahao Fan","Deng Xiong","Jiancheng Lv","Huajin Tang"],"pdf_url":"https://arxiv.org/pdf/2504.10012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10007v1","updated":"2025-04-14T09:09:01Z","published":"2025-04-14T09:09:01Z","title":"Balancing Two Classifiers via A Simplex ETF Structure for Model\n  Calibration","summary":"  In recent years, deep neural networks (DNNs) have demonstrated\nstate-of-the-art performance across various domains. However, despite their\nsuccess, they often face calibration issues, particularly in safety-critical\napplications such as autonomous driving and healthcare, where unreliable\npredictions can have serious consequences. Recent research has started to\nimprove model calibration from the view of the classifier. However, the\nexploration of designing the classifier to solve the model calibration problem\nis insufficient. Let alone most of the existing methods ignore the calibration\nerrors arising from underconfidence. In this work, we propose a novel method by\nbalancing learnable and ETF classifiers to solve the overconfidence or\nunderconfidence problem for model Calibration named BalCAL. By introducing a\nconfidence-tunable module and a dynamic adjustment method, we ensure better\nalignment between model confidence and its true accuracy. Extensive\nexperimental validation shows that ours significantly improves model\ncalibration performance while maintaining high predictive accuracy,\noutperforming existing techniques. This provides a novel solution to the\ncalibration challenges commonly encountered in deep learning.\n","authors":["Jiani Ni","He Zhao","Jintong Gao","Dandan Guo","Hongyuan Zha"],"pdf_url":"https://arxiv.org/pdf/2504.10007v1.pdf","comment":"CVPR2025"},{"id":"http://arxiv.org/abs/2504.10004v1","updated":"2025-04-14T09:07:11Z","published":"2025-04-14T09:07:11Z","title":"An Image is Worth $K$ Topics: A Visual Structural Topic Model with\n  Pretrained Image Embeddings","summary":"  Political scientists are increasingly interested in analyzing visual content\nat scale. However, the existing computational toolbox is still in need of\nmethods and models attuned to the specific challenges and goals of social and\npolitical inquiry. In this article, we introduce a visual Structural Topic\nModel (vSTM) that combines pretrained image embeddings with a structural topic\nmodel. This has important advantages compared to existing approaches. First,\npretrained embeddings allow the model to capture the semantic complexity of\nimages relevant to political contexts. Second, the structural topic model\nprovides the ability to analyze how topics and covariates are related, while\nmaintaining a nuanced representation of images as a mixture of multiple topics.\nIn our empirical application, we show that the vSTM is able to identify topics\nthat are interpretable, coherent, and substantively relevant to the study of\nonline political communication.\n","authors":["Matías Piqueras","Alexandra Segerberg","Matteo Magnani","Måns Magnusson","Nataša Sladoje"],"pdf_url":"https://arxiv.org/pdf/2504.10004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10003v1","updated":"2025-04-14T09:06:02Z","published":"2025-04-14T09:06:02Z","title":"NaviDiffusor: Cost-Guided Diffusion Model for Visual Navigation","summary":"  Visual navigation, a fundamental challenge in mobile robotics, demands\nversatile policies to handle diverse environments. Classical methods leverage\ngeometric solutions to minimize specific costs, offering adaptability to new\nscenarios but are prone to system errors due to their multi-modular design and\nreliance on hand-crafted rules. Learning-based methods, while achieving high\nplanning success rates, face difficulties in generalizing to unseen\nenvironments beyond the training data and often require extensive training. To\naddress these limitations, we propose a hybrid approach that combines the\nstrengths of learning-based methods and classical approaches for RGB-only\nvisual navigation. Our method first trains a conditional diffusion model on\ndiverse path-RGB observation pairs. During inference, it integrates the\ngradients of differentiable scene-specific and task-level costs, guiding the\ndiffusion model to generate valid paths that meet the constraints. This\napproach alleviates the need for retraining, offering a plug-and-play solution.\nExtensive experiments in both indoor and outdoor settings, across simulated and\nreal-world scenarios, demonstrate zero-shot transfer capability of our\napproach, achieving higher success rates and fewer collisions compared to\nbaseline methods. Code will be released at\nhttps://github.com/SYSU-RoboticsLab/NaviD.\n","authors":["Yiming Zeng","Hao Ren","Shuhang Wang","Junlong Huang","Hui Cheng"],"pdf_url":"https://arxiv.org/pdf/2504.10003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10001v1","updated":"2025-04-14T09:04:01Z","published":"2025-04-14T09:04:01Z","title":"GaussVideoDreamer: 3D Scene Generation with Video Diffusion and\n  Inconsistency-Aware Gaussian Splatting","summary":"  Single-image 3D scene reconstruction presents significant challenges due to\nits inherently ill-posed nature and limited input constraints. Recent advances\nhave explored two promising directions: multiview generative models that train\non 3D consistent datasets but struggle with out-of-distribution generalization,\nand 3D scene inpainting and completion frameworks that suffer from cross-view\ninconsistency and suboptimal error handling, as they depend exclusively on\ndepth data or 3D smoothness, which ultimately degrades output quality and\ncomputational performance. Building upon these approaches, we present\nGaussVideoDreamer, which advances generative multimedia approaches by bridging\nthe gap between image, video, and 3D generation, integrating their strengths\nthrough two key innovations: (1) A progressive video inpainting strategy that\nharnesses temporal coherence for improved multiview consistency and faster\nconvergence. (2) A 3D Gaussian Splatting consistency mask to guide the video\ndiffusion with 3D consistent multiview evidence. Our pipeline combines three\ncore components: a geometry-aware initialization protocol, Inconsistency-Aware\nGaussian Splatting, and a progressive video inpainting strategy. Experimental\nresults demonstrate that our approach achieves 32% higher LLaVA-IQA scores and\nat least 2x speedup compared to existing methods while maintaining robust\nperformance across diverse scenes.\n","authors":["Junlin Hao","Peiheng Wang","Haoyang Wang","Xinggong Zhang","Zongming Guo"],"pdf_url":"https://arxiv.org/pdf/2504.10001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10000v1","updated":"2025-04-14T09:03:51Z","published":"2025-04-14T09:03:51Z","title":"Do We Really Need Curated Malicious Data for Safety Alignment in\n  Multi-modal Large Language Models?","summary":"  Multi-modal large language models (MLLMs) have made significant progress, yet\ntheir safety alignment remains limited. Typically, current open-source MLLMs\nrely on the alignment inherited from their language module to avoid harmful\ngenerations. However, the lack of safety measures specifically designed for\nmulti-modal inputs creates an alignment gap, leaving MLLMs vulnerable to\nvision-domain attacks such as typographic manipulation. Current methods utilize\na carefully designed safety dataset to enhance model defense capability, while\nthe specific knowledge or patterns acquired from the high-quality dataset\nremain unclear. Through comparison experiments, we find that the alignment gap\nprimarily arises from data distribution biases, while image content, response\nquality, or the contrastive behavior of the dataset makes little contribution\nto boosting multi-modal safety. To further investigate this and identify the\nkey factors in improving MLLM safety, we propose finetuning MLLMs on a small\nset of benign instruct-following data with responses replaced by simple, clear\nrejection sentences. Experiments show that, without the need for\nlabor-intensive collection of high-quality malicious data, model safety can\nstill be significantly improved, as long as a specific fraction of rejection\ndata exists in the finetuning set, indicating the security alignment is not\nlost but rather obscured during multi-modal pretraining or instruction\nfinetuning. Simply correcting the underlying data bias could narrow the safety\ngap in the vision domain.\n","authors":["Yanbo Wang","Jiyang Guan","Jian Liang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2504.10000v1.pdf","comment":"Accepted to CVPR 2025, codes in process"},{"id":"http://arxiv.org/abs/2410.24037v2","updated":"2025-04-14T09:02:09Z","published":"2024-10-31T15:34:49Z","title":"TPC: Test-time Procrustes Calibration for Diffusion-based Human Image\n  Animation","summary":"  Human image animation aims to generate a human motion video from the inputs\nof a reference human image and a target motion video. Current diffusion-based\nimage animation systems exhibit high precision in transferring human identity\ninto targeted motion, yet they still exhibit irregular quality in their\noutputs. Their optimal precision is achieved only when the physical\ncompositions (i.e., scale and rotation) of the human shapes in the reference\nimage and target pose frame are aligned. In the absence of such alignment,\nthere is a noticeable decline in fidelity and consistency. Especially, in\nreal-world environments, this compositional misalignment commonly occurs,\nposing significant challenges to the practical usage of current systems. To\nthis end, we propose Test-time Procrustes Calibration (TPC), which enhances the\nrobustness of diffusion-based image animation systems by maintaining optimal\nperformance even when faced with compositional misalignment, effectively\naddressing real-world scenarios. The TPC provides a calibrated reference image\nfor the diffusion model, enhancing its capability to understand the\ncorrespondence between human shapes in the reference and target images. Our\nmethod is simple and can be applied to any diffusion-based image animation\nsystem in a model-agnostic manner, improving the effectiveness at test time\nwithout additional training.\n","authors":["Sunjae Yoon","Gwanhyeong Koo","Younghwan Lee","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2410.24037v2.pdf","comment":"24 pages, 16 figures, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2504.09998v1","updated":"2025-04-14T09:01:49Z","published":"2025-04-14T09:01:49Z","title":"Metric-Guided Synthesis of Class Activation Mapping","summary":"  Class activation mapping (CAM) is a widely adopted class of saliency methods\nused to explain the behavior of convolutional neural networks (CNNs). These\nmethods generate heatmaps that highlight the parts of the input most relevant\nto the CNN output. Various CAM methods have been proposed, each distinguished\nby the expressions used to derive heatmaps. In general, users look for heatmaps\nwith specific properties that reflect different aspects of CNN functionality.\nThese may include similarity to ground truth, robustness, equivariance, and\nmore. Although existing CAM methods implicitly encode some of these properties\nin their expressions, they do not allow for variability in heatmap generation\nfollowing the user's intent or domain knowledge. In this paper, we address this\nlimitation by introducing SyCAM, a metric-based approach for synthesizing CAM\nexpressions. Given a predefined evaluation metric for saliency maps, SyCAM\nautomatically generates CAM expressions optimized for that metric. We\nspecifically explore a syntax-guided synthesis instantiation of SyCAM, where\nCAM expressions are derived based on predefined syntactic constraints and the\ngiven metric. Using several established evaluation metrics, we demonstrate the\nefficacy and flexibility of our approach in generating targeted heatmaps. We\ncompare SyCAM with other well-known CAM methods on three prominent models:\nResNet50, VGG16, and VGG19.\n","authors":["Alejandro Luque-Cerpa","Elizabeth Polgreen","Ajitha Rajan","Hazem Torfah"],"pdf_url":"https://arxiv.org/pdf/2504.09998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09990v1","updated":"2025-04-14T08:52:50Z","published":"2025-04-14T08:52:50Z","title":"Correlative and Discriminative Label Grouping for Multi-Label Visual\n  Prompt Tuning","summary":"  Modeling label correlations has always played a pivotal role in multi-label\nimage classification (MLC), attracting significant attention from researchers.\nHowever, recent studies have overemphasized co-occurrence relationships among\nlabels, which can lead to overfitting risk on this overemphasis, resulting in\nsuboptimal models. To tackle this problem, we advocate for balancing\ncorrelative and discriminative relationships among labels to mitigate the risk\nof overfitting and enhance model performance. To this end, we propose the\nMulti-Label Visual Prompt Tuning framework, a novel and parameter-efficient\nmethod that groups classes into multiple class subsets according to label\nco-occurrence and mutual exclusivity relationships, and then models them\nrespectively to balance the two relationships. In this work, since each group\ncontains multiple classes, multiple prompt tokens are adopted within Vision\nTransformer (ViT) to capture the correlation or discriminative label\nrelationship within each group, and effectively learn correlation or\ndiscriminative representations for class subsets. On the other hand, each group\ncontains multiple group-aware visual representations that may correspond to\nmultiple classes, and the mixture of experts (MoE) model can cleverly assign\nthem from the group-aware to the label-aware, adaptively obtaining label-aware\nrepresentation, which is more conducive to classification. Experiments on\nmultiple benchmark datasets show that our proposed approach achieves\ncompetitive results and outperforms SOTA methods on multiple pre-trained\nmodels.\n","authors":["LeiLei Ma","Shuo Xu","MingKun Xie","Lei Wang","Dengdi Sun","Haifeng Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.09990v1.pdf","comment":"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n  2025"},{"id":"http://arxiv.org/abs/2502.03897v3","updated":"2025-04-14T08:45:19Z","published":"2025-02-06T09:18:30Z","title":"UniForm: A Unified Multi-Task Diffusion Transformer for Audio-Video\n  Generation","summary":"  With the rise of diffusion models, audio-video generation has been\nrevolutionized. However, most existing methods rely on separate modules for\neach modality, with limited exploration of unified generative architectures. In\naddition, many are confined to a single task and small-scale datasets. To\naddress these limitations, we first propose UniForm, a unified multi-task\ndiffusion transformer that jointly generates audio and visual modalities in a\nshared latent space. A single diffusion process models both audio and video,\ncapturing the inherent correlations between sound and vision. Second, we\nintroduce task-specific noise schemes and task tokens, enabling a single model\nto support multiple tasks, including text-to-audio-video, audio-to-video, and\nvideo-to-audio generation. Furthermore, by leveraging large language models and\na large-scale text-audio-video combined dataset, UniForm achieves greater\ngenerative diversity than prior approaches. Extensive experiments show that\nUniForm achieves the state-of-the-art performance across audio-video generation\ntasks, producing content that is both well-aligned and close to real-world data\ndistributions. Our demos are available at https://uniform-t2av.github.io/.\n","authors":["Lei Zhao","Linfeng Feng","Dongxu Ge","Rujin Chen","Fangqiu Yi","Chi Zhang","Xiao-Lei Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2502.03897v3.pdf","comment":"Our demos are available at https://uniform-t2av.github.io/"},{"id":"http://arxiv.org/abs/2408.14358v3","updated":"2025-04-14T08:43:34Z","published":"2024-08-26T15:32:31Z","title":"An Embedding is Worth a Thousand Noisy Labels","summary":"  The performance of deep neural networks scales with dataset size and label\nquality, rendering the efficient mitigation of low-quality data annotations\ncrucial for building robust and cost-effective systems. Existing strategies to\naddress label noise exhibit severe limitations due to computational complexity\nand application dependency. In this work, we propose WANN, a Weighted Adaptive\nNearest Neighbor approach that builds on self-supervised feature\nrepresentations obtained from foundation models. To guide the weighted voting\nscheme, we introduce a reliability score $\\eta$, which measures the likelihood\nof a data label being correct. WANN outperforms reference methods, including a\nlinear layer trained with robust loss functions, on diverse datasets of varying\nsize and under various noise types and severities. WANN also exhibits superior\ngeneralization on imbalanced data compared to both Adaptive-NNs (ANN) and fixed\nk-NNs. Furthermore, the proposed weighting scheme enhances supervised\ndimensionality reduction under noisy labels. This yields a significant boost in\nclassification performance with 10x and 100x smaller image embeddings,\nminimizing latency and storage requirements. Our approach, emphasizing\nefficiency and explainability, emerges as a simple, robust solution to overcome\ninherent limitations of deep neural network training. The code is available at\nhttps://github.com/francescodisalvo05/wann-noisy-labels .\n","authors":["Francesco Di Salvo","Sebastian Doerrich","Ines Rieger","Christian Ledig"],"pdf_url":"https://arxiv.org/pdf/2408.14358v3.pdf","comment":"Accepted to Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2504.09979v1","updated":"2025-04-14T08:43:00Z","published":"2025-04-14T08:43:00Z","title":"Resampling Benchmark for Efficient Comprehensive Evaluation of Large\n  Vision-Language Models","summary":"  We propose an efficient evaluation protocol for large vision-language models\n(VLMs). Given their broad knowledge and reasoning capabilities, multiple\nbenchmarks are needed for comprehensive assessment, making evaluation\ncomputationally expensive. To improve efficiency, we construct a subset that\nyields results comparable to full benchmark evaluations. Our benchmark\nclassification experiments reveal that no single benchmark fully covers all\nchallenges. We then introduce a subset construction method using farthest point\nsampling (FPS). Our experiments show that FPS-based benchmarks maintain a\nstrong correlation (> 0.96) with full evaluations while using only ~1\\% of the\ndata. Additionally, applying FPS to an existing benchmark improves correlation\nwith overall evaluation results, suggesting its potential to reduce unintended\ndataset biases.\n","authors":["Teppei Suzuki","Keisuke Ozawa"],"pdf_url":"https://arxiv.org/pdf/2504.09979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15166v2","updated":"2025-04-14T08:38:46Z","published":"2025-03-19T12:47:37Z","title":"Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive\n  Learning: Adapting Alignment Calibration to MERU","summary":"  Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC\n","authors":["Àlex Pujol Vidal","Sergio Escalera","Kamal Nasrollahi","Thomas B. Moeslund"],"pdf_url":"https://arxiv.org/pdf/2503.15166v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2412.11609v3","updated":"2025-04-14T08:35:10Z","published":"2024-12-16T09:50:09Z","title":"CLIP-SR: Collaborative Linguistic and Image Processing for\n  Super-Resolution","summary":"  Convolutional Neural Networks (CNNs) have significantly advanced Image\nSuper-Resolution (SR), yet most CNN-based methods rely solely on pixel-based\ntransformations, often leading to artifacts and blurring, particularly under\nsevere downsampling rates (\\eg, 8$\\times$ or 16$\\times$). The recently\ndeveloped text-guided SR approaches leverage textual descriptions to enhance\ntheir detail restoration capabilities but frequently struggle with effectively\nperforming alignment, resulting in semantic inconsistencies. To address these\nchallenges, we propose a multi-modal semantic enhancement framework that\nintegrates textual semantics with visual features, effectively mitigating\nsemantic mismatches and detail losses in highly degraded low-resolution (LR)\nimages. Our method enables realistic, high-quality SR to be performed at large\nupscaling factors, with a maximum scaling ratio of 16$\\times$. The framework\nintegrates both text and image inputs using the prompt predictor, the\nText-Image Fusion Block (TIFBlock), and the Iterative Refinement Module,\nleveraging Contrastive Language-Image Pretraining (CLIP) features to guide a\nprogressive enhancement process with fine-grained alignment. This synergy\nproduces high-resolution outputs with sharp textures and strong semantic\ncoherence, even at substantial scaling factors. Extensive comparative\nexperiments and ablation studies validate the effectiveness of our approach.\nFurthermore, by leveraging textual semantics, our method offers a degree of\nsuper-resolution editability, allowing for controlled enhancements while\npreserving semantic consistency. The code is available at\nhttps://github.com/hengliusky/CLIP-SR.\n","authors":["Bingwen Hu","Heng Liu","Zhedong Zheng","Ping Liu"],"pdf_url":"https://arxiv.org/pdf/2412.11609v3.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2504.09975v1","updated":"2025-04-14T08:31:26Z","published":"2025-04-14T08:31:26Z","title":"OctGPT: Octree-based Multiscale Autoregressive Models for 3D Shape\n  Generation","summary":"  Autoregressive models have achieved remarkable success across various\ndomains, yet their performance in 3D shape generation lags significantly behind\nthat of diffusion models. In this paper, we introduce OctGPT, a novel\nmultiscale autoregressive model for 3D shape generation that dramatically\nimproves the efficiency and performance of prior 3D autoregressive approaches,\nwhile rivaling or surpassing state-of-the-art diffusion models. Our method\nemploys a serialized octree representation to efficiently capture the\nhierarchical and spatial structures of 3D shapes. Coarse geometry is encoded\nvia octree structures, while fine-grained details are represented by binary\ntokens generated using a vector quantized variational autoencoder (VQVAE),\ntransforming 3D shapes into compact \\emph{multiscale binary sequences} suitable\nfor autoregressive prediction. To address the computational challenges of\nhandling long sequences, we incorporate octree-based transformers enhanced with\n3D rotary positional encodings, scale-specific embeddings, and token-parallel\ngeneration schemes. These innovations reduce training time by 13 folds and\ngeneration time by 69 folds, enabling the efficient training of high-resolution\n3D shapes, e.g.,$1024^3$, on just four NVIDIA 4090 GPUs only within days.\nOctGPT showcases exceptional versatility across various tasks, including text-,\nsketch-, and image-conditioned generation, as well as scene-level synthesis\ninvolving multiple objects. Extensive experiments demonstrate that OctGPT\naccelerates convergence and improves generation quality over prior\nautoregressive methods, offering a new paradigm for high-quality, scalable 3D\ncontent creation.\n","authors":["Si-Tong Wei","Rui-Huan Wang","Chuan-Zhi Zhou","Baoquan Chen","Peng-Shuai Wang"],"pdf_url":"https://arxiv.org/pdf/2504.09975v1.pdf","comment":"SIGGRAPH 2025"},{"id":"http://arxiv.org/abs/2504.09973v1","updated":"2025-04-14T08:24:57Z","published":"2025-04-14T08:24:57Z","title":"Beyond Degradation Redundancy: Contrastive Prompt Learning for\n  All-in-One Image Restoration","summary":"  All-in-one image restoration, addressing diverse degradation types with a\nunified model, presents significant challenges in designing task-specific\nprompts that effectively guide restoration across multiple degradation\nscenarios. While adaptive prompt learning enables end-to-end optimization, it\noften yields overlapping or redundant task representations. Conversely,\nexplicit prompts derived from pretrained classifiers enhance discriminability\nbut may discard critical visual information for reconstruction. To address\nthese limitations, we introduce Contrastive Prompt Learning (CPL), a novel\nframework that fundamentally enhances prompt-task alignment through two\ncomplementary innovations: a \\emph{Sparse Prompt Module (SPM)} that efficiently\ncaptures degradation-specific features while minimizing redundancy, and a\n\\emph{Contrastive Prompt Regularization (CPR)} that explicitly strengthens task\nboundaries by incorporating negative prompt samples across different\ndegradation types. Unlike previous approaches that focus primarily on\ndegradation classification, CPL optimizes the critical interaction between\nprompts and the restoration model itself. Extensive experiments across five\ncomprehensive benchmarks demonstrate that CPL consistently enhances\nstate-of-the-art all-in-one restoration models, achieving significant\nimprovements in both standard multi-task scenarios and challenging composite\ndegradation settings. Our framework establishes new state-of-the-art\nperformance while maintaining parameter efficiency, offering a principled\nsolution for unified image restoration.\n","authors":["Gang Wu","Junjun Jiang","Kui Jiang","Xianming Liu","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2504.09973v1.pdf","comment":"Project page: https://github.com/Aitical/CPLIR"},{"id":"http://arxiv.org/abs/2409.09386v2","updated":"2025-04-14T08:21:38Z","published":"2024-09-14T09:34:05Z","title":"AMBER -- Advanced SegFormer for Multi-Band Image Segmentation: an\n  application to Hyperspectral Imaging","summary":"  Deep learning has revolutionized the field of hyperspectral image (HSI)\nanalysis, enabling the extraction of complex spectral and spatial features.\nWhile convolutional neural networks (CNNs) have been the backbone of HSI\nclassification, their limitations in capturing global contextual features have\nled to the exploration of Vision Transformers (ViTs). This paper introduces\nAMBER, an advanced SegFormer specifically designed for multi-band image\nsegmentation. AMBER enhances the original SegFormer by incorporating\nthree-dimensional convolutions, custom kernel sizes, and a Funnelizer layer.\nThis architecture enables processing hyperspectral data directly, without\nrequiring spectral dimensionality reduction during preprocessing. Our\nexperiments, conducted on three benchmark datasets (Salinas, Indian Pines, and\nPavia University) and on a dataset from the PRISMA satellite, show that AMBER\noutperforms traditional CNN-based methods in terms of Overall Accuracy, Kappa\ncoefficient, and Average Accuracy on the first three datasets, and achieves\nstate-of-the-art performance on the PRISMA dataset. These findings highlight\nAMBER's robustness, adaptability to both airborne and spaceborne data, and its\npotential as a powerful solution for remote sensing and other domains requiring\nadvanced analysis of high-dimensional data.\n","authors":["Andrea Dosi","Massimo Brescia","Stefano Cavuoti","Mariarca D'Aniello","Michele Delli Veneri","Carlo Donadio","Adriano Ettari","Giuseppe Longo","Alvi Rownok","Luca Sannino","Maria Zampella"],"pdf_url":"https://arxiv.org/pdf/2409.09386v2.pdf","comment":"submitted to Neural Computing & Applications (Springer). Accepted\n  with minor revisions"},{"id":"http://arxiv.org/abs/2406.01867v4","updated":"2025-04-14T08:11:25Z","published":"2024-06-04T00:38:44Z","title":"MoLA: Motion Generation and Editing with Latent Diffusion Enhanced by\n  Adversarial Training","summary":"  In text-to-motion generation, controllability as well as generation quality\nand speed has become increasingly critical. The controllability challenges\ninclude generating a motion of a length that matches the given textual\ndescription and editing the generated motions according to control signals,\nsuch as the start-end positions and the pelvis trajectory. In this paper, we\npropose MoLA, which provides fast, high-quality, variable-length motion\ngeneration and can also deal with multiple editing tasks in a single framework.\nOur approach revisits the motion representation used as inputs and outputs in\nthe model, incorporating an activation variable to enable variable-length\nmotion generation. Additionally, we integrate a variational autoencoder and a\nlatent diffusion model, further enhanced through adversarial training, to\nachieve high-quality and fast generation. Moreover, we apply a training-free\nguided generation framework to achieve various editing tasks with motion\ncontrol inputs. We quantitatively show the effectiveness of adversarial\nlearning in text-to-motion generation, and demonstrate the applicability of our\nediting framework to multiple editing tasks in the motion domain.\n","authors":["Kengo Uchida","Takashi Shibuya","Yuhta Takida","Naoki Murata","Julian Tanke","Shusuke Takahashi","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2406.01867v4.pdf","comment":"CVPR 2025 HuMoGen Workshop"},{"id":"http://arxiv.org/abs/2406.14510v2","updated":"2025-04-14T08:10:45Z","published":"2024-06-20T17:14:43Z","title":"V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data","summary":"  Diffusion-based generative models have recently shown remarkable image and\nvideo editing capabilities. However, local video editing, particularly removal\nof small attributes like glasses, remains a challenge. Existing methods either\nalter the videos excessively, generate unrealistic artifacts, or fail to\nperform the requested edit consistently throughout the video. In this work, we\nfocus on consistent and identity-preserving removal of glasses in videos, using\nit as a case study for consistent local attribute removal in videos. Due to the\nlack of paired data, we adopt a weakly supervised approach and generate\nsynthetic imperfect data, using an adjusted pretrained diffusion model. We show\nthat despite data imperfection, by learning from our generated data and\nleveraging the prior of pretrained diffusion models, our model is able to\nperform the desired edit consistently while preserving the original video\ncontent. Furthermore, we exemplify the generalization ability of our method to\nother local video editing tasks by applying it successfully to facial\nsticker-removal. Our approach demonstrates significant improvement over\nexisting methods, showcasing the potential of leveraging synthetic data and\nstrong video priors for local video editing tasks.\n","authors":["Rotem Shalev-Arkushin","Aharon Azulay","Tavi Halperin","Eitan Richardson","Amit H. Bermano","Ohad Fried"],"pdf_url":"https://arxiv.org/pdf/2406.14510v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09967v1","updated":"2025-04-14T08:09:37Z","published":"2025-04-14T08:09:37Z","title":"Enhancing Multi-task Learning Capability of Medical Generalist\n  Foundation Model via Image-centric Multi-annotation Data","summary":"  The emergence of medical generalist foundation models has revolutionized\nconventional task-specific model development paradigms, aiming to better handle\nmultiple tasks through joint training on large-scale medical datasets. However,\nrecent advances prioritize simple data scaling or architectural component\nenhancement, while neglecting to re-examine multi-task learning from a\ndata-centric perspective. Critically, simply aggregating existing data\nresources leads to decentralized image-task alignment, which fails to cultivate\ncomprehensive image understanding or align with clinical needs for\nmulti-dimensional image interpretation. In this paper, we introduce the\nimage-centric multi-annotation X-ray dataset (IMAX), the first attempt to\nenhance the multi-task learning capabilities of medical multi-modal large\nlanguage models (MLLMs) from the data construction level. To be specific, IMAX\nis featured from the following attributes: 1) High-quality data curation. A\ncomprehensive collection of more than 354K entries applicable to seven\ndifferent medical tasks. 2) Image-centric dense annotation. Each X-ray image is\nassociated with an average of 4.10 tasks and 7.46 training entries, ensuring\nmulti-task representation richness per image. Compared to the general\ndecentralized multi-annotation X-ray dataset (DMAX), IMAX consistently\ndemonstrates significant multi-task average performance gains ranging from\n3.20% to 21.05% across seven open-source state-of-the-art medical MLLMs.\nMoreover, we investigate differences in statistical patterns exhibited by IMAX\nand DMAX training processes, exploring potential correlations between\noptimization dynamics and multi-task performance. Finally, leveraging the core\nconcept of IMAX data construction, we propose an optimized DMAX-based training\nstrategy to alleviate the dilemma of obtaining high-quality IMAX data in\npractical scenarios.\n","authors":["Xun Zhu","Fanbin Mo","Zheng Zhang","Jiaxi Wang","Yiming Shi","Ming Wu","Chuang Zhang","Miao Li","Ji Wu"],"pdf_url":"https://arxiv.org/pdf/2504.09967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09966v1","updated":"2025-04-14T08:09:17Z","published":"2025-04-14T08:09:17Z","title":"SemiETS: Integrating Spatial and Content Consistencies for\n  Semi-Supervised End-to-end Text Spotting","summary":"  Most previous scene text spotting methods rely on high-quality manual\nannotations to achieve promising performance. To reduce their expensive costs,\nwe study semi-supervised text spotting (SSTS) to exploit useful information\nfrom unlabeled images. However, directly applying existing semi-supervised\nmethods of general scenes to SSTS will face new challenges: 1) inconsistent\npseudo labels between detection and recognition tasks, and 2) sub-optimal\nsupervisions caused by inconsistency between teacher/student. Thus, we propose\na new Semi-supervised framework for End-to-end Text Spotting, namely SemiETS\nthat leverages the complementarity of text detection and recognition.\nSpecifically, it gradually generates reliable hierarchical pseudo labels for\neach task, thereby reducing noisy labels. Meanwhile, it extracts important\ninformation in locations and transcriptions from bidirectional flows to improve\nconsistency. Extensive experiments on three datasets under various settings\ndemonstrate the effectiveness of SemiETS on arbitrary-shaped text. For example,\nit outperforms previous state-of-the-art SSL methods by a large margin on\nend-to-end spotting (+8.7%, +5.6%, and +2.6% H-mean under 0.5%, 1%, and 2%\nlabeled data settings on Total-Text, respectively). More importantly, it still\nimproves upon a strongly supervised text spotter trained with plenty of labeled\ndata by 2.0%. Compelling domain adaptation ability shows practical potential.\nMoreover, our method demonstrates consistent improvement on different text\nspotters.\n","authors":["Dongliang Luo","Hanshen Zhu","Ziyang Zhang","Dingkang Liang","Xudong Xie","Yuliang Liu","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2504.09966v1.pdf","comment":"Accepted by CVPR2025. Code will be available at\n  \\url{https://github.com/DrLuo/SemiETS}"},{"id":"http://arxiv.org/abs/2504.08593v2","updated":"2025-04-14T08:07:48Z","published":"2025-04-11T14:52:59Z","title":"Hands-On: Segmenting Individual Signs from Continuous Sequences","summary":"  This work tackles the challenge of continuous sign language segmentation, a\nkey task with huge implications for sign language translation and data\nannotation. We propose a transformer-based architecture that models the\ntemporal dynamics of signing and frames segmentation as a sequence labeling\nproblem using the Begin-In-Out (BIO) tagging scheme. Our method leverages the\nHaMeR hand features, and is complemented with 3D Angles. Extensive experiments\nshow that our model achieves state-of-the-art results on the DGS Corpus, while\nour features surpass prior benchmarks on BSLCorpus.\n","authors":["Low Jian He","Harry Walsh","Ozge Mercanoglu Sincan","Richard Bowden"],"pdf_url":"https://arxiv.org/pdf/2504.08593v2.pdf","comment":"Accepted in the 19th IEEE International Conference on Automatic Face\n  and Gesture Recognition"},{"id":"http://arxiv.org/abs/2504.09960v1","updated":"2025-04-14T07:57:22Z","published":"2025-04-14T07:57:22Z","title":"Dual-Path Enhancements in Event-Based Eye Tracking: Augmented Robustness\n  and Adaptive Temporal Modeling","summary":"  Event-based eye tracking has become a pivotal technology for augmented\nreality and human-computer interaction. Yet, existing methods struggle with\nreal-world challenges such as abrupt eye movements and environmental noise.\nBuilding on the efficiency of the Lightweight Spatiotemporal Network-a causal\narchitecture optimized for edge devices-we introduce two key advancements.\nFirst, a robust data augmentation pipeline incorporating temporal shift,\nspatial flip, and event deletion improves model resilience, reducing Euclidean\ndistance error by 12% (1.61 vs. 1.70 baseline) on challenging samples. Second,\nwe propose KnightPupil, a hybrid architecture combining an EfficientNet-B3\nbackbone for spatial feature extraction, a bidirectional GRU for contextual\ntemporal modeling, and a Linear Time-Varying State-Space Module to adapt to\nsparse inputs and noise dynamically. Evaluated on the 3ET+ benchmark, our\nframework achieved 1.61 Euclidean distance on the private test set of the\nEvent-based Eye Tracking Challenge at CVPR 2025, demonstrating its\neffectiveness for practical deployment in AR/VR systems while providing a\nfoundation for future innovations in neuromorphic vision.\n","authors":["Hoang M. Truong","Vinh-Thuan Ly","Huy G. Tran","Thuan-Phat Nguyen","Tram T. Doan"],"pdf_url":"https://arxiv.org/pdf/2504.09960v1.pdf","comment":"Camera-ready version for CVPRW 2025. Accepted for presentation at the\n  IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops\n  (CVPRW 2025)"},{"id":"http://arxiv.org/abs/2504.09956v1","updated":"2025-04-14T07:44:34Z","published":"2025-04-14T07:44:34Z","title":"Semantic Depth Matters: Explaining Errors of Deep Vision Networks\n  through Perceived Class Similarities","summary":"  Understanding deep neural network (DNN) behavior requires more than\nevaluating classification accuracy alone; analyzing errors and their\npredictability is equally crucial. Current evaluation methodologies lack\ntransparency, particularly in explaining the underlying causes of network\nmisclassifications. To address this, we introduce a novel framework that\ninvestigates the relationship between the semantic hierarchy depth perceived by\na network and its real-data misclassification patterns. Central to our\nframework is the Similarity Depth (SD) metric, which quantifies the semantic\nhierarchy depth perceived by a network along with a method of evaluation of how\nclosely the network's errors align with its internally perceived similarity\nstructure. We also propose a graph-based visualization of model semantic\nrelationships and misperceptions. A key advantage of our approach is that\nleveraging class templates -- representations derived from classifier layer\nweights -- is applicable to already trained networks without requiring\nadditional data or experiments. Our approach reveals that deep vision networks\nencode specific semantic hierarchies and that high semantic depth improves the\ncompliance between perceived class similarities and actual errors.\n","authors":["Katarzyna Filus","Michał Romaszewski","Mateusz Żarski"],"pdf_url":"https://arxiv.org/pdf/2504.09956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20021v4","updated":"2025-04-14T07:42:52Z","published":"2024-07-29T13:57:40Z","title":"MimiQ: Low-Bit Data-Free Quantization of Vision Transformers with\n  Encouraging Inter-Head Attention Similarity","summary":"  Data-free quantization (DFQ) is a technique that creates a lightweight\nnetwork from its full-precision counterpart without the original training data,\noften through a synthetic dataset. Although several DFQ methods have been\nproposed for vision transformer (ViT) architectures, they fail to achieve\nefficacy in low-bit settings. Examining the existing methods, we observe that\ntheir synthetic data produce misaligned attention maps, while those of the real\nsamples are highly aligned. From this observation, we find that aligning\nattention maps of synthetic data helps improve the overall performance of\nquantized ViTs. Motivated by this finding, we devise MimiQ, a novel DFQ method\ndesigned for ViTs that enhances inter-head attention similarity. First, we\ngenerate synthetic data by aligning head-wise attention outputs from each\nspatial query patch. Then, we align the attention maps of the quantized network\nto those of the full-precision teacher by applying head-wise structural\nattention distillation. The experimental results show that the proposed method\nsignificantly outperforms baselines, setting a new state-of-the-art for\nViT-DFQ. This paper is an extended version of our work published in the\nproceedings of AAAI 2025, including additional supplementary material.\n","authors":["Kanghyun Choi","Hye Yoon Lee","Dain Kwon","SunJong Park","Kyuyeun Kim","Noseong Park","Jonghyun Choi","Jinho Lee"],"pdf_url":"https://arxiv.org/pdf/2407.20021v4.pdf","comment":"Published to AAAI 2025"},{"id":"http://arxiv.org/abs/2409.17671v4","updated":"2025-04-14T07:36:54Z","published":"2024-09-26T09:30:37Z","title":"Leveraging Anthropometric Measurements to Improve Human Mesh Estimation\n  and Ensure Consistent Body Shapes","summary":"  The basic body shape (i.e., the body shape in T-pose) of a person does not\nchange within a single video. However, most SOTA human mesh estimation (HME)\nmodels output a slightly different, thus inconsistent basic body shape for each\nvideo frame. Furthermore, we find that SOTA 3D human pose estimation (HPE)\nmodels outperform HME models regarding the precision of the estimated 3D\nkeypoint positions. We solve the problem of inconsistent body shapes by\nleveraging anthropometric measurements like taken by tailors from humans. We\ncreate a model called A2B that converts given anthropometric measurements to\nbasic body shape parameters of human mesh models. We obtain superior and\nconsistent human meshes by combining the A2B model results with the keypoints\nof 3D HPE models using inverse kinematics. We evaluate our approach on\nchallenging datasets like ASPset or fit3D, where we can lower the MPJPE by over\n30 mm compared to SOTA HME models. Further, replacing estimates of the body\nshape parameters from existing HME models with A2B results not only increases\nthe performance of these HME models, but also guarantees consistent body\nshapes.\n","authors":["Katja Ludwig","Julian Lorenz","Daniel Kienzle","Tuan Bui","Rainer Lienhart"],"pdf_url":"https://arxiv.org/pdf/2409.17671v4.pdf","comment":"accepted for CVSports@CVPR'25"},{"id":"http://arxiv.org/abs/2504.09953v1","updated":"2025-04-14T07:32:28Z","published":"2025-04-14T07:32:28Z","title":"Efficient 2D to Full 3D Human Pose Uplifting including Joint Rotations","summary":"  In sports analytics, accurately capturing both the 3D locations and rotations\nof body joints is essential for understanding an athlete's biomechanics. While\nHuman Mesh Recovery (HMR) models can estimate joint rotations, they often\nexhibit lower accuracy in joint localization compared to 3D Human Pose\nEstimation (HPE) models. Recent work addressed this limitation by combining a\n3D HPE model with inverse kinematics (IK) to estimate both joint locations and\nrotations. However, IK is computationally expensive. To overcome this, we\npropose a novel 2D-to-3D uplifting model that directly estimates 3D human\nposes, including joint rotations, in a single forward pass. We investigate\nmultiple rotation representations, loss functions, and training strategies -\nboth with and without access to ground truth rotations. Our models achieve\nstate-of-the-art accuracy in rotation estimation, are 150 times faster than the\nIK-based approach, and surpass HMR models in joint localization precision.\n","authors":["Katja Ludwig","Yuliia Oksymets","Robin Schön","Daniel Kienzle","Rainer Lienhart"],"pdf_url":"https://arxiv.org/pdf/2504.09953v1.pdf","comment":"accepted at CVSports@CVPR'25"},{"id":"http://arxiv.org/abs/2504.09949v1","updated":"2025-04-14T07:24:03Z","published":"2025-04-14T07:24:03Z","title":"Pseudo-Label Guided Real-World Image De-weathering: A Learning Framework\n  with Imperfect Supervision","summary":"  Real-world image de-weathering aims at removingvarious undesirable\nweather-related artifacts, e.g., rain, snow,and fog. To this end, acquiring\nideal training pairs is crucial.Existing real-world datasets are typically\nconstructed paired databy extracting clean and degraded images from live\nstreamsof landscape scene on the Internet. Despite the use of strictfiltering\nmechanisms during collection, training pairs inevitablyencounter inconsistency\nin terms of lighting, object position, scenedetails, etc, making de-weathering\nmodels possibly suffer fromdeformation artifacts under non-ideal supervision.\nIn this work,we propose a unified solution for real-world image\nde-weatheringwith non-ideal supervision, i.e., a pseudo-label guided\nlearningframework, to address various inconsistencies within the realworld\npaired dataset. Generally, it consists of a de-weatheringmodel (De-W) and a\nConsistent Label Constructor (CLC), bywhich restoration result can be\nadaptively supervised by originalground-truth image to recover sharp textures\nwhile maintainingconsistency with the degraded inputs in non-weather\ncontentthrough the supervision of pseudo-labels. Particularly, a Crossframe\nSimilarity Aggregation (CSA) module is deployed withinCLC to enhance the\nquality of pseudo-labels by exploring thepotential complementary information of\nmulti-frames throughgraph model. Moreover, we introduce an Information\nAllocationStrategy (IAS) to integrate the original ground-truth imagesand\npseudo-labels, thereby facilitating the joint supervision forthe training of\nde-weathering model. Extensive experimentsdemonstrate that our method exhibits\nsignificant advantageswhen trained on imperfectly aligned de-weathering\ndatasets incomparison with other approaches.\n","authors":["Heming Xu","Xiaohui Liu","Zhilu Zhang","Hongzhi Zhang","Xiaohe Wu","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2504.09949v1.pdf","comment":"15 pages, 16 figures"},{"id":"http://arxiv.org/abs/2308.06985v2","updated":"2025-04-14T07:22:23Z","published":"2023-08-14T07:45:54Z","title":"PatchContrast: Self-Supervised Pre-training for 3D Object Detection","summary":"  Accurately detecting objects in the environment is a key challenge for\nautonomous vehicles. However, obtaining annotated data for detection is\nexpensive and time-consuming. We introduce PatchContrast, a novel\nself-supervised point cloud pre-training framework for 3D object detection. We\npropose to utilize two levels of abstraction to learn discriminative\nrepresentation from unlabeled data: proposal-level and patch-level. The\nproposal-level aims at localizing objects in relation to their surroundings,\nwhereas the patch-level adds information about the internal connections between\nthe object's components, hence distinguishing between different objects based\non their individual components. We demonstrate how these levels can be\nintegrated into self-supervised pre-training for various backbones to enhance\nthe downstream 3D detection task. We show that our method outperforms existing\nstate-of-the-art models on three commonly-used 3D detection datasets.\n","authors":["Oren Shrout","Ori Nizan","Yizhak Ben-Shabat","Ayellet Tal"],"pdf_url":"https://arxiv.org/pdf/2308.06985v2.pdf","comment":"CVPRW 2025"},{"id":"http://arxiv.org/abs/2504.09948v1","updated":"2025-04-14T07:18:32Z","published":"2025-04-14T07:18:32Z","title":"Omni-Dish: Photorealistic and Faithful Image Generation and Editing for\n  Arbitrary Chinese Dishes","summary":"  Dish images play a crucial role in the digital era, with the demand for\nculturally distinctive dish images continuously increasing due to the\ndigitization of the food industry and e-commerce. In general cases, existing\ntext-to-image generation models excel in producing high-quality images;\nhowever, they struggle to capture diverse characteristics and faithful details\nof specific domains, particularly Chinese dishes. To address this limitation,\nwe propose Omni-Dish, the first text-to-image generation model specifically\ntailored for Chinese dishes. We develop a comprehensive dish curation pipeline,\nbuilding the largest dish dataset to date. Additionally, we introduce a\nrecaption strategy and employ a coarse-to-fine training scheme to help the\nmodel better learn fine-grained culinary nuances. During inference, we enhance\nthe user's textual input using a pre-constructed high-quality caption library\nand a large language model, enabling more photorealistic and faithful image\ngeneration. Furthermore, to extend our model's capability for dish editing\ntasks, we propose Concept-Enhanced P2P. Based on this approach, we build a dish\nediting dataset and train a specialized editing model. Extensive experiments\ndemonstrate the superiority of our methods.\n","authors":["Huijie Liu","Bingcan Wang","Jie Hu","Xiaoming Wei","Guoliang Kang"],"pdf_url":"https://arxiv.org/pdf/2504.09948v1.pdf","comment":"10 pages, 10 figures, 3 tables"},{"id":"http://arxiv.org/abs/2211.13613v3","updated":"2025-04-14T06:52:30Z","published":"2022-11-24T13:59:32Z","title":"Ham2Pose: Animating Sign Language Notation into Pose Sequences","summary":"  Translating spoken languages into Sign languages is necessary for open\ncommunication between the hearing and hearing-impaired communities. To achieve\nthis goal, we propose the first method for animating a text written in\nHamNoSys, a lexical Sign language notation, into signed pose sequences. As\nHamNoSys is universal, our proposed method offers a generic solution invariant\nto the target Sign language. Our method gradually generates pose predictions\nusing transformer encoders that create meaningful representations of the text\nand poses while considering their spatial and temporal information. We use weak\nsupervision for the training process and show that our method succeeds in\nlearning from partial and inaccurate data. Additionally, we offer a new\ndistance measurement for pose sequences, normalized Dynamic Time Warping\n(nDTW), based on DTW over normalized keypoints trajectories, and validate its\ncorrectness using AUTSL, a large-scale Sign language dataset. We show that it\nmeasures the distance between pose sequences more accurately than existing\nmeasurements and use it to assess the quality of our generated pose sequences.\nCode for the data pre-processing, the model, and the distance measurement is\npublicly released for future research.\n","authors":["Rotem Shalev-Arkushin","Amit Moryossef","Ohad Fried"],"pdf_url":"https://arxiv.org/pdf/2211.13613v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09925v1","updated":"2025-04-14T06:33:29Z","published":"2025-04-14T06:33:29Z","title":"FUSION: Fully Integration of Vision-Language Representations for Deep\n  Cross-Modal Understanding","summary":"  We introduce FUSION, a family of multimodal large language models (MLLMs)\nwith a fully vision-language alignment and integration paradigm. Unlike\nexisting methods that primarily rely on late-stage modality interaction during\nLLM decoding, our approach achieves deep, dynamic integration throughout the\nentire processing pipeline. To this end, we propose Text-Guided Unified Vision\nEncoding, incorporating textual information in vision encoding to achieve\npixel-level integration. We further design Context-Aware Recursive Alignment\nDecoding that recursively aggregates visual features conditioned on textual\ncontext during decoding, enabling fine-grained, question-level semantic\nintegration. To guide feature mapping and mitigate modality discrepancies, we\ndevelop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a\nSynthesized Language-Driven Question-Answer (QA) dataset through a new data\nsynthesis method, prioritizing high-quality QA pairs to optimize text-guided\nfeature integration. Building on these foundations, we train FUSION at two\nscales-3B, 8B-and demonstrate that our full-modality integration approach\nsignificantly outperforms existing methods with only 630 vision tokens.\nNotably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most\nbenchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited\nto 300 vision tokens. Our ablation studies show that FUSION outperforms\nLLaVA-NeXT on over half of the benchmarks under same configuration without\ndynamic resolution, highlighting the effectiveness of our approach. We release\nour code, model weights, and dataset. https://github.com/starriver030515/FUSION\n","authors":["Zheng Liu","Mengjie Liu","Jingzhou Chen","Jingwei Xu","Bin Cui","Conghui He","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.09925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09914v1","updated":"2025-04-14T06:23:44Z","published":"2025-04-14T06:23:44Z","title":"Improving Multimodal Hateful Meme Detection Exploiting LMM-Generated\n  Knowledge","summary":"  Memes have become a dominant form of communication in social media in recent\nyears. Memes are typically humorous and harmless, however there are also memes\nthat promote hate speech, being in this way harmful to individuals and groups\nbased on their identity. Therefore, detecting hateful content in memes has\nemerged as a task of critical importance. The need for understanding the\ncomplex interactions of images and their embedded text renders the hateful meme\ndetection a challenging multimodal task. In this paper we propose to address\nthe aforementioned task leveraging knowledge encoded in powerful Large\nMultimodal Models (LMM). Specifically, we propose to exploit LMMs in a two-fold\nmanner. First, by extracting knowledge oriented to the hateful meme detection\ntask in order to build strong meme representations. Specifically, generic\nsemantic descriptions and emotions that the images along with their embedded\ntexts elicit are extracted, which are then used to train a simple\nclassification head for hateful meme detection. Second, by developing a novel\nhard mining approach introducing directly LMM-encoded knowledge to the training\nprocess, providing further improvements. We perform extensive experiments on\ntwo datasets that validate the effectiveness of the proposed method, achieving\nstate-of-the-art performance. Our code and trained models are publicly\navailable at: https://github.com/IDT-ITI/LMM-CLIP-meme.\n","authors":["Maria Tzelepi","Vasileios Mezaris"],"pdf_url":"https://arxiv.org/pdf/2504.09914v1.pdf","comment":"Accepted for publication, Multimodal Learning and Applications\n  Workshop (MULA 2025) @ IEEE/CVF CVPR 2025, Nashville, TN, USA, June 2025.\n  This is the authors' \"accepted version\""},{"id":"http://arxiv.org/abs/2310.13019v5","updated":"2025-04-14T06:22:56Z","published":"2023-10-18T18:50:39Z","title":"Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class\n  Manipulation Using DeepFool Algorithm","summary":"  The susceptibility of deep neural networks (DNNs) to adversarial attacks\nundermines their reliability across numerous applications, underscoring the\nnecessity for an in-depth exploration of these vulnerabilities and the\nformulation of robust defense strategies. The DeepFool algorithm by\nMoosavi-Dezfooli et al. (2016) represents a pivotal step in identifying minimal\nperturbations required to induce misclassification of input images.\nNonetheless, its generic methodology falls short in scenarios necessitating\ntargeted interventions. Additionally, previous research studies have\npredominantly concentrated on the success rate of attacks without adequately\naddressing the consequential distortion of images, the maintenance of image\nquality, or the confidence threshold required for misclassification. To bridge\nthese gaps, we introduce the Enhanced Targeted DeepFool (ET DeepFool)\nalgorithm, an evolution of DeepFool that not only facilitates the specification\nof desired misclassification targets but also incorporates a configurable\nminimum confidence score. Our empirical investigations demonstrate the\nsuperiority of this refined approach in maintaining the integrity of images and\nminimizing perturbations across a variety of DNN architectures. Unlike previous\niterations, such as the Targeted DeepFool by Gajjar et al. (2022), our method\ngrants unparalleled control over the perturbation process, enabling precise\nmanipulation of model responses. Preliminary outcomes reveal that certain\nmodels, including AlexNet and the advanced Vision Transformer, display\ncommendable robustness to such manipulations. This discovery of varying levels\nof model robustness, as unveiled through our confidence level adjustments,\ncould have far-reaching implications for the field of image recognition. Our\ncode is available at https://github.com/FazleLabib/et_deepfool.\n","authors":["S. M. Fazle Rabby Labib","Joyanta Jyoti Mondal","Meem Arafat Manab","Xi Xiao","Sarfaraz Newaz"],"pdf_url":"https://arxiv.org/pdf/2310.13019v5.pdf","comment":"18 pages, 5 figures. Accepted by Nature Scientific Reports"},{"id":"http://arxiv.org/abs/2504.09904v1","updated":"2025-04-14T05:53:57Z","published":"2025-04-14T05:53:57Z","title":"LiteTracker: Leveraging Temporal Causality for Accurate Low-latency\n  Tissue Tracking","summary":"  Tissue tracking plays a critical role in various surgical navigation and\nextended reality (XR) applications. While current methods trained on large\nsynthetic datasets achieve high tracking accuracy and generalize well to\nendoscopic scenes, their runtime performances fail to meet the low-latency\nrequirements necessary for real-time surgical applications. To address this\nlimitation, we propose LiteTracker, a low-latency method for tissue tracking in\nendoscopic video streams. LiteTracker builds on a state-of-the-art long-term\npoint tracking method, and introduces a set of training-free runtime\noptimizations. These optimizations enable online, frame-by-frame tracking by\nleveraging a temporal memory buffer for efficient feature reuse and utilizing\nprior motion for accurate track initialization. LiteTracker demonstrates\nsignificant runtime improvements being around 7x faster than its predecessor\nand 2x than the state-of-the-art. Beyond its primary focus on efficiency,\nLiteTracker delivers high-accuracy tracking and occlusion prediction,\nperforming competitively on both the STIR and SuPer datasets. We believe\nLiteTracker is an important step toward low-latency tissue tracking for\nreal-time surgical applications in the operating room.\n","authors":["Mert Asim Karaoglu","Wenbo Ji","Ahmed Abbas","Nassir Navab","Benjamin Busam","Alexander Ladikos"],"pdf_url":"https://arxiv.org/pdf/2504.09904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09900v1","updated":"2025-04-14T05:49:31Z","published":"2025-04-14T05:49:31Z","title":"Small Object Detection with YOLO: A Performance Analysis Across Model\n  Versions and Hardware","summary":"  This paper provides an extensive evaluation of YOLO object detection models\n(v5, v8, v9, v10, v11) by com- paring their performance across various hardware\nplatforms and optimization libraries. Our study investigates inference speed\nand detection accuracy on Intel and AMD CPUs using popular libraries such as\nONNX and OpenVINO, as well as on GPUs through TensorRT and other GPU-optimized\nframeworks. Furthermore, we analyze the sensitivity of these YOLO models to\nobject size within the image, examining performance when detecting objects that\noccupy 1%, 2.5%, and 5% of the total area of the image. By identifying the\ntrade-offs in efficiency, accuracy, and object size adaptability, this paper\noffers insights for optimal model selection based on specific hardware\nconstraints and detection requirements, aiding practitioners in deploying YOLO\nmodels effectively for real-world applications.\n","authors":["Muhammad Fasih Tariq","Muhammad Azeem Javed"],"pdf_url":"https://arxiv.org/pdf/2504.09900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09899v1","updated":"2025-04-14T05:48:05Z","published":"2025-04-14T05:48:05Z","title":"Digital Staining with Knowledge Distillation: A Unified Framework for\n  Unpaired and Paired-But-Misaligned Data","summary":"  Staining is essential in cell imaging and medical diagnostics but poses\nsignificant challenges, including high cost, time consumption, labor intensity,\nand irreversible tissue alterations. Recent advances in deep learning have\nenabled digital staining through supervised model training. However, collecting\nlarge-scale, perfectly aligned pairs of stained and unstained images remains\ndifficult. In this work, we propose a novel unsupervised deep learning\nframework for digital cell staining that reduces the need for extensive paired\ndata using knowledge distillation. We explore two training schemes: (1)\nunpaired and (2) paired-but-misaligned settings. For the unpaired case, we\nintroduce a two-stage pipeline, comprising light enhancement followed by\ncolorization, as a teacher model. Subsequently, we obtain a student staining\ngenerator through knowledge distillation with hybrid non-reference losses. To\nleverage the pixel-wise information between adjacent sections, we further\nextend to the paired-but-misaligned setting, adding the Learning to Align\nmodule to utilize pixel-level information. Experiment results on our dataset\ndemonstrate that our proposed unsupervised deep staining method can generate\nstained images with more accurate positions and shapes of the cell targets in\nboth settings. Compared with competing methods, our method achieves improved\nresults both qualitatively and quantitatively (e.g., NIQE and PSNR).We applied\nour digital staining method to the White Blood Cell (WBC) dataset,\ninvestigating its potential for medical applications.\n","authors":["Ziwang Xu","Lanqing Guo","Satoshi Tsutsui","Shuyan Zhang","Alex C. Kot","Bihan Wen"],"pdf_url":"https://arxiv.org/pdf/2504.09899v1.pdf","comment":"Accepted to IEEE Transactions on Medical Imaging"},{"id":"http://arxiv.org/abs/2504.09897v1","updated":"2025-04-14T05:44:38Z","published":"2025-04-14T05:44:38Z","title":"TAMP: Token-Adaptive Layerwise Pruning in Multimodal Large Language\n  Models","summary":"  Multimodal Large Language Models (MLLMs) have shown remarkable versatility in\nunderstanding diverse multimodal data and tasks. However, these capabilities\ncome with an increased model scale. While post-training pruning reduces model\nsize in unimodal models, its application to MLLMs often yields limited success.\nOur analysis discovers that conventional methods fail to account for the unique\ntoken attributes across layers and modalities inherent to MLLMs. Inspired by\nthis observation, we propose TAMP, a simple yet effective pruning framework\ntailored for MLLMs, featuring two key components: (1) Diversity-Aware Sparsity,\nwhich adjusts sparsity ratio per layer based on diversities among multimodal\noutput tokens, preserving more parameters in high-diversity layers; and (2)\nAdaptive Multimodal Input Activation, which identifies representative\nmultimodal input tokens using attention scores to guide unstructured weight\npruning. We validate our method on two state-of-the-art MLLMs: LLaVA-NeXT,\ndesigned for vision-language tasks, and VideoLLaMA2, capable of processing\naudio, visual, and language modalities. Empirical experiments across various\nmultimodal evaluation benchmarks demonstrate that each component of our\napproach substantially outperforms existing pruning techniques.\n","authors":["Jaewoo Lee","Keyang Xuan","Chanakya Ekbote","Sandeep Polisetty","Yi R."," Fung","Paul Pu Liang"],"pdf_url":"https://arxiv.org/pdf/2504.09897v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2111.12664v3","updated":"2025-04-14T05:41:01Z","published":"2021-11-24T17:51:29Z","title":"MIO : Mutual Information Optimization using Self-Supervised Binary\n  Contrastive Learning","summary":"  Self-supervised contrastive learning frameworks have progressed rapidly over\nthe last few years. In this paper, we propose a novel loss function for\ncontrastive learning. We model our pre-training task as a binary classification\nproblem to induce an implicit contrastive effect. We further improve the\nn\\\"aive loss function after removing the effect of the positive-positive\nrepulsion and incorporating the upper bound of the negative pair repulsion.\nUnlike existing methods, the proposed loss function optimizes the mutual\ninformation in positive and negative pairs. We also present a closed-form\nexpression for the parameter gradient flow and compare the behaviour of\nself-supervised contrastive frameworks using Hessian eigenspectrum to\nanalytically study their convergence. The proposed method outperforms SOTA\nself-supervised contrastive frameworks on benchmark datasets such as CIFAR-10,\nCIFAR-100, STL-10, and Tiny-ImageNet. After 200 pretraining epochs with\nResNet-18 as the backbone, the proposed model achieves an accuracy of 86.36%,\n58.18%, 80.50%, and 30.87% on the CIFAR-10, CIFAR-100, STL-10, and\nTiny-ImageNet datasets, respectively, and surpasses the SOTA contrastive\nbaseline by 1.93%, 3.57%, 4.85%, and 0.33%, respectively. The proposed\nframework also achieves a state-of-the-art accuracy of 78.4% (200 epochs) and\n65.22% (100 epochs) Top-1 Linear Evaluation accuracy on ImageNet100 and\nImageNet1K datasets, respectively.\n","authors":["Siladittya Manna","Umapada Pal","Saumik Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2111.12664v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.22268v2","updated":"2025-04-14T05:34:33Z","published":"2025-03-28T09:34:11Z","title":"Segment Any Motion in Videos","summary":"  Moving object segmentation is a crucial task for achieving a high-level\nunderstanding of visual scenes and has numerous downstream applications. Humans\ncan effortlessly segment moving objects in videos. Previous work has largely\nrelied on optical flow to provide motion cues; however, this approach often\nresults in imperfect predictions due to challenges such as partial motion,\ncomplex deformations, motion blur and background distractions. We propose a\nnovel approach for moving object segmentation that combines long-range\ntrajectory motion cues with DINO-based semantic features and leverages SAM2 for\npixel-level mask densification through an iterative prompting strategy. Our\nmodel employs Spatio-Temporal Trajectory Attention and Motion-Semantic\nDecoupled Embedding to prioritize motion while integrating semantic support.\nExtensive testing on diverse datasets demonstrates state-of-the-art\nperformance, excelling in challenging scenarios and fine-grained segmentation\nof multiple objects. Our code is available at https://motion-seg.github.io/.\n","authors":["Nan Huang","Wenzhao Zheng","Chenfeng Xu","Kurt Keutzer","Shanghang Zhang","Angjoo Kanazawa","Qianqian Wang"],"pdf_url":"https://arxiv.org/pdf/2503.22268v2.pdf","comment":"CVPR 2025. Website: https://motion-seg.github.io/"},{"id":"http://arxiv.org/abs/2504.09887v1","updated":"2025-04-14T05:26:24Z","published":"2025-04-14T05:26:24Z","title":"Enhanced Semantic Extraction and Guidance for UGC Image Super Resolution","summary":"  Due to the disparity between real-world degradations in user-generated\ncontent(UGC) images and synthetic degradations, traditional super-resolution\nmethods struggle to generalize effectively, necessitating a more robust\napproach to model real-world distortions. In this paper, we propose a novel\napproach to UGC image super-resolution by integrating semantic guidance into a\ndiffusion framework. Our method addresses the inconsistency between\ndegradations in wild and synthetic datasets by separately simulating the\ndegradation processes on the LSDIR dataset and combining them with the official\npaired training set. Furthermore, we enhance degradation removal and detail\ngeneration by incorporating a pretrained semantic extraction model (SAM2) and\nfine-tuning key hyperparameters for improved perceptual fidelity. Extensive\nexperiments demonstrate the superiority of our approach against\nstate-of-the-art methods. Additionally, the proposed model won second place in\nthe CVPR NTIRE 2025 Short-form UGC Image Super-Resolution Challenge, further\nvalidating its effectiveness. The code is available at\nhttps://github.c10pom/Moonsofang/NTIRE-2025-SRlab.\n","authors":["Yiwen Wang","Ying Liang","Yuxuan Zhang","Xinning Chai","Zhengxue Cheng","Yingsheng Qin","Yucai Yang","Rong Xie","Li Song"],"pdf_url":"https://arxiv.org/pdf/2504.09887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09885v1","updated":"2025-04-14T05:17:41Z","published":"2025-04-14T05:17:41Z","title":"Separate to Collaborate: Dual-Stream Diffusion Model for Coordinated\n  Piano Hand Motion Synthesis","summary":"  Automating the synthesis of coordinated bimanual piano performances poses\nsignificant challenges, particularly in capturing the intricate choreography\nbetween the hands while preserving their distinct kinematic signatures. In this\npaper, we propose a dual-stream neural framework designed to generate\nsynchronized hand gestures for piano playing from audio input, addressing the\ncritical challenge of modeling both hand independence and coordination. Our\nframework introduces two key innovations: (i) a decoupled diffusion-based\ngeneration framework that independently models each hand's motion via\ndual-noise initialization, sampling distinct latent noise for each while\nleveraging a shared positional condition, and (ii) a Hand-Coordinated\nAsymmetric Attention (HCAA) mechanism suppresses symmetric (common-mode) noise\nto highlight asymmetric hand-specific features, while adaptively enhancing\ninter-hand coordination during denoising. The system operates hierarchically:\nit first predicts 3D hand positions from audio features and then generates\njoint angles through position-aware diffusion models, where parallel denoising\nstreams interact via HCAA. Comprehensive evaluations demonstrate that our\nframework outperforms existing state-of-the-art methods across multiple\nmetrics.\n","authors":["Zihao Liu","Mingwen Ou","Zunnan Xu","Jiaqi Huang","Haonan Han","Ronghui Li","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2504.09885v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.09881v1","updated":"2025-04-14T05:04:51Z","published":"2025-04-14T05:04:51Z","title":"Focus on Local: Finding Reliable Discriminative Regions for Visual Place\n  Recognition","summary":"  Visual Place Recognition (VPR) is aimed at predicting the location of a query\nimage by referencing a database of geotagged images. For VPR task, often fewer\ndiscriminative local regions in an image produce important effects while\nmundane background regions do not contribute or even cause perceptual aliasing\nbecause of easy overlap. However, existing methods lack precisely modeling and\nfull exploitation of these discriminative regions. In this paper, we propose\nthe Focus on Local (FoL) approach to stimulate the performance of image\nretrieval and re-ranking in VPR simultaneously by mining and exploiting\nreliable discriminative local regions in images and introducing\npseudo-correlation supervision. First, we design two losses,\nExtraction-Aggregation Spatial Alignment Loss (SAL) and Foreground-Background\nContrast Enhancement Loss (CEL), to explicitly model reliable discriminative\nlocal regions and use them to guide the generation of global representations\nand efficient re-ranking. Second, we introduce a weakly-supervised local\nfeature training strategy based on pseudo-correspondences obtained from\naggregating global features to alleviate the lack of local correspondences\nground truth for the VPR task. Third, we suggest an efficient re-ranking\npipeline that is efficiently and precisely based on discriminative region\nguidance. Finally, experimental results show that our FoL achieves the\nstate-of-the-art on multiple VPR benchmarks in both image retrieval and\nre-ranking stages and also significantly outperforms existing two-stage VPR\nmethods in terms of computational efficiency. Code and models are available at\nhttps://github.com/chenshunpeng/FoL\n","authors":["Changwei Wang","Shunpeng Chen","Yukun Song","Rongtao Xu","Zherui Zhang","Jiguang Zhang","Haoran Yang","Yu Zhang","Kexue Fu","Shide Du","Zhiwei Xu","Longxiang Gao","Li Guo","Shibiao Xu"],"pdf_url":"https://arxiv.org/pdf/2504.09881v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2504.09878v1","updated":"2025-04-14T04:59:28Z","published":"2025-04-14T04:59:28Z","title":"MCBlock: Boosting Neural Radiance Field Training Speed by MCTS-based\n  Dynamic-Resolution Ray Sampling","summary":"  Neural Radiance Field (NeRF) is widely known for high-fidelity novel view\nsynthesis. However, even the state-of-the-art NeRF model, Gaussian Splatting,\nrequires minutes for training, far from the real-time performance required by\nmultimedia scenarios like telemedicine. One of the obstacles is its inefficient\nsampling, which is only partially addressed by existing works. Existing\npoint-sampling algorithms uniformly sample simple-texture regions (easy to fit)\nand complex-texture regions (hard to fit), while existing ray-sampling\nalgorithms sample these regions all in the finest granularity (i.e. the pixel\nlevel), both wasting GPU training resources. Actually, regions with different\ntexture intensities require different sampling granularities. To this end, we\npropose a novel dynamic-resolution ray-sampling algorithm, MCBlock, which\nemploys Monte Carlo Tree Search (MCTS) to partition each training image into\npixel blocks with different sizes for active block-wise training. Specifically,\nthe trees are initialized according to the texture of training images to boost\nthe initialization speed, and an expansion/pruning module dynamically optimizes\nthe block partition. MCBlock is implemented in Nerfstudio, an open-source\ntoolset, and achieves a training acceleration of up to 2.33x, surpassing other\nray-sampling algorithms. We believe MCBlock can apply to any cone-tracing NeRF\nmodel and contribute to the multimedia community.\n","authors":["Yunpeng Tan","Junlin Hao","Jiangkai Wu","Liming Liu","Qingyang Li","Xinggong Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.09878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09876v1","updated":"2025-04-14T04:52:24Z","published":"2025-04-14T04:52:24Z","title":"HDC: Hierarchical Distillation for Multi-level Noisy Consistency in\n  Semi-Supervised Fetal Ultrasound Segmentation","summary":"  Transvaginal ultrasound is a critical imaging modality for evaluating\ncervical anatomy and detecting physiological changes. However, accurate\nsegmentation of cervical structures remains challenging due to low contrast,\nshadow artifacts, and fuzzy boundaries. While convolutional neural networks\n(CNNs) have shown promising results in medical image segmentation, their\nperformance is often limited by the need for large-scale annotated datasets -\nan impractical requirement in clinical ultrasound imaging. Semi-supervised\nlearning (SSL) offers a compelling solution by leveraging unlabeled data, but\nexisting teacher-student frameworks often suffer from confirmation bias and\nhigh computational costs. We propose HDC, a novel semi-supervised segmentation\nframework that integrates Hierarchical Distillation and Consistency learning\nwithin a multi-level noise mean-teacher framework. Unlike conventional\napproaches that rely solely on pseudo-labeling, we introduce a hierarchical\ndistillation mechanism that guides feature-level learning via two novel\nobjectives: (1) Correlation Guidance Loss to align feature representations\nbetween the teacher and main student branch, and (2) Mutual Information Loss to\nstabilize representations between the main and noisy student branches. Our\nframework reduces model complexity while improving generalization. Extensive\nexperiments on two fetal ultrasound datasets, FUGC and PSFH, demonstrate that\nour method achieves competitive performance with significantly lower\ncomputational overhead than existing multi-teacher models.\n","authors":["Tran Quoc Khanh Le","Nguyen Lan Vi Vu","Ha-Hieu Pham","Xuan-Loc Huynh","Tien-Huy Nguyen","Minh Huu Nhat Le","Quan Nguyen","Hien D. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2504.09876v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13573v2","updated":"2025-04-14T04:22:48Z","published":"2024-12-18T07:45:30Z","title":"Seeking Consistent Flat Minima for Better Domain Generalization via\n  Refining Loss Landscapes","summary":"  Domain generalization aims to learn a model from multiple training domains\nand generalize it to unseen test domains. Recent theory has shown that seeking\nthe deep models, whose parameters lie in the flat minima of the loss landscape,\ncan significantly reduce the out-of-domain generalization error. However,\nexisting methods often neglect the consistency of loss landscapes in different\ndomains, resulting in models that are not simultaneously in the optimal flat\nminima in all domains, which limits their generalization ability. To address\nthis issue, this paper proposes an iterative Self-Feedback Training (SFT)\nframework to seek consistent flat minima that are shared across different\ndomains by progressively refining loss landscapes during training. It\nalternatively generates a feedback signal by measuring the inconsistency of\nloss landscapes in different domains and refines these loss landscapes for\ngreater consistency using this feedback signal. Benefiting from the consistency\nof the flat minima within these refined loss landscapes, our SFT helps achieve\nbetter out-of-domain generalization. Extensive experiments on DomainBed\ndemonstrate superior performances of SFT when compared to state-of-the-art\nsharpness-aware methods and other prevalent DG baselines. On average across\nfive DG benchmarks, SFT surpasses the sharpness-aware minimization by 2.6% with\nResNet-50 and 1.5% with ViT-B/16, respectively. The code will be available\nsoon.\n","authors":["Aodi Li","Liansheng Zhuang","Xiao Long","Minghong Yao","Shafei Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13573v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08395v2","updated":"2025-04-14T03:51:04Z","published":"2024-11-13T07:27:56Z","title":"MambaXCTrack: Mamba-based Tracker with SSM Cross-correlation and Motion\n  Prompt for Ultrasound Needle Tracking","summary":"  Ultrasound (US)-guided needle insertion is widely employed in percutaneous\ninterventions. However, providing feedback on the needle tip position via US\nimaging presents challenges due to noise, artifacts, and the thin imaging plane\nof US, which degrades needle features and leads to intermittent tip visibility.\nIn this paper, a Mamba-based US needle tracker MambaXCTrack utilizing\nstructured state space models cross-correlation (SSMX-Corr) and implicit motion\nprompt is proposed, which is the first application of Mamba in US needle\ntracking. The SSMX-Corr enhances cross-correlation by long-range modeling and\nglobal searching of distant semantic features between template and search maps,\nbenefiting the tracking under noise and artifacts by implicitly learning\npotential distant semantic cues. By combining with cross-map interleaved scan\n(CIS), local pixel-wise interaction with positional inductive bias can also be\nintroduced to SSMX-Corr. The implicit low-level motion descriptor is proposed\nas a non-visual prompt to enhance tracking robustness, addressing the\nintermittent tip visibility problem. Extensive experiments on a dataset with\nmotorized needle insertion in both phantom and tissue samples demonstrate that\nthe proposed tracker outperforms other state-of-the-art trackers while ablation\nstudies further highlight the effectiveness of each proposed tracking module.\n","authors":["Yuelin Zhang","Long Lei","Wanquan Yan","Tianyi Zhang","Raymond Shing-Yan Tang","Shing Shin Cheng"],"pdf_url":"https://arxiv.org/pdf/2411.08395v2.pdf","comment":"Accepted by RAL"}]},"2025-04-13T00:00:00Z":{"Statistical Machine Learning (Theory)":[{"id":"http://arxiv.org/abs/2504.09708v1","updated":"2025-04-13T20:06:49Z","published":"2025-04-13T20:06:49Z","title":"Preconditioned Gradient Descent for Over-Parameterized Nonconvex Matrix\n  Factorization","summary":"  In practical instances of nonconvex matrix factorization, the rank of the\ntrue solution $r^{\\star}$ is often unknown, so the rank $r$ of the model can be\noverspecified as $r>r^{\\star}$. This over-parameterized regime of matrix\nfactorization significantly slows down the convergence of local search\nalgorithms, from a linear rate with $r=r^{\\star}$ to a sublinear rate when\n$r>r^{\\star}$. We propose an inexpensive preconditioner for the matrix sensing\nvariant of nonconvex matrix factorization that restores the convergence rate of\ngradient descent back to linear, even in the over-parameterized case, while\nalso making it agnostic to possible ill-conditioning in the ground truth.\nClassical gradient descent in a neighborhood of the solution slows down due to\nthe need for the model matrix factor to become singular. Our key result is that\nthis singularity can be corrected by $\\ell_{2}$ regularization with a specific\nrange of values for the damping parameter. In fact, a good damping parameter\ncan be inexpensively estimated from the current iterate. The resulting\nalgorithm, which we call preconditioned gradient descent or PrecGD, is stable\nunder noise, and converges linearly to an information theoretically optimal\nerror bound. Our numerical experiments find that PrecGD works equally well in\nrestoring the linear convergence of other variants of nonconvex matrix\nfactorization in the over-parameterized regime.\n","authors":["Gavin Zhang","Salar Fattahi","Richard Y. Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.09708v1.pdf","comment":"NeurIPS 2021. See also\n  https://proceedings.neurips.cc/paper/2021/hash/2f2cd5c753d3cee48e47dbb5bbaed331-Abstract.html"},{"id":"http://arxiv.org/abs/2504.09706v1","updated":"2025-04-13T19:57:10Z","published":"2025-04-13T19:57:10Z","title":"Modeling Discrete Coating Degradation Events via Hawkes Processes","summary":"  Forecasting the degradation of coated materials has long been a topic of\ncritical interest in engineering, as it has enormous implications for both\nsystem maintenance and sustainable material use. Material degradation is\naffected by many factors, including the history of corrosion and\ncharacteristics of the environment, which can be measured by high-frequency\nsensors. However, the high volume of data produced by such sensors can inhibit\nefficient modeling and prediction. To alleviate this issue, we propose novel\nmetrics for representing material degradation, taking the form of discrete\ndegradation events. These events maintain the statistical properties of\ncontinuous sensor readings, such as correlation with time to coating failure\nand coefficient of variation at failure, but are composed of orders of\nmagnitude fewer measurements. To forecast future degradation of the coating\nsystem, a marked Hawkes process models the events. We use the forecast of\ndegradation to predict a future time of failure, exhibiting superior\nperformance to the approach based on direct modeling of galvanic corrosion\nusing continuous sensor measurements. While such maintenance is typically done\non a regular basis, degradation models can enable informed condition-based\nmaintenance, reducing unnecessary excess maintenance and preventing unexpected\nfailures.\n","authors":["Matthew Repasky","Henry Yuchi","Fritz Friedersdorf","Yao Xie"],"pdf_url":"https://arxiv.org/pdf/2504.09706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09663v1","updated":"2025-04-13T17:26:44Z","published":"2025-04-13T17:26:44Z","title":"Ordinary Least Squares as an Attention Mechanism","summary":"  I show that ordinary least squares (OLS) predictions can be rewritten as the\noutput of a restricted attention module, akin to those forming the backbone of\nlarge language models. This connection offers an alternative perspective on\nattention beyond the conventional information retrieval framework, making it\nmore accessible to researchers and analysts with a background in traditional\nstatistics. It falls into place when OLS is framed as a similarity-based method\nin a transformed regressor space, distinct from the standard view based on\npartial correlations. In fact, the OLS solution can be recast as the outcome of\nan alternative problem: minimizing squared prediction errors by optimizing the\nembedding space in which training and test vectors are compared via inner\nproducts. Rather than estimating coefficients directly, we equivalently learn\noptimal encoding and decoding operations for predictors. From this vantage\npoint, OLS maps naturally onto the query-key-value structure of attention\nmechanisms. Building on this foundation, I discuss key elements of\nTransformer-style attention and draw connections to classic ideas from time\nseries econometrics.\n","authors":["Philippe Goulet Coulombe"],"pdf_url":"https://arxiv.org/pdf/2504.09663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09629v1","updated":"2025-04-13T15:56:00Z","published":"2025-04-13T15:56:00Z","title":"Quantization Error Propagation: Revisiting Layer-Wise Post-Training\n  Quantization","summary":"  Layer-wise post-training quantization has emerged as a widely used technique\nfor compressing large language models (LLMs) without retraining. However,\nrecent progress in this line of research is saturating, underscoring the need\nto revisit its core limitation and explore further improvements. This study\nidentifies a critical bottleneck in existing layer-wise PTQ methods: the\naccumulation of quantization errors across layers significantly degrades\nperformance, particularly in low-bit regimes. To address this, we propose\nQuantization Error Propagation (QEP), a lightweight and general framework that\nenhances layer-wise PTQ by explicitly propagating the quantization error which\nenable compensating for accumulated quantization errors. Additionally, we\nintroduce a tunable propagation mechanism that allows for control over both\npropagation strength and computational overhead, making the framework adaptable\nto various architectures and resource constraints. Empirical evaluation on\nLLaMA2 models (7B, 13B, 70B) demonstrate that incorporating QEP into standard\nlayer-wise PTQ pipelines outperforms standard PTQ methods. Notably, QEP yields\nsubstantial performance improvements under extreme low-bit quantization\nsettings.\n","authors":["Yamato Arai","Yuma Ichikawa"],"pdf_url":"https://arxiv.org/pdf/2504.09629v1.pdf","comment":"16 pages, 1 figure"},{"id":"http://arxiv.org/abs/2504.09567v1","updated":"2025-04-13T13:38:25Z","published":"2025-04-13T13:38:25Z","title":"Conditional Independence Test Based on Transport Maps","summary":"  Testing conditional independence between two random vectors given a third is\na fundamental and challenging problem in statistics, particularly in\nmultivariate nonparametric settings due to the complexity of conditional\nstructures. We propose a novel framework for testing conditional independence\nusing transport maps. At the population level, we show that two well-defined\ntransport maps can transform the conditional independence test into an\nunconditional independence test, this substantially simplifies the problem.\nThese transport maps are estimated from data using conditional continuous\nnormalizing flow models. Within this framework, we derive a test statistic and\nprove its consistency under both the null and alternative hypotheses. A\npermutation-based procedure is employed to evaluate the significance of the\ntest. We validate the proposed method through extensive simulations and\nreal-data analysis. Our numerical studies demonstrate the practical\neffectiveness of the proposed method for conditional independence testing.\n","authors":["Chenxuan He","Yuan Gao","Liping Zhu","Jian Huang"],"pdf_url":"https://arxiv.org/pdf/2504.09567v1.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2412.12807v2","updated":"2025-04-13T12:19:53Z","published":"2024-12-17T11:25:51Z","title":"Ask for More Than Bayes Optimal: A Theory of Indecisions for\n  Classification","summary":"  Selective classification is a powerful tool for automated decision-making in\nhigh-risk scenarios, allowing classifiers to make only highly confident\ndecisions while abstaining when uncertainty is too high. Given a target\nclassification accuracy, our goal is to minimize the number of indecisions,\nwhich are observations that we do not automate. For problems that are hard, the\ntarget accuracy may not be achievable without using indecisions. In contrast,\nby using indecisions, we are able to control the misclassification rate to any\nuser-specified level, even below the Bayes optimal error rate, while minimizing\nthe frequency of identifying an indecision. We provide a full characterization\nof the minimax risk in selective classification, proving key continuity and\nmonotonicity properties that enable optimal indecision selection. Our results\nextend to hypothesis testing, where we control type II error given a fixed type\nI error, introducing a novel perspective in selective inference. We analyze the\nimpact of estimating the regression function $\\eta$, showing that plug-in\nclassifiers remain consistent and that accuracy-based calibration effectively\ncontrols indecision levels. Additionally, we develop finite-sample calibration\nmethods and identify cases where no training data is needed under the Monotone\nLikelihood Ratio (MLR) property. In the binary Gaussian mixture model, we\nestablish sharp phase transition results, demonstrating that minimal\nindecisions can yield near-optimal accuracy even with suboptimal class\nseparation. These findings highlight the potential of selective classification\nto significantly reduce misclassification rates with a relatively small cost in\nterms of indecisions.\n","authors":["Mohamed Ndaoud","Peter Radchenko","Bradley Rava"],"pdf_url":"https://arxiv.org/pdf/2412.12807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02191v2","updated":"2025-04-13T11:30:08Z","published":"2023-07-05T10:33:45Z","title":"Evaluating AI systems under uncertain ground truth: a case study in\n  dermatology","summary":"  For safety, medical AI systems undergo thorough evaluations before\ndeployment, validating their predictions against a ground truth which is\nassumed to be fixed and certain. However, this ground truth is often curated in\nthe form of differential diagnoses. While a single differential diagnosis\nreflects the uncertainty in one expert assessment, multiple experts introduce\nanother layer of uncertainty through disagreement. Both forms of uncertainty\nare ignored in standard evaluation which aggregates these differential\ndiagnoses to a single label. In this paper, we show that ignoring uncertainty\nleads to overly optimistic estimates of model performance, therefore\nunderestimating risk associated with particular diagnostic decisions. To this\nend, we propose a statistical aggregation approach, where we infer a\ndistribution on probabilities of underlying medical condition candidates\nthemselves, based on observed annotations. This formulation naturally accounts\nfor the potential disagreements between different experts, as well as\nuncertainty stemming from individual differential diagnoses, capturing the\nentire ground truth uncertainty. Our approach boils down to generating multiple\nsamples of medical condition probabilities, then evaluating and averaging\nperformance metrics based on these sampled probabilities. In skin condition\nclassification, we find that a large portion of the dataset exhibits\nsignificant ground truth uncertainty and standard evaluation severely\nover-estimates performance without providing uncertainty estimates. In\ncontrast, our framework provides uncertainty estimates on common metrics of\ninterest such as top-k accuracy and average overlap, showing that performance\ncan change multiple percentage points. We conclude that, while assuming a crisp\nground truth can be acceptable for many AI applications, a more nuanced\nevaluation protocol should be utilized in medical diagnosis.\n","authors":["David Stutz","Ali Taylan Cemgil","Abhijit Guha Roy","Tatiana Matejovicova","Melih Barsbey","Patricia Strachan","Mike Schaekermann","Jan Freyberg","Rajeev Rikhye","Beverly Freeman","Javier Perez Matos","Umesh Telang","Dale R. Webster","Yuan Liu","Greg S. Corrado","Yossi Matias","Pushmeet Kohli","Yun Liu","Arnaud Doucet","Alan Karthikesalingam"],"pdf_url":"https://arxiv.org/pdf/2307.02191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19711v2","updated":"2025-04-13T10:24:57Z","published":"2024-12-27T16:10:03Z","title":"Causal machine learning for heterogeneous treatment effects in the\n  presence of missing outcome data","summary":"  When estimating heterogeneous treatment effects, missing outcome data can\ncomplicate treatment effect estimation, causing certain subgroups of the\npopulation to be poorly represented. In this work, we discuss this commonly\noverlooked problem and consider the impact that missing at random (MAR) outcome\ndata has on causal machine learning estimators for the conditional average\ntreatment effect (CATE). We propose two de-biased machine learning estimators\nfor the CATE, the mDR-learner and mEP-learner, which address the issue of\nunder-representation by integrating inverse probability of censoring weights\ninto the DR-learner and EP-learner respectively. We show that under reasonable\nconditions, these estimators are oracle efficient, and illustrate their\nfavorable performance through simulated data settings, comparing them to\nexisting CATE estimators, including comparison to estimators which use common\nmissing data techniques. We present an example of their application using the\nGBSG2 trial, exploring treatment effect heterogeneity when comparing hormonal\ntherapies to non-hormonal therapies among breast cancer patients post surgery,\nand offer guidance on the decisions a practitioner must make when implementing\nthese estimators.\n","authors":["Matthew Pryce","Karla Diaz-Ordaz","Ruth H. Keogh","Stijn Vansteelandt"],"pdf_url":"https://arxiv.org/pdf/2412.19711v2.pdf","comment":"34 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2504.09509v1","updated":"2025-04-13T10:21:35Z","published":"2025-04-13T10:21:35Z","title":"Optimal sparse phase retrieval via a quasi-Bayesian approach","summary":"  This paper addresses the problem of sparse phase retrieval, a fundamental\ninverse problem in applied mathematics, physics, and engineering, where a\nsignal need to be reconstructed using only the magnitude of its transformation\nwhile phase information remains inaccessible. Leveraging the inherent sparsity\nof many real-world signals, we introduce a novel sparse quasi-Bayesian approach\nand provide the first theoretical guarantees for such an approach.\nSpecifically, we employ a scaled Student distribution as a continuous shrinkage\nprior to enforce sparsity and analyze the method using the PAC-Bayesian\ninequality framework. Our results establish that the proposed Bayesian\nestimator achieves minimax-optimal convergence rates under sub-exponential\nnoise, matching those of state-of-the-art frequentist methods. To ensure\ncomputational feasibility, we develop an efficient Langevin Monte Carlo\nsampling algorithm. Through numerical experiments, we demonstrate that our\nmethod performs comparably to existing frequentist techniques, highlighting its\npotential as a principled alternative for sparse phase retrieval in noisy\nsettings.\n","authors":["The Tien Mai"],"pdf_url":"https://arxiv.org/pdf/2504.09509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07611v2","updated":"2025-04-13T09:26:44Z","published":"2024-12-10T15:50:43Z","title":"Deep Partially Linear Transformation Model for Right-Censored Survival\n  Data","summary":"  Although the Cox proportional hazards model is well established and\nextensively used in the analysis of survival data, the proportional hazards\n(PH) assumption may not always hold in practical scenarios. The semiparametric\ntransformation model extends the conventional Cox model and also includes many\nother survival models as special cases. This paper introduces a deep partially\nlinear transformation model (DPLTM) as a general and flexible framework for\nestimation, inference and prediction. The proposed method is capable of\navoiding the curse of dimensionality while still retaining the interpretability\nof some covariates of interest. We derive the overall convergence rate of the\nmaximum likelihood estimators, the minimax lower bound of the nonparametric\ndeep neural network (DNN) estimator, the asymptotic normality and the\nsemiparametric efficiency of the parametric estimator. Comprehensive simulation\nstudies demonstrate the impressive performance of the proposed estimation\nprocedure in terms of both estimation accuracy and prediction power, which is\nfurther validated by an application to a real-world dataset.\n","authors":["Junkai Yin","Yue Zhang","Zhangsheng Yu"],"pdf_url":"https://arxiv.org/pdf/2412.07611v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09449v1","updated":"2025-04-13T06:17:35Z","published":"2025-04-13T06:17:35Z","title":"aweSOM: a CPU/GPU-accelerated Self-organizing Map and Statistically\n  Combined Ensemble Framework for Machine-learning Clustering Analysis","summary":"  We introduce aweSOM, an open-source Python package for machine learning (ML)\nclustering and classification, using a Self-organizing Maps (SOM) algorithm\nthat incorporates CPU/GPU acceleration to accommodate large ($N > 10^6$, where\n$N$ is the number of data points), multidimensional datasets. aweSOM consists\nof two main modules, one that handles the initialization and training of the\nSOM, and another that stacks the results of multiple SOM realizations to obtain\nmore statistically robust clusters. Existing Python-based SOM implementations\n(e.g., POPSOM, Yuan (2018); MiniSom, Vettigli (2018); sklearn-som) primarily\nserve as proof-of-concept demonstrations, optimized for smaller datasets, but\nlacking scalability for large, multidimensional data. aweSOM provides a\nsolution for this gap in capability, with good performance scaling up to $\\sim\n10^8$ individual points, and capable of utilizing multiple features per point.\nWe compare the code performance against the legacy implementations it is based\non, and find a 10-100x speed up, as well as significantly improved memory\nefficiency, due to several built-in optimizations.\n","authors":["Trung Ha","Joonas Nättilä","Jordy Davelaar"],"pdf_url":"https://arxiv.org/pdf/2504.09449v1.pdf","comment":"Published in the Journal of Open Source Software; method paper for\n  arXiv: 2410.01878"},{"id":"http://arxiv.org/abs/2503.08643v2","updated":"2025-04-13T06:15:52Z","published":"2025-03-11T17:36:11Z","title":"Rethinking Diffusion Model in High Dimension","summary":"  Curse of Dimensionality is an unavoidable challenge in statistical\nprobability models, yet diffusion models seem to overcome this limitation,\nachieving impressive results in high-dimensional data generation. Diffusion\nmodels assume that they can learn the statistical properties of the underlying\nprobability distribution, enabling sampling from this distribution to generate\nrealistic samples. But is this really how they work? To address this question,\nthis paper conducts a detailed analysis of the objective function and inference\nmethods of diffusion models, leading to several important conclusions that help\nanswer the above question: 1) In high-dimensional sparse scenarios, the target\nof the objective function fitting degrades from a weighted sum of multiple\nsamples to a single sample. 2) The mainstream inference methods can all be\nrepresented within a simple unified framework, without requiring statistical\nconcepts such as Markov chains and SDE, while aligning with the degraded\nobjective function. 3) Guided by this simple framework, more efficient\ninference methods can be discovered.\n","authors":["Zhenxin Zheng","Zhenjie Zheng"],"pdf_url":"https://arxiv.org/pdf/2503.08643v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14949v6","updated":"2025-04-13T06:13:45Z","published":"2024-10-19T02:36:11Z","title":"On the Wasserstein Convergence and Straightness of Rectified Flow","summary":"  Diffusion models have emerged as a powerful tool for image generation and\ndenoising. Typically, generative models learn a trajectory between the starting\nnoise distribution and the target data distribution. Recently Liu et al.\n(2023b) proposed Rectified Flow (RF), a generative model that aims to learn\nstraight flow trajectories from noise to data using a sequence of convex\noptimization problems with close ties to optimal transport. If the trajectory\nis curved, one must use many Euler discretization steps or novel strategies,\nsuch as exponential integrators, to achieve a satisfactory generation quality.\nIn contrast, RF has been shown to theoretically straighten the trajectory\nthrough successive rectifications, reducing the number of function evaluations\n(NFEs) while sampling. It has also been shown empirically that RF may improve\nthe straightness in two rectifications if one can solve the underlying\noptimization problem within a sufficiently small error. In this paper, we make\ntwo contributions. First, we provide a theoretical analysis of the Wasserstein\ndistance between the sampling distribution of RF and the target distribution.\nOur error rate is characterized by the number of discretization steps and a\nnovel formulation of straightness stronger than that in the original work.\nSecondly, we present general conditions guaranteeing uniqueness and\nstraightness of 1-RF, which is in line with previous empirical findings. As a\nbyproduct of our analysis, we show that, in one dimension, RF started at the\nstandard Gaussian distribution yields the Monge map. Additionally, we also\npresent empirical results on both simulated and real datasets to validate our\ntheoretical findings. The code is available at\nhttps://github.com/bansal-vansh/rectified-flow.\n","authors":["Vansh Bansal","Saptarshi Roy","Purnamrita Sarkar","Alessandro Rinaldo"],"pdf_url":"https://arxiv.org/pdf/2410.14949v6.pdf","comment":"38 pages"},{"id":"http://arxiv.org/abs/2504.09434v1","updated":"2025-04-13T04:57:34Z","published":"2025-04-13T04:57:34Z","title":"Constants of motion network revisited","summary":"  Discovering constants of motion is meaningful in helping understand the\ndynamical systems, but inevitably needs proficient mathematical skills and keen\nanalytical capabilities. With the prevalence of deep learning, methods\nemploying neural networks, such as Constant Of Motion nETwork (COMET), are\npromising in handling this scientific problem. Although the COMET method can\nproduce better predictions on dynamics by exploiting the discovered constants\nof motion, there is still plenty of room to sharpen it. In this paper, we\npropose a novel neural network architecture, built using the\nsingular-value-decomposition (SVD) technique, and a two-phase training\nalgorithm to improve the performance of COMET. Extensive experiments show that\nour approach not only retains the advantages of COMET, such as applying to\nnon-Hamiltonian systems and indicating the number of constants of motion, but\nalso can be more lightweight and noise-robust than COMET.\n","authors":["Wenqi Fang","Chao Chen","Yongkui Yang","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2504.09434v1.pdf","comment":"under revision"},{"id":"http://arxiv.org/abs/2504.09396v1","updated":"2025-04-13T01:43:25Z","published":"2025-04-13T01:43:25Z","title":"Adaptive Insurance Reserving with CVaR-Constrained Reinforcement\n  Learning under Macroeconomic Regimes","summary":"  This paper proposes a reinforcement learning (RL) framework for insurance\nreserving that integrates tail-risk sensitivity, macroeconomic regime modeling,\nand regulatory compliance. The reserving problem is formulated as a\nfinite-horizon Markov Decision Process (MDP), in which reserve adjustments are\noptimized using Proximal Policy Optimization (PPO) subject to Conditional\nValue-at-Risk (CVaR) constraints. To enhance policy robustness across varying\neconomic conditions, the agent is trained using a regime-aware curriculum that\nprogressively increases volatility exposure.\n  The reward structure penalizes reserve shortfall, capital inefficiency, and\nsolvency floor violations, with design elements informed by Solvency II and Own\nRisk and Solvency Assessment (ORSA) frameworks. Empirical evaluations on two\nindustry datasets--Workers' Compensation, and Other Liability--demonstrate that\nthe RL-CVaR agent achieves superior performance relative to classical reserving\nmethods across multiple criteria, including tail-risk control (CVaR$_{0.95}$),\ncapital efficiency, and regulatory violation rate. The framework also\naccommodates fixed-shock stress testing and regime-stratified analysis,\nproviding a principled and extensible approach to reserving under uncertainty.\n","authors":["Stella C. Dong","James R. Finlay"],"pdf_url":"https://arxiv.org/pdf/2504.09396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10540v1","updated":"2025-04-13T08:29:58Z","published":"2025-04-13T08:29:58Z","title":"AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse","summary":"  Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality.\n","authors":["Zichao Yu","Zhen Zou","Guojiang Shao","Chengwei Zhang","Shengze Xu","Jie Huang","Feng Zhao","Xiaodong Cun","Wenyi Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.10540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.12680v5","updated":"2025-04-13T03:28:09Z","published":"2022-06-25T16:03:48Z","title":"Topology-aware Generalization of Decentralized SGD","summary":"  This paper studies the algorithmic stability and generalizability of\ndecentralized stochastic gradient descent (D-SGD). We prove that the consensus\nmodel learned by D-SGD is $\\mathcal{O}{(N^{-1}+m^{-1} +\\lambda^2)}$-stable in\nexpectation in the non-convex non-smooth setting, where $N$ is the total sample\nsize, $m$ is the worker number, and $1+\\lambda$ is the spectral gap that\nmeasures the connectivity of the communication topology. These results then\ndeliver an $\\mathcal{O}{(N^{-(1+\\alpha)/2}+\nm^{-(1+\\alpha)/2}+\\lambda^{1+\\alpha} + \\phi_{\\mathcal{S}})}$ in-average\ngeneralization bound, which is non-vacuous even when $\\lambda$ is closed to\n$1$, in contrast to vacuous as suggested by existing literature on the\nprojected version of D-SGD. Our theory indicates that the generalizability of\nD-SGD is positively correlated with the spectral gap, and can explain why\nconsensus control in initial training phase can ensure better generalization.\nExperiments of VGG-11 and ResNet-18 on CIFAR-10, CIFAR-100 and Tiny-ImageNet\njustify our theory. To our best knowledge, this is the first work on the\ntopology-aware generalization of vanilla D-SGD. Code is available at\nhttps://github.com/Raiden-Zhu/Generalization-of-DSGD.\n","authors":["Tongtian Zhu","Fengxiang He","Lan Zhang","Zhengyang Niu","Mingli Song","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2206.12680v5.pdf","comment":"Accepted for publication in the 39th International Conference on\n  Machine Learning (ICML 2022)"}],"Optimization":[{"id":"http://arxiv.org/abs/2504.09748v1","updated":"2025-04-13T22:55:35Z","published":"2025-04-13T22:55:35Z","title":"Level-set topology optimisation with unfitted finite elements and\n  automatic shape differentiation","summary":"  In this paper we develop automatic shape differentiation techniques for\nunfitted discretisations and link these to recent advances in shape calculus\nfor unfitted methods. We extend existing analytic shape calculus results to the\ncase where the domain boundary intersects with the boundary of the background\ndomain. We further show that we can recover these analytic derivatives to\nmachine precision regardless of the mesh size using the developed automatic\nshape differentiation techniques. In addition, we show that we can also recover\nthe symmetric shape Hessian. We implement these techniques for both serial and\ndistributed computing frameworks in the Julia package GridapTopOpt and the\nwider Gridap ecosystem. As part of this implementation we propose a novel\ngraph-based approach for isolated volume detection. We demonstrate the\napplicability of the unfitted automatic shape differentiation framework and our\nimplementation by considering the three-dimensional minimum compliance topology\noptimisation of a linear elastic wheel and of a linear elastic structure in a\nfluid-structure interaction problem with Stokes flow. The implementation is\ngeneral and allows GridapTopOpt to solve a wide range of problems without\nanalytic calculation of shape derivatives and avoiding issues that arise when\nmaterial properties are smoothed at the domain boundary. The software is open\nsource and available at https://github.com/zjwegert/GridapTopOpt.jl.\n","authors":["Zachary J. Wegert","Jordi Manyer","Connor Mallon","Santiago Badia","Vivien J. Challis"],"pdf_url":"https://arxiv.org/pdf/2504.09748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09730v1","updated":"2025-04-13T21:35:16Z","published":"2025-04-13T21:35:16Z","title":"Learning-based decentralized control with collision avoidance for\n  multi-agent systems","summary":"  In this paper, we present a learning-based tracking controller based on\nGaussian processes (GP) for collision avoidance of multi-agent systems where\nthe agents evolve in the special Euclidean group in the space SE(3). In\nparticular, we use GPs to estimate certain uncertainties that appear in the\ndynamics of the agents. The control algorithm is designed to learn and mitigate\nthese uncertainties by using GPs as a learning-based model for the predictions.\nIn particular, the presented approach guarantees that the tracking error\nremains bounded with high probability. We present some simulation results to\nshow how the control algorithm is implemented.\n","authors":["Omayra Yago Nieto","Alexandre Anahory Simoes","Juan I. Giribet","Leonardo J. Colombo"],"pdf_url":"https://arxiv.org/pdf/2504.09730v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2504.09708v1","updated":"2025-04-13T20:06:49Z","published":"2025-04-13T20:06:49Z","title":"Preconditioned Gradient Descent for Over-Parameterized Nonconvex Matrix\n  Factorization","summary":"  In practical instances of nonconvex matrix factorization, the rank of the\ntrue solution $r^{\\star}$ is often unknown, so the rank $r$ of the model can be\noverspecified as $r>r^{\\star}$. This over-parameterized regime of matrix\nfactorization significantly slows down the convergence of local search\nalgorithms, from a linear rate with $r=r^{\\star}$ to a sublinear rate when\n$r>r^{\\star}$. We propose an inexpensive preconditioner for the matrix sensing\nvariant of nonconvex matrix factorization that restores the convergence rate of\ngradient descent back to linear, even in the over-parameterized case, while\nalso making it agnostic to possible ill-conditioning in the ground truth.\nClassical gradient descent in a neighborhood of the solution slows down due to\nthe need for the model matrix factor to become singular. Our key result is that\nthis singularity can be corrected by $\\ell_{2}$ regularization with a specific\nrange of values for the damping parameter. In fact, a good damping parameter\ncan be inexpensively estimated from the current iterate. The resulting\nalgorithm, which we call preconditioned gradient descent or PrecGD, is stable\nunder noise, and converges linearly to an information theoretically optimal\nerror bound. Our numerical experiments find that PrecGD works equally well in\nrestoring the linear convergence of other variants of nonconvex matrix\nfactorization in the over-parameterized regime.\n","authors":["Gavin Zhang","Salar Fattahi","Richard Y. Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.09708v1.pdf","comment":"NeurIPS 2021. See also\n  https://proceedings.neurips.cc/paper/2021/hash/2f2cd5c753d3cee48e47dbb5bbaed331-Abstract.html"},{"id":"http://arxiv.org/abs/2208.04883v7","updated":"2025-04-13T19:16:48Z","published":"2022-08-09T16:25:49Z","title":"Neural-Rendezvous: Provably Robust Guidance and Control to Encounter\n  Interstellar Objects","summary":"  Interstellar objects (ISOs) are likely representatives of primitive materials\ninvaluable in understanding exoplanetary star systems. Due to their poorly\nconstrained orbits with generally high inclinations and relative velocities,\nhowever, exploring ISOs with conventional human-in-the-loop approaches is\nsignificantly challenging. This paper presents Neural-Rendezvous -- a deep\nlearning-based guidance and control framework for encountering fast-moving\nobjects, including ISOs, robustly, accurately, and autonomously in real time.\nIt uses pointwise minimum norm tracking control on top of a guidance policy\nmodeled by a spectrally-normalized deep neural network, where its\nhyperparameters are tuned with a loss function directly penalizing the MPC\nstate trajectory tracking error. We show that Neural-Rendezvous provides a high\nprobability exponential bound on the expected spacecraft delivery error, the\nproof of which leverages stochastic incremental stability analysis. In\nparticular, it is used to construct a non-negative function with a\nsupermartingale property, explicitly accounting for the ISO state uncertainty\nand the local nature of nonlinear state estimation guarantees. In numerical\nsimulations, Neural-Rendezvous is demonstrated to satisfy the expected error\nbound for 100 ISO candidates. This performance is also empirically validated\nusing our spacecraft simulator and in high-conflict and distributed UAV swarm\nreconfiguration with up to 20 UAVs.\n","authors":["Hiroyasu Tsukamoto","Soon-Jo Chung","Yashwanth Kumar Nakka","Benjamin Donitz","Declan Mages","Michel Ingham"],"pdf_url":"https://arxiv.org/pdf/2208.04883v7.pdf","comment":"Preprint Version, Accepted: October, 2024 (One-minute YouTube\n  summary: https://youtu.be/q3e0LYS2IYQ, DOI:\n  https://doi.org/10.2514/1.G007671)"},{"id":"http://arxiv.org/abs/2504.09680v1","updated":"2025-04-13T18:14:38Z","published":"2025-04-13T18:14:38Z","title":"SPOT: Spatio-Temporal Pattern Mining and Optimization for Load\n  Consolidation in Freight Transportation Networks","summary":"  Freight consolidation has significant potential to reduce transportation\ncosts and mitigate congestion and pollution. An effective load consolidation\nplan relies on carefully chosen consolidation points to ensure alignment with\nexisting transportation management processes, such as driver scheduling,\npersonnel planning, and terminal operations. This complexity represents a\nsignificant challenge when searching for optimal consolidation strategies.\nTraditional optimization-based methods provide exact solutions, but their\ncomputational complexity makes them impractical for large-scale instances and\nthey fail to leverage historical data. Machine learning-based approaches\naddress these issues but often ignore operational constraints, leading to\ninfeasible consolidation plans.\n  This work proposes SPOT, an end-to-end approach that integrates the benefits\nof machine learning (ML) and optimization for load consolidation. The ML\ncomponent plays a key role in the planning phase by identifying the\nconsolidation points through spatio-temporal clustering and constrained\nfrequent itemset mining, while the optimization selects the most cost-effective\nfeasible consolidation routes for a given operational day. Extensive\nexperiments conducted on industrial load data demonstrate that SPOT\nsignificantly reduces travel distance and transportation costs (by about 50% on\nlarge terminals) compared to the existing industry-standard load planning\nstrategy and a neighborhood-based heuristic. Moreover, the ML component\nprovides valuable tactical-level insights by identifying frequently recurring\nconsolidation opportunities that guide proactive planning. In addition, SPOT is\ncomputationally efficient and can be easily scaled to accommodate large\ntransportation networks.\n","authors":["Sikai Cheng","Amira Hijazi","Jeren Konak","Alan Erera","Pascal Van Hentenryck"],"pdf_url":"https://arxiv.org/pdf/2504.09680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09657v1","updated":"2025-04-13T17:11:28Z","published":"2025-04-13T17:11:28Z","title":"Nonlinear Online Optimization for Vehicle-Home-Grid Integration\n  including Household Load Prediction and Battery Degradation","summary":"  This paper investigates the economic impact of vehicle-home-grid integration,\nby proposing an online energy management algorithm that optimizes energy flows\nbetween an electric vehicle (EV), a household, and the electrical grid. The\nalgorithm leverages vehicle-to-home (V2H) for self-consumption and\nvehicle-to-grid (V2G) for energy trading, adapting to real-time conditions\nthrough a hybrid long short-term memory (LSTM) neural network for accurate\nhousehold load prediction, alongside a comprehensive nonlinear battery\ndegradation model accounting for both cycle and calendar aging. Simulation\nresults reveal significant economic advantages: compared to smart\nunidirectional charging, the proposed method yields an annual economic benefit\nof up to EUR 3046.81, despite a modest 1.96% increase in battery degradation.\nEven under unfavorable market conditions, where V2G energy selling generates no\nrevenue, V2H alone ensures yearly savings of EUR 425.48. A systematic\nsensitivity analysis investigates how variations in battery capacity, household\nload, and price ratios affect economic outcomes, confirming the consistent\nbenefits of bidirectional energy exchange. These findings highlight the\npotential of EVs as active energy nodes, enabling sustainable energy management\nand cost-effective battery usage in real-world conditions.\n","authors":["Francesco Popolizio","Torsten Wik","Chih Feng Lee","Changfu Zou"],"pdf_url":"https://arxiv.org/pdf/2504.09657v1.pdf","comment":"Submitted to the 2025 IEEE Conference on Decision and Control (CDC)"},{"id":"http://arxiv.org/abs/2504.09638v1","updated":"2025-04-13T16:25:34Z","published":"2025-04-13T16:25:34Z","title":"Data-Driven Two-Stage Distributionally Robust Dispatch of Multi-Energy\n  Microgrid","summary":"  This paper studies adaptive distributionally robust dispatch (DRD) of the\nmulti-energy microgrid under supply and demand uncertainties. A Wasserstein\nambiguity set is constructed to support data-driven decision-making. By fully\nleveraging the special structure of worst-case expectation from the primal\nperspective, a novel and high-efficient decomposition algorithm under the\nframework of column-and-constraint generation is customized and developed to\naddress the computational burden. Numerical studies demonstrate the\neffectiveness of our DRD approach, and shed light on the interrelationship of\nit with the traditional dispatch approaches through stochastic programming and\nrobust optimization schemes. Also, comparisons with popular algorithms in the\nliterature for two-stage distributionally robust optimization verify the\npowerful capacity of our algorithm in computing the DRD problem.\n","authors":["Xunhang Sun","Xiaoyu Cao","Bo Zeng","Miaomiao Li","Xiaohong Guan","Tamer Başar"],"pdf_url":"https://arxiv.org/pdf/2504.09638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09611v1","updated":"2025-04-13T15:04:02Z","published":"2025-04-13T15:04:02Z","title":"An Operator-Theoretic Framework for the Optimal Control Problem of\n  Nonlinear Caputo Fractional Systems","summary":"  This paper addresses the optimal control problem for a class of nonlinear\nfractional systems involving Caputo derivatives and nonlocal initial\nconditions. The system is reformulated as an abstract Hammerstein-type operator\nequation, enabling the application of operator-theoretic techniques. Sufficient\nconditions are established to guarantee the existence of mild solutions and\noptimal control-state pairs. The analysis covers both convex and non-convex\nscenarios through various sets of assumptions on the involved operators. An\noptimality system is derived for quadratic cost functionals using the G\\^ateaux\nderivative, and the connection with Pontryagin-type minimum principles is\ndiscussed. Illustrative examples demonstrate the effectiveness of the proposed\ntheoretical framework.\n","authors":["Dev Prakash Jha","Raju K. George"],"pdf_url":"https://arxiv.org/pdf/2504.09611v1.pdf","comment":"29 pages,2 figures"},{"id":"http://arxiv.org/abs/2405.16058v3","updated":"2025-04-13T13:17:33Z","published":"2024-05-25T04:56:54Z","title":"A Novel Privacy Enhancement Scheme with Dynamic Quantization for\n  Federated Learning","summary":"  Federated learning (FL) has been widely regarded as a promising paradigm for\nprivacy preservation of raw data in machine learning. Although, the data\nprivacy in FL is locally protected to some extent, it is still a desideratum to\nenhance privacy and alleviate communication overhead caused by repetitively\ntransmitting model parameters. Typically, these challenges are addressed\nseparately, or jointly via a unified scheme that consists of noise-injected\nprivacy mechanism and communication compression, which may lead to model\ncorruption due to the introduced composite noise. In this work, we propose a\nnovel model-splitting privacy-preserving FL (MSP-FL) scheme to achieve private\nFL with precise accuracy guarantee. Based upon MSP-FL, we further propose a\nmodel-splitting privacy-preserving FL with dynamic quantization (MSPDQ-FL) to\nmitigate the communication overhead, which incorporates a shrinking\nquantization interval to reduce the quantization error. We provide privacy and\nconvergence analysis for both MSP-FL and MSPDQ-FL under non-i.i.d. dataset,\npartial clients participation and finite quantization level. Numerical results\nare presented to validate the superiority of the proposed schemes.\n","authors":["Yifan Wang","Xianghui Cao","Shi Jin","Mo-Yuen Chow"],"pdf_url":"https://arxiv.org/pdf/2405.16058v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00592v4","updated":"2025-04-13T10:53:35Z","published":"2023-12-31T21:59:40Z","title":"Majority voting is not good for heaven or hell, with mirrored\n  performance","summary":"  Within the ViSE (Voting in Stochastic Environment) model, we study the\neffectiveness of majority voting in various environments. By the pit of losses\nparadox identified in previous work, majority decisions in apparently hostile\nenvironments tend to reduce the capital of society. In such cases, the simple\nsocial decision rule of \"rejecting all proposals without voting\" outperforms\nmajority voting. In this paper, we identify another pit of losses appearing in\nfavorable environments. Here, the simple social decision rule of \"accepting all\nproposals without voting\" is superior to majority voting. We prove that under a\nversion of simple majority called symmetrized majority and the antisymmetry of\nthe voting body, the second pit of losses is a mirror image of the pit of\nlosses in hostile environments and explain this phenomenon. Technically, we\nconsider a voting society consisting of individualists whose strategy is\nsupporting all proposals that increase their capital and a group (groups) whose\nmembers vote to increase the wealth of their group. According to the main\nresult, the expected capital gain of each agent in the environment whose\ngenerator $X$ has mean $\\mu>0$ exceeds by $\\mu$ their expected capital gain\nunder generator $-X$. This result extends to location families of generators\nwith distributions symmetric about their mean. The mentioned result determines\nthe symmetry of the difference between the expected capital gain under the\nsymmetrized majority and that under the \"basic\" social decision rule that\nrejects (resp. accepts) all proposals in unfavorable (resp. favorable)\nenvironments.\n","authors":["Pavel Chebotarev","Vadim Afonkin"],"pdf_url":"https://arxiv.org/pdf/2401.00592v4.pdf","comment":"17 pages, 3 figures. Submitted to a journal. Compared to the previous\n  version, the results have been generalized"},{"id":"http://arxiv.org/abs/2503.22124v2","updated":"2025-04-13T08:19:48Z","published":"2025-03-28T03:56:12Z","title":"Scheduling problem of aircrafts on a same runway and dual runways","summary":"  In this paper, the scheduling problems of landing and takeoff aircrafts on a\nsame runway and dual runways are addressed. In contrast to the approaches based\non mixed integer optimization models in existing works, our approach focuses on\nthe minimum separation times between aircrafts by introducing some necessary\nassumptions. A dynamic programming algorithm is proposed and numerical examples\nare presented to show the effectiveness of the theoretical results.\n","authors":["Peng Lin","Haopeng Yang","Weihua Gui"],"pdf_url":"https://arxiv.org/pdf/2503.22124v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07373v4","updated":"2025-04-13T08:01:52Z","published":"2023-12-12T15:43:20Z","title":"Mean-field limits for Consensus-Based Optimization and Sampling","summary":"  For algorithms based on interacting particle systems that admit a mean-field\ndescription, convergence analysis is often more accessible at the mean-field\nlevel. In order to transfer convergence results obtained at the mean-field\nlevel to the finite ensemble size setting, it is desirable to show that the\nparticle dynamics converge in an appropriate sense to the corresponding\nmean-field dynamics. In this paper, we prove quantitative mean-field limit\nresults for two related interacting particle systems: Consensus-Based\nOptimization and Consensus-Based Sampling. Our approach requires a\ngeneralization of Sznitman's classical argument: in order to circumvent issues\nrelated to the lack of global Lipschitz continuity of the coefficients, we\ndiscard an event of small probability, the contribution of which is controlled\nusing moment estimates for the particle systems. In addition, we present new\nresults on the well-posedness of the particle systems and their mean-field\nlimit, and provide novel stability estimates for the weighted mean and the\nweighted covariance.\n","authors":["Nicolai Jurek Gerber","Franca Hoffmann","Urbain Vaes"],"pdf_url":"https://arxiv.org/pdf/2312.07373v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10454v5","updated":"2025-04-13T04:20:56Z","published":"2023-09-19T09:12:26Z","title":"Maximum Principle of Stochastic Optimal Control Problems with Model\n  Uncertainty","summary":"  This paper is concerned with the maximum principle of stochastic optimal\ncontrol problems, where the coefficients of the state equation and the cost\nfunctional are uncertain, and the system is generally under Markovian regime\nswitching. Firstly, the $ L^\\beta$-solutions of forward-backward stochastic\ndifferential equations with regime switching are given. Secondly, we obtain the\nvariational inequality by making use of the continuity of solutions to\nvariational equations with respect to the uncertainty parameter $\\theta$.\nThirdly, utilizing the linearization and weak convergence techniques, we prove\nthe necessary stochastic maximum principle and provide sufficient conditions\nfor the stochastic optimal control. Finally, as an application, a\nrisk-minimizing portfolio selection problem is studied.\n","authors":["Tao Hao","Jiaqiang Wen","Jie Xiong"],"pdf_url":"https://arxiv.org/pdf/2309.10454v5.pdf","comment":"35 Pages"},{"id":"http://arxiv.org/abs/2504.09425v1","updated":"2025-04-13T04:16:38Z","published":"2025-04-13T04:16:38Z","title":"Optimal Control for Kuramoto Model: from Many-Particle Liouville\n  Equation to Diffusive Mean-Field Problem","summary":"  In this paper, we investigate the mean-field optimal control problem of a\nswarm of Kuramoto oscillators. The controls exploit the self-synchronization\nproperty of the oscillators to achieve target density and target phase\ncoherence. In the limit of an infinite number of oscillators the collective\ndynamics of the agents' density is described by a diffusive mean-field model in\nthe form of a non-local PDE, where the non-locality arises from the\nsynchronization mechanism. We prove the existence of the optimal control of the\nmean-field model by using $\\Gamma$-convergence strategy of the cost functional\ncorresponding to the Liouville equation on the particle level. In the\ndiscussion of propagation of chaos for fixed control functions we complete the\nrelative entropy estimate by using large deviation estimate given by\n\\cite{MR3858403}.\n","authors":["Li Chen","Yucheng Wang","Valeriia Zhidkova"],"pdf_url":"https://arxiv.org/pdf/2504.09425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09409v1","updated":"2025-04-13T02:44:47Z","published":"2025-04-13T02:44:47Z","title":"Bregman Linearized Augmented Lagrangian Method for Nonconvex Constrained\n  Stochastic Zeroth-order Optimization","summary":"  In this paper, we study nonconvex constrained stochastic zeroth-order\noptimization problems, for which we have access to exact information of\nconstraints and noisy function values of the objective. We propose a Bregman\nlinearized augmented Lagrangian method that utilizes stochastic zeroth-order\ngradient estimators combined with a variance reduction technique. We analyze\nits oracle complexity, in terms of the total number of stochastic function\nvalue evaluations required to achieve an \\(\\epsilon\\)-KKT point in\n\\(\\ell_p\\)-norm metrics with \\(p \\ge 2\\), where \\(p\\) is a parameter associated\nwith the selected Bregman distance. In particular, starting from a\nnear-feasible initial point and using Rademacher smoothing, the oracle\ncomplexity is in order \\(O(p d^{2/p} \\epsilon^{-3})\\) for \\(p \\in [2, 2 \\ln\nd]\\), and \\(O(\\ln d \\cdot \\epsilon^{-3})\\) for \\(p > 2 \\ln d\\), where \\(d\\)\ndenotes the problem dimension. Those results show that the complexity of the\nproposed method can achieve a dimensional dependency lower than \\(O(d)\\)\nwithout requiring additional assumptions, provided that a Bregman distance is\nchosen properly. This offers a significant improvement in the high-dimensional\nsetting over existing work, and matches the lowest complexity order with\nrespect to the tolerance \\(\\epsilon\\) reported in the literature. Numerical\nexperiments on constrained Lasso and black-box adversarial attack problems\nhighlight the promising performances of the proposed method.\n","authors":["Qiankun Shi","Xiao Wang","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2504.09409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09401v1","updated":"2025-04-13T02:08:52Z","published":"2025-04-13T02:08:52Z","title":"Linear Quadratic Mean Field Stackelberg Games: Open-loop and Feedback\n  Solutions","summary":"  This paper investigates open-loop and feedback solutions of linear quadratic\nmean field (MF) games with a leader and a large number of followers. The leader\nfirst gives its strategy and then all the followers cooperate to optimize the\nsocial cost as the sum of their costs. By variational analysis with MF\napproximations, we obtain a set of open-loop controls of players in terms of\nsolutions to MF forward-backward stochastic differential equations (FBSDEs),\nwhich is further shown be to an asymptotic Stackelberg equilibrium. By applying\nthe matrix maximum principle, a set of decentralized feedback strategies is\nconstructed for all the players. For open-loop and feedback solutions, the\ncorresponding optimal costs of all players are explicitly given by virtue of\nthe solutions to two Riccati equations, respectively. The performances of two\nsolutions are compared by the numerical simulation.\n","authors":["Bing-Chang Wang","Juanjuan Xu","Huanshui Zhang","Yong Liang"],"pdf_url":"https://arxiv.org/pdf/2504.09401v1.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2504.09385v1","updated":"2025-04-13T00:40:17Z","published":"2025-04-13T00:40:17Z","title":"Expressivity of Quadratic Neural ODEs","summary":"  This work focuses on deriving quantitative approximation error bounds for\nneural ordinary differential equations having at most quadratic nonlinearities\nin the dynamics. The simple dynamics of this model form demonstrates how\nexpressivity can be derived primarily from iteratively composing many basic\nelementary operations, versus from the complexity of those elementary\noperations themselves. Like the analog differential analyzer and universal\npolynomial DAEs, the expressivity is derived instead primarily from the \"depth\"\nof the model. These results contribute to our understanding of what depth\nspecifically imparts to the capabilities of deep learning architectures.\n","authors":["Joshua Hanson","Maxim Raginsky"],"pdf_url":"https://arxiv.org/pdf/2504.09385v1.pdf","comment":"9 pages, 1 figure"}],"Computation and Language":[{"id":"http://arxiv.org/abs/2502.11901v2","updated":"2025-04-13T23:38:44Z","published":"2025-02-17T15:24:11Z","title":"Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o\n  Under Data Scarcity","summary":"  Existing LMs struggle with proof-oriented programming due to data scarcity,\nwhich manifest in two key ways: (1) a lack of sufficient corpora for\nproof-oriented programming languages such as F*, and (2) the absence of\nlarge-scale, project-level proof-oriented implementations that can teach the\nmodel the intricate reasoning process when performing proof-oriented\nprogramming. We present the first on synthetic data augmentation for project\nlevel proof oriented programming for both generation and repair. Our method\naddresses data scarcity by synthesizing basic proof-oriented programming\nproblems for proficiency in that language; incorporating diverse coding data\nfor reasoning capability elicitation and creating new proofs and repair data\nwithin existing repositories. This approach enables language models to both\nsynthesize and repair proofs for function- and repository-level code. We show\nthat our fine-tuned 14B parameter model, PoPilot, can exceed the performance of\nthe models that outperforms GPT-4o in project-level proof-oriented programming\nby 64% relative margin, and can improve GPT-4o's performance by 54% by\nrepairing its outputs over GPT-4o's self-repair.\n","authors":["Dylan Zhang","Justin Wang","Tianran Sun"],"pdf_url":"https://arxiv.org/pdf/2502.11901v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04639v2","updated":"2025-04-13T23:31:22Z","published":"2025-04-06T23:08:05Z","title":"Ineffectiveness for Search and Undecidability of PCSP Meta-Problems","summary":"  It is an open question whether the search and decision versions of promise\nCSPs are equivalent. Most known algorithms for PCSPs solve only their\n\\emph{decision} variant, and it is unknown whether they can be adapted to solve\n\\emph{search} as well. The main approaches, called BLP, AIP and BLP+AIP, handle\na PCSP by finding a solution to a relaxation of some integer program. We prove\nthat rounding those solutions to a proper search certificate can be as hard as\nany problem in the class TFNP. In other words, these algorithms are ineffective\nfor search. Building on the algebraic approach to PCSPs, we find sufficient\nconditions that imply ineffectiveness for search. Our tools are tailored to\nalgorithms that are characterized by minions in a suitable way, and can also be\nused to prove undecidability results for meta-problems. This way, we show that\nthe families of templates solvable via BLP, AIP, and BLP+AIP are undecidable.\n  Using the same techniques we also analyze several algebraic conditions that\nare known to guarantee the tractability of finite-template CSPs. We prove that\nseveral meta-problems related to cyclic polymorphims and WNUs are undecidable\nfor PCSPs. In particular, there is no algorithm deciding whether a finite PCSP\ntemplate (1) admits cyclic a polymorphism, (2) admits a WNU.\n","authors":["Alberto Larrauri"],"pdf_url":"https://arxiv.org/pdf/2504.04639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09753v1","updated":"2025-04-13T23:10:13Z","published":"2025-04-13T23:10:13Z","title":"Improving Multilingual Capabilities with Cultural and Local Knowledge in\n  Large Language Models While Enhancing Native Performance","summary":"  Large Language Models (LLMs) have shown remarkable capabilities, but their\ndevelopment has primarily focused on English and other high-resource languages,\nleaving many languages underserved. We present our latest Hindi-English\nbi-lingual LLM \\textbf{Mantra-14B} with ~3\\% average improvement in benchmark\nscores over both languages, outperforming models twice its size. Using a\ncurated dataset composed of English and Hindi instruction data of 485K samples,\nwe instruction tuned models such as Qwen-2.5-14B-Instruct and Phi-4 to improve\nperformance over both English and Hindi. Our experiments encompassing seven\ndifferent LLMs of varying parameter sizes and over 140 training attempts with\nvarying English-Hindi training data ratios demonstrated that it is possible to\nsignificantly improve multilingual performance without compromising native\nperformance. Further, our approach avoids resource-intensive techniques like\nvocabulary expansion or architectural modifications, thus keeping the model\nsize small. Our results indicate that modest fine-tuning with culturally and\nlocally informed data can bridge performance gaps without incurring significant\ncomputational overhead. We release our training code, datasets, and models\nunder mit and apache licenses to aid further research towards under-represented\nand low-resource languages.\n","authors":["Ram Mohan Rao Kadiyala","Siddartha Pullakhandam","Siddhant Gupta","Drishti Sharma","Jebish Purbey","Kanwal Mehreen","Muhammad Arham","Hamza Farooq"],"pdf_url":"https://arxiv.org/pdf/2504.09753v1.pdf","comment":"ARR Feb 2025 submission"},{"id":"http://arxiv.org/abs/2504.09737v1","updated":"2025-04-13T22:01:25Z","published":"2025-04-13T22:01:25Z","title":"Can LLM feedback enhance review quality? A randomized study of 20K\n  reviews at ICLR 2025","summary":"  Peer review at AI conferences is stressed by rapidly rising submission\nvolumes, leading to deteriorating review quality and increased author\ndissatisfaction. To address these issues, we developed Review Feedback Agent, a\nsystem leveraging multiple large language models (LLMs) to improve review\nclarity and actionability by providing automated feedback on vague comments,\ncontent misunderstandings, and unprofessional remarks to reviewers. Implemented\nat ICLR 2025 as a large randomized control study, our system provided optional\nfeedback to more than 20,000 randomly selected reviews. To ensure high-quality\nfeedback for reviewers at this scale, we also developed a suite of automated\nreliability tests powered by LLMs that acted as guardrails to ensure feedback\nquality, with feedback only being sent to reviewers if it passed all the tests.\nThe results show that 27% of reviewers who received feedback updated their\nreviews, and over 12,000 feedback suggestions from the agent were incorporated\nby those reviewers. This suggests that many reviewers found the AI-generated\nfeedback sufficiently helpful to merit updating their reviews. Incorporating AI\nfeedback led to significantly longer reviews (an average increase of 80 words\namong those who updated after receiving feedback) and more informative reviews,\nas evaluated by blinded researchers. Moreover, reviewers who were selected to\nreceive AI feedback were also more engaged during paper rebuttals, as seen in\nlonger author-reviewer discussions. This work demonstrates that carefully\ndesigned LLM-generated review feedback can enhance peer review quality by\nmaking reviews more specific and actionable while increasing engagement between\nreviewers and authors. The Review Feedback Agent is publicly available at\nhttps://github.com/zou-group/review_feedback_agent.\n","authors":["Nitya Thakkar","Mert Yuksekgonul","Jake Silberg","Animesh Garg","Nanyun Peng","Fei Sha","Rose Yu","Carl Vondrick","James Zou"],"pdf_url":"https://arxiv.org/pdf/2504.09737v1.pdf","comment":"30 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.09723v1","updated":"2025-04-13T21:10:56Z","published":"2025-04-13T21:10:56Z","title":"AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM\n  Agents","summary":"  A/B testing experiment is a widely adopted method for evaluating UI/UX design\ndecisions in modern web applications. Yet, traditional A/B testing remains\nconstrained by its dependence on the large-scale and live traffic of human\nparticipants, and the long time of waiting for the testing result. Through\nformative interviews with six experienced industry practitioners, we identified\ncritical bottlenecks in current A/B testing workflows. In response, we present\nAgentA/B, a novel system that leverages Large Language Model-based autonomous\nagents (LLM Agents) to automatically simulate user interaction behaviors with\nreal webpages. AgentA/B enables scalable deployment of LLM agents with diverse\npersonas, each capable of navigating the dynamic webpage and interactively\nexecuting multi-step interactions like search, clicking, filtering, and\npurchasing. In a demonstrative controlled experiment, we employ AgentA/B to\nsimulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and\ncompare agent behaviors with real human shopping behaviors at a scale. Our\nfindings suggest AgentA/B can emulate human-like behavior patterns.\n","authors":["Dakuo Wang","Ting-Yao Hsu","Yuxuan Lu","Limeng Cui","Yaochen Xie","William Headean","Bingsheng Yao","Akash Veeragouni","Jiapeng Liu","Sreyashi Nag","Jessie Wang"],"pdf_url":"https://arxiv.org/pdf/2504.09723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09714v1","updated":"2025-04-13T20:45:49Z","published":"2025-04-13T20:45:49Z","title":"Evaluating the Quality of Benchmark Datasets for Low-Resource Languages:\n  A Case Study on Turkish","summary":"  The reliance on translated or adapted datasets from English or multilingual\nresources introduces challenges regarding linguistic and cultural suitability.\nThis study addresses the need for robust and culturally appropriate benchmarks\nby evaluating the quality of 17 commonly used Turkish benchmark datasets. Using\na comprehensive framework that assesses six criteria, both human and LLM-judge\nannotators provide detailed evaluations to identify dataset strengths and\nshortcomings.\n  Our results reveal that 70% of the benchmark datasets fail to meet our\nheuristic quality standards. The correctness of the usage of technical terms is\nthe strongest criterion, but 85% of the criteria are not satisfied in the\nexamined datasets. Although LLM judges demonstrate potential, they are less\neffective than human annotators, particularly in understanding cultural common\nsense knowledge and interpreting fluent, unambiguous text. GPT-4o has stronger\nlabeling capabilities for grammatical and technical tasks, while Llama3.3-70B\nexcels at correctness and cultural knowledge evaluation. Our findings emphasize\nthe urgent need for more rigorous quality control in creating and adapting\ndatasets for low-resource languages.\n","authors":["Ayşe Aysu Cengiz","Ahmet Kaan Sever","Elif Ecem Ümütlü","Naime Şeyma Erdem","Burak Aytan","Büşra Tufan","Abdullah Topraksoy","Esra Darıcı","Cagri Toraman"],"pdf_url":"https://arxiv.org/pdf/2504.09714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09710v1","updated":"2025-04-13T20:10:27Z","published":"2025-04-13T20:10:27Z","title":"DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM\n  Post-training","summary":"  Recent advances in reinforcement learning (RL)-based post-training have led\nto notable improvements in large language models (LLMs), particularly in\nenhancing their reasoning capabilities to handle complex tasks. However, most\nexisting methods treat the training data as a unified whole, overlooking the\nfact that modern LLM training often involves a mixture of data from diverse\ndistributions-varying in both source and difficulty. This heterogeneity\nintroduces a key challenge: how to adaptively schedule training across\ndistributions to optimize learning efficiency. In this paper, we present a\nprincipled curriculum learning framework grounded in the notion of\ndistribution-level learnability. Our core insight is that the magnitude of\npolicy advantages reflects how much a model can still benefit from further\ntraining on a given distribution. Based on this, we propose a\ndistribution-level curriculum learning framework for RL-based LLM\npost-training, which leverages the Upper Confidence Bound (UCB) principle to\ndynamically adjust sampling probabilities for different distrubutions. This\napproach prioritizes distributions with either high average advantage\n(exploitation) or low sample count (exploration), yielding an adaptive and\ntheoretically grounded training schedule. We instantiate our curriculum\nlearning framework with GRPO as the underlying RL algorithm and demonstrate its\neffectiveness on logic reasoning datasets with multiple difficulties and\nsources. Our experiments show that our framework significantly improves\nconvergence speed and final performance, highlighting the value of\ndistribution-aware curriculum strategies in LLM post-training. Code:\nhttps://github.com/ZhentingWang/DUMP.\n","authors":["Zhenting Wang","Guofeng Cui","Kun Wan","Wentian Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.09710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09696v1","updated":"2025-04-13T19:07:45Z","published":"2025-04-13T19:07:45Z","title":"GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for\n  Concise Mathematical Reasoning in Language Models","summary":"  Recent advances in R1-like reasoning models leveraging Group Relative Policy\nOptimization (GRPO) have significantly improved the performance of language\nmodels on mathematical reasoning tasks. However, current GRPO implementations\nencounter critical challenges, including reward sparsity due to binary accuracy\nmetrics, limited incentives for conciseness, and insufficient focus on complex\nreasoning tasks. To address these issues, we propose GRPO-LEAD, a suite of\nnovel enhancements tailored for mathematical reasoning. Specifically, GRPO-LEAD\nintroduces (1) a length-dependent accuracy reward to encourage concise and\nprecise solutions, (2) an explicit penalty mechanism for incorrect answers to\nsharpen decision boundaries, and (3) a difficulty-aware advantage reweighting\nstrategy that amplifies learning signals for challenging problems. Furthermore,\nwe systematically examine the impact of model scale and supervised fine-tuning\n(SFT) strategies, demonstrating that larger-scale base models and carefully\ncurated datasets significantly enhance reinforcement learning effectiveness.\nExtensive empirical evaluations and ablation studies confirm that GRPO-LEAD\nsubstantially mitigates previous shortcomings, resulting in language models\nthat produce more concise, accurate, and robust reasoning across diverse\nmathematical tasks.\n","authors":["Jixiao Zhang","Chunsheng Zuo"],"pdf_url":"https://arxiv.org/pdf/2504.09696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09689v1","updated":"2025-04-13T18:47:22Z","published":"2025-04-13T18:47:22Z","title":"EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental\n  Health Safety","summary":"  The rise of LLM-driven AI characters raises safety concerns, particularly for\nvulnerable human users with psychological disorders. To address these risks, we\npropose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate\nmental health hazards in human-AI interactions. EmoAgent comprises two\ncomponents: EmoEval simulates virtual users, including those portraying\nmentally vulnerable individuals, to assess mental health changes before and\nafter interactions with AI characters. It uses clinically proven psychological\nand psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks\ninduced by LLM. EmoGuard serves as an intermediary, monitoring users' mental\nstatus, predicting potential harm, and providing corrective feedback to\nmitigate risks. Experiments conducted in popular character-based chatbots show\nthat emotionally engaging dialogues can lead to psychological deterioration in\nvulnerable users, with mental state deterioration in more than 34.4% of the\nsimulations. EmoGuard significantly reduces these deterioration rates,\nunderscoring its role in ensuring safer AI-human interactions. Our code is\navailable at: https://github.com/1akaman/EmoAgent\n","authors":["Jiahao Qiu","Yinghui He","Xinzhe Juan","Yiming Wang","Yuhan Liu","Zixin Yao","Yue Wu","Xun Jiang","Ling Yang","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2504.09689v1.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2504.09687v1","updated":"2025-04-13T18:40:32Z","published":"2025-04-13T18:40:32Z","title":"Domain-Adaptive Continued Pre-Training of Small Language Models","summary":"  Continued pre-training of small language models offers a promising path for\ndomain adaptation with limited computational resources. I've investigated this\napproach within educational domains, evaluating it as a resource-efficient\nalternative to training models from scratch. Using a 125M parameter model, I\ndemonstrate significant performance improvements through incremental training\non 400 million tokens, followed by further training to reach 1 billion tokens.\nMy approach includes comprehensive data preprocessing, memory-optimized\ntraining configurations, and benchmark-based evaluation. Results show notable\ngains in knowledge-intensive tasks (MMLU +8.1%) and contextual understanding\n(HellaSwag +7.6%), while revealing educational domain specialization\ntrade-offs. I analyze token efficiency, catastrophic forgetting mitigation\nstrategies, and scaling patterns. My findings suggest that thoughtful\npreprocessing and training methodologies enable meaningful improvements in\nlanguage model capabilities even with constrained computational resources,\nopening pathways for domain-specific adaptation of smaller language models.\n","authors":["Salman Faroz"],"pdf_url":"https://arxiv.org/pdf/2504.09687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09665v1","updated":"2025-04-13T17:34:35Z","published":"2025-04-13T17:34:35Z","title":"CLEAR-KGQA: Clarification-Enhanced Ambiguity Resolution for Knowledge\n  Graph Question Answering","summary":"  This study addresses the challenge of ambiguity in knowledge graph question\nanswering (KGQA). While recent KGQA systems have made significant progress,\nparticularly with the integration of large language models (LLMs), they\ntypically assume user queries are unambiguous, which is an assumption that\nrarely holds in real-world applications. To address these limitations, we\npropose a novel framework that dynamically handles both entity ambiguity (e.g.,\ndistinguishing between entities with similar names) and intent ambiguity (e.g.,\nclarifying different interpretations of user queries) through interactive\nclarification. Our approach employs a Bayesian inference mechanism to quantify\nquery ambiguity and guide LLMs in determining when and how to request\nclarification from users within a multi-turn dialogue framework. We further\ndevelop a two-agent interaction framework where an LLM-based user simulator\nenables iterative refinement of logical forms through simulated user feedback.\nExperimental results on the WebQSP and CWQ dataset demonstrate that our method\nsignificantly improves performance by effectively resolving semantic\nambiguities. Additionally, we contribute a refined dataset of disambiguated\nqueries, derived from interaction histories, to facilitate future research in\nthis direction.\n","authors":["Liqiang Wen","Guanming Xiong","Tong Mo","Bing Li","Weiping Li","Wen Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.09665v1.pdf","comment":"This work has been accepted by the IJCNN 2025 main track"},{"id":"http://arxiv.org/abs/2004.13821v4","updated":"2025-04-13T16:39:37Z","published":"2020-04-20T09:34:16Z","title":"Fine-tuning Multi-hop Question Answering with Hierarchical Graph Network","summary":"  In this paper, we present a two stage model for multi-hop question answering.\nThe first stage is a hierarchical graph network, which is used to reason over\nmulti-hop question and is capable to capture different levels of granularity\nusing the nature structure(i.e., paragraphs, questions, sentences and entities)\nof documents. The reasoning process is convert to node classify task(i.e.,\nparagraph nodes and sentences nodes). The second stage is a language model\nfine-tuning task. In a word, stage one use graph neural network to select and\nconcatenate support sentences as one paragraph, and stage two find the answer\nspan in language model fine-tuning paradigm.\n","authors":["Guanming Xiong"],"pdf_url":"https://arxiv.org/pdf/2004.13821v4.pdf","comment":"Incomplete Work"},{"id":"http://arxiv.org/abs/2504.09645v1","updated":"2025-04-13T16:36:59Z","published":"2025-04-13T16:36:59Z","title":"Myanmar XNLI: Building a Dataset and Exploring Low-resource Approaches\n  to Natural Language Inference with Myanmar","summary":"  Despite dramatic recent progress in NLP, it is still a major challenge to\napply Large Language Models (LLM) to low-resource languages. This is made\nvisible in benchmarks such as Cross-Lingual Natural Language Inference (XNLI),\na key task that demonstrates cross-lingual capabilities of NLP systems across a\nset of 15 languages. In this paper, we extend the XNLI task for one additional\nlow-resource language, Myanmar, as a proxy challenge for broader low-resource\nlanguages, and make three core contributions. First, we build a dataset called\nMyanmar XNLI (myXNLI) using community crowd-sourced methods, as an extension to\nthe existing XNLI corpus. This involves a two-stage process of community-based\nconstruction followed by expert verification; through an analysis, we\ndemonstrate and quantify the value of the expert verification stage in the\ncontext of community-based construction for low-resource languages. We make the\nmyXNLI dataset available to the community for future research. Second, we carry\nout evaluations of recent multilingual language models on the myXNLI benchmark,\nas well as explore data-augmentation methods to improve model performance. Our\ndata-augmentation methods improve model accuracy by up to 2 percentage points\nfor Myanmar, while uplifting other languages at the same time. Third, we\ninvestigate how well these data-augmentation methods generalise to other\nlow-resource languages in the XNLI dataset.\n","authors":["Aung Kyaw Htet","Mark Dras"],"pdf_url":"https://arxiv.org/pdf/2504.09645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09643v1","updated":"2025-04-13T16:34:17Z","published":"2025-04-13T16:34:17Z","title":"Iterative Self-Training for Code Generation via Reinforced Re-Ranking","summary":"  Generating high-quality code that solves complex programming tasks is\nchallenging, especially with current decoder-based models that produce highly\nstochastic outputs. In code generation, even minor errors can easily break the\nentire solution. Leveraging multiple sampled solutions can significantly\nimprove the overall output quality.\n  One effective way to enhance code generation is by pairing a code generation\nmodel with a reranker model, which selects the best solution from the generated\nsamples. We propose a novel iterative self-training approach for self-training\nreranker models using Proximal Policy Optimization (PPO), aimed at improving\nboth reranking accuracy and the overall code generation process. Unlike\ntraditional PPO approaches, where the focus is on optimizing a generative model\nwith a reward model, our approach emphasizes the development of a robust\nreward/reranking model. This model improves the quality of generated code\nthrough reranking and addresses problems and errors that the reward model might\noverlook during PPO alignment with the reranker. Our method iteratively refines\nthe training dataset by re-evaluating outputs, identifying high-scoring\nnegative examples, and incorporating them into the training loop, that boosting\nmodel performance.\n  Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter\nmodel outperforms a 33B model in code generation quality while being three\ntimes faster. Moreover, it achieves performance comparable to GPT-4 and\nsurpasses it in one programming language.\n","authors":["Nikita Sorokin","Ivan Sedykh","Valentin Malykh"],"pdf_url":"https://arxiv.org/pdf/2504.09643v1.pdf","comment":"Published at ECIR 2025"},{"id":"http://arxiv.org/abs/2504.09639v1","updated":"2025-04-13T16:26:56Z","published":"2025-04-13T16:26:56Z","title":"Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model\n  Capability","summary":"  Recent advancements in large language models (LLMs), such as DeepSeek-R1 and\nOpenAI-o1, have demonstrated the significant effectiveness of test-time\nscaling, achieving substantial performance gains across various benchmarks.\nThese advanced models utilize deliberate \"thinking\" steps to systematically\nenhance answer quality. In this paper, we propose leveraging these high-quality\noutputs generated by reasoning-intensive models to improve less computationally\ndemanding, non-reasoning models. We explore and compare methodologies for\nutilizing the answers produced by reasoning models to train and improve\nnon-reasoning models. Through straightforward Supervised Fine-Tuning (SFT)\nexperiments on established benchmarks, we demonstrate consistent improvements\nacross various benchmarks, underscoring the potential of this approach for\nadvancing the ability of models to answer questions directly.\n","authors":["Haotian Wang","Han Zhao","Shuaiting Chen","Xiaoyu Tian","Sitong Zhao","Yunjie Ji","Yiping Peng","Xiangang Li"],"pdf_url":"https://arxiv.org/pdf/2504.09639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10924v4","updated":"2025-04-13T16:17:45Z","published":"2024-12-14T18:18:52Z","title":"Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning","summary":"  Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens and current\nstructural constraints motivate changes to existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokens and pretraining can act as a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being arguably\nmeaningfully insulated from the main system intelligence. [First uploaded to\narXiv in December, 2024.]\n","authors":["Julia Witte Zimmerman","Denis Hudon","Kathryn Cramer","Alejandro J. Ruiz","Calla Beauregard","Ashley Fehr","Mikaela Irene Fudolig","Bradford Demarest","Yoshi Meke Bird","Milo Z. Trujillo","Christopher M. Danforth","Peter Sheridan Dodds"],"pdf_url":"https://arxiv.org/pdf/2412.10924v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09620v1","updated":"2025-04-13T15:28:09Z","published":"2025-04-13T15:28:09Z","title":"Metropolis-Hastings Captioning Game: Knowledge Fusion of Vision Language\n  Models via Decentralized Bayesian Inference","summary":"  We propose the Metropolis-Hastings Captioning Game (MHCG), a method to fuse\nknowledge of multiple vision-language models (VLMs) by learning from each\nother. Although existing methods that combine multiple models suffer from\ninference costs and architectural constraints, MHCG avoids these problems by\nperforming decentralized Bayesian inference through a process resembling a\nlanguage game. The knowledge fusion process establishes communication between\ntwo VLM agents alternately captioning images and learning from each other. We\nconduct two image-captioning experiments with two VLMs, each pre-trained on a\ndifferent dataset. The first experiment demonstrates that MHCG achieves\nconsistent improvement in reference-free evaluation metrics. The second\nexperiment investigates how MHCG contributes to sharing VLMs' category-level\nvocabulary by observing the occurrence of the vocabulary in the generated\ncaptions.\n","authors":["Yuta Matsui","Ryosuke Yamaki","Ryo Ueda","Seitaro Shinagawa","Tadahiro Taniguchi"],"pdf_url":"https://arxiv.org/pdf/2504.09620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04569v2","updated":"2025-04-13T14:53:45Z","published":"2025-04-06T17:58:08Z","title":"KnowsLM: A framework for evaluation of small language models for\n  knowledge augmentation and humanised conversations","summary":"  In the evolving landscape of conversational AI, generating concise,\ncontext-aware, and human-like dialogue using small and medium-sized language\nmodels (LLMs) remains a complex challenge. This study investigates the\ninfluence of LoRA rank, dataset scale, and prompt prefix design on both\nknowledge retention and stylistic alignment. While fine-tuning improves fluency\nand enables stylistic customization, its ability to integrate unseen knowledge\nis constrained -- particularly with smaller datasets. Conversely, RAG-augmented\nmodels, equipped to incorporate external documents at inference, demonstrated\nsuperior factual accuracy on out-of-distribution prompts, though they lacked\nthe stylistic consistency achieved by fine-tuning. Evaluations by LLM-based\njudges across knowledge accuracy, conversational quality, and conciseness\nsuggest that fine-tuning is best suited for tone adaptation, whereas RAG excels\nat real-time knowledge augmentation.\n","authors":["Chitranshu Harbola","Anupam Purwar"],"pdf_url":"https://arxiv.org/pdf/2504.04569v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09602v1","updated":"2025-04-13T14:35:30Z","published":"2025-04-13T14:35:30Z","title":"Fine-tuning an Large Language Model for Automating Computational Fluid\n  Dynamics Simulations","summary":"  Configuring computational fluid dynamics (CFD) simulations typically demands\nextensive domain expertise, limiting broader access. Although large language\nmodels (LLMs) have advanced scientific computing, their use in automating CFD\nworkflows is underdeveloped. We introduce a novel approach centered on\ndomain-specific LLM adaptation. By fine-tuning Qwen2.5-7B-Instruct on NL2FOAM,\nour custom dataset of 28716 natural language-to-OpenFOAM configuration pairs\nwith chain-of-thought (CoT) annotations, we enable direct translation from\nnatural language descriptions to executable CFD setups. A multi-agent framework\norchestrates the process, autonomously verifying inputs, generating\nconfigurations, running simulations, and correcting errors. Evaluation on a\nbenchmark of 21 diverse flow cases demonstrates state-of-the-art performance,\nachieving 88.7% solution accuracy and 82.6% first-attempt success rate. This\nsignificantly outperforms larger general-purpose models like\nQwen2.5-72B-Instruct, DeepSeek-R1, and Llama3.3-70B-Instruct, while also\nrequiring fewer correction iterations and maintaining high computational\nefficiency. The results highlight the critical role of domain-specific\nadaptation in deploying LLM assistants for complex engineering workflows.\n","authors":["Zhehao Dong","Zhen Lu","Yue Yang"],"pdf_url":"https://arxiv.org/pdf/2504.09602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12280v2","updated":"2025-04-13T14:17:57Z","published":"2024-02-19T16:47:04Z","title":"Plato: Plan to Efficiently Decode for Large Language Model Inference","summary":"  Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%.\n","authors":["Shuowei Jin","Xueshen Liu","Yongji Wu","Haizhong Zheng","Qingzhao Zhang","Atul Prakash","Matthew Lentz","Danyang Zhuo","Feng Qian","Z. Morley Mao"],"pdf_url":"https://arxiv.org/pdf/2402.12280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09586v1","updated":"2025-04-13T14:12:14Z","published":"2025-04-13T14:12:14Z","title":"Short-Path Prompting in LLMs: Analyzing Reasoning Instability and\n  Solutions for Robust Performance","summary":"  Recent years have witnessed significant progress in large language models'\n(LLMs) reasoning, which is largely due to the chain-of-thought (CoT)\napproaches, allowing models to generate intermediate reasoning steps before\nreaching the final answer. Building on these advances, state-of-the-art LLMs\nare instruction-tuned to provide long and detailed CoT pathways when responding\nto reasoning-related questions. However, human beings are naturally cognitive\nmisers and will prompt language models to give rather short responses, thus\nraising a significant conflict with CoT reasoning. In this paper, we delve into\nhow LLMs' reasoning performance changes when users provide short-path prompts.\nThe results and analysis reveal that language models can reason effectively and\nrobustly without explicit CoT prompts, while under short-path prompting, LLMs'\nreasoning ability drops significantly and becomes unstable, even on\ngrade-school problems. To address this issue, we propose two approaches: an\ninstruction-guided approach and a fine-tuning approach, both designed to\neffectively manage the conflict. Experimental results show that both methods\nachieve high accuracy, providing insights into the trade-off between\ninstruction adherence and reasoning accuracy in current models.\n","authors":["Zuoli Tang","Junjie Ou","Kaiqin Hu","Chunwei Wu","Zhaoxin Huan","Chilin Fu","Xiaolu Zhang","Jun Zhou","Chenliang Li"],"pdf_url":"https://arxiv.org/pdf/2504.09586v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2504.09582v1","updated":"2025-04-13T14:05:40Z","published":"2025-04-13T14:05:40Z","title":"Reduction of Supervision for Biomedical Knowledge Discovery","summary":"  Knowledge discovery is hindered by the increasing volume of publications and\nthe scarcity of extensive annotated data. To tackle the challenge of\ninformation overload, it is essential to employ automated methods for knowledge\nextraction and processing. Finding the right balance between the level of\nsupervision and the effectiveness of models poses a significant challenge.\nWhile supervised techniques generally result in better performance, they have\nthe major drawback of demanding labeled data. This requirement is\nlabor-intensive and time-consuming and hinders scalability when exploring new\ndomains. In this context, our study addresses the challenge of identifying\nsemantic relationships between biomedical entities (e.g., diseases, proteins)\nin unstructured text while minimizing dependency on supervision. We introduce a\nsuite of unsupervised algorithms based on dependency trees and attention\nmechanisms and employ a range of pointwise binary classification methods.\nTransitioning from weakly supervised to fully unsupervised settings, we assess\nthe methods' ability to learn from data with noisy labels. The evaluation on\nbiomedical benchmark datasets explores the effectiveness of the methods. Our\napproach tackles a central issue in knowledge discovery: balancing performance\nwith minimal supervision. By gradually decreasing supervision, we assess the\nrobustness of pointwise binary classification techniques in handling noisy\nlabels, revealing their capability to shift from weakly supervised to entirely\nunsupervised scenarios. Comprehensive benchmarking offers insights into the\neffectiveness of these techniques, suggesting an encouraging direction toward\nadaptable knowledge discovery systems, representing progress in creating\ndata-efficient methodologies for extracting useful insights when annotated data\nis limited.\n","authors":["Christos Theodoropoulos","Andrei Catalin Coman","James Henderson","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2504.09582v1.pdf","comment":"Published as part of the PhD dissertation: Theodoropoulos, Christos,\n  Marie-Francine Moens, and Matthew Blaschko. \"Deep Learning Models for the\n  Extraction of Knowledge from Text.\" (2025)"},{"id":"http://arxiv.org/abs/2409.15355v5","updated":"2025-04-13T14:02:47Z","published":"2024-09-14T02:34:26Z","title":"Block-Attention for Efficient Prefilling","summary":"  We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section.\n","authors":["Dongyang Ma","Yan Wang","Lan Tian"],"pdf_url":"https://arxiv.org/pdf/2409.15355v5.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2504.09570v1","updated":"2025-04-13T13:45:53Z","published":"2025-04-13T13:45:53Z","title":"LLMs Can Achieve High-quality Simultaneous Machine Translation as\n  Efficiently as Offline","summary":"  When the complete source sentence is provided, Large Language Models (LLMs)\nperform excellently in offline machine translation even with a simple prompt\n\"Translate the following sentence from [src lang] into [tgt lang]:\". However,\nin many real scenarios, the source tokens arrive in a streaming manner and\nsimultaneous machine translation (SiMT) is required, then the efficiency and\nperformance of decoder-only LLMs are significantly limited by their\nauto-regressive nature. To enable LLMs to achieve high-quality SiMT as\nefficiently as offline translation, we propose a novel paradigm that includes\nconstructing supervised fine-tuning (SFT) data for SiMT, along with new\ntraining and inference strategies. To replicate the token input/output stream\nin SiMT, the source and target tokens are rearranged into an interleaved\nsequence, separated by special tokens according to varying latency\nrequirements. This enables powerful LLMs to learn read and write operations\nadaptively, based on varying latency prompts, while still maintaining efficient\nauto-regressive decoding. Experimental results show that, even with limited SFT\ndata, our approach achieves state-of-the-art performance across various SiMT\nbenchmarks, and preserves the original abilities of offline translation.\nMoreover, our approach generalizes well to document-level SiMT setting without\nrequiring specific fine-tuning, even beyond the offline translation model.\n","authors":["Biao Fu","Minpeng Liao","Kai Fan","Chengxi Li","Liang Zhang","Yidong Chen","Xiaodong Shi"],"pdf_url":"https://arxiv.org/pdf/2504.09570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09566v1","updated":"2025-04-13T13:35:41Z","published":"2025-04-13T13:35:41Z","title":"Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution","summary":"  Chain-of-Thought (CoT) prompting enhances the reasoning of large language\nmodels (LLMs) by decomposing problems into sequential steps, mimicking human\nlogic and reducing errors. However, complex tasks with vast solution spaces and\nvague constraints often exceed the capacity of a single reasoning chain.\nInspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic\ngeometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends\nCoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper\nlogical dependencies, enabling more robust and structured problem-solving. MFR\ndecomposes a module into a sequence of free modules with minimal rank,\nproviding a structured analytical approach to complex systems. This method\nintroduces the concepts of \"Module\", \"Betti numbers\",\"Freeness\", \"Mapping\",\n\"Exactness\" and \"Minimality\", enabling the systematic decomposition of the\noriginal complex problem into logically complete minimal subproblems while\npreserving key problem features and reducing reasoning length. We tested SoT\nacross diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini,\nQwen2.5), achieving inference accuracy that matches or surpasses mainstream\nCoTs standards. Additionally, by aligning the sampling process with algebraic\nconstraints, our approach enhances the scalability of inference time in LLMs,\nensuring both transparent reasoning and high performance. Our code will be\npublicly available at https://github.com/dlMARiA/Syzygy-of-thoughts.\n","authors":["Chenghao Li","Chaoning Zhang","Yi Lu","Jiaquan Zhang","Qigan Sun","Xudong Wang","Jiwei Wei","Guoqing Wang","Yang Yang","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2504.09566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10999v5","updated":"2025-04-13T13:03:09Z","published":"2024-06-16T16:25:22Z","title":"Balancing Rigor and Utility: Mitigating Cognitive Biases in Large\n  Language Models for Multiple-Choice Questions","summary":"  This paper examines the role of cognitive biases in the decision-making\nprocesses of large language models (LLMs), challenging the conventional goal of\neliminating all biases. When properly balanced, we show that certain cognitive\nbiases can enhance decision-making efficiency through rational deviations and\nheuristic shortcuts. By introducing heuristic moderation and an abstention\noption, which allows LLMs to withhold responses when uncertain, we reduce error\nrates, improve decision accuracy, and optimize decision rates. Using the\nBalance Rigor and Utility (BRU) dataset, developed through expert\ncollaboration, our findings demonstrate that targeted inspection of cognitive\nbiases aligns LLM decisions more closely with human reasoning, enhancing\nreliability and suggesting strategies for future improvements. This approach\noffers a novel way to leverage cognitive biases to improve the practical\nutility of LLMs across various applications.\n","authors":["Hanyang Zhong","Liman Wang","Wenting Cao","Zeyuan Sun"],"pdf_url":"https://arxiv.org/pdf/2406.10999v5.pdf","comment":"This work has been accepted as a full paper at the 2025 Annual\n  Conference of the Cognitive Science Society (CogSci 2025) and will be\n  presented in the form of a poster. The dataset and project website are\n  available at: https://hanyangzhong.github.io/BRU-website/"},{"id":"http://arxiv.org/abs/2503.13208v3","updated":"2025-04-13T12:38:06Z","published":"2025-03-17T14:20:48Z","title":"Improving Complex Reasoning with Dynamic Prompt Corruption: A soft\n  prompt Optimization Approach","summary":"  Prompt-tuning (PT) for large language models (LLMs) can facilitate the\nperformance on various conventional NLP tasks with significantly fewer\ntrainable parameters. However, our investigation reveals that PT provides\nlimited improvement and may even degrade the primitive performance of LLMs on\ncomplex reasoning tasks. Such a phenomenon suggests that soft prompts can\npositively impact certain instances while negatively affecting others,\nparticularly during the later phases of reasoning. To address these challenges,\nWe first identify an information accumulation within the soft prompts. Through\ndetailed analysis, we demonstrate that this phenomenon is often accompanied by\nerroneous information flow patterns in the deeper layers of the model, which\nultimately lead to incorrect reasoning outcomes. we propose a novel method\ncalled Dynamic Prompt Corruption (DPC) to take better advantage of soft prompts\nin complex reasoning tasks, which dynamically adjusts the influence of soft\nprompts based on their impact on the reasoning process. Specifically, DPC\nconsists of two stages: Dynamic Trigger and Dynamic Corruption. First, Dynamic\nTrigger measures the impact of soft prompts, identifying whether beneficial or\ndetrimental. Then, Dynamic Corruption mitigates the negative effects of soft\nprompts by selectively masking key tokens that interfere with the reasoning\nprocess. We validate the proposed approach through extensive experiments on\nvarious LLMs and reasoning tasks, including GSM8K, MATH, and AQuA. Experimental\nresults demonstrate that DPC can consistently enhance the performance of PT,\nachieving 4%-8% accuracy gains compared to vanilla prompt tuning, highlighting\nthe effectiveness of our approach and its potential to enhance complex\nreasoning in LLMs.\n","authors":["Sinan Fan","Liang Xie","Chen Shen","Ge Teng","Xiaosong Yuan","Xiaofeng Zhang","Chenxi Huang","Wenxiao Wang","Xiaofei He","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2503.13208v3.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2504.03786v2","updated":"2025-04-13T12:33:54Z","published":"2025-04-03T17:43:45Z","title":"Do \"New Snow Tablets\" Contain Snow? Large Language Models Over-Rely on\n  Names to Identify Ingredients of Chinese Drugs","summary":"  Traditional Chinese Medicine (TCM) has seen increasing adoption in\nhealthcare, with specialized Large Language Models (LLMs) emerging to support\nclinical applications. A fundamental requirement for these models is accurate\nidentification of TCM drug ingredients. In this paper, we evaluate how general\nand TCM-specialized LLMs perform when identifying ingredients of Chinese drugs.\nOur systematic analysis reveals consistent failure patterns: models often\ninterpret drug names literally, overuse common herbs regardless of relevance,\nand exhibit erratic behaviors when faced with unfamiliar formulations. LLMs\nalso fail to understand the verification task. These findings demonstrate that\ncurrent LLMs rely primarily on drug names rather than possessing systematic\npharmacological knowledge. To address these limitations, we propose a Retrieval\nAugmented Generation (RAG) approach focused on ingredient names. Experiments\nacross 220 TCM formulations show our method significantly improves accuracy\nfrom approximately 50% to 82% in ingredient verification tasks. Our work\nhighlights critical weaknesses in current TCM-specific LLMs and offers a\npractical solution for enhancing their clinical reliability.\n","authors":["Sifan Li","Yujun Cai","Bryan Hooi","Nanyun Peng","Yiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2504.03786v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09522v1","updated":"2025-04-13T11:25:04Z","published":"2025-04-13T11:25:04Z","title":"How new data permeates LLM knowledge and how to dilute it","summary":"  Large language models learn and continually learn through the accumulation of\ngradient-based updates, but how individual pieces of new information affect\nexisting knowledge, leading to both beneficial generalization and problematic\nhallucination, remains poorly understood. We demonstrate that when learning new\ninformation, LLMs exhibit a \"priming\" effect: learning a new fact can cause the\nmodel to inappropriately apply that knowledge in unrelated contexts. To\nsystematically study this phenomenon, we introduce \"Outlandish,\" a carefully\ncurated dataset of 1320 diverse text samples designed to probe how new\nknowledge permeates through an LLM's existing knowledge base. Using this\ndataset, we show that the degree of priming after learning new information can\nbe predicted by measuring the token probability of key words before learning.\nThis relationship holds robustly across different model architectures (PALM-2,\nGemma, Llama), sizes, and training stages. Finally, we develop two novel\ntechniques to modulate how new knowledge affects existing model behavior: (1) a\n``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update\npruning method. These approaches reduce undesirable priming effects by 50-95\\%\nwhile preserving the model's ability to learn new information. Our findings\nprovide both empirical insights into how LLMs learn and practical tools for\nimproving the specificity of knowledge insertion in language models. Further\nmaterials: https://sunchipsster1.github.io/projects/outlandish/\n","authors":["Chen Sun","Renat Aksitov","Andrey Zhmoginov","Nolan Andrew Miller","Max Vladymyrov","Ulrich Rueckert","Been Kim","Mark Sandler"],"pdf_url":"https://arxiv.org/pdf/2504.09522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09736v2","updated":"2025-04-13T10:51:40Z","published":"2023-12-15T12:20:24Z","title":"HEAR: Hearing Enhanced Audio Response for Video-grounded Dialogue","summary":"  Video-grounded Dialogue (VGD) aims to answer questions regarding a given\nmulti-modal input comprising video, audio, and dialogue history. Although there\nhave been numerous efforts in developing VGD systems to improve the quality of\ntheir responses, existing systems are competent only to incorporate the\ninformation in the video and text and tend to struggle in extracting the\nnecessary information from the audio when generating appropriate responses to\nthe question. The VGD system seems to be deaf, and thus, we coin this symptom\nof current systems' ignoring audio data as a deaf response. To overcome the\ndeaf response problem, Hearing Enhanced Audio Response (HEAR) framework is\nproposed to perform sensible listening by selectively attending to audio\nwhenever the question requires it. The HEAR framework enhances the accuracy and\naudibility of VGD systems in a model-agnostic manner. HEAR is validated on VGD\ndatasets (i.e., AVSD@DSTC7 and AVSD@DSTC8) and shows effectiveness with various\nVGD systems.\n","authors":["Sunjae Yoon","Dahyun Kim","Eunseop Yoon","Hee Suk Yoon","Junyeong Kim","Chnag D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2312.09736v2.pdf","comment":"EMNLP 2023, 14 pages, 13 figures"},{"id":"http://arxiv.org/abs/2504.09504v1","updated":"2025-04-13T10:07:52Z","published":"2025-04-13T10:07:52Z","title":"MADLLM: Multivariate Anomaly Detection via Pre-trained LLMs","summary":"  When applying pre-trained large language models (LLMs) to address anomaly\ndetection tasks, the multivariate time series (MTS) modality of anomaly\ndetection does not align with the text modality of LLMs. Existing methods\nsimply transform the MTS data into multiple univariate time series sequences,\nwhich can cause many problems. This paper introduces MADLLM, a novel\nmultivariate anomaly detection method via pre-trained LLMs. We design a new\ntriple encoding technique to align the MTS modality with the text modality of\nLLMs. Specifically, this technique integrates the traditional patch embedding\nmethod with two novel embedding approaches: Skip Embedding, which alters the\norder of patch processing in traditional methods to help LLMs retain knowledge\nof previous features, and Feature Embedding, which leverages contrastive\nlearning to allow the model to better understand the correlations between\ndifferent features. Experimental results demonstrate that our method\noutperforms state-of-the-art methods in various public anomaly detection\ndatasets.\n","authors":["Wei Tao","Xiaoyang Qu","Kai Lu","Jiguang Wan","Guokuan Li","Jianzong Wang"],"pdf_url":"https://arxiv.org/pdf/2504.09504v1.pdf","comment":"Accepted by IEEE International Conference on Multimedia & Expo 2025\n  (ICME 2025)"},{"id":"http://arxiv.org/abs/2408.02479v2","updated":"2025-04-13T09:42:30Z","published":"2024-08-05T14:01:15Z","title":"From LLMs to LLM-based Agents for Software Engineering: A Survey of\n  Current, Challenges and Future","summary":"  With the rise of large language models (LLMs), researchers are increasingly\nexploring their applications in var ious vertical domains, such as software\nengineering. LLMs have achieved remarkable success in areas including code\ngeneration and vulnerability detection. However, they also exhibit numerous\nlimitations and shortcomings. LLM-based agents, a novel tech nology with the\npotential for Artificial General Intelligence (AGI), combine LLMs as the core\nfor decision-making and action-taking, addressing some of the inherent\nlimitations of LLMs such as lack of autonomy and self-improvement. Despite\nnumerous studies and surveys exploring the possibility of using LLMs in\nsoftware engineering, it lacks a clear distinction between LLMs and LLM based\nagents. It is still in its early stage for a unified standard and benchmarking\nto qualify an LLM solution as an LLM-based agent in its domain. In this survey,\nwe broadly investigate the current practice and solutions for LLMs and\nLLM-based agents for software engineering. In particular we summarise six key\ntopics: requirement engineering, code generation, autonomous decision-making,\nsoftware design, test generation, and software maintenance. We review and\ndifferentiate the work of LLMs and LLM-based agents from these six topics,\nexamining their differences and similarities in tasks, benchmarks, and\nevaluation metrics. Finally, we discuss the models and benchmarks used,\nproviding a comprehensive analysis of their applications and effectiveness in\nsoftware engineering. We anticipate this work will shed some lights on pushing\nthe boundaries of LLM-based agents in software engineering for future research.\n","authors":["Haolin Jin","Linghan Huang","Haipeng Cai","Jun Yan","Bo Li","Huaming Chen"],"pdf_url":"https://arxiv.org/pdf/2408.02479v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09488v1","updated":"2025-04-13T09:01:05Z","published":"2025-04-13T09:01:05Z","title":"Kongzi: A Historical Large Language Model with Fact Enhancement","summary":"  The capabilities of the latest large language models (LLMs) have been\nextended from pure natural language understanding to complex reasoning tasks.\nHowever, current reasoning models often exhibit factual inaccuracies in longer\nreasoning chains, which poses challenges for historical reasoning and limits\nthe potential of LLMs in complex, knowledge-intensive tasks. Historical studies\nrequire not only the accurate presentation of factual information but also the\nability to establish cross-temporal correlations and derive coherent\nconclusions from fragmentary and often ambiguous sources. To address these\nchallenges, we propose Kongzi, a large language model specifically designed for\nhistorical analysis. Through the integration of curated, high-quality\nhistorical data and a novel fact-reinforcement learning strategy, Kongzi\ndemonstrates strong factual alignment and sophisticated reasoning depth.\nExtensive experiments on tasks such as historical question answering and\nnarrative generation demonstrate that Kongzi outperforms existing models in\nboth factual accuracy and reasoning depth. By effectively addressing the unique\nchallenges inherent in historical texts, Kongzi sets a new standard for the\ndevelopment of accurate and reliable LLMs in professional domains.\n","authors":["Jiashu Yang","Ningning Wang","Yian Zhao","Chaoran Feng","Junjia Du","Hao Pang","Zhirui Fang","Xuxin Cheng"],"pdf_url":"https://arxiv.org/pdf/2504.09488v1.pdf","comment":"22 pages, 12 figures"},{"id":"http://arxiv.org/abs/2504.09482v1","updated":"2025-04-13T08:35:22Z","published":"2025-04-13T08:35:22Z","title":"HalluShift: Measuring Distribution Shifts towards Hallucination\n  Detection in LLMs","summary":"  Large Language Models (LLMs) have recently garnered widespread attention due\nto their adeptness at generating innovative responses to the given prompts\nacross a multitude of domains. However, LLMs often suffer from the inherent\nlimitation of hallucinations and generate incorrect information while\nmaintaining well-structured and coherent responses. In this work, we\nhypothesize that hallucinations stem from the internal dynamics of LLMs. Our\nobservations indicate that, during passage generation, LLMs tend to deviate\nfrom factual accuracy in subtle parts of responses, eventually shifting toward\nmisinformation. This phenomenon bears a resemblance to human cognition, where\nindividuals may hallucinate while maintaining logical coherence, embedding\nuncertainty within minor segments of their speech. To investigate this further,\nwe introduce an innovative approach, HalluShift, designed to analyze the\ndistribution shifts in the internal state space and token probabilities of the\nLLM-generated responses. Our method attains superior performance compared to\nexisting baselines across various benchmark datasets. Our codebase is available\nat https://github.com/sharanya-dasgupta001/hallushift.\n","authors":["Sharanya Dasgupta","Sujoy Nath","Arkaprabha Basu","Pourya Shamsolmoali","Swagatam Das"],"pdf_url":"https://arxiv.org/pdf/2504.09482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07824v4","updated":"2025-04-13T08:22:51Z","published":"2025-01-14T03:59:48Z","title":"Real-time Verification and Refinement of Language Model Text Generation","summary":"  Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods.\n","authors":["Joonho Ko","Jinheon Baek","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2501.07824v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09479v1","updated":"2025-04-13T08:22:09Z","published":"2025-04-13T08:22:09Z","title":"Draw with Thought: Unleashing Multimodal Reasoning for Scientific\n  Diagram Generation","summary":"  Scientific diagrams are vital tools for communicating structured knowledge\nacross disciplines. However, they are often published as static raster images,\nlosing symbolic semantics and limiting reuse. While Multimodal Large Language\nModels (MLLMs) offer a pathway to bridging vision and structure, existing\nmethods lack semantic control and structural interpretability, especially on\ncomplex diagrams. We propose Draw with Thought (DwT), a training-free framework\nthat guides MLLMs to reconstruct diagrams into editable mxGraph XML code\nthrough cognitively-grounded Chain-of-Thought reasoning. DwT enables\ninterpretable and controllable outputs without model fine-tuning by dividing\nthe task into two stages: Coarse-to-Fine Planning, which handles perceptual\nstructuring and semantic specification, and Structure-Aware Code Generation,\nenhanced by format-guided refinement. To support evaluation, we release\nPlot2XML, a benchmark of 247 real-world scientific diagrams with gold-standard\nXML annotations. Extensive experiments across eight MLLMs show that our\napproach yields high-fidelity, semantically aligned, and structurally valid\nreconstructions, with human evaluations confirming strong alignment in both\naccuracy and visual aesthetics, offering a scalable solution for converting\nstatic visuals into executable representations and advancing machine\nunderstanding of scientific graphics.\n","authors":["Zhiqing Cui","Jiahao Yuan","Hanqing Wang","Yanshu Li","Chenxu Du","Zhenglong Ding"],"pdf_url":"https://arxiv.org/pdf/2504.09479v1.pdf","comment":"26 pages, 14 figures"},{"id":"http://arxiv.org/abs/2504.09466v1","updated":"2025-04-13T07:39:17Z","published":"2025-04-13T07:39:17Z","title":"AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender","summary":"  Despite extensive efforts in safety alignment, large language models (LLMs)\nremain vulnerable to jailbreak attacks. Activation steering offers a\ntraining-free defense method but relies on fixed steering coefficients,\nresulting in suboptimal protection and increased false rejections of benign\ninputs. To address this, we propose AdaSteer, an adaptive activation steering\nmethod that dynamically adjusts model behavior based on input characteristics.\nWe identify two key properties: Rejection Law (R-Law), which shows that\nstronger steering is needed for jailbreak inputs opposing the rejection\ndirection, and Harmfulness Law (H-Law), which differentiates adversarial and\nbenign inputs. AdaSteer steers input representations along both the Rejection\nDirection (RD) and Harmfulness Direction (HD), with adaptive coefficients\nlearned via logistic regression, ensuring robust jailbreak defense while\npreserving benign input handling. Experiments on LLaMA-3.1, Gemma-2, and\nQwen2.5 show that AdaSteer outperforms baseline methods across multiple\njailbreak attacks with minimal impact on utility. Our results highlight the\npotential of interpretable model internals for real-time, flexible safety\nenforcement in LLMs.\n","authors":["Weixiang Zhao","Jiahe Guo","Yulin Hu","Yang Deng","An Zhang","Xingyu Sui","Xinyang Han","Yanyan Zhao","Bing Qin","Tat-Seng Chua","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2504.09466v1.pdf","comment":"17 pages, 6 figures, 9 tables"},{"id":"http://arxiv.org/abs/2410.18491v2","updated":"2025-04-13T06:16:49Z","published":"2024-10-24T07:25:29Z","title":"ChineseSafe: A Chinese Benchmark for Evaluating Safety in Large Language\n  Models","summary":"  With the rapid development of Large language models (LLMs), understanding the\ncapabilities of LLMs in identifying unsafe content has become increasingly\nimportant. While previous works have introduced several benchmarks to evaluate\nthe safety risk of LLMs, the community still has a limited understanding of\ncurrent LLMs' capability to recognize illegal and unsafe content in Chinese\ncontexts. In this work, we present a Chinese safety benchmark (ChineseSafe) to\nfacilitate research on the content safety of large language models. To align\nwith the regulations for Chinese Internet content moderation, our ChineseSafe\ncontains 205,034 examples across 4 classes and 10 sub-classes of safety issues.\nFor Chinese contexts, we add several special types of illegal content:\npolitical sensitivity, pornography, and variant/homophonic words. Moreover, we\nemploy two methods to evaluate the legal risks of popular LLMs, including\nopen-sourced models and APIs. The results reveal that many LLMs exhibit\nvulnerability to certain types of safety issues, leading to legal risks in\nChina. Our work provides a guideline for developers and researchers to\nfacilitate the safety of LLMs. Our results are also available at\nhttps://huggingface.co/spaces/SUSTech/ChineseSafe-Benchmark. Additionally, we\nrelease a test set comprising 200,000 examples, which is publicly accessible at\nhttps://huggingface.co/datasets/SUSTech/ChineseSafe.\n","authors":["Hengxiang Zhang","Hongfu Gao","Qiang Hu","Guanhua Chen","Lili Yang","Bingyi Jing","Hongxin Wei","Bing Wang","Haifeng Bai","Lei Yang"],"pdf_url":"https://arxiv.org/pdf/2410.18491v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09426v1","updated":"2025-04-13T04:17:12Z","published":"2025-04-13T04:17:12Z","title":"BabyVLM: Data-Efficient Pretraining of VLMs Inspired by Infant Learning","summary":"  Human infants rapidly develop visual reasoning skills from minimal input,\nsuggesting that developmentally inspired pretraining could significantly\nenhance the efficiency of vision-language models (VLMs). Although recent\nefforts have leveraged infant-inspired datasets like SAYCam, existing\nevaluation benchmarks remain misaligned--they are either too simplistic,\nnarrowly scoped, or tailored for large-scale pretrained models. Additionally,\ntraining exclusively on infant data overlooks the broader, diverse input from\nwhich infants naturally learn. To address these limitations, we propose\nBabyVLM, a novel framework comprising comprehensive in-domain evaluation\nbenchmarks and a synthetic training dataset created via child-directed\ntransformations of existing datasets. We demonstrate that VLMs trained with our\nsynthetic dataset achieve superior performance on BabyVLM tasks compared to\nmodels trained solely on SAYCam or general-purpose data of the SAYCam size.\nBabyVLM thus provides a robust, developmentally aligned evaluation tool and\nillustrates how compact models trained on carefully curated data can generalize\neffectively, opening pathways toward data-efficient vision-language learning\nparadigms.\n","authors":["Shengao Wang","Arjun Chandra","Aoming Liu","Venkatesh Saligrama","Boqing Gong"],"pdf_url":"https://arxiv.org/pdf/2504.09426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09421v1","updated":"2025-04-13T04:00:40Z","published":"2025-04-13T04:00:40Z","title":"ClinicalGPT-R1: Pushing reasoning capability of generalist disease\n  diagnosis with large language model","summary":"  Recent advances in reasoning with large language models (LLMs)has shown\nremarkable reasoning capabilities in domains such as mathematics and coding,\nyet their application to clinical diagnosis remains underexplored. Here, we\nintroduce ClinicalGPT-R1, a reasoning enhanced generalist large language model\nfor disease diagnosis. Trained on a dataset of 20,000 real-world clinical\nrecords, ClinicalGPT-R1 leverages diverse training strategies to enhance\ndiagnostic reasoning. To benchmark performance, we curated MedBench-Hard, a\nchallenging dataset spanning seven major medical specialties and representative\ndiseases. Experimental results demonstrate that ClinicalGPT-R1 outperforms\nGPT-4o in Chinese diagnostic tasks and achieves comparable performance to GPT-4\nin English settings. This comparative study effectively validates the superior\nperformance of ClinicalGPT-R1 in disease diagnosis tasks. Resources are\navailable at https://github.com/medfound/medfound.\n","authors":["Wuyang Lan","Wenzheng Wang","Changwei Ji","Guoxing Yang","Yongbo Zhang","Xiaohong Liu","Song Wu","Guangyu Wang"],"pdf_url":"https://arxiv.org/pdf/2504.09421v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.14763v2","updated":"2025-04-13T03:42:38Z","published":"2024-10-18T14:17:03Z","title":"Enabling Scalable Evaluation of Bias Patterns in Medical LLMs","summary":"  Large language models (LLMs) have shown impressive potential in helping with\nnumerous medical challenges. Deploying LLMs in high-stakes applications such as\nmedicine, however, brings in many concerns. One major area of concern relates\nto biased behaviors of LLMs in medical applications, leading to unfair\ntreatment of individuals. To pave the way for the responsible and impactful\ndeployment of Med LLMs, rigorous evaluation is a key prerequisite. Due to the\nhuge complexity and variability of different medical scenarios, existing work\nin this domain has primarily relied on using manually crafted datasets for bias\nevaluation. In this study, we present a new method to scale up such bias\nevaluations by automatically generating test cases based on rigorous medical\nevidence. We specifically target the challenges of a) domain-specificity of\nbias characterization, b) hallucinating while generating the test cases, and c)\nvarious dependencies between the health outcomes and sensitive attributes. To\nthat end, we offer new methods to address these challenges integrated with our\ngenerative pipeline, using medical knowledge graphs, medical ontologies, and\ncustomized general LLM evaluation frameworks in our method. Through a series of\nextensive experiments, we show that the test cases generated by our proposed\nmethod can effectively reveal bias patterns in Med LLMs at larger and more\nflexible scales than human-crafted datasets. We publish a large bias evaluation\ndataset using our pipeline, which is dedicated to a few medical case studies. A\nlive demo of our application for vignette generation is available at\nhttps://vignette.streamlit.app. Our code is also available at\nhttps://github.com/healthylaife/autofair.\n","authors":["Hamed Fayyaz","Raphael Poulain","Rahmatollah Beheshti"],"pdf_url":"https://arxiv.org/pdf/2410.14763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09420v1","updated":"2025-04-13T03:36:06Z","published":"2025-04-13T03:36:06Z","title":"SaRO: Enhancing LLM Safety through Reasoning-based Alignment","summary":"  Current safety alignment techniques for large language models (LLMs) face two\nkey challenges: (1) under-generalization, which leaves models vulnerable to\nnovel jailbreak attacks, and (2) over-alignment, which leads to the excessive\nrefusal of benign instructions. Our preliminary investigation reveals semantic\noverlap between jailbreak/harmful queries and normal prompts in embedding\nspace, suggesting that more effective safety alignment requires a deeper\nsemantic understanding. This motivates us to incorporate safety-policy-driven\nreasoning into the alignment process. To this end, we propose the\nSafety-oriented Reasoning Optimization Framework (SaRO), which consists of two\nstages: (1) Reasoning-style Warmup (RW) that enables LLMs to internalize\nlong-chain reasoning through supervised fine-tuning, and (2) Safety-oriented\nReasoning Process Optimization (SRPO) that promotes safety reflection via\ndirect preference optimization (DPO). Extensive experiments demonstrate the\nsuperiority of SaRO over traditional alignment methods.\n","authors":["Yutao Mou","Yuxiao Luo","Shikun Zhang","Wei Ye"],"pdf_url":"https://arxiv.org/pdf/2504.09420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09407v1","updated":"2025-04-13T02:34:22Z","published":"2025-04-13T02:34:22Z","title":"UXAgent: A System for Simulating Usability Testing of Web Design with\n  LLM Agents","summary":"  Usability testing is a fundamental research method that user experience (UX)\nresearchers use to evaluate and iterate a web design, but\\textbf{ how to\nevaluate and iterate the usability testing study design } itself? Recent\nadvances in Large Language Model-simulated Agent (\\textbf{LLM Agent}) research\ninspired us to design \\textbf{UXAgent} to support UX researchers in evaluating\nand reiterating their usability testing study design before they conduct the\nreal human-subject study. Our system features a Persona Generator module, an\nLLM Agent module, and a Universal Browser Connector module to automatically\ngenerate thousands of simulated users to interactively test the target website.\nThe system also provides an Agent Interview Interface and a Video Replay\nInterface so that the UX researchers can easily review and analyze the\ngenerated qualitative and quantitative log data. Through a heuristic\nevaluation, five UX researcher participants praised the innovation of our\nsystem but also expressed concerns about the future of LLM Agent usage in UX\nstudies.\n","authors":["Yuxuan Lu","Bingsheng Yao","Hansu Gu","Jing Huang","Jessie Wang","Yang Li","Jiri Gesi","Qi He","Toby Jia-Jun Li","Dakuo Wang"],"pdf_url":"https://arxiv.org/pdf/2504.09407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09402v1","updated":"2025-04-13T02:10:18Z","published":"2025-04-13T02:10:18Z","title":"Question Tokens Deserve More Attention: Enhancing Large Language Models\n  without Training through Step-by-Step Reading and Question Attention\n  Recalibration","summary":"  Large Language Models (LLMs) often struggle with tasks that require a deep\nunderstanding of complex questions, especially when faced with long-range\ndependencies or multi-step reasoning. This work investigates the limitations of\ncurrent LLMs in question comprehension and identifies three insights: (1)\nrepeating question tokens improves comprehension by increasing attention to\nquestion regions, (2) increased backward dependencies negatively affect\nperformance due to unidirectional attentional constraints, and (3)\nrecalibrating attentional mechanisms to prioritize question-relevant regions\nimproves performance.\n  Based on these findings, we first propose a family of prompt-based strategies\n- Step-by-Step Reading (SSR), SSR+, and SSR++ - that guide LLMs to\nincrementally process question tokens and align their reasoning with the input\nstructure. These methods significantly improve performance, with SSR++\nachieving state-of-the-art results on several benchmarks: 96.66% on GSM8K,\n94.61% on ASDiv, and 76.28% on AQuA. Second, we introduce a training-free\nattention recalibration mechanism that dynamically adjusts attention\ndistributions during inference to emphasize question-relevant regions. This\nmethod improves the accuracy of LLaMA 3.1-8B on AQuA by 5.17% without changing\nmodel parameters or input prompts.\n  Taken together, our results highlight the importance of structured prompt\ndesign and attention optimization in improving LLM comprehension, providing\nlightweight yet effective tools for improving performance in various NLP tasks.\n","authors":["Feijiang Han","Licheng Guo","Hengtao Cui","Zhiyuan Lyu"],"pdf_url":"https://arxiv.org/pdf/2504.09402v1.pdf","comment":"CIS 5300"},{"id":"http://arxiv.org/abs/2504.09398v1","updated":"2025-04-13T01:48:13Z","published":"2025-04-13T01:48:13Z","title":"Composable NLP Workflows for BERT-based Ranking and QA System","summary":"  There has been a lot of progress towards building NLP models that scale to\nmultiple tasks. However, real-world systems contain multiple components and it\nis tedious to handle cross-task interaction with varying levels of text\ngranularity. In this work, we built an end-to-end Ranking and\nQuestion-Answering (QA) system using Forte, a toolkit that makes composable NLP\npipelines. We utilized state-of-the-art deep learning models such as BERT,\nRoBERTa in our pipeline, evaluated the performance on MS-MARCO and Covid-19\ndatasets using metrics such as BLUE, MRR, F1 and compared the results of\nranking and QA systems with their corresponding benchmark results. The modular\nnature of our pipeline and low latency of reranker makes it easy to build\ncomplex NLP applications easily.\n","authors":["Gaurav Kumar","Murali Mohana Krishna Dandu"],"pdf_url":"https://arxiv.org/pdf/2504.09398v1.pdf","comment":"6 pages, 3 figures, 6 tables"},{"id":"http://arxiv.org/abs/2504.09394v1","updated":"2025-04-13T01:36:47Z","published":"2025-04-13T01:36:47Z","title":"Evaluation Under Imperfect Benchmarks and Ratings: A Case Study in Text\n  Simplification","summary":"  Despite the successes of language models, their evaluation remains a daunting\nchallenge for new and existing tasks. We consider the task of text\nsimplification, commonly used to improve information accessibility, where\nevaluation faces two major challenges. First, the data in existing benchmarks\nmight not reflect the capabilities of current language models on the task,\noften containing disfluent, incoherent, or simplistic examples. Second,\nexisting human ratings associated with the benchmarks often contain a high\ndegree of disagreement, resulting in inconsistent ratings; nevertheless,\nexisting metrics still have to show higher correlations with these imperfect\nratings. As a result, evaluation for the task is not reliable and does not\nreflect expected trends (e.g., more powerful models being assigned higher\nscores). We address these challenges for the task of text simplification\nthrough three contributions. First, we introduce SynthSimpliEval, a synthetic\nbenchmark for text simplification featuring simplified sentences generated by\nmodels of varying sizes. Through a pilot study, we show that human ratings on\nour benchmark exhibit high inter-annotator agreement and reflect the expected\ntrend: larger models produce higher-quality simplifications. Second, we show\nthat auto-evaluation with a panel of LLM judges (LLMs-as-a-jury) often suffices\nto obtain consistent ratings for the evaluation of text simplification. Third,\nwe demonstrate that existing learnable metrics for text simplification benefit\nfrom training on our LLMs-as-a-jury-rated synthetic data, closing the gap with\npure LLMs-as-a-jury for evaluation. Overall, through our case study on text\nsimplification, we show that a reliable evaluation requires higher quality test\ndata, which could be obtained through synthetic data and LLMs-as-a-jury\nratings.\n","authors":["Joseph Liu","Yoonsoo Nam","Xinyue Cui","Swabha Swayamdipta"],"pdf_url":"https://arxiv.org/pdf/2504.09394v1.pdf","comment":"Submitted to COLM 2025. 9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2504.09389v1","updated":"2025-04-13T00:48:58Z","published":"2025-04-13T00:48:58Z","title":"Beyond Memorization: Mapping the Originality-Quality Frontier of\n  Language Models","summary":"  As large language models (LLMs) are increasingly used for ideation and\nscientific discovery, it is important to evaluate their ability to generate\nnovel output. Prior work evaluates novelty as the originality with respect to\ntraining data, but original outputs can be low quality. In contrast, non-expert\njudges may favor high-quality but memorized outputs, limiting the reliability\nof human preference as a metric. We propose a new novelty metric for LLM\ngenerations that balances originality and quality -- the harmonic mean of the\nfraction of \\ngrams unseen during training and a task-specific quality score.\nWe evaluate the novelty of generations from two families of open-data models\n(OLMo and Pythia) on three creative tasks: story completion, poetry writing,\nand creative tool use. We find that LLM generated text is less novel than human\nwritten text. To elicit more novel outputs, we experiment with various\ninference-time methods, which reveals a trade-off between originality and\nquality. While these methods can boost novelty, they do so by increasing\noriginality at the expense of quality. In contrast, increasing model size or\napplying post-training reliably shifts the Pareto frontier, highlighting that\nstarting with a stronger base model is a more effective way to improve novelty.\n","authors":["Vishakh Padmakumar","Chen Yueh-Han","Jane Pan","Valerie Chen","He He"],"pdf_url":"https://arxiv.org/pdf/2504.09389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09387v1","updated":"2025-04-13T00:43:06Z","published":"2025-04-13T00:43:06Z","title":"On Language Models' Sensitivity to Suspicious Coincidences","summary":"  Humans are sensitive to suspicious coincidences when generalizing inductively\nover data, as they make assumptions as to how the data was sampled. This\nresults in smaller, more specific hypotheses being favored over more general\nones. For instance, when provided the set {Austin, Dallas, Houston}, one is\nmore likely to think that this is sampled from \"Texas Cities\" over \"US Cities\"\neven though both are compatible. Suspicious coincidence is strongly connected\nto pragmatic reasoning, and can serve as a testbed to analyze systems on their\nsensitivity towards the communicative goals of the task (i.e., figuring out the\ntrue category underlying the data). In this paper, we analyze whether\nsuspicious coincidence effects are reflected in language models' (LMs)\nbehavior. We do so in the context of two domains: 1) the number game, where\nhumans made judgments of whether a number (e.g., 4) fits a list of given\nnumbers (e.g., 16, 32, 2); and 2) by extending the number game setup to\nprominent cities. For both domains, the data is compatible with multiple\nhypotheses and we study which hypothesis is most consistent with the models'\nbehavior. On analyzing five models, we do not find strong evidence for\nsuspicious coincidences in LMs' zero-shot behavior. However, when provided\naccess to the hypotheses space via chain-of-thought or explicit prompting, LMs\nstart to show an effect resembling suspicious coincidences, sometimes even\nshowing effects consistent with humans. Our study suggests that inductive\nreasoning behavior in LMs can be enhanced with explicit access to the\nhypothesis landscape.\n","authors":["Sriram Padmanabhan","Kanishka Misra","Kyle Mahowald","Eunsol Choi"],"pdf_url":"https://arxiv.org/pdf/2504.09387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09378v1","updated":"2025-04-13T00:01:22Z","published":"2025-04-13T00:01:22Z","title":"Can you map it to English? The Role of Cross-Lingual Alignment in\n  Multilingual Performance of LLMs","summary":"  Large language models (LLMs) pre-trained predominantly on English text\nexhibit surprising multilingual capabilities, yet the mechanisms driving\ncross-lingual generalization remain poorly understood. This work investigates\nhow the alignment of representations for text written in different languages\ncorrelates with LLM performance on natural language understanding tasks and\ntranslation tasks, both at the language and the instance level. For this\npurpose, we introduce cross-lingual alignment metrics such as the\nDiscriminative Alignment Index (DALI) to quantify the alignment at an instance\nlevel for discriminative tasks. Through experiments on three natural language\nunderstanding tasks (Belebele, XStoryCloze, XCOPA), and machine translation, we\nfind that while cross-lingual alignment metrics strongly correlate with task\naccuracy at the language level, the sample-level alignment often fails to\ndistinguish correct from incorrect predictions, exposing alignment as a\nnecessary but insufficient condition for success.\n","authors":["Kartik Ravisankar","Hyojung Han","Marine Carpuat"],"pdf_url":"https://arxiv.org/pdf/2504.09378v1.pdf","comment":null}]},"2025-04-12T00:00:00Z":{"Statistical Machine Learning (Theory)":[{"id":"http://arxiv.org/abs/2502.08736v2","updated":"2025-04-12T21:46:53Z","published":"2025-02-12T19:18:50Z","title":"Recurrent Memory for Online Interdomain Gaussian Processes","summary":"  We propose a novel online Gaussian process (GP) model that is capable of\ncapturing long-term memory in sequential data in an online regression setting.\nOur model, Online HiPPO Sparse Variational Gaussian Process Regression\n(OHSGPR), leverages the HiPPO (High-order Polynomial Projection Operators)\nframework, which is popularized in the RNN domain due to its long-range memory\nmodeling capabilities. We interpret the HiPPO time-varying orthogonal\nprojections as inducing variables with time-dependent orthogonal polynomial\nbasis functions, which allows the SGPR inducing points to memorize the process\nhistory. We show that the HiPPO framework fits naturally into the interdomain\nGP framework and demonstrate that the kernel matrices can also be updated\nonline in a recurrence form based on the ODE evolution of HiPPO. We evaluate\nour method on time series regression tasks, showing that it outperforms the\nexisting online GP method in terms of predictive performance and computational\nefficiency\n","authors":["Wenlong Chen","Naoki Kiyohara","Harrison Bo Hua Zhu","Yingzhen Li"],"pdf_url":"https://arxiv.org/pdf/2502.08736v2.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.09349v1","updated":"2025-04-12T21:37:11Z","published":"2025-04-12T21:37:11Z","title":"Neural Posterior Estimation on Exponential Random Graph Models:\n  Evaluating Bias and Implementation Challenges","summary":"  Exponential random graph models (ERGMs) are flexible probabilistic frameworks\nto model statistical networks through a variety of network summary statistics.\nConventional Bayesian estimation for ERGMs involves iteratively exchanging with\nan auxiliary variable due to the intractability of ERGMs, however, this\napproach lacks scalability to large-scale implementations. Neural posterior\nestimation (NPE) is a recent advancement in simulation-based inference, using a\nneural network based density estimator to infer the posterior for models with\ndoubly intractable likelihoods for which simulations can be generated. While\nNPE has been successfully adopted in various fields such as cosmology, little\nresearch has investigated its use for ERGMs. Performing NPE on ERGM not only\nprovides a differing angle of resolving estimation for the intractable ERGM\nlikelihoods but also allows more efficient and scalable inference using the\namortisation properties of NPE, and therefore, we investigate how NPE can be\neffectively implemented in ERGMs.\n  In this study, we present the first systematic implementation of NPE for\nERGMs, rigorously evaluating potential biases, interpreting the biases\nmagnitudes, and comparing NPE fittings against conventional Bayesian ERGM\nfittings. More importantly, our work highlights ERGM-specific areas that may\nimpose particular challenges for the adoption of NPE.\n","authors":["Yefeng Fan","Simon White"],"pdf_url":"https://arxiv.org/pdf/2504.09349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09347v1","updated":"2025-04-12T21:32:42Z","published":"2025-04-12T21:32:42Z","title":"Inferring Outcome Means of Exponential Family Distributions Estimated by\n  Deep Neural Networks","summary":"  Despite the widespread use of deep neural networks (DNNs) for prediction,\ninference on estimated means for categorical or exponential family outcomes\nremains underexplored. We address this gap by framing the problem within the\ngeneralized linear models (GLMs) framework and developing a rigorous\nstatistical approach for inference on DNN-estimated means. To address a key\nlimitation of assuming independence between prediction errors and input\nvariables in the literature, which often fails in GLMs, we introduce a\ntruncation technique that partitions the problem into regimes with distinct\nnoise behaviors, enabling refined analysis and robust theoretical guarantees\nunder general GLM frameworks. To implement inference, we consider an Ensemble\nSubsampling Method (ESM) that leverages U-statistics and the Hoeffding\ndecomposition to construct reliable confidence intervals. This method enables\nmodel-free variance estimation and accounts for heterogeneity among individuals\nin the population. Through extensive simulations across Binary, Poisson and\nBinomial models, we demonstrate the effectiveness and efficiency of our method.\nWe further apply the method to real-world data from the eICU dataset to predict\npatient readmission risks, providing actionable insights for clinical\ndecision-making.\n","authors":["Xuran Meng","Yi Li"],"pdf_url":"https://arxiv.org/pdf/2504.09347v1.pdf","comment":"44 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2504.09339v1","updated":"2025-04-12T20:51:51Z","published":"2025-04-12T20:51:51Z","title":"Towards Optimal Differentially Private Regret Bounds in Linear MDPs","summary":"  We study regret minimization under privacy constraints in episodic\ninhomogeneous linear Markov Decision Processes (MDPs), motivated by the growing\nuse of reinforcement learning (RL) in personalized decision-making systems that\nrely on sensitive user data. In this setting, both transition probabilities and\nreward functions are assumed to be linear in a feature mapping $\\phi(s, a)$,\nand we aim to ensure privacy through joint differential privacy (JDP), a\nrelaxation of differential privacy suited to online learning. Prior work has\nestablished suboptimal regret bounds by privatizing the LSVI-UCB algorithm,\nwhich achieves $\\widetilde{O}(\\sqrt{d^3 H^4 K})$ regret in the non-private\nsetting. Building on recent advances that improve this to minimax optimal\nregret $\\widetilde{O}(HD\\sqrt{K})$ via LSVI-UCB++ with Bernstein-style bonuses,\nwe design a new differentially private algorithm by privatizing LSVI-UCB++ and\nadapting techniques for variance-aware analysis from offline RL. Our algorithm\nachieves a regret bound of $\\widetilde{O}(d \\sqrt{H^3 K} + H^{4.5} d^{7/6}\nK^{1/2} / \\epsilon)$, improving over previous private methods. Empirical\nresults show that our algorithm retains near-optimal utility compared to\nnon-private baselines, indicating that privacy can be achieved with minimal\nperformance degradation in this setting.\n","authors":["Sharan Sahu"],"pdf_url":"https://arxiv.org/pdf/2504.09339v1.pdf","comment":"28 pages, 2 figures"},{"id":"http://arxiv.org/abs/2504.09330v1","updated":"2025-04-12T20:21:24Z","published":"2025-04-12T20:21:24Z","title":"Regretful Decisions under Label Noise","summary":"  Machine learning models are routinely used to support decisions that affect\nindividuals -- be it to screen a patient for a serious illness or to gauge\ntheir response to treatment. In these tasks, we are limited to learning models\nfrom datasets with noisy labels. In this paper, we study the instance-level\nimpact of learning under label noise. We introduce a notion of regret for this\nregime which measures the number of unforeseen mistakes due to noisy labels. We\nshow that standard approaches to learning under label noise can return models\nthat perform well at a population level while subjecting individuals to a\nlottery of mistakes. We present a versatile approach to estimate the likelihood\nof mistakes at the individual level from a noisy dataset by training models\nover plausible realizations of datasets without label noise. This is supported\nby a comprehensive empirical study of label noise in clinical prediction tasks.\nOur results reveal how failure to anticipate mistakes can compromise model\nreliability and adoption, and demonstrate how we can address these challenges\nby anticipating and avoiding regretful decisions.\n","authors":["Sujay Nagaraj","Yang Liu","Flavio P. Calmon","Berk Ustun"],"pdf_url":"https://arxiv.org/pdf/2504.09330v1.pdf","comment":"The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)"},{"id":"http://arxiv.org/abs/2501.02406v3","updated":"2025-04-12T18:05:42Z","published":"2025-01-04T23:51:43Z","title":"Zero-Shot Statistical Tests for LLM-Generated Text Detection using\n  Finite Sample Concentration Inequalities","summary":"  Verifying the provenance of content is crucial to the function of many\norganizations, e.g., educational institutions, social media platforms, firms,\netc. This problem is becoming increasingly challenging as text generated by\nLarge Language Models (LLMs) becomes almost indistinguishable from\nhuman-generated content. In addition, many institutions utilize in-house LLMs\nand want to ensure that external, non-sanctioned LLMs do not produce content\nwithin the institution. We answer the following question: Given a piece of\ntext, can we identify whether it was produced by LLM $A$ or $B$ (where $B$ can\nbe a human)? We model LLM-generated text as a sequential stochastic process\nwith complete dependence on history and design zero-shot statistical tests to\ndistinguish between (i) the text generated by two different sets of LLMs $A$\n(in-house) and $B$ (non-sanctioned) and also (ii) LLM-generated and\nhuman-generated texts. We prove that our tests' type I and type II errors\ndecrease exponentially as text length increases. For designing our tests for a\ngiven string, we demonstrate that if the string is generated by the evaluator\nmodel $A$, the log-perplexity of the string under $A$ converges to the average\nentropy of the string under $A$, except with an exponentially small probability\nin the string length. We also show that if $B$ generates the text, except with\nan exponentially small probability in string length, the log-perplexity of the\nstring under $A$ converges to the average cross-entropy of $B$ and $A$. For our\nexperiments: First, we present experiments using open-source LLMs to support\nour theoretical results, and then we provide experiments in a black-box setting\nwith adversarial attacks. Practically, our work enables guaranteed finding of\nthe origin of harmful or false LLM-generated text, which can be useful for\ncombating misinformation and compliance with emerging AI regulations.\n","authors":["Tara Radvand","Mojtaba Abdolmaleki","Mohamed Mostagir","Ambuj Tewari"],"pdf_url":"https://arxiv.org/pdf/2501.02406v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09279v1","updated":"2025-04-12T16:51:02Z","published":"2025-04-12T16:51:02Z","title":"No-Regret Generative Modeling via Parabolic Monge-Ampère PDE","summary":"  We introduce a novel generative modeling framework based on a discretized\nparabolic Monge-Amp\\`ere PDE, which emerges as a continuous limit of the\nSinkhorn algorithm commonly used in optimal transport. Our method performs\niterative refinement in the space of Brenier maps using a mirror gradient\ndescent step. We establish theoretical guarantees for generative modeling\nthrough the lens of no-regret analysis, demonstrating that the iterates\nconverge to the optimal Brenier map under a variety of step-size schedules. As\na technical contribution, we derive a new Evolution Variational Inequality\ntailored to the parabolic Monge-Amp\\`ere PDE, connecting geometry,\ntransportation cost, and regret. Our framework accommodates non-log-concave\ntarget distributions, constructs an optimal sampling process via the Brenier\nmap, and integrates favorable learning techniques from generative adversarial\nnetworks and score-based diffusion models. As direct applications, we\nillustrate how our theory paves new pathways for generative modeling and\nvariational inference.\n","authors":["Nabarun Deb","Tengyuan Liang"],"pdf_url":"https://arxiv.org/pdf/2504.09279v1.pdf","comment":"30 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.16241v3","updated":"2025-04-12T14:51:39Z","published":"2023-03-28T18:34:52Z","title":"Convergence of the Stochastic Heavy Ball Method With Approximate\n  Gradients and/or Block Updating","summary":"  In this paper, we establish the convergence of the stochastic Heavy Ball\n(SHB) algorithm under more general conditions than in the current literature.\nSpecifically, (i) The stochastic gradient is permitted to be biased, and also,\nto have conditional variance that grows over time (or iteration number). This\nfeature is essential when applying SHB with zeroth-order methods, which use\nonly two function evaluations to approximate the gradient. In contrast, all\nexisting papers assume that the stochastic gradient is unbiased and/or has\nbounded conditional variance. (ii) The step sizes are permitted to be random,\nwhich is essential when applying SHB with block updating. The sufficient\nconditions for convergence are stochastic analogs of the well-known\nRobbins-Monro conditions. This is in contrast to existing papers where more\nrestrictive conditions are imposed on the step size sequence. (iii) Our\nanalysis embraces not only convex functions, but also more general functions\nthat satisfy the PL (Polyak-{\\L}ojasiewicz) and KL (Kurdyka-{\\L}ojasiewicz)\nconditions. (iv) If the stochastic gradient is unbiased and has bounded\nvariance, and the objective function satisfies (PL), then the iterations of SHB\nmatch the known best rates for convex functions. (v) We establish the\nalmost-sure convergence of the iterations, as opposed to convergence in the\nmean or convergence in probability, which is the case in much of the\nliterature. (vi) Each of the above convergence results continue to hold if\nfull-coordinate updating is replaced by any one of three widely-used updating\nmethods. In addition, numerical computations are carried out to illustrate the\nabove points.\n","authors":["Uday Kiran Reddy Tadipatri","Mathukumalli Vidyasagar"],"pdf_url":"https://arxiv.org/pdf/2303.16241v3.pdf","comment":"37 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.09210v1","updated":"2025-04-12T13:32:11Z","published":"2025-04-12T13:32:11Z","title":"FairACE: Achieving Degree Fairness in Graph Neural Networks via\n  Contrastive and Adversarial Group-Balanced Training","summary":"  Fairness has been a significant challenge in graph neural networks (GNNs)\nsince degree biases often result in un-equal prediction performance among nodes\nwith varying degrees. Existing GNN models focus on prediction accuracy,\nfrequently overlooking fairness across different degree groups. To addressthis\nissue, we propose a novel GNN framework, namely Fairness- Aware Asymmetric\nContrastive Ensemble (FairACE), which inte-grates asymmetric contrastive\nlearning with adversarial training to improve degree fairness. FairACE captures\none-hop local neighborhood information and two-hop monophily similarity to\ncreate fairer node representations and employs a degree fairness regulator to\nbalance performance between high-degree and low-degree nodes. During model\ntraining, a novel group-balanced fairness loss is proposed to minimize\nclassification disparities across degree groups. In addition, we also propose a\nnovel fairness metric, the Accuracy Distribution Gap (ADG), which can\nquantitatively assess and ensure equitable performance across different\ndegree-based node groups. Experimental results on both synthetic and real-world\ndatasets demonstrate that FairACE significantly improves degree fairness\nmetrics while maintaining competitive accuracy in comparison to the\nstate-of-the-art GNN models.\n","authors":["Jiaxin Liu","Xiaoqian Jiang","Cangqi Zhou","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.09210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09206v1","updated":"2025-04-12T13:14:35Z","published":"2025-04-12T13:14:35Z","title":"Rethinking Remaining Useful Life Prediction with Scarce Time Series\n  Data: Regression under Indirect Supervision","summary":"  Supervised time series prediction relies on directly measured target\nvariables, but real-world use cases such as predicting remaining useful life\n(RUL) involve indirect supervision, where the target variable is labeled as a\nfunction of another dependent variable. Trending temporal regression techniques\nrely on sequential time series inputs to capture temporal patterns, requiring\ninterpolation when dealing with sparsely and irregularly sampled covariates\nalong the timeline. However, interpolation can introduce significant biases,\nparticularly with highly scarce data. In this paper, we address the RUL\nprediction problem with data scarcity as time series regression under indirect\nsupervision. We introduce a unified framework called parameterized static\nregression, which takes single data points as inputs for regression of target\nvalues, inherently handling data scarcity without requiring interpolation. The\ntime dependency under indirect supervision is captured via a parametrical\nrectification (PR) process, approximating a parametric function during\ninference with historical posteriori estimates, following the same underlying\ndistribution used for labeling during training. Additionally, we propose a\nnovel batch training technique for tasks in indirect supervision to prevent\noverfitting and enhance efficiency. We evaluate our model on public benchmarks\nfor RUL prediction with simulated data scarcity. Our method demonstrates\ncompetitive performance in prediction accuracy when dealing with highly scarce\ntime series data.\n","authors":["Jiaxiang Cheng","Yipeng Pang","Guoqiang Hu"],"pdf_url":"https://arxiv.org/pdf/2504.09206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09157v1","updated":"2025-04-12T09:44:20Z","published":"2025-04-12T09:44:20Z","title":"Dose-finding design based on level set estimation in phase I cancer\n  clinical trials","summary":"  The primary objective of phase I cancer clinical trials is to evaluate the\nsafety of a new experimental treatment and to find the maximum tolerated dose\n(MTD). We show that the MTD estimation problem can be regarded as a level set\nestimation (LSE) problem whose objective is to determine the regions where an\nunknown function value is above or below a given threshold. Then, we propose a\nnovel dose-finding design in the framework of LSE. The proposed design\ndetermines the next dose on the basis of an acquisition function incorporating\nuncertainty in the posterior distribution of the dose-toxicity curve as well as\noverdose control. Simulation experiments show that the proposed LSE design\nachieves a higher accuracy in estimating the MTD and involves a lower risk of\noverdosing allocation compared to existing designs, thereby indicating that it\nprovides an effective methodology for phase I cancer clinical trial design.\n","authors":["Keiichiro Seno","Kota Matsui","Shogo Iwazaki","Yu Inatsu","Shion Takeno","Shigeyuki Matsui"],"pdf_url":"https://arxiv.org/pdf/2504.09157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19449v3","updated":"2025-04-12T08:04:13Z","published":"2024-10-25T10:23:34Z","title":"Learned Reference-based Diffusion Sampling for multi-modal distributions","summary":"  Over the past few years, several approaches utilizing score-based diffusion\nhave been proposed to sample from probability distributions, that is without\nhaving access to exact samples and relying solely on evaluations of\nunnormalized densities. The resulting samplers approximate the time-reversal of\na noising diffusion process, bridging the target distribution to an\neasy-to-sample base distribution. In practice, the performance of these methods\nheavily depends on key hyperparameters that require ground truth samples to be\naccurately tuned. Our work aims to highlight and address this fundamental\nissue, focusing in particular on multi-modal distributions, which pose\nsignificant challenges for existing sampling methods. Building on existing\napproaches, we introduce Learned Reference-based Diffusion Sampler (LRDS), a\nmethodology specifically designed to leverage prior knowledge on the location\nof the target modes in order to bypass the obstacle of hyperparameter tuning.\nLRDS proceeds in two steps by (i) learning a reference diffusion model on\nsamples located in high-density space regions and tailored for multimodality,\nand (ii) using this reference model to foster the training of a diffusion-based\nsampler. We experimentally demonstrate that LRDS best exploits prior knowledge\non the target distribution compared to competing algorithms on a variety of\nchallenging distributions.\n","authors":["Maxence Noble","Louis Grenioux","Marylou Gabrié","Alain Oliviero Durmus"],"pdf_url":"https://arxiv.org/pdf/2410.19449v3.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2410.02321v2","updated":"2025-04-12T07:56:07Z","published":"2024-10-03T09:07:13Z","title":"Convergence of Score-Based Discrete Diffusion Models: A Discrete-Time\n  Analysis","summary":"  Diffusion models have achieved great success in generating high-dimensional\nsamples across various applications. While the theoretical guarantees for\ncontinuous-state diffusion models have been extensively studied, the\nconvergence analysis of the discrete-state counterparts remains under-explored.\nIn this paper, we study the theoretical aspects of score-based discrete\ndiffusion models under the Continuous Time Markov Chain (CTMC) framework. We\nintroduce a discrete-time sampling algorithm in the general state space $[S]^d$\nthat utilizes score estimators at predefined time points. We derive convergence\nbounds for the Kullback-Leibler (KL) divergence and total variation (TV)\ndistance between the generated sample distribution and the data distribution,\nconsidering both scenarios with and without early stopping under reasonable\nassumptions. Notably, our KL divergence bounds are nearly linear in the\ndimension $d$, aligning with state-of-the-art results for diffusion models. Our\nconvergence analysis employs a Girsanov-based method and establishes key\nproperties of the discrete score function, which are essential for\ncharacterizing the discrete-time sampling process.\n","authors":["Zikun Zhang","Zixiang Chen","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2410.02321v2.pdf","comment":"26 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.06564v2","updated":"2025-04-12T07:17:53Z","published":"2025-02-10T15:31:57Z","title":"Nearly Optimal Robust Covariance and Scatter Matrix Estimation Beyond\n  Gaussians","summary":"  We study the problem of computationally efficient robust estimation of the\ncovariance/scatter matrix of elliptical distributions -- that is, affine\ntransformations of spherically symmetric distributions -- under the strong\ncontamination model in the high-dimensional regime $d \\gtrsim 1/\\varepsilon^2$,\nwhere $d$ is the dimension and $\\varepsilon$ is the fraction of adversarial\ncorruptions.\n  We propose an algorithm that, under a very mild assumption on the scatter\nmatrix $\\Sigma$, and given a nearly optimal number of samples $n =\n\\tilde{O}(d^2/\\varepsilon^2)$, computes in polynomial time an estimator\n$\\hat{\\Sigma}$ such that, with high probability, \\[ \\left\\| \\Sigma^{-1/2}\n\\hat{\\Sigma} \\Sigma^{-1/2} - Id \\right\\|_{\\text F} \\le O(\\varepsilon\n\\log(1/\\varepsilon))\\,. \\]\n  As an application of our result, we obtain the first efficiently computable,\nnearly optimal robust covariance estimators that extend beyond the Gaussian\ncase. Specifically, for elliptical distributions satisfying the Hanson--Wright\ninequality (such as Gaussians and uniform distributions over ellipsoids), our\nestimator $\\hat{\\Sigma}$ of the covariance $\\Sigma$ achieves the same error\nguarantee as in the Gaussian case. Moreover, for elliptical distributions with\nsub-exponential tails (such as the multivariate Laplace distribution), we\nconstruct an estimator $\\hat{\\Sigma}$ satisfying the spectral norm bound \\[\n\\left\\| \\Sigma^{-1/2} \\hat{\\Sigma} \\Sigma^{-1/2} - Id \\right\\| \\le\nO(\\varepsilon \\log(1/\\varepsilon))\\,. \\]\n  Our approach is based on estimating the covariance of the spatial sign of\nelliptical distributions. The estimation proceeds in several stages, one of\nwhich involves a novel spectral covariance filtering algorithm. This algorithm\ncombines covariance filtering techniques with degree-4 sum-of-squares\nrelaxations, and we believe it may be of independent interest for future\napplications.\n","authors":["Gleb Novikov"],"pdf_url":"https://arxiv.org/pdf/2502.06564v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09096v1","updated":"2025-04-12T06:28:05Z","published":"2025-04-12T06:28:05Z","title":"High dimensional online calibration in polynomial time","summary":"  In online (sequential) calibration, a forecaster predicts probability\ndistributions over a finite outcome space $[d]$ over a sequence of $T$ days,\nwith the goal of being calibrated. While asymptotically calibrated strategies\nare known to exist, they suffer from the curse of dimensionality: the best\nknown algorithms require $\\exp(d)$ days to achieve non-trivial calibration.\n  In this work, we present the first asymptotically calibrated strategy that\nguarantees non-trivial calibration after a polynomial number of rounds.\nSpecifically, for any desired accuracy $\\epsilon > 0$, our forecaster becomes\n$\\epsilon$-calibrated after $T = d^{O(1/\\epsilon^2)}$ days. We complement this\nresult with a lower bound, proving that at least $T =\nd^{\\Omega(\\log(1/\\epsilon))}$ rounds are necessary to achieve\n$\\epsilon$-calibration. Our results resolve the open questions posed by\n[Abernethy-Mannor'11, Hazan-Kakade'12].\n  Our algorithm is inspired by recent breakthroughs in swap regret minimization\n[Peng-Rubinstein'24, Dagan et al.'24]. Despite its strong theoretical\nguarantees, the approach is remarkably simple and intuitive: it randomly\nselects among a set of sub-forecasters, each of which predicts the empirical\noutcome frequency over recent time windows.\n","authors":["Binghui Peng"],"pdf_url":"https://arxiv.org/pdf/2504.09096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15669v2","updated":"2025-04-12T04:01:10Z","published":"2024-11-23T23:13:24Z","title":"Implicit High-Order Moment Tensor Estimation and Learning Latent\n  Variable Models","summary":"  We study the task of learning latent-variable models. A common algorithmic\ntechnique for this task is the method of moments. Unfortunately, moment-based\napproaches are hampered by the fact that the moment tensors of super-constant\ndegree cannot even be written down in polynomial time. Motivated by such\nlearning applications, we develop a general efficient algorithm for {\\em\nimplicit moment tensor computation}. Our framework generalizes the work\nof~\\cite{LL21-opt} which developed an efficient algorithm for the specific\nmoment tensors that arise in clustering mixtures of spherical Gaussians.\n  By leveraging our implicit moment estimation algorithm, we obtain the first\n$\\mathrm{poly}(d, k)$-time learning algorithms for the following models.\n  * {\\bf Mixtures of Linear Regressions} We give a $\\mathrm{poly}(d, k,\n1/\\epsilon)$-time algorithm for this task, where $\\epsilon$ is the desired\nerror.\n  * {\\bf Mixtures of Spherical Gaussians} For density estimation, we give a\n$\\mathrm{poly}(d, k, 1/\\epsilon)$-time learning algorithm, where $\\epsilon$ is\nthe desired total variation error, under the condition that the means lie in a\nball of radius $O(\\sqrt{\\log k})$. For parameter estimation, we give a\n$\\mathrm{poly}(d, k, 1/\\epsilon)$-time algorithm under the {\\em optimal} mean\nseparation of $\\Omega(\\log^{1/2}(k/\\epsilon))$.\n  * {\\bf Positive Linear Combinations of Non-Linear Activations} We give a\ngeneral algorithm for this task with complexity $\\mathrm{poly}(d, k)\ng(\\epsilon)$, where $\\epsilon$ is the desired error and the function $g$\ndepends on the Hermite concentration of the target class of functions.\nSpecifically, for positive linear combinations of ReLU activations, our\nalgorithm has complexity $\\mathrm{poly}(d, k) 2^{\\mathrm{poly}(1/\\epsilon)}$.\n","authors":["Ilias Diakonikolas","Daniel M. Kane"],"pdf_url":"https://arxiv.org/pdf/2411.15669v2.pdf","comment":"Abstract shortened due to arxiv requirements"},{"id":"http://arxiv.org/abs/2504.09068v1","updated":"2025-04-12T03:54:05Z","published":"2025-04-12T03:54:05Z","title":"An Incremental Non-Linear Manifold Approximation Method","summary":"  Analyzing high-dimensional data presents challenges due to the \"curse of\ndimensionality'', making computations intensive. Dimension reduction\ntechniques, categorized as linear or non-linear, simplify such data. Non-linear\nmethods are particularly essential for efficiently visualizing and processing\ncomplex data structures in interactive and graphical applications. This\nresearch develops an incremental non-linear dimension reduction method using\nthe Geometric Multi-Resolution Analysis (GMRA) framework for streaming data.\nThe proposed method enables real-time data analysis and visualization by\nincrementally updating the cluster map, PCA basis vectors, and wavelet\ncoefficients. Numerical experiments show that the incremental GMRA accurately\nrepresents non-linear manifolds even with small initial samples and aligns\nclosely with batch GMRA, demonstrating efficient updates and maintaining the\nmultiscale structure. The findings highlight the potential of Incremental GMRA\nfor real-time visualization and interactive graphics applications that require\nadaptive high-dimensional data representations.\n","authors":["Praveen T. W. Hettige","Benjamin W. Ong"],"pdf_url":"https://arxiv.org/pdf/2504.09068v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2203.10651v3","updated":"2025-04-12T13:56:39Z","published":"2022-03-20T21:22:39Z","title":"Forecasting Sparse Movement Speed of Urban Road Networks with\n  Nonstationary Temporal Matrix Factorization","summary":"  Movement speed data from urban road networks, computed from ridesharing\nvehicles or taxi trajectories, is often high-dimensional, sparse, and\nnonstationary (e.g., exhibiting seasonality). To address these challenges, we\npropose a Nonstationary Temporal Matrix Factorization (NoTMF) model that\nleverages matrix factorization to project high-dimensional and sparse movement\nspeed data into low-dimensional latent spaces. This results in a concise\nformula with the multiplication between spatial and temporal factor matrices.\nTo characterize the temporal correlations, NoTMF takes a latent equation on the\nseasonal differenced temporal factors using higher-order vector autoregression\n(VAR). This approach not only preserves the low-rank structure of sparse\nmovement speed data but also maintains consistent temporal dynamics, including\nseasonality information. The learning process for NoTMF involves optimizing the\nspatial and temporal factor matrices along with a collection of VAR coefficient\nmatrices. To solve this efficiently, we introduce an alternating minimization\nframework, which tackles a challenging procedure of estimating the temporal\nfactor matrix using conjugate gradient method, as the subproblem involves both\npartially observed matrix factorization and seasonal differenced VAR. To\nevaluate the forecasting performance of NoTMF, we conduct extensive experiments\non Uber movement speed datasets, which are estimated from ridesharing vehicle\ntrajectories. These datasets contain a large proportion of missing values due\nto insufficient ridesharing vehicles on the urban road network. Despite the\npresence of missing data, NoTMF demonstrates superior forecasting accuracy and\neffectiveness compared to baseline models. Moreover, as the seasonality of\nmovement speed data is of great concern, the experiment results highlight the\nsignificance of addressing the nonstationarity of movement speed data.\n","authors":["Xinyu Chen","Chengyuan Zhang","Xi-Le Zhao","Nicolas Saunier","Lijun Sun"],"pdf_url":"https://arxiv.org/pdf/2203.10651v3.pdf","comment":"Data and Python codes: https://github.com/xinychen/tracebase"}],"Optimization":[{"id":"http://arxiv.org/abs/2504.09375v1","updated":"2025-04-12T23:49:17Z","published":"2025-04-12T23:49:17Z","title":"Efficient Gradient-Enhanced Bayesian Optimizer with Comparisons to\n  Quasi-Newton Optimizers for Unconstrained Local Optimization","summary":"  The probabilistic surrogates used by Bayesian optimizers make them popular\nmethods when function evaluations are noisy or expensive to evaluate. While\nBayesian optimizers are traditionally used for global optimization, their\nbenefits are also valuable for local optimization. In this paper, a framework\nfor gradient-enhanced unconstrained local Bayesian optimization is presented.\nIt involves selecting a subset of the evaluation points to construct the\nsurrogate and using a probabilistic trust region for the minimization of the\nacquisition function. The Bayesian optimizer is compared to quasi-Newton\noptimizers from MATLAB and SciPy for unimodal problems with 2 to 40 dimensions.\nThe Bayesian optimizer converges the optimality as deeply as the quasi-Newton\noptimizer and often does so using significantly fewer function evaluations. For\nthe minimization of the 40-dimensional Rosenbrock function for example, the\nBayesian optimizer requires half as many function evaluations as the\nquasi-Newton optimizers to reduce the optimality by 10 orders of magnitude. For\ntest cases with noisy gradients, the probabilistic surrogate of the Bayesian\noptimizer enables it to converge the optimality several additional orders of\nmagnitude relative to the quasi-Newton optimizers. The final test case involves\nthe chaotic Lorenz 63 model and inaccurate gradients. For this problem, the\nBayesian optimizer achieves a lower final objective evaluation than the SciPy\nquasi-Newton optimizer for all initial starting solutions. The results\ndemonstrate that a Bayesian optimizer can be competitive with quasi-Newton\noptimizers when accurate gradients are available, and significantly outperforms\nthem when the gradients are innacurate.\n","authors":["André L. Marchildon","David W. Zingg"],"pdf_url":"https://arxiv.org/pdf/2504.09375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09356v1","updated":"2025-04-12T22:11:30Z","published":"2025-04-12T22:11:30Z","title":"Weak equilibria of a mean-field market model under asymmetric\n  information","summary":"  We investigate how asymmetric information affects the equilibrium dynamics in\na setting where a large number of players interacts. Motivated by the analysis\nof the mechanism of equilibrium price formation, we consider the mean-field\nlimit of a model with two subpopulations of asymmetrically informed players.\nOne subpopulation observes a stochastic factor that remains inaccessible to the\nother. We derive an equation for the mean-field equilibrium and prove the\nexistence of solutions in probabilistic weak sense. We rely on a discretization\nof the trajectories and on weak convergence arguments. We also study the\nconditions under which a mean-field equilibrium provides an approximation of\nthe equilibrium price for an economy populated by finitely many players.\nFinally, we illustrate how, in the case of a single informed agent, her\nstrategy can be characterized in terms of the equilibrium.\n","authors":["Alekos Cecchin","Markus Fischer","Claudio Fontana","Giacomo Lanaro"],"pdf_url":"https://arxiv.org/pdf/2504.09356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03692v4","updated":"2025-04-12T21:15:15Z","published":"2024-01-08T06:46:39Z","title":"Boosting Column Generation with Graph Neural Networks for Joint Rider\n  Trip Planning and Crew Shift Scheduling","summary":"  Optimizing service schedules is pivotal to the reliable, efficient, and\ninclusive on-demand mobility. This pressing challenge is further exacerbated by\nthe increasing needs of an aging population, the oversubscription of existing\nservices, and the lack of effective solution methods. This study addresses the\nintricacies of service scheduling, by jointly optimizing rider trip planning\nand crew scheduling for a complex dynamic mobility service. The resulting\noptimization problems are extremely challenging computationally for\nstate-of-the-art methods. To address this fundamental gap, this paper\nintroduces the Joint Rider Trip Planning and Crew Shift Scheduling Problem\n(JRTPCSSP) and a novel solution method, called Attention and Gated GNN-Informed\nColumn Generation (AGGNNI-CG), that hybridizes column generation and machine\nlearning to obtain near-optimal solutions to the JRTPCSSP with real-life\nconstraints of the application. The key idea of the machine-learning component\nis to dramatically reduce the number of paths to explore in the pricing\nproblem, accelerating the most time-consuming component of the column\ngeneration. The machine learning component is a graph neural network with an\nattention mechanism and a gated architecture, which is particularly suited to\ncater for the different input sizes coming from daily operations. AGGNNI-CG has\nbeen applied to a challenging, real-world dataset from the Paratransit system\nof Chatham County in Georgia. It produces substantial improvements compared to\nthe baseline column generation approach, which typically cannot produce\nhigh-quality feasible solutions in reasonable time on large-scale complex\ninstances. AGGNNI-CG also produces significant improvements in service quality\ncompared to the existing system.\n","authors":["Jiawei Lu","Tinghan Ye","Wenbo Chen","Pascal Van Hentenryck"],"pdf_url":"https://arxiv.org/pdf/2401.03692v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09487v2","updated":"2025-04-12T19:48:13Z","published":"2024-09-14T17:23:20Z","title":"Evaluating probabilistic and data-driven inference models for\n  fiber-coupled NV-diamond temperature sensors","summary":"  We evaluate the impact of inference model on uncertainties when using\ncontinuous wave Optically Detected Magnetic Resonance (ODMR) measurements to\ninfer temperature. Our approach leverages a probabilistic feedforward inference\nmodel designed to maximize the likelihood of observed ODMR spectra through\nautomatic differentiation. This model effectively utilizes the temperature\ndependence of spin Hamiltonian parameters to infer temperature from spectral\nfeatures in the ODMR data. We achieve prediction uncertainty of $\\pm$ 1 K\nacross a temperature range of 243 K to 323 K. To benchmark our probabilistic\nmodel, we compare it with a non-parametric peak-finding technique and\ndata-driven methodologies such as Principal Component Regression (PCR) and a 1D\nConvolutional Neural Network (CNN). We find that when validated against\nout-of-sample dataset that encompasses the same temperature range as the\ntraining dataset, data driven methods can show uncertainties that are as much\nas 0.67 K lower without incorporating expert-level understanding of the\nspectroscopic-temperature relationship. However, our results show that the\nprobabilistic model outperforms both PCR and CNN when tasked with extrapolating\nbeyond the temperature range used in training set, indicating robustness and\ngeneralizability. In contrast, data-driven methods like PCR and CNN demonstrate\nup to ten times worse uncertainties when tasked with extrapolating outside\ntheir training data range.\n","authors":["Shraddha Rajpal","Zeeshan Ahmed","Tyrus Berry"],"pdf_url":"https://arxiv.org/pdf/2409.09487v2.pdf","comment":"15 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2504.09279v1","updated":"2025-04-12T16:51:02Z","published":"2025-04-12T16:51:02Z","title":"No-Regret Generative Modeling via Parabolic Monge-Ampère PDE","summary":"  We introduce a novel generative modeling framework based on a discretized\nparabolic Monge-Amp\\`ere PDE, which emerges as a continuous limit of the\nSinkhorn algorithm commonly used in optimal transport. Our method performs\niterative refinement in the space of Brenier maps using a mirror gradient\ndescent step. We establish theoretical guarantees for generative modeling\nthrough the lens of no-regret analysis, demonstrating that the iterates\nconverge to the optimal Brenier map under a variety of step-size schedules. As\na technical contribution, we derive a new Evolution Variational Inequality\ntailored to the parabolic Monge-Amp\\`ere PDE, connecting geometry,\ntransportation cost, and regret. Our framework accommodates non-log-concave\ntarget distributions, constructs an optimal sampling process via the Brenier\nmap, and integrates favorable learning techniques from generative adversarial\nnetworks and score-based diffusion models. As direct applications, we\nillustrate how our theory paves new pathways for generative modeling and\nvariational inference.\n","authors":["Nabarun Deb","Tengyuan Liang"],"pdf_url":"https://arxiv.org/pdf/2504.09279v1.pdf","comment":"30 pages, 3 figures"},{"id":"http://arxiv.org/abs/2504.09272v1","updated":"2025-04-12T16:22:30Z","published":"2025-04-12T16:22:30Z","title":"Bouligand Analysis and Discrete Optimal Control of Total Variation-Based\n  Variational Inequalities","summary":"  We investigate differentiability and subdifferentiability properties of the\nsolution mapping associated with variational inequalities (VI) of the second\nkind involving the discrete total-variation. Bouligand differentiability of the\nsolution operator is established via a direct quotient analysis applied to a\nprimal-dual reformulation of the VI. By exploiting the structure of the\ndirectional derivative and introducing a suitable subspace, we fully\ncharacterize the Bouligand subdifferential of the solution mapping. We then\nderive optimality conditions characterizing Bouligand-stationary and\nstrongly-stationary points for discrete VI-constrained optimal control\nproblems. A trust-region algorithm for solving these control problems is\nproposed based on the obtained characterizations, and a numerical experiment is\npresented to illustrate the main properties of both the solution and the\nproposed algorithm.\n","authors":["Juan Carlos De Los Reyes"],"pdf_url":"https://arxiv.org/pdf/2504.09272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09262v1","updated":"2025-04-12T15:45:57Z","published":"2025-04-12T15:45:57Z","title":"Exact Controllability for a Refined Stochastic Hyperbolic Equation with\n  Internal Controls","summary":"  We establish the internal exact controllability of a refined stochastic\nhyperbolic equation by deriving a suitable observability inequality via\nCarleman estimates for the associated backward stochastic hyperbolic equation.\nIn contrast to existing results on boundary exact controllability--which\nrequire longer waiting times, we demonstrate that the required waiting time for\ninternal exact controllability in stochastic hyperbolic equations coincides\nexactly with that of their deterministic counterparts.\n","authors":["Zengyu Li","Zhonghua Liao","Qi Lü"],"pdf_url":"https://arxiv.org/pdf/2504.09262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16241v3","updated":"2025-04-12T14:51:39Z","published":"2023-03-28T18:34:52Z","title":"Convergence of the Stochastic Heavy Ball Method With Approximate\n  Gradients and/or Block Updating","summary":"  In this paper, we establish the convergence of the stochastic Heavy Ball\n(SHB) algorithm under more general conditions than in the current literature.\nSpecifically, (i) The stochastic gradient is permitted to be biased, and also,\nto have conditional variance that grows over time (or iteration number). This\nfeature is essential when applying SHB with zeroth-order methods, which use\nonly two function evaluations to approximate the gradient. In contrast, all\nexisting papers assume that the stochastic gradient is unbiased and/or has\nbounded conditional variance. (ii) The step sizes are permitted to be random,\nwhich is essential when applying SHB with block updating. The sufficient\nconditions for convergence are stochastic analogs of the well-known\nRobbins-Monro conditions. This is in contrast to existing papers where more\nrestrictive conditions are imposed on the step size sequence. (iii) Our\nanalysis embraces not only convex functions, but also more general functions\nthat satisfy the PL (Polyak-{\\L}ojasiewicz) and KL (Kurdyka-{\\L}ojasiewicz)\nconditions. (iv) If the stochastic gradient is unbiased and has bounded\nvariance, and the objective function satisfies (PL), then the iterations of SHB\nmatch the known best rates for convex functions. (v) We establish the\nalmost-sure convergence of the iterations, as opposed to convergence in the\nmean or convergence in probability, which is the case in much of the\nliterature. (vi) Each of the above convergence results continue to hold if\nfull-coordinate updating is replaced by any one of three widely-used updating\nmethods. In addition, numerical computations are carried out to illustrate the\nabove points.\n","authors":["Uday Kiran Reddy Tadipatri","Mathukumalli Vidyasagar"],"pdf_url":"https://arxiv.org/pdf/2303.16241v3.pdf","comment":"37 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.09241v1","updated":"2025-04-12T14:46:03Z","published":"2025-04-12T14:46:03Z","title":"Real-rooted integer polynomial enumeration algorithms and interlacing\n  polynomials via linear programming","summary":"  We extend the algorithms of Robinson, Smyth, and McKee--Smyth to enumerate\nall real-rooted integer polynomials of a fixed degree, where the first few (at\nleast three) leading coefficients are specified. Additionally, we introduce new\nlinear programming algorithms to enumerate all feasible interlacing polynomials\nof a given polynomial that comes from a certain family of real-rooted integer\npolynomials. These algorithms are further specialised for the study of real\nequiangular lines, incorporating additional number-theoretic constraints to\nrestrict the enumeration. Our improvements significantly enhance the efficiency\nof the methods presented in previous work by the authors.\n","authors":["Gary R. W. Greaves","Jeven Syatriadi"],"pdf_url":"https://arxiv.org/pdf/2504.09241v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2504.09190v1","updated":"2025-04-12T12:08:08Z","published":"2025-04-12T12:08:08Z","title":"Krasovskiĭ Stability Theorem for FDEs in the Extended Sense","summary":"  The analysis of the stability of systems' equilibria plays a central role in\nthe study of dynamical systems and control theory. This note establishes an\nextension of the celebrated Krasovski\\u{\\i} stability theorem for functional\ndifferential equations (FDEs) in the extended sense. Namely, the FDEs hold for\n$t \\geq t_0$ almost everywhere with respect to the Lebesgue measure. The\nexistence and uniqueness of such FDEs were briefly discussed in J.K Hale's\nclassical treatise on FDEs, yet a corresponding stability theorem was not\nprovided. A key step in proving the proposed stability theorem was to utilize\nan alternative strategy instead of relying on the mean value theorem of\ndifferentiable functions. The proposed theorem can be useful in the stability\nanalysis of cybernetic systems, which are often subject to noise and glitches\nthat have a countably infinite number of jumps. To demonstrate the usefulness\nof the proposed theorem, we provide examples of linear systems with\ntime-varying delays in which the FDEs cannot be defined in the conventional\nsense.\n","authors":["Qian Feng","Wilfrid Perruquetti"],"pdf_url":"https://arxiv.org/pdf/2504.09190v1.pdf","comment":"Accepted by 23rd European Control Conference (ECC) 2025"},{"id":"http://arxiv.org/abs/2410.10486v2","updated":"2025-04-12T11:58:01Z","published":"2024-10-14T13:28:36Z","title":"Consensus in Multiagent Systems under communication failure","summary":"  We consider multi-agent systems with cooperative interactions and study the\nconvergence to consensus in the case of time-dependent connections, with\npossible communication failure.\n  We prove a new condition ensuring consensus: we define a graph in which\ndirected arrows correspond to connection functions that converge (in the weak\nsense) to some function with a positive integral on all intervals of the form\n$[t,+\\infty)$. If the graph has a node reachable from all other indices,\ni.e.~``globally reachable'', then the system converges to consensus. We show\nthat this requirement generalizes some known sufficient conditions for\nconvergence, such as Moreau's or the Persistent Excitation one. We also give a\nsecond new condition, transversal to the known ones: total connectedness of the\nundirected graph formed by the non-vanishing of limiting functions.\n","authors":["Mohamed Bentaibi","Laura Caravenna","Jean-Paul A. Gauthier","Francesco Rossi"],"pdf_url":"https://arxiv.org/pdf/2410.10486v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07796v2","updated":"2025-04-12T09:00:43Z","published":"2025-04-10T14:36:56Z","title":"Numerical solution by shape optimization method to an inverse shape\n  problem in multi-dimensional advection-diffusion problem with space dependent\n  coefficients","summary":"  This work focuses on numerically solving a shape identification problem\nrelated to advection-diffusion processes with space-dependent coefficients\nusing shape optimization techniques. Two boundary-type cost functionals are\nconsidered, and their corresponding variations with respect to shapes are\nderived using the adjoint method, employing the chain rule approach. This\ninvolves firstly utilizing the material derivative of the state system and\nsecondly using its shape derivative. Subsequently, an alternating direction\nmethod of multipliers (ADMM) combined with the Sobolev-gradient-descent\nalgorithm is applied to stably solve the shape reconstruction problem.\nNumerical experiments in two and three dimensions are conducted to demonstrate\nthe feasibility of the methods.\n","authors":["Elmehdi Cherrat","Lekbir Afraites","Julius Fergy Tiongson Rabago"],"pdf_url":"https://arxiv.org/pdf/2504.07796v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03933v2","updated":"2025-04-12T08:11:58Z","published":"2025-01-07T16:49:01Z","title":"Data-driven Optimization for the Evolve-Filter-Relax regularization of\n  convection-dominated flows","summary":"  Numerical stabilization techniques are often employed in under-resolved\nsimulations of convection-dominated flows to improve accuracy and mitigate\nspurious oscillations. Specifically, the evolve--filter--relax (EFR) algorithm\nis a framework which consists in evolving the solution, applying a filtering\nstep to remove high-frequency noise, and relaxing through a convex combination\nof filtered and original solutions. The stability and accuracy of the EFR\nsolution strongly depend on two parameters, the filter radius $\\delta$ and the\nrelaxation parameter $\\chi$. Standard choices for these parameters are usually\nfixed in time, and related to the full order model setting, i.e., the grid size\nfor $\\delta$ and the time step for $\\chi$. The key novelties with respect to\nthe standard EFR approach are: (i) time-dependent parameters $\\delta(t)$ and\n$\\chi(t)$, and (ii) data-driven adaptive optimization of the parameters in\ntime, considering a fully-resolved simulation as reference. In particular, we\npropose three different classes of optimized-EFR (Opt-EFR) strategies, aiming\nto optimize one or both parameters. The new Opt-EFR strategies are tested in\nthe under-resolved simulation of a turbulent flow past a cylinder at $Re=1000$.\nThe Opt-EFR proved to be more accurate than standard approaches by up to\n99$\\%$, while maintaining a similar computational time. In particular, the key\nnew finding of our analysis is that such accuracy can be obtained only if the\noptimized objective function includes: (i) a global metric (as the kinetic\nenergy), and (ii) spatial gradients' information.\n","authors":["Anna Ivagnes","Maria Strazzullo","Michele Girfoglio","Traian Iliescu","Gianluigi Rozza"],"pdf_url":"https://arxiv.org/pdf/2501.03933v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09078v1","updated":"2025-04-12T04:36:54Z","published":"2025-04-12T04:36:54Z","title":"Role of Intra-specific Competition and Additional Food on Prey-Predator\n  Systems exhibiting Holling Type-IV Functional Response","summary":"  In recent years, the study on the impact of competition on additional food\nprovided prey-predator systems have gained significant attention from\nresearchers in the field of mathematical biology. In this study, we consider an\nadditional food provided prey-predator model exhibiting Holling type-IV\nfunctional response and the intra-specific competition among predators. We\nprove the existence and uniqueness of global positive solutions for the\nproposed model. We study the existence and stability of equilibrium points and\nfurther explore the codimension-$1$ and $2$ bifurcations with respect to the\nadditional food and competition. We further study the global dynamics of the\nsystem and discuss the consequences of providing additional food. Later, we do\nthe time-optimal control studies with respect to the quality and quantity of\nadditional food as control variables by transforming the independent variable\nin the control system. Making use of the Pontraygin maximum principle, we\ncharacterize the optimal quality of additional food and optimal quantity of\nadditional food. We show that the findings of these dynamics and control\nstudies have the potential to be applied to a variety of problems in pest\nmanagement.\n","authors":["D Bhanu Prakash","D K K Vamsi"],"pdf_url":"https://arxiv.org/pdf/2504.09078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11742v3","updated":"2025-04-12T03:12:19Z","published":"2024-12-16T13:02:10Z","title":"A particle system approach towards the global well-posedness of master\n  equations for potential mean field games of control","summary":"  This paper studies the $N$-particle systems as well as the HJB/master\nequations for a class of generalized mean field control (MFC) problems and the\ncorresponding potential mean field games of control (MFGC). A local in time\nclassical solution for the HJB equation is generated via a probabilistic\napproach based on the mean field maximum principle. Given an extension of the\nso called displacement convexity condition, we obtain the uniform estimates on\nthe HJB equation for the $N$-particle system. Such estimates imply the\ndisplacement convexity/semi-concavity and thus the prior estimates on the\nsolution to the HJB equation for generalized MFC problems. The global\nwell-posedness of HJB/master equation for generalized MFC/potential MFGC is\nthen proved thanks to the local well-posedness and the prior estimates. In view\nof the nature of the displacement convexity condition, such well-posedness is\nalso true for the degenerated case. Our analysis on the $N$-particle system\nalso induces an Lipschitz approximator to the optimal feedback function in\ngeneralized MFC/potential MFGC where an algebraic convergence rate is obtained.\nFurthermore, an alternative approximate Nash equilibrium is proposed based on\nthe $N$-particle system, where the approximation error is quantified thanks to\nthe aforementioned uniform estimates.\n","authors":["Huafu Liao","Chenchen Mou"],"pdf_url":"https://arxiv.org/pdf/2412.11742v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09043v1","updated":"2025-04-12T01:55:35Z","published":"2025-04-12T01:55:35Z","title":"Cooperation, Competition, and Common Pool Resources in Mean Field Games","summary":"  Mean field games (MFGs) have been introduced to study Nash equilibria in very\nlarge population of self-interested agents. However, when applied to common\npool resource (CPR) games, MFG equilibria lead to the so-called tragedy of the\ncommons (TOTC). Empirical studies have shown that in many situations, TOTC does\nnot materialize which hints at the fact that standard MFG models cannot explain\nthe behavior of agents in CPR games. In this work, we study two models which\nincorporate a mix of cooperative and non-cooperative behaviors, either at the\nindividual level or the population level. After defining these models, we study\noptimality conditions in the form of forward-backward stochastic differential\nequations and we prove that the mean field models provide approximate\nequilibria controls for corresponding finite-agent games. We then show an\napplication to a model of fish stock management, for which the solution can be\ncomputed by solving systems of ordinary differential equations, which we prove\nto have a unique solution. Numerical results illustrate the impact of the level\nof cooperation at the individual and the population levels on the CPR.\n","authors":["Gokce Dayanikli","Mathieu Lauriere"],"pdf_url":"https://arxiv.org/pdf/2504.09043v1.pdf","comment":"53 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.09035v1","updated":"2025-04-12T01:18:53Z","published":"2025-04-12T01:18:53Z","title":"InterQ: A DQN Framework for Optimal Intermittent Control","summary":"  In this letter, we explore the communication-control co-design of\ndiscrete-time stochastic linear systems through reinforcement learning.\nSpecifically, we examine a closed-loop system involving two sequential\ndecision-makers: a scheduler and a controller. The scheduler continuously\nmonitors the system's state but transmits it to the controller intermittently\nto balance the communication cost and control performance. The controller, in\nturn, determines the control input based on the intermittently received\ninformation. Given the partially nested information structure, we show that the\noptimal control policy follows a certainty-equivalence form. Subsequently, we\nanalyze the qualitative behavior of the scheduling policy. To develop the\noptimal scheduling policy, we propose InterQ, a deep reinforcement learning\nalgorithm which uses a deep neural network to approximate the Q-function.\nThrough extensive numerical evaluations, we analyze the scheduling landscape\nand further compare our approach against two baseline strategies: (a) a\nmulti-period periodic scheduling policy, and (b) an event-triggered policy. The\nresults demonstrate that our proposed method outperforms both baselines. The\nopen source implementation can be found at https://github.com/AC-sh/InterQ.\n","authors":["Shubham Aggarwal","Dipankar Maity","Tamer Başar"],"pdf_url":"https://arxiv.org/pdf/2504.09035v1.pdf","comment":"Submitted to IEEE for possible publication"}]},"2025-04-15T00:00:00Z":{"Statistical Machine Learning (Theory)":[{"id":"http://arxiv.org/abs/2504.11396v1","updated":"2025-04-15T17:10:38Z","published":"2025-04-15T17:10:38Z","title":"Property Inheritance for Subtensors in Tensor Train Decompositions","summary":"  Tensor dimensionality reduction is one of the fundamental tools for modern\ndata science. To address the high computational overhead, fiber-wise sampled\nsubtensors that preserve the original tensor rank are often used in designing\nefficient and scalable tensor dimensionality reduction. However, the theory of\nproperty inheritance for subtensors is still underdevelopment, that is, how the\nessential properties of the original tensor will be passed to its subtensors.\nThis paper theoretically studies the property inheritance of the two key tensor\nproperties, namely incoherence and condition number, under the tensor train\nsetting. We also show how tensor train rank is preserved through fiber-wise\nsampling. The key parameters introduced in theorems are numerically evaluated\nunder various settings. The results show that the properties of interest can be\nwell preserved to the subtensors formed via fiber-wise sampling. Overall, this\npaper provides several handy analytic tools for developing efficient tensor\nanalysis\n","authors":["HanQin Cai","Longxiu Huang"],"pdf_url":"https://arxiv.org/pdf/2504.11396v1.pdf","comment":"2025 IEEE International Symposium on Information Theory (ISIT 2025)"},{"id":"http://arxiv.org/abs/2503.09722v3","updated":"2025-04-15T16:25:03Z","published":"2025-03-12T18:11:37Z","title":"The Pitfalls of Imitation Learning when Actions are Continuous","summary":"  We study the problem of imitating an expert demonstrator in a discrete-time,\ncontinuous state-and-action control system. We show that, even if the dynamics\nsatisfy a control-theoretic property called exponentially stability (i.e. the\neffects of perturbations decay exponentially quickly), and the expert is smooth\nand deterministic, any smooth, deterministic imitator policy necessarily\nsuffers error on execution that is exponentially larger, as a function of\nproblem horizon, than the error under the distribution of expert training data.\nOur negative result applies to any algorithm which learns solely from expert\ndata, including both behavior cloning and offline-RL algorithms, unless the\nalgorithm produces highly \"improper\" imitator policies--those which are\nnon-smooth, non-Markovian, or which exhibit highly state-dependent\nstochasticity--or unless the expert trajectory distribution is sufficiently\n\"spread.\" We provide experimental evidence of the benefits of these more\ncomplex policy parameterizations, explicating the benefits of today's popular\npolicy parameterizations in robot learning (e.g. action-chunking and Diffusion\nPolicies). We also establish a host of complementary negative and positive\nresults for imitation in control systems.\n","authors":["Max Simchowitz","Daniel Pfrommer","Ali Jadbabaie"],"pdf_url":"https://arxiv.org/pdf/2503.09722v3.pdf","comment":"98 pages, 2 figures, updated proof sketch"},{"id":"http://arxiv.org/abs/2504.11353v1","updated":"2025-04-15T16:23:25Z","published":"2025-04-15T16:23:25Z","title":"An Adaptive Dropout Approach for High-Dimensional Bayesian Optimization","summary":"  Bayesian optimization (BO) is a widely used algorithm for solving expensive\nblack-box optimization problems. However, its performance decreases\nsignificantly on high-dimensional problems due to the inherent\nhigh-dimensionality of the acquisition function. In the proposed algorithm, we\nadaptively dropout the variables of the acquisition function along the\niterations. By gradually reducing the dimension of the acquisition function,\nthe proposed approach has less and less difficulty to optimize the acquisition\nfunction. Numerical experiments demonstrate that AdaDropout effectively tackle\nhigh-dimensional challenges and improve solution quality where standard\nBayesian optimization methods often struggle. Moreover, it achieves superior\nresults when compared with state-of-the-art high-dimensional Bayesian\noptimization approaches. This work provides a simple yet efficient solution for\nhigh-dimensional expensive optimization.\n","authors":["Jundi Huang","Dawei Zhan"],"pdf_url":"https://arxiv.org/pdf/2504.11353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11344v1","updated":"2025-04-15T16:15:16Z","published":"2025-04-15T16:15:16Z","title":"Interpretable Hybrid-Rule Temporal Point Processes","summary":"  Temporal Point Processes (TPPs) are widely used for modeling event sequences\nin various medical domains, such as disease onset prediction, progression\nanalysis, and clinical decision support. Although TPPs effectively capture\ntemporal dynamics, their lack of interpretability remains a critical challenge.\nRecent advancements have introduced interpretable TPPs. However, these methods\nfail to incorporate numerical features, thereby limiting their ability to\ngenerate precise predictions. To address this issue, we propose Hybrid-Rule\nTemporal Point Processes (HRTPP), a novel framework that integrates temporal\nlogic rules with numerical features, improving both interpretability and\npredictive accuracy in event modeling. HRTPP comprises three key components:\nbasic intensity for intrinsic event likelihood, rule-based intensity for\nstructured temporal dependencies, and numerical feature intensity for dynamic\nprobability modulation. To effectively discover valid rules, we introduce a\ntwo-phase rule mining strategy with Bayesian optimization. To evaluate our\nmethod, we establish a multi-criteria assessment framework, incorporating rule\nvalidity, model fitting, and temporal predictive accuracy. Experimental results\non real-world medical datasets demonstrate that HRTPP outperforms\nstate-of-the-art interpretable TPPs in terms of predictive performance and\nclinical interpretability. In case studies, the rules extracted by HRTPP\nexplain the disease progression, offering valuable contributions to medical\ndiagnosis.\n","authors":["Yunyang Cao","Juekai Lin","Hongye Wang","Wenhao Li","Bo Jin"],"pdf_url":"https://arxiv.org/pdf/2504.11344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11343v1","updated":"2025-04-15T16:15:02Z","published":"2025-04-15T16:15:02Z","title":"A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce","summary":"  Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training.\n","authors":["Wei Xiong","Jiarui Yao","Yuhui Xu","Bo Pang","Lei Wang","Doyen Sahoo","Junnan Li","Nan Jiang","Tong Zhang","Caiming Xiong","Hanze Dong"],"pdf_url":"https://arxiv.org/pdf/2504.11343v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.11320v1","updated":"2025-04-15T16:00:21Z","published":"2025-04-15T16:00:21Z","title":"Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints","summary":"  Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.\n","authors":["Ruicheng Ao","Gan Luo","David Simchi-Levi","Xinshang Wang"],"pdf_url":"https://arxiv.org/pdf/2504.11320v1.pdf","comment":"42 pages, 18 figures"},{"id":"http://arxiv.org/abs/2504.09347v2","updated":"2025-04-15T15:55:26Z","published":"2025-04-12T21:32:42Z","title":"Inferring Outcome Means of Exponential Family Distributions Estimated by\n  Deep Neural Networks","summary":"  While deep neural networks (DNNs) are widely used for prediction, inference\non DNN-estimated subject-specific means for categorical or exponential family\noutcomes remains underexplored. We address this by proposing a DNN estimator\nunder generalized nonparametric regression models (GNRMs) and developing a\nrigorous inference framework. Unlike existing approaches that assume\nindependence between prediction errors and inputs to establish the error bound,\na condition often violated in GNRMs, we allow for dependence and our\ntheoretical analysis demonstrates the feasibility of drawing inference under\nGNRMs. To implement inference, we consider an Ensemble Subsampling Method (ESM)\nthat leverages U-statistics and the Hoeffding decomposition to construct\nreliable confidence intervals for DNN estimates. We show that, under GNRM\nsettings, ESM enables model-free variance estimation and accounts for\nheterogeneity among individuals in the population. Through simulations under\nnonparametric logistic, Poisson, and binomial regression models, we demonstrate\nthe effectiveness and efficiency of our method. We further apply the method to\nthe electronic Intensive Care Unit (eICU) dataset, a large-scale collection of\nanonymized health records from ICU patients, to predict ICU readmission risk\nand offer patient-centric insights for clinical decision-making.\n","authors":["Xuran Meng","Yi Li"],"pdf_url":"https://arxiv.org/pdf/2504.09347v2.pdf","comment":"44 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2504.11304v1","updated":"2025-04-15T15:45:48Z","published":"2025-04-15T15:45:48Z","title":"Differentially Private Geodesic and Linear Regression","summary":"  In statistical applications it has become increasingly common to encounter\ndata structures that live on non-linear spaces such as manifolds. Classical\nlinear regression, one of the most fundamental methodologies of statistical\nlearning, captures the relationship between an independent variable and a\nresponse variable which both are assumed to live in Euclidean space. Thus,\ngeodesic regression emerged as an extension where the response variable lives\non a Riemannian manifold. The parameters of geodesic regression, as with linear\nregression, capture the relationship of sensitive data and hence one should\nconsider the privacy protection practices of said parameters. We consider\nreleasing Differentially Private (DP) parameters of geodesic regression via the\nK-Norm Gradient (KNG) mechanism for Riemannian manifolds. We derive theoretical\nbounds for the sensitivity of the parameters showing they are tied to their\nrespective Jacobi fields and hence the curvature of the space. This\ncorroborates recent findings of differential privacy for the Fr\\'echet mean. We\ndemonstrate the efficacy of our methodology on the sphere,\n$\\mbS^2\\subset\\mbR^3$ and, since it is general to Riemannian manifolds, the\nmanifold of Euclidean space which simplifies geodesic regression to a case of\nlinear regression. Our methodology is general to any Riemannian manifold and\nthus it is suitable for data in domains such as medical imaging and computer\nvision.\n","authors":["Aditya Kulkarni","Carlos Soto"],"pdf_url":"https://arxiv.org/pdf/2504.11304v1.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.11284v1","updated":"2025-04-15T15:25:27Z","published":"2025-04-15T15:25:27Z","title":"Bipartite Ranking From Multiple Labels: On Loss Versus Label Aggregation","summary":"  Bipartite ranking is a fundamental supervised learning problem, with the goal\nof learning a ranking over instances with maximal area under the ROC curve\n(AUC) against a single binary target label. However, one may often observe\nmultiple binary target labels, e.g., from distinct human annotators. How can\none synthesize such labels into a single coherent ranking? In this work, we\nformally analyze two approaches to this problem -- loss aggregation and label\naggregation -- by characterizing their Bayes-optimal solutions. Based on this,\nwe show that while both methods can yield Pareto-optimal solutions, loss\naggregation can exhibit label dictatorship: one can inadvertently (and\nundesirably) favor one label over others. This suggests that label aggregation\ncan be preferable to loss aggregation, which we empirically verify.\n","authors":["Michal Lukasik","Lin Chen","Harikrishna Narasimhan","Aditya Krishna Menon","Wittawat Jitkrittum","Felix X. Yu","Sashank J. Reddi","Gang Fu","Mohammadhossein Bateni","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2504.11284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11279v1","updated":"2025-04-15T15:18:58Z","published":"2025-04-15T15:18:58Z","title":"Simulation-based inference for stochastic nonlinear mixed-effects models\n  with applications in systems biology","summary":"  The analysis of data from multiple experiments, such as observations of\nseveral individuals, is commonly approached using mixed-effects models, which\naccount for variation between individuals through hierarchical representations.\nThis makes mixed-effects models widely applied in fields such as biology,\npharmacokinetics, and sociology. In this work, we propose a novel methodology\nfor scalable Bayesian inference in hierarchical mixed-effects models. Our\nframework first constructs amortized approximations of the likelihood and the\nposterior distribution, which are then rapidly refined for each individual\ndataset, to ultimately approximate the parameters posterior across many\nindividuals. The framework is easily trainable, as it uses mixtures of experts\nbut without neural networks, leading to parsimonious yet expressive surrogate\nmodels of the likelihood and the posterior. We demonstrate the effectiveness of\nour methodology using challenging stochastic models, such as mixed-effects\nstochastic differential equations emerging in systems biology-driven problems.\nHowever, the approach is broadly applicable and can accommodate both stochastic\nand deterministic models. We show that our approach can seamlessly handle\ninference for many parameters. Additionally, we applied our method to a\nreal-data case study of mRNA transfection. When compared to exact\npseudomarginal Bayesian inference, our approach proved to be both fast and\ncompetitive in terms of statistical accuracy.\n","authors":["Henrik Häggström","Sebastian Persson","Marija Cvijovic","Umberto Picchini"],"pdf_url":"https://arxiv.org/pdf/2504.11279v1.pdf","comment":"36 pages, 20 figures"},{"id":"http://arxiv.org/abs/2504.11249v1","updated":"2025-04-15T14:46:25Z","published":"2025-04-15T14:46:25Z","title":"Cryo-em images are intrinsically low dimensional","summary":"  Simulation-based inference provides a powerful framework for cryo-electron\nmicroscopy, employing neural networks in methods like CryoSBI to infer\nbiomolecular conformations via learned latent representations. This latent\nspace represents a rich opportunity, encoding valuable information about the\nphysical system and the inference process. Harnessing this potential hinges on\nunderstanding the underlying geometric structure of these representations. We\ninvestigate this structure by applying manifold learning techniques to CryoSBI\nrepresentations of hemagglutinin (simulated and experimental). We reveal that\nthese high-dimensional data inherently populate low-dimensional, smooth\nmanifolds, with simulated data effectively covering the experimental\ncounterpart. By characterizing the manifold's geometry using Diffusion Maps and\nidentifying its principal axes of variation via coordinate interpretation\nmethods, we establish a direct link between the latent structure and key\nphysical parameters. Discovering this intrinsic low-dimensionality and\ninterpretable geometric organization not only validates the CryoSBI approach\nbut enables us to learn more from the data structure and provides opportunities\nfor improving future inference strategies by exploiting this revealed manifold\ngeometry.\n","authors":["Luke Evans","Octavian-Vlad Murad","Lars Dingeldein","Pilar Cossio","Roberto Covino","Marina Meila"],"pdf_url":"https://arxiv.org/pdf/2504.11249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03369v2","updated":"2025-04-15T14:29:02Z","published":"2024-06-05T15:24:20Z","title":"Posterior and variational inference for deep neural networks with\n  heavy-tailed weights","summary":"  We consider deep neural networks in a Bayesian framework with a prior\ndistribution sampling the network weights at random. Following a recent idea of\nAgapiou and Castillo (2023), who show that heavy-tailed prior distributions\nachieve automatic adaptation to smoothness, we introduce a simple Bayesian deep\nlearning prior based on heavy-tailed weights and ReLU activation. We show that\nthe corresponding posterior distribution achieves near-optimal minimax\ncontraction rates, simultaneously adaptive to both intrinsic dimension and\nsmoothness of the underlying function, in a variety of contexts including\nnonparametric regression, geometric data and Besov spaces. While most works so\nfar need a form of model selection built-in within the prior distribution, a\nkey aspect of our approach is that it does not require to sample\nhyperparameters to learn the architecture of the network. We also provide\nvariational Bayes counterparts of the results, that show that mean-field\nvariational approximations still benefit from near-optimal theoretical support.\n","authors":["Ismaël Castillo","Paul Egels"],"pdf_url":"https://arxiv.org/pdf/2406.03369v2.pdf","comment":"58 pages"},{"id":"http://arxiv.org/abs/2504.06372v2","updated":"2025-04-15T13:25:02Z","published":"2025-04-08T18:44:33Z","title":"A Metropolis-Adjusted Langevin Algorithm for Sampling Jeffreys Prior","summary":"  Inference and estimation are fundamental aspects of statistics, system\nidentification and machine learning. For most inference problems, prior\nknowledge is available on the system to be modeled, and Bayesian analysis is a\nnatural framework to impose such prior information in the form of a prior\ndistribution. However, in many situations, coming out with a fully specified\nprior distribution is not easy, as prior knowledge might be too vague, so\npractitioners prefer to use a prior distribution that is as `ignorant' or\n`uninformative' as possible, in the sense of not imposing subjective beliefs,\nwhile still supporting reliable statistical analysis. Jeffreys prior is an\nappealing uninformative prior because it offers two important benefits: (i) it\nis invariant under any re-parameterization of the model, (ii) it encodes the\nintrinsic geometric structure of the parameter space through the Fisher\ninformation matrix, which in turn enhances the diversity of parameter samples.\nDespite these benefits, drawing samples from Jeffreys prior is a challenging\ntask. In this paper, we propose a general sampling scheme using the\nMetropolis-Adjusted Langevin Algorithm that enables sampling of parameter\nvalues from Jeffreys prior, and provide numerical illustrations of our approach\nthrough several examples.\n","authors":["Yibo Shi","Braghadeesh Lakshminarayanan","Cristian R. Rojas"],"pdf_url":"https://arxiv.org/pdf/2504.06372v2.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2504.11133v1","updated":"2025-04-15T12:34:09Z","published":"2025-04-15T12:34:09Z","title":"Hessian stability and convergence rates for entropic and Sinkhorn\n  potentials via semiconcavity","summary":"  In this paper we determine quantitative stability bounds for the Hessian of\nentropic potentials, i.e., the dual solution to the entropic optimal transport\nproblem. Up to authors' knowledge this is the first work addressing this\nsecond-order quantitative stability estimate in general unbounded settings. Our\nproof strategy relies on semiconcavity properties of entropic potentials and on\nthe representation of entropic transport plans as laws of forward and backward\ndiffusion processes, known as Schr\\\"odinger bridges. Moreover, our approach\nallows to deduce a stochastic proof of quantitative stability entropic\nestimates and integrated gradient estimates as well. Finally, as a direct\nconsequence of these stability bounds, we deduce exponential convergence rates\nfor gradient and Hessian of Sinkhorn iterates along Sinkhorn's algorithm, a\nproblem that was still open in unbounded settings. Our rates have a polynomial\ndependence on the regularization parameter.\n","authors":["Giacomo Greco","Luca Tamanini"],"pdf_url":"https://arxiv.org/pdf/2504.11133v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2504.11130v1","updated":"2025-04-15T12:30:21Z","published":"2025-04-15T12:30:21Z","title":"Divergence of Empirical Neural Tangent Kernel in Classification Problems","summary":"  This paper demonstrates that in classification problems, fully connected\nneural networks (FCNs) and residual neural networks (ResNets) cannot be\napproximated by kernel logistic regression based on the Neural Tangent Kernel\n(NTK) under overtraining (i.e., when training time approaches infinity).\nSpecifically, when using the cross-entropy loss, regardless of how large the\nnetwork width is (as long as it is finite), the empirical NTK diverges from the\nNTK on the training samples as training time increases. To establish this\nresult, we first demonstrate the strictly positive definiteness of the NTKs for\nmulti-layer FCNs and ResNets. Then, we prove that during training, % with the\ncross-entropy loss, the neural network parameters diverge if the smallest\neigenvalue of the empirical NTK matrix (Gram matrix) with respect to training\nsamples is bounded below by a positive constant. This behavior contrasts\nsharply with the lazy training regime commonly observed in regression problems.\nConsequently, using a proof by contradiction, we show that the empirical NTK\ndoes not uniformly converge to the NTK across all times on the training samples\nas the network width increases. We validate our theoretical results through\nexperiments on both synthetic data and the MNIST classification task. This\nfinding implies that NTK theory is not applicable in this context, with\nsignificant theoretical implications for understanding neural networks in\nclassification problems.\n","authors":["Zixiong Yu","Songtao Tian","Guhan Chen"],"pdf_url":"https://arxiv.org/pdf/2504.11130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11044v1","updated":"2025-04-15T10:12:26Z","published":"2025-04-15T10:12:26Z","title":"On relative universality, regression operator, and conditional\n  independence","summary":"  The notion of relative universality with respect to a {\\sigma}-field was\nintroduced to establish the unbiasedness and Fisher consistency of an estimator\nin nonlinear sufficient dimension reduction. However, there is a gap in the\nproof of this result in the existing literature. The existing definition of\nrelative universality seems to be too strong for the proof to be valid. In this\nnote we modify the definition of relative universality using the concept of\n\\k{o}-measurability, and rigorously establish the mentioned unbiasedness and\nFisher consistency. The significance of this result is beyond its original\ncontext of sufficient dimension reduction, because relative universality allows\nus to use the regression operator to fully characterize conditional\nindependence, a crucially important statistical relation that sits at the core\nof many areas and methodologies in statistics and machine learning, such as\ndimension reduction, graphical models, probability embedding, causal inference,\nand Bayesian estimation.\n","authors":["Bing Li","Ben Jones","Andreas Artemiou"],"pdf_url":"https://arxiv.org/pdf/2504.11044v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2504.03784v3","updated":"2025-04-15T09:29:06Z","published":"2025-04-03T16:16:35Z","title":"Robust Reinforcement Learning from Human Feedback for Large Language\n  Models Fine-Tuning","summary":"  Reinforcement learning from human feedback (RLHF) has emerged as a key\ntechnique for aligning the output of large language models (LLMs) with human\npreferences. To learn the reward function, most existing RLHF algorithms use\nthe Bradley-Terry model, which relies on assumptions about human preferences\nthat may not reflect the complexity and variability of real-world judgments. In\nthis paper, we propose a robust algorithm to enhance the performance of\nexisting approaches under such reward model misspecifications. Theoretically,\nour algorithm reduces the variance of reward and policy estimators, leading to\nimproved regret bounds. Empirical evaluations on LLM benchmark datasets\ndemonstrate that the proposed algorithm consistently outperforms existing\nmethods, with 77-81% of responses being favored over baselines on the Anthropic\nHelpful and Harmless dataset.\n","authors":["Kai Ye","Hongyi Zhou","Jin Zhu","Francesco Quinzan","Chengchung Shi"],"pdf_url":"https://arxiv.org/pdf/2504.03784v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22248v2","updated":"2025-04-15T04:39:57Z","published":"2024-10-29T17:11:33Z","title":"Model-free Estimation of Latent Structure via Multiscale Nonparametric\n  Maximum Likelihood","summary":"  Multivariate distributions often carry latent structures that are difficult\nto identify and estimate, and which better reflect the data generating\nmechanism than extrinsic structures exhibited simply by the raw data. In this\npaper, we propose a model-free approach for estimating such latent structures\nwhenever they are present, without assuming they exist a priori. Given an\narbitrary density $p_0$, we construct a multiscale representation of the\ndensity and propose data-driven methods for selecting representative models\nthat capture meaningful discrete structure. Our approach uses a nonparametric\nmaximum likelihood estimator to estimate the latent structure at different\nscales and we further characterize their asymptotic limits. By carrying out\nsuch a multiscale analysis, we obtain coarseto-fine structures inherent in the\noriginal distribution, which are integrated via a model selection procedure to\nyield an interpretable discrete representation of it. As an application, we\ndesign a clustering algorithm based on the proposed procedure and demonstrate\nits effectiveness in capturing a wide range of latent structures.\n","authors":["Bryon Aragam","Ruiyi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.22248v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07031v2","updated":"2025-04-15T03:43:54Z","published":"2024-03-11T04:19:05Z","title":"Cramming Contextual Bandits for On-policy Statistical Evaluation","summary":"  We introduce the cram method as a general statistical framework for\nevaluating the final learned policy from a multi-armed contextual bandit\nalgorithm, using the dataset generated by the same bandit algorithm. The\nproposed on-policy evaluation methodology differs from most existing methods\nthat focus on off-policy performance evaluation of contextual bandit\nalgorithms. Cramming utilizes an entire bandit sequence through a single pass\nof data, leading to both statistically and computationally efficient\nevaluation. We prove that if a bandit algorithm satisfies a certain stability\ncondition, the resulting crammed evaluation estimator is consistent and\nasymptotically normal under mild regularity conditions. Furthermore, we show\nthat this stability condition holds for commonly used linear contextual bandit\nalgorithms, including epsilon-greedy, Thompson Sampling, and Upper Confidence\nBound algorithms. Using both synthetic and publicly available datasets, we\ncompare the empirical performance of cramming with the state-of-the-art\nmethods. The results demonstrate that the proposed cram method reduces the\nevaluation standard error by approximately 40% relative to off-policy\nevaluation methods while preserving unbiasedness and valid confidence interval\ncoverage.\n","authors":["Zeyang Jia","Kosuke Imai","Michael Lingzhi Li"],"pdf_url":"https://arxiv.org/pdf/2403.07031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08178v3","updated":"2025-04-15T03:35:43Z","published":"2025-04-11T00:20:37Z","title":"A Piecewise Lyapunov Analysis of Sub-quadratic SGD: Applications to\n  Robust and Quantile Regression","summary":"  Motivated by robust and quantile regression problems, we investigate the\nstochastic gradient descent (SGD) algorithm for minimizing an objective\nfunction $f$ that is locally strongly convex with a sub--quadratic tail. This\nsetting covers many widely used online statistical methods. We introduce a\nnovel piecewise Lyapunov function that enables us to handle functions $f$ with\nonly first-order differentiability, which includes a wide range of popular loss\nfunctions such as Huber loss. Leveraging our proposed Lyapunov function, we\nderive finite-time moment bounds under general diminishing stepsizes, as well\nas constant stepsizes. We further establish the weak convergence, central limit\ntheorem and bias characterization under constant stepsize, providing the first\ngeometrical convergence result for sub--quadratic SGD. Our results have wide\napplications, especially in online statistical methods. In particular, we\ndiscuss two applications of our results. 1) Online robust regression: We\nconsider a corrupted linear model with sub--exponential covariates and\nheavy--tailed noise. Our analysis provides convergence rates comparable to\nthose for corrupted models with Gaussian covariates and noise. 2) Online\nquantile regression: Importantly, our results relax the common assumption in\nprior work that the conditional density is continuous and provide a more\nfine-grained analysis for the moment bounds.\n","authors":["Yixuan Zhang","Dongyan Huo","Yudong Chen","Qiaomin Xie"],"pdf_url":"https://arxiv.org/pdf/2504.08178v3.pdf","comment":"ACM SIGMETRICS 2025. 40 pages, 12 figures"},{"id":"http://arxiv.org/abs/2504.09210v2","updated":"2025-04-15T02:22:16Z","published":"2025-04-12T13:32:11Z","title":"FairACE: Achieving Degree Fairness in Graph Neural Networks via\n  Contrastive and Adversarial Group-Balanced Training","summary":"  Fairness has been a significant challenge in graph neural networks (GNNs)\nsince degree biases often result in un-equal prediction performance among nodes\nwith varying degrees. Existing GNN models focus on prediction accuracy,\nfrequently overlooking fairness across different degree groups. To addressthis\nissue, we propose a novel GNN framework, namely Fairness- Aware Asymmetric\nContrastive Ensemble (FairACE), which inte-grates asymmetric contrastive\nlearning with adversarial training to improve degree fairness. FairACE captures\none-hop local neighborhood information and two-hop monophily similarity to\ncreate fairer node representations and employs a degree fairness regulator to\nbalance performance between high-degree and low-degree nodes. During model\ntraining, a novel group-balanced fairness loss is proposed to minimize\nclassification disparities across degree groups. In addition, we also propose a\nnovel fairness metric, the Accuracy Distribution Gap (ADG), which can\nquantitatively assess and ensure equitable performance across different\ndegree-based node groups. Experimental results on both synthetic and real-world\ndatasets demonstrate that FairACE significantly improves degree fairness\nmetrics while maintaining competitive accuracy in comparison to the\nstate-of-the-art GNN models.\n","authors":["Jiaxin Liu","Xiaoqian Jiang","Xiang Li","Bohan Zhang","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.09210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11610v1","updated":"2025-04-15T20:49:31Z","published":"2025-04-15T20:49:31Z","title":"Generalized probabilistic canonical correlation analysis for multi-modal\n  data integration with full or partial observations","summary":"  Background: The integration and analysis of multi-modal data are increasingly\nessential across various domains including bioinformatics. As the volume and\ncomplexity of such data grow, there is a pressing need for computational models\nthat not only integrate diverse modalities but also leverage their\ncomplementary information to improve clustering accuracy and insights,\nespecially when dealing with partial observations with missing data. Results:\nWe propose Generalized Probabilistic Canonical Correlation Analysis (GPCCA), an\nunsupervised method for the integration and joint dimensionality reduction of\nmulti-modal data. GPCCA addresses key challenges in multi-modal data analysis\nby handling missing values within the model, enabling the integration of more\nthan two modalities, and identifying informative features while accounting for\ncorrelations within individual modalities. The model demonstrates robustness to\nvarious missing data patterns and provides low-dimensional embeddings that\nfacilitate downstream clustering and analysis. In a range of simulation\nsettings, GPCCA outperforms existing methods in capturing essential patterns\nacross modalities. Additionally, we demonstrate its applicability to\nmulti-omics data from TCGA cancer datasets and a multi-view image dataset.\nConclusion: GPCCA offers a useful framework for multi-modal data integration,\neffectively handling missing data and providing informative low-dimensional\nembeddings. Its performance across cancer genomics and multi-view image data\nhighlights its robustness and potential for broad application. To make the\nmethod accessible to the wider research community, we have released an R\npackage, GPCCA, which is available at https://github.com/Kaversoniano/GPCCA.\n","authors":["Tianjian Yang","Wei Vivian Li"],"pdf_url":"https://arxiv.org/pdf/2504.11610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11609v1","updated":"2025-04-15T20:46:42Z","published":"2025-04-15T20:46:42Z","title":"Towards Interpretable Deep Generative Models via Causal Representation\n  Learning","summary":"  Recent developments in generative artificial intelligence (AI) rely on\nmachine learning techniques such as deep learning and generative modeling to\nachieve state-of-the-art performance across wide-ranging domains. These\nmethods' surprising performance is due in part to their ability to learn\nimplicit \"representations'' of complex, multi-modal data. Unfortunately, deep\nneural networks are notoriously black boxes that obscure these representations,\nmaking them difficult to interpret or analyze. To resolve these difficulties,\none approach is to build new interpretable neural network models from the\nground up. This is the goal of the emerging field of causal representation\nlearning (CRL) that uses causality as a vector for building flexible,\ninterpretable, and transferable generative AI. CRL can be seen as a culmination\nof three intrinsically statistical problems: (i) latent variable models such as\nfactor analysis; (ii) causal graphical models with latent variables; and (iii)\nnonparametric statistics and deep learning. This paper reviews recent progress\nin CRL from a statistical perspective, focusing on connections to classical\nmodels and statistical and causal identifiablity results. This review also\nhighlights key application areas, implementation strategies, and open\nstatistical questions in CRL.\n","authors":["Gemma E. Moran","Bryon Aragam"],"pdf_url":"https://arxiv.org/pdf/2504.11609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12668v3","updated":"2025-04-15T19:39:47Z","published":"2024-02-20T02:36:26Z","title":"Randomization Can Reduce Both Bias and Variance: A Case Study in Random\n  Forests","summary":"  We study the often overlooked phenomenon, first noted in\n\\cite{breiman2001random}, that random forests appear to reduce bias compared to\nbagging. Motivated by an interesting paper by \\cite{mentch2020randomization},\nwhere the authors explain the success of random forests in low signal-to-noise\nratio (SNR) settings through regularization, we explore how random forests can\ncapture patterns in the data that bagging ensembles fail to capture. We\nempirically demonstrate that in the presence of such patterns, random forests\nreduce bias along with variance and can increasingly outperform bagging\nensembles when SNR is high. Our observations offer insights into the real-world\nsuccess of random forests across a range of SNRs and enhance our understanding\nof the difference between random forests and bagging ensembles. Our\ninvestigations also yield practical insights into the importance of tuning\n$mtry$ in random forests.\n","authors":["Brian Liu","Rahul Mazumder"],"pdf_url":"https://arxiv.org/pdf/2402.12668v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13950v3","updated":"2025-04-15T19:35:15Z","published":"2024-05-22T19:33:58Z","title":"Learning to sample fibers for goodness-of-fit testing","summary":"  We consider the problem of constructing exact goodness-of-fit tests for\ndiscrete exponential family models. This classical problem remains practically\nunsolved for many types of structured or sparse data, as it rests on a\ncomputationally difficult core task: to produce a reliable sample from lattice\npoints in a high-dimensional polytope. We translate the problem into a Markov\ndecision process and demonstrate a reinforcement learning approach for learning\n`good moves' for sampling. We illustrate the approach on data sets and models\nfor which traditional MCMC samplers converge too slowly due to problem size,\nsparsity structure, and the requirement to use prohibitive non-linear algebra\ncomputations in the process. The differentiating factor is the use of scalable\ntools from \\emph{linear} algebra in the context of theoretical guarantees\nprovided by \\emph{non-linear} algebra. Our algorithm is based on an\nactor-critic sampling scheme, with provable convergence.\n  The discovered moves can be used to efficiently obtain an exchangeable\nsample, significantly cutting computational times with regards to statistical\ntesting.\n","authors":["Ivan Gvozdanović","Sonja Petrović"],"pdf_url":"https://arxiv.org/pdf/2405.13950v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11555v1","updated":"2025-04-15T18:53:51Z","published":"2025-04-15T18:53:51Z","title":"Sub-optimality of the Separation Principle for Quadratic Control from\n  Bilinear Observations","summary":"  We consider the problem of controlling a linear dynamical system from\nbilinear observations with minimal quadratic cost. Despite the similarity of\nthis problem to standard linear quadratic Gaussian (LQG) control, we show that\nwhen the observation model is bilinear, neither does the Separation Principle\nhold, nor is the optimal controller affine in the estimated state. Moreover,\nthe cost-to-go is non-convex in the control input. Hence, finding an analytical\nexpression for the optimal feedback controller is difficult in general. Under\ncertain settings, we show that the standard LQG controller locally maximizes\nthe cost instead of minimizing it. Furthermore, the optimal controllers\n(derived analytically) are not unique and are nonlinear in the estimated state.\nWe also introduce a notion of input-dependent observability and derive\nconditions under which the Kalman filter covariance remains bounded. We\nillustrate our theoretical results through numerical experiments in multiple\nsynthetic settings.\n","authors":["Yahya Sattar","Sunmook Choi","Yassir Jedra","Maryam Fazel","Sarah Dean"],"pdf_url":"https://arxiv.org/pdf/2504.11555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11554v1","updated":"2025-04-15T18:52:33Z","published":"2025-04-15T18:52:33Z","title":"Normalizing Flow Regression for Bayesian Inference with Offline\n  Likelihood Evaluations","summary":"  Bayesian inference with computationally expensive likelihood evaluations\nremains a significant challenge in many scientific domains. We propose\nnormalizing flow regression (NFR), a novel offline inference method for\napproximating posterior distributions. Unlike traditional surrogate approaches\nthat require additional sampling or inference steps, NFR directly yields a\ntractable posterior approximation through regression on existing log-density\nevaluations. We introduce training techniques specifically for flow regression,\nsuch as tailored priors and likelihood functions, to achieve robust posterior\nand model evidence estimation. We demonstrate NFR's effectiveness on synthetic\nbenchmarks and real-world applications from neuroscience and biology, showing\nsuperior or comparable performance to existing methods. NFR represents a\npromising approach for Bayesian inference when standard methods are\ncomputationally prohibitive or existing model evaluations can be recycled.\n","authors":["Chengkun Li","Bobby Huggins","Petrus Mikkola","Luigi Acerbi"],"pdf_url":"https://arxiv.org/pdf/2504.11554v1.pdf","comment":"Accepted at the Proceedings track of the 7th Symposium on Advances in\n  Approximate Bayesian Inference (AABI 2025). 40 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.08307v2","updated":"2025-04-15T18:34:06Z","published":"2024-06-12T15:08:15Z","title":"Measuring training variability from stochastic optimization using robust\n  nonparametric testing","summary":"  Deep neural network training often involves stochastic optimization, meaning\neach run will produce a different model. This implies that hyperparameters of\nthe training process, such as the random seed itself, can potentially have\nsignificant influence on the variability in the trained models. Measuring model\nquality by summary statistics, such as test accuracy, can obscure this\ndependence. We propose a robust hypothesis testing framework and a novel\nsummary statistic, the $\\alpha$-trimming level, to measure model similarity.\nApplying hypothesis testing directly with the $\\alpha$-trimming level is\nchallenging because we cannot accurately describe the distribution under the\nnull hypothesis. Our framework addresses this issue by determining how closely\nan approximate distribution resembles the expected distribution of a group of\nindividually trained models and using this approximation as our reference. We\nthen use the $\\alpha$-trimming level to suggest how many training runs should\nbe sampled to ensure that an ensemble is a reliable representative of the true\nmodel performance. We also show how to use the $\\alpha$-trimming level to\nmeasure model variability and demonstrate experimentally that it is more\nexpressive than performance metrics like validation accuracy, churn, or\nexpected calibration error when taken alone. An application of fine-tuning over\nrandom seed in transfer learning illustrates the advantage of our new metric.\n","authors":["Sinjini Banerjee","Tim Marrinan","Reilly Cannon","Tony Chiang","Anand D. Sarwate"],"pdf_url":"https://arxiv.org/pdf/2406.08307v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11516v1","updated":"2025-04-15T15:16:18Z","published":"2025-04-15T15:16:18Z","title":"FEAT: Free energy Estimators with Adaptive Transport","summary":"  We present Free energy Estimators with Adaptive Transport (FEAT), a novel\nframework for free energy estimation -- a critical challenge across scientific\ndomains. FEAT leverages learned transports implemented via stochastic\ninterpolants and provides consistent, minimum-variance estimators based on\nescorted Jarzynski equality and controlled Crooks theorem, alongside\nvariational upper and lower bounds on free energy differences. Unifying\nequilibrium and non-equilibrium methods under a single theoretical framework,\nFEAT establishes a principled foundation for neural free energy calculations.\nExperimental validation on toy examples, molecular simulations, and quantum\nfield theory demonstrates improvements over existing learning-based methods.\n","authors":["Jiajun He","Yuanqi Du","Francisco Vargas","Yuanqing Wang","Carla P. Gomes","José Miguel Hernández-Lobato","Eric Vanden-Eijnden"],"pdf_url":"https://arxiv.org/pdf/2504.11516v1.pdf","comment":"29 pages, 2 tables, 3 figures"},{"id":"http://arxiv.org/abs/2504.12353v1","updated":"2025-04-15T22:03:38Z","published":"2025-04-15T22:03:38Z","title":"TransST: Transfer Learning Embedded Spatial Factor Modeling of Spatial\n  Transcriptomics Data","summary":"  Background: Spatial transcriptomics have emerged as a powerful tool in\nbiomedical research because of its ability to capture both the spatial contexts\nand abundance of the complete RNA transcript profile in organs of interest.\nHowever, limitations of the technology such as the relatively low resolution\nand comparatively insufficient sequencing depth make it difficult to reliably\nextract real biological signals from these data. To alleviate this challenge,\nwe propose a novel transfer learning framework, referred to as TransST, to\nadaptively leverage the cell-labeled information from external sources in\ninferring cell-level heterogeneity of a target spatial transcriptomics data.\n  Results: Applications in several real studies as well as a number of\nsimulation settings show that our approach significantly improves existing\ntechniques. For example, in the breast cancer study, TransST successfully\nidentifies five biologically meaningful cell clusters, including the two\nsubgroups of cancer in situ and invasive cancer; in addition, only TransST is\nable to separate the adipose tissues from the connective issues among all the\nstudied methods.\n  Conclusions: In summary, the proposed method TransST is both effective and\nrobust in identifying cell subclusters and detecting corresponding driving\nbiomarkers in spatial transcriptomics data.\n","authors":["Shuo Shuo Liu","Shikun Wang","Yuxuan Chen","Anil K. Rustgi","Ming Yuan","Jianhua Hu"],"pdf_url":"https://arxiv.org/pdf/2504.12353v1.pdf","comment":null}],"Optimization":[{"id":"http://arxiv.org/abs/2504.11440v1","updated":"2025-04-15T17:54:21Z","published":"2025-04-15T17:54:21Z","title":"Greedy Restart Schedules: A Baseline for Dynamic Algorithm Selection on\n  Numerical Black-box Optimization Problems","summary":"  In many optimization domains, there are multiple different solvers that\ncontribute to the overall state-of-the-art, each performing better on some, and\nworse on other types of problem instances. Meta-algorithmic approaches, such as\ninstance-based algorithm selection, configuration and scheduling, aim to close\nthis gap by extracting the most performance possible from a set of\n(configurable) optimizers. In this context, the best performing individual\nalgorithms are often hand-crafted hybrid heuristics which perform many restarts\nof fast local optimization approaches. However, data-driven techniques to\ncreate optimized restart schedules have not yet been extensively studied.\n  Here, we present a simple scheduling approach that iteratively selects the\nalgorithm performing best on the distribution of unsolved training problems at\ntime of selection, resulting in a problem-independent solver schedule. We\ndemonstrate our approach using well-known optimizers from numerical black-box\noptimization on the BBOB testbed, bridging much of the gap between single and\nvirtual best solver from the original portfolio across various evaluation\nprotocols. Our greedy restart schedule presents a powerful baseline for more\ncomplex dynamic algorithm selection models.\n","authors":["Lennart Schäpermeier"],"pdf_url":"https://arxiv.org/pdf/2504.11440v1.pdf","comment":"Author version. Accepted as full paper to be presented at the GECCO\n  2025 conference, July 14-18, M\\'alaga, Spain. (DOI 10.1145/3712256.3726408)"},{"id":"http://arxiv.org/abs/2504.11410v1","updated":"2025-04-15T17:26:40Z","published":"2025-04-15T17:26:40Z","title":"Randomized block proximal method with locally Lipschitz continuous\n  gradient","summary":"  Block-coordinate algorithms are recognized to furnish efficient iterative\nschemes for addressing large-scale problems, especially when the computation of\nfull derivatives entails substantial memory requirements and computational\nefforts. In this paper, we investigate a randomized block proximal gradient\nalgorithm for minimizing the sum of a differentiable function and a separable\nproper lower-semicontinuous function, both possibly nonconvex. In contrast to\nprevious works, we only assume that the partial gradients of the differentiable\nfunction are locally Lipschitz continuous. At each iteration, the method\nadaptively selects a proximal stepsize to satisfy a sufficient decrease\ncondition without prior knowledge of the local Lipschitz moduli of the partial\ngradients of the differentiable function. In addition, we incorporate the\npossibility of conducting an additional linesearch to enhance the performance\nof the algorithm. Our main result establishes subsequential convergence to a\nstationary point of the problem almost surely. Finally, we provide numerical\nvalidation of the method in an experiment in image compression using a\nnonnegative matrix factorization model.\n","authors":["Pedro Pérez-Aros","David Torregrosa-Belén"],"pdf_url":"https://arxiv.org/pdf/2504.11410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05627v2","updated":"2025-04-15T17:10:26Z","published":"2024-11-08T15:17:40Z","title":"Large problems are not necessarily hard: A case study on distributed\n  NMPC paying off","summary":"  A key motivation in the development of Distributed Model Predictive Control\n(DMPC) is to accelerate centralized Model Predictive Control (MPC) for\nlarge-scale systems. DMPC has the prospect of scaling well by parallelizing\ncomputations among subsystems. However, communication delays may deteriorate\nthe performance of decentralized optimization, if excessively many iterations\nare required per control step. Moreover, centralized solvers often exhibit\nfaster asymptotic convergence rates and, by parallelizing costly linear algebra\noperations, they can also benefit from modern multicore computing\narchitectures. On this canvas, we study the computational performance of\ncooperative DMPC for linear and nonlinear systems. To this end, we apply a\ntailored decentralized real-time iteration scheme to frequency control for\npower systems. DMPC scales well for the considered linear and nonlinear\nbenchmarks, as the iteration number does not depend on the number of\nsubsystems. Comparisons with multi-threaded centralized solvers demonstrate\ncompetitive performance of the proposed decentralized optimization algorithms.\n","authors":["Gösta Stomberg","Maurice Raetsch","Alexander Engelmann","Timm Faulwasser"],"pdf_url":"https://arxiv.org/pdf/2411.05627v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11385v1","updated":"2025-04-15T16:54:25Z","published":"2025-04-15T16:54:25Z","title":"GLL-type Nonmonotone Descent Methods Revisited under\n  Kurdyka-Łojasiewicz Property","summary":"  The purpose of this paper is to extend the full convergence results of the\nclassic GLL-type (Grippo-Lampariello-Lucidi) nonmonotone methods to nonconvex\nand nonsmooth optimization. We propose a novel iterative framework for the\nminimization of a proper and lower semicontinuous function $\\Phi$. The\nframework consists of the GLL-type nonmonotone decrease condition for a\nsequence, a relative error condition for its augmented sequence with respect to\na Kurdyka-{\\L}ojasiewicz (KL) function $\\Theta$, and a relative gap condition\nfor the partial maximum objective value sequence. The last condition is shown\nto be a product of the prox-regularity of $\\Phi$ on the set of cluster points,\nand to hold automatically under a mild condition on the objective value\nsequence. We prove that for any sequence and its bounded augmented sequence\ntogether falling within the framework, the sequence itself is convergent.\nFurthermore, when $\\Theta$ is a KL function of exponent $\\theta\\in(0, 1)$, the\nconvergence admits a linear rate if $\\theta\\in(0, 1/2]$ and a sublinear rate if\n$\\theta\\in(1/2, 1)$. As applications, we prove, for the first time, that the\ntwo existing algorithms, namely the nonmonotone proximal gradient (NPG) method\nwith majorization and NPG with extrapolation both enjoy the full convergence of\nthe iterate sequences for nonconvex and nonsmooth KL composite optimization\nproblems.\n","authors":["Yitian Qian","Ting Tao","Shaohua Pan","Houduo Qi"],"pdf_url":"https://arxiv.org/pdf/2504.11385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11382v1","updated":"2025-04-15T16:53:48Z","published":"2025-04-15T16:53:48Z","title":"The tangent cone to the real determinantal variety: various expressions\n  and four proofs","summary":"  The set of real matrices of upper-bounded rank is a real algebraic variety\ncalled the real generic determinantal variety. An explicit description of the\ntangent cone to that variety is given in Theorem 3.2 of Schneider and Uschmajew\n[SIAM J. Optim., 25 (2015), pp. 622-646]. The present paper shows that the\nproof therein is incomplete and provides four proofs. It also reviews\nequivalent descriptions of the tangent cone to that variety. Moreover, it shows\nthat the tangent cone and the algebraic tangent cone to that variety coincide,\nwhich is not true for all real algebraic varieties.\n","authors":["Guillaume Olikier","Petar Mlinarić","P. -A. Absil","André Uschmajew"],"pdf_url":"https://arxiv.org/pdf/2504.11382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11945v2","updated":"2025-04-15T16:27:37Z","published":"2024-03-18T16:40:07Z","title":"Kernel Modelling of Fading Memory Systems","summary":"  The paper is a follow-up of the recently introduced kernel-based framework to\nidentify nonlinear input-output systems regularized by desirable input-output\nincremental properties. Assuming that the system has fading memory, we propose\nto learn the functional that maps the past input to the present output rather\nthan the operator mapping input trajectories to output trajectories. While\nretaining the benefits of the previously proposed framework, this modification\nsimplifies the selection of the kernel, enforces causality, and enables\ntemporal simulation.\n","authors":["Yongkang Huo","Thomas Chaffey","Rodolphe Sepulchre"],"pdf_url":"https://arxiv.org/pdf/2403.11945v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11320v1","updated":"2025-04-15T16:00:21Z","published":"2025-04-15T16:00:21Z","title":"Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints","summary":"  Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.\n","authors":["Ruicheng Ao","Gan Luo","David Simchi-Levi","Xinshang Wang"],"pdf_url":"https://arxiv.org/pdf/2504.11320v1.pdf","comment":"42 pages, 18 figures"},{"id":"http://arxiv.org/abs/2504.11313v1","updated":"2025-04-15T15:53:10Z","published":"2025-04-15T15:53:10Z","title":"A Composed Alternating Relaxed Projection Algorithm for Feasibility\n  Problem","summary":"  Feasibility problem aims to find a common point of two or more closed\n(convex) sets whose intersection is nonempty. In the literature, projection\nbased algorithms are widely adopted to solve the problem, such as the method of\nalternating projection (MAP), and Douglas--Rachford splitting method (DR). The\nperformance of the methods are governed by the geometric properties of the\nunderlying sets. For example, the fixed-point sequence of the Douglas--Rachford\nsplitting method exhibits a spiraling behavior when solving the feasibility\nproblem of two subspaces, leading to a slow convergence speed and slower than\nMAP. However, when the problem at hand is non-polyhedral, DR can demonstrate\nsignificant faster performance. Motivated by the behaviors of the DR method, in\nthis paper we propose a new algorithm for solving convex feasibility problems.\nThe method is designed based on DR method by further incorporating a\ncomposition of projection and reflection. A non-stationary version of the\nmethod is also designed, aiming to achieve faster practical performance.\nTheoretical guarantees of the proposed schemes are provided and supported by\nnumerical experiments.\n","authors":["Yuting Shen","Jingwei Liang"],"pdf_url":"https://arxiv.org/pdf/2504.11313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06184v3","updated":"2025-04-15T15:41:22Z","published":"2024-09-10T03:26:25Z","title":"A Policy Iteration Method for Inverse Mean Field Games","summary":"  We propose a policy iteration method to solve an inverse problem for a\nmean-field game (MFG) model, specifically to reconstruct the obstacle function\nin the game from the partial observation data of value functions, which\nrepresent the optimal costs for agents. The proposed approach decouples this\ncomplex inverse problem, which is an optimization problem constrained by a\ncoupled nonlinear forward and backward PDE system in the MFG, into several\niterations of solving linear PDEs and linear inverse problems. This method can\nalso be viewed as a fixed-point iteration that simultaneously solves the MFG\nsystem and inversion. We prove its linear rate of convergence. In addition,\nnumerical examples in 1D and 2D, along with performance comparisons to a direct\nleast-squares method, demonstrate the superior efficiency and accuracy of the\nproposed method for solving inverse MFGs.\n","authors":["Kui Ren","Nathan Soedjak","Shanyin Tong"],"pdf_url":"https://arxiv.org/pdf/2409.06184v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11261v1","updated":"2025-04-15T15:00:34Z","published":"2025-04-15T15:00:34Z","title":"Robust MPC for Uncertain Linear Systems -- Combining Model Adaptation\n  and Iterative Learning","summary":"  This paper presents a robust adaptive learning Model Predictive Control (MPC)\nframework for linear systems with parametric uncertainties and additive\ndisturbances performing iterative tasks. The approach iteratively refines the\nparameter estimates using set membership estimation. Performance enhancement\nover iterations is achieved by learning the terminal cost from data. Safety is\nenforced using a terminal set, which is also learned iteratively. The proposed\nmethod guarantees recursive feasibility, constraint satisfaction, and a robust\nbound on the closed-loop cost. Numerical simulations on a mass-spring-damper\nsystem demonstrate improved computational efficiency and control performance\ncompared to an existing robust adaptive MPC approach.\n","authors":["Hannes Petrenz","Johannes Köhler","Francesco Borrelli"],"pdf_url":"https://arxiv.org/pdf/2504.11261v1.pdf","comment":"Github link to the example:\n  https://github.com/HannesPetrenz/RALMPC_Linear_Uncertain_Systems"},{"id":"http://arxiv.org/abs/2504.11254v1","updated":"2025-04-15T14:51:11Z","published":"2025-04-15T14:51:11Z","title":"Model Consistency of Iterative Regularization for Low-Complexity\n  Regularization","summary":"  Regularization is a core component of modern inverse problems as it allows to\nestablish well-posedness to the solution of interests. Popular regularization\napproaches include variational regularization and iterative regularization. The\nformer one can be tackled by solving a variational optimization problem, which\nis the sum of a regularization term and a data-fidelity term balanced by a\nproper weight, while the latter one chooses a proper stopping time to avoid\noverfitting to the noise. In the study of regularization, an important topic is\nthe relation between the solution obtained by regularization and the original\nground truth. When the ground truth has low-complexity structure which is\nencoded as the \"model\", a sensitivity property shows that the solution obtained\nfrom proper regularization that promotes the same structure is robust to small\nperturbations, this is called \"model consistency\". For variational\nregularization, model consistency of linear inverse problem is studied in [1].\nWhile, for iterative regularization, the existence of model consistency is an\nopen problem. In this paper, based on a recent development of partial\nsmoothness which is also considered in [1], we show that if the noise level is\nsufficiently small and a proper stopping time is chosen, the solution by\niterative regularization also achieves model consistency and more exhibit local\nlinear convergence behavior. Numerical simulations are provided to verify our\ntheoretical findings.\n","authors":["Jie Gao","Cesare Molinari","Silvia Villa","Jingwei Liang"],"pdf_url":"https://arxiv.org/pdf/2504.11254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08654v2","updated":"2025-04-15T14:50:39Z","published":"2025-03-11T17:44:31Z","title":"Some commutation principles for optimization problems over\n  transformation groups and semi-FTvN systems","summary":"  We introduce the concepts of commutativity relative to a transformation group\nand strong commutativity in the setting of a semi-FTvN system and show their\nappearance as optimality conditions in certain optimization problems. In the\nsetting of a semi-FTvN system (in particular, in an FTvN system), we show that\nstrong commutativity implies commutativity and observe that in the special case\nof Euclidean Jordan algebra, commutativity and strong commutativity concepts\nreduce, respectively, to those of operator and strong operator commutativity.\nWe demonstrate that every complete hyperbolic polynomial induces a semi-FTvN\nsystem. By way of an application, we describe several commutation principles.\n","authors":["M. Seetharama Gowda","David Sossa"],"pdf_url":"https://arxiv.org/pdf/2503.08654v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.00846v2","updated":"2025-04-15T14:24:24Z","published":"2025-04-01T14:33:23Z","title":"The effect of latency on optimal order execution policy","summary":"  Market participants regularly send bid and ask quotes to exchange-operated\nlimit order books. This creates an optimization challenge where their potential\nprofit is determined by their quoted price and how often their orders are\nsuccessfully executed. The expected profit from successful execution at a\nfavorable limit price needs to be balanced against two key risks: (1) the\npossibility that orders will remain unfilled, which hinders the trading agenda\nand leads to greater price uncertainty, and (2) the danger that limit orders\nwill be executed as market orders, particularly in the presence of order\nsubmission latency, which in turn results in higher transaction costs. In this\npaper, we consider a stochastic optimal control problem where a risk-averse\ntrader attempts to maximize profit while balancing risk. The market is modeled\nusing Brownian motion to represent the price uncertainty. We analyze the\nrelationship between fill probability, limit price, and order submission\nlatency. We derive closed-form approximations of these quantities that perform\nwell in the practical regime of interest. Then, we utilize a mean-variance\nmethod where our total reward function features a risk-tolerance parameter to\nquantify the combined risk and profit.\n","authors":["Chutian Ma","Giacinto Paolo Saggese","Paul Smith"],"pdf_url":"https://arxiv.org/pdf/2504.00846v2.pdf","comment":"15 figures, 2 tables; additional references added in v2"},{"id":"http://arxiv.org/abs/2401.03734v3","updated":"2025-04-15T14:07:38Z","published":"2024-01-08T08:54:04Z","title":"Risk-averse decision strategies for influence diagrams using rooted\n  junction trees","summary":"  This paper presents how a mixed-integer programming (MIP) formulation for\ninfluence diagrams, based on a gradual rooted junction tree representation of\nthe diagram, can be generalized to incorporate risk considerations such as\nconditional value-at-risk and chance constraints. We present two algorithms on\nhow targeted modifications can be made to the underlying influence diagram or\nto the gradual rooted junction tree representation to enable our\nreformulations. We present computational results comparing our reformulation\nwith another MIP formulation for influence diagrams.\n","authors":["Olli Herrala","Topias Terho","Fabricio Oliveira"],"pdf_url":"https://arxiv.org/pdf/2401.03734v3.pdf","comment":"8+3 pages, 7+8 figures"},{"id":"http://arxiv.org/abs/2503.16092v2","updated":"2025-04-15T12:54:33Z","published":"2025-03-20T12:33:45Z","title":"Well-Posedness and Stability of Infinite-Dimensional Systems Under\n  Monotone Feedback","summary":"  We study the well-posedness and stability of an impedance passive\ninfinite-dimensional linear system under nonlinear feedback of the form\n$u(t)=\\phi(v(t)-y(t))$, where $\\phi$ is a monotone function. Our first main\nresult introduces conditions guaranteeing the existence of classical and\ngeneralised solutions in a situation where the original linear system is\nwell-posed. In the absence of the external input $v$ we establish the existence\nof strong and generalised solutions under strictly weaker conditions. Finally,\nwe introduce conditions guaranteeing that the origin is a globally\nasymptotically stable equilibrium point of the closed-loop system. Motivated by\nthe analysis of partial differential equations with nonlinear boundary\nconditions, we use our results to investigate the well-posedness and stablility\nof abstract boundary control systems, port-Hamiltonian systems, a Timoshenko\nbeam model, and a two-dimensional boundary controlled heat equation.\n","authors":["Anthony Hastir","Lassi Paunonen"],"pdf_url":"https://arxiv.org/pdf/2503.16092v2.pdf","comment":"41 pages, 1 figure. Submitted. In this version, the stability results\n  in Section 4 were generalised to system nodes which are impedance passive but\n  not necessarily well-posed"},{"id":"http://arxiv.org/abs/2504.11141v1","updated":"2025-04-15T12:42:42Z","published":"2025-04-15T12:42:42Z","title":"A definition of the background state of the atmosphere using optimal\n  transport","summary":"  The dynamics of atmospheric disturbances are often described in terms of\ndisplacements of air parcels relative to their locations in a notional\nbackground state. Modified Lagrangian Mean (MLM) states have been proposed by\nM. E. McIntyre using the Lagrangian conserved variables potential vorticity and\npotential temperature to label air parcels, thus avoiding the need to calculate\ntrajectories explicitly. Methven and Berrisford further defined a zonally\nsymmetric MLM state for global atmospheric flow in terms of mass in zonal\nangular momentum ($z$) and potential temperature ($\\theta$) coordinates. We\nprove that for any snapshot of an atmospheric flow in a single hemisphere,\nthere exists a unique energy-minimising MLM state in geophysical coordinates\n(latitude and pressure). Since the state is an energy minimum, it is suitable\nfor quantification of finite amplitude disturbances and examining atmospheric\ninstability. This state is obtained by solving a free surface problem, which we\nframe as the minimisation of an optimal transport cost over a class of source\nmeasures. The solution consists of a source measure, encoding surface pressure,\nand an optimal transport map, connecting the distribution of mass in\ngeophysical coordinates to the known distribution of mass in $(z, \\theta)$. We\nshow that this problem reduces to an optimal transport problem with a known\nsource measure, which has a numerically feasible discretisation. Additionally,\nour results hold for a large class of cost functions, and generalise analogous\nresults on free surface variants of the semi-geostrophic equations.\n","authors":["Charlie Egan","John Methven","David P. Bourne","Mike J. P. Cullen"],"pdf_url":"https://arxiv.org/pdf/2504.11141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11133v1","updated":"2025-04-15T12:34:09Z","published":"2025-04-15T12:34:09Z","title":"Hessian stability and convergence rates for entropic and Sinkhorn\n  potentials via semiconcavity","summary":"  In this paper we determine quantitative stability bounds for the Hessian of\nentropic potentials, i.e., the dual solution to the entropic optimal transport\nproblem. Up to authors' knowledge this is the first work addressing this\nsecond-order quantitative stability estimate in general unbounded settings. Our\nproof strategy relies on semiconcavity properties of entropic potentials and on\nthe representation of entropic transport plans as laws of forward and backward\ndiffusion processes, known as Schr\\\"odinger bridges. Moreover, our approach\nallows to deduce a stochastic proof of quantitative stability entropic\nestimates and integrated gradient estimates as well. Finally, as a direct\nconsequence of these stability bounds, we deduce exponential convergence rates\nfor gradient and Hessian of Sinkhorn iterates along Sinkhorn's algorithm, a\nproblem that was still open in unbounded settings. Our rates have a polynomial\ndependence on the regularization parameter.\n","authors":["Giacomo Greco","Luca Tamanini"],"pdf_url":"https://arxiv.org/pdf/2504.11133v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2407.00359v5","updated":"2025-04-15T12:28:45Z","published":"2024-06-29T08:12:36Z","title":"Multicriteria Optimization and Decision Making: Principles, Algorithms\n  and Case Studies","summary":"  Real-world decision and optimization problems, often involve constraints and\nconflicting criteria. For example, choosing a travel method must balance speed,\ncost, environmental footprint, and convenience. Similarly, designing an\nindustrial process must consider safety, environmental impact, and cost\nefficiency. Ideal solutions where all objectives are optimally met are rare;\ninstead, we seek good compromises and aim to avoid lose-lose scenarios.\nMulticriteria optimization offers computational techniques to compute Pareto\noptimal solutions, aiding decision analysis and decision making. This reader\noffers an introduction to this topic and has been developed on the basis of the\nrevised edition of the reader for the MSc computer science course\n\"Multicriteria Optimization and Decision Analysis\" at the Leiden Institute of\nAdvanced Computer Science, Leiden University, The Netherlands. This course was\ntaught annually by the first author from 2007 to 2023 as a single semester\ncourse with lectures and practicals. Our aim was to make the material\naccessible to MSc students who do not study mathematics as their core\ndiscipline by introducing basic numerical analysis concepts when necessary and\nproviding numerical examples for interesting cases. The introduction is\norganized in a unique didactic manner developed by the authors, starting from\nmore simple concepts such as linear programming and single-point methods, and\nadvancing from these to more difficult concepts such as optimality conditions\nfor nonlinear optimization and set-oriented solution algorithms. Besides, we\nfocus on the mathematical modeling and foundations rather than on specific\nalgorithms, though not excluding the discussion of some representative examples\nof solution algorithms.\n","authors":["Michael Emmerich","André Deutz"],"pdf_url":"https://arxiv.org/pdf/2407.00359v5.pdf","comment":"102 pages, Lecture notes"},{"id":"http://arxiv.org/abs/2504.11125v1","updated":"2025-04-15T12:14:20Z","published":"2025-04-15T12:14:20Z","title":"A mixed-integer framework for analyzing neural network-based controllers\n  for piecewise affine systems with bounded disturbances","summary":"  We present a method for representing the closed-loop dynamics of piecewise\naffine (PWA) systems with bounded additive disturbances and neural\nnetwork-based controllers as mixed-integer (MI) linear constraints. We show\nthat such representations enable the computation of robustly positively\ninvariant (RPI) sets for the specified system class by solving MI linear\nprograms. These RPI sets can subsequently be used to certify stability and\nconstraint satisfaction. Furthermore, the approach allows to handle non-linear\nsystems based on suitable PWA approximations and corresponding error bounds,\nwhich can be interpreted as the bounded disturbances from above.\n","authors":["Dieter Teichrib","Moritz Schulze Darup"],"pdf_url":"https://arxiv.org/pdf/2504.11125v1.pdf","comment":"8 pages, 3 figures, to be published in the proceedings of the 23rd\n  European Control Conference (2025)"},{"id":"http://arxiv.org/abs/2504.11120v1","updated":"2025-04-15T12:08:07Z","published":"2025-04-15T12:08:07Z","title":"Improved approximation ratios for the Quantum Max-Cut problem on\n  general, triangle-free and bipartite graphs","summary":"  We study polynomial-time approximation algorithms for the Quantum Max-Cut\n(QMC) problem. Given an edge-weighted graph $G$ on n vertices, the QMC problem\nis to determine the largest eigenvalue of a particular $2^n \\times 2^n$ matrix\nthat corresponds to $G$. We provide a sharpened analysis of the currently\nbest-known QMC approximation algorithm for general graphs. This algorithm\nachieves an approximation ratio of $0.599$, which our analysis improves to\n$0.603$. Additionally, we propose two new approximation algorithms for the QMC\nproblem on triangle-free and bipartite graphs, that achieve approximation\nratios of $0.61383$ and $0.8162$, respectively. These are the best-known\napproximation ratios for their respective graph classes.\n","authors":["Sander Gribling","Lennart Sinjorgo","Renata Sotirov"],"pdf_url":"https://arxiv.org/pdf/2504.11120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11100v1","updated":"2025-04-15T11:48:55Z","published":"2025-04-15T11:48:55Z","title":"Uncertainty modeling method for wind and solar power output in building\n  integrated energy systems under continuous anomalous weather","summary":"  The increasing occurrence of continuous anomalous weather events has\nintensified the uncertainty in wind and photovoltaic power generation, posing\nsignificant challenges to the operation and optimization of building integrated\nenergy systems. Existing studies often neglect the interdependence between\nsuccessive anomalous weather events and their collective impact on wind and\nsolar power output. Additionally, conventional modeling approaches struggle to\naccurately capture the nonlinear fluctuations induced by these weather\nconditions. To address this gap, this study proposes an uncertainty modeling\nmethod based on stochastic optimization and scenario generation. The Weibull\nand Beta distributions characterize the probabilistic properties of wind speed\nand solar irradiance, respectively, while the Copula function captures the\ndependence between wind speed and precipitation, enabling the construction of a\nwind-solar power uncertainty model that incorporates the joint distribution of\nconsecutive anomalous weather events. A Monte Carlo-based scenario generation\napproach is employed to construct a dataset representing anomalous weather\ncharacteristics, followed by a probabilistic distance-based scenario reduction\ntechnique to enhance modeling efficiency. Furthermore, the unscented\ntransformation method is introduced to mitigate nonlinear propagation errors in\nwind and solar power state estimation. Case studies demonstrate that the\nproposed method effectively characterizes the fluctuation patterns of wind and\nsolar power under continuous anomalous weather conditions while preserving the\nstatistical properties of the original data. These findings provide a reliable\nbasis for improving the operational resilience of building integrated energy\nsystems under extreme weather scenarios.\n","authors":["Deyi Shao","Hongru Li","Jingsheng Li","Xia Yu","Xiaoyu Sun","Bowen Han"],"pdf_url":"https://arxiv.org/pdf/2504.11100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11086v1","updated":"2025-04-15T11:32:46Z","published":"2025-04-15T11:32:46Z","title":"Obtuse almost-equiangular sets","summary":"  For $t \\in [-1, 1)$, a set of points on the $(n-1)$-dimensional unit sphere\nis called $t$-almost equiangular if among any three distinct points there is a\npair with inner product $t$. We propose a semidefinite programming upper bound\nfor the maximum cardinality $\\alpha(n, t)$ of such a set based on an extension\nof the Lov\\'asz theta number to hypergraphs. This bound is at least as good as\npreviously known bounds and for many values of $n$ and $t$ it is better.\n  We also refine existing spectral methods to show that $\\alpha(n, t) \\leq\n2(n+1)$ for all $n$ and $t \\leq 0$, with equality only at $t = -1/n$. This\nallows us to show the uniqueness of the optimal construction at $t = -1/n$ for\n$n \\leq 5$ and to enumerate all possible constructions for $n \\leq 3$ and $t\n\\leq 0$.\n","authors":["Christine Bachoc","Bram Bekker","Philippe Moustrou","Fernando Mário de Oliveira Filho"],"pdf_url":"https://arxiv.org/pdf/2504.11086v1.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2402.07593v2","updated":"2025-04-15T11:07:47Z","published":"2024-02-12T11:48:00Z","title":"Source reconstruction algorithms for coupled parabolic systems from\n  internal measurements of one scalar state","summary":"  This paper is devoted to the study of source reconstruction algorithms for\ncoupled systems of heat equations with constant or spatial-dependent coupling\nterms and whose internal measurements involve a reduced number of observed\nstates. The analysis is developed for two kinds of systems: the first one\nconsists of parabolic equations with constant zero-order coupling terms\n(coupling given by a matrix potential term, or, coupling through the diffusion\nmatrix). The second type considers parabolic equations coupled by a matrix\npotential depending on spatial variables, which drives to analyze a\nnon-self-adjoint operator. In all configurations, the source is decomposed into\nseparate variables, where the temporal part is known and scalar, whereas the\nspatial dependence is an unknown vector field. Numerical algorithms through the\nfinite element method in 1D and 2D are performed. Several examples show that\nthe algorithms make it possible to recover space-dependent sources.\n","authors":["Cristhian Montoya","Ignacio Brevis","David Bolivar"],"pdf_url":"https://arxiv.org/pdf/2402.07593v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11043v1","updated":"2025-04-15T10:09:28Z","published":"2025-04-15T10:09:28Z","title":"Riemannian optimization for model order reduction of linear systems with\n  quadratic outputs","summary":"  This paper investigates the optimal $H_2$ model order reduction for linear\nsystems with quadratic outputs. In the framework of Galerkin projection, we\nfirst formulate the optimal $H_2$ MOR as an unconstrained Riemannian\noptimization problem on the Stiefel manifold. The Riemannian gradient of the\nspecific cost function is derived with the aid of Gramians of systems, and the\nDai-Yuan-type Riemannian conjugate gradient method is adopted to generate\nstructure-preserving reduced models. We also consider the optimal $H_2$ MOR\nbased on the product manifold, where some coefficient matrices of reduced\nmodels are determined directly via the iteration of optimization problem,\ninstead of the Galerkin projection method. In addition, we provide a scheme to\ncompute low-rank approximate solutions of Sylvester equations based on the\ntruncated polynomial expansions, which fully exploits the specific structure of\nSylvester equations in the optimization problems, and enables an efficient\nexecution of our approach. Finally, two numerical examples are simulated to\ndemonstrate the efficiency of our methods.\n","authors":["Xiaolong Wang","Tongtu Tian"],"pdf_url":"https://arxiv.org/pdf/2504.11043v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2405.16058v4","updated":"2025-04-15T10:03:07Z","published":"2024-05-25T04:56:54Z","title":"A Novel Privacy Enhancement Scheme with Dynamic Quantization for\n  Federated Learning","summary":"  Federated learning (FL) has been widely regarded as a promising paradigm for\nprivacy preservation of raw data in machine learning. Although, the data\nprivacy in FL is locally protected to some extent, it is still a desideratum to\nenhance privacy and alleviate communication overhead caused by repetitively\ntransmitting model parameters. Typically, these challenges are addressed\nseparately, or jointly via a unified scheme that consists of noise-injected\nprivacy mechanism and communication compression, which may lead to model\ncorruption due to the introduced composite noise. In this work, we propose a\nnovel model-splitting privacy-preserving FL (MSP-FL) scheme to achieve private\nFL with precise accuracy guarantee. Based upon MSP-FL, we further propose a\nmodel-splitting privacy-preserving FL with dynamic quantization (MSPDQ-FL) to\nmitigate the communication overhead, which incorporates a shrinking\nquantization interval to reduce the quantization error. We provide privacy and\nconvergence analysis for both MSP-FL and MSPDQ-FL under non-i.i.d. dataset,\npartial clients participation and finite quantization level. Numerical results\nare presented to validate the superiority of the proposed schemes.\n","authors":["Yifan Wang","Xianghui Cao","Shi Jin","Mo-Yuen Chow"],"pdf_url":"https://arxiv.org/pdf/2405.16058v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.05690v5","updated":"2025-04-15T09:53:43Z","published":"2021-09-13T03:52:57Z","title":"Inexact Bregman Proximal Gradient Method and its Inertial Variant with\n  Absolute and Partial Relative Stopping Criteria","summary":"  The Bregman proximal gradient method (BPGM), which uses the Bregman distance\nas a proximity measure in the iterative scheme, has recently been re-developed\nfor minimizing convex composite problems without the global Lipschitz gradient\ncontinuity assumption. This makes the BPGM appealing for a wide range of\napplications, and hence it has received growing attention in recent years.\nHowever, most existing convergence results are only obtained under the\nassumption that the involved subproblems are solved exactly, which is\nunrealistic in many applications and limits the applicability of the BPGM. To\nmake the BPGM implementable and practical, in this paper, we develop inexact\nversions of the BPGM (denoted by iBPGM) by employing either an absolute-type\nstopping criterion or a partial relative-type stopping criterion for solving\nthe subproblems. The $\\mathcal{O}(1/k)$ convergence rate and the convergence of\nthe sequence are also established for our iBPGM under some conditions.\nMoreover, we develop an inertial variant of our iBPGM (denoted by v-iBPGM) and\nestablish the $\\mathcal{O}(1/k^{\\gamma})$ convergence rate, where $\\gamma\\geq1$\nis a restricted relative smoothness exponent depending on the smooth function\nin the objective and the kernel function. Specially, when the smooth function\nin the objective has a Lipschitz continuous gradient and the kernel function is\nstrongly convex, we have $\\gamma=2$ and thus the v-iBPGM improves the\nconvergence rate of the iBPGM from $\\mathcal{O}(1/k)$ to $\\mathcal{O}(1/k^2)$,\nin accordance with the existing results on the exact accelerated BPGM. Finally,\nsome preliminary numerical experiments for solving the discrete quadratic\nregularized optimal transport problem are conducted to illustrate the\nconvergence behaviors of our iBPGM and v-iBPGM under different inexactness\nsettings.\n","authors":["Lei Yang","Kim-Chuan Toh"],"pdf_url":"https://arxiv.org/pdf/2109.05690v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11528v2","updated":"2025-04-15T09:50:45Z","published":"2024-11-18T12:37:43Z","title":"Optimal Control of 1D Semilinear Heat Equations with Moment-SOS\n  Relaxations","summary":"  We use moment-SOS (Sum Of Squares) relaxations to address the optimal control\nproblem of the 1D heat equation perturbed with a nonlinear term. We extend the\ncurrent framework of moment-based optimal control of PDEs to consider a\nquadratic cost on the control. We develop a new method to extract a nonlinear\ncontroller from approximate moments of the solution. The control law acts on\nthe boundary of the domain and depends on the solution over the whole domain.\nOur method is validated numerically and compared to a linear-quadratic\ncontroller.\n","authors":["Charlie Lebarbé","Emilien Flayac","Michel Fournié","Didier Henrion","Milan Korda"],"pdf_url":"https://arxiv.org/pdf/2411.11528v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11023v1","updated":"2025-04-15T09:48:27Z","published":"2025-04-15T09:48:27Z","title":"An Inexact Variable Metric Proximal Gradient-subgradient Algorithm for a\n  Class of Fractional Optimization Problems","summary":"  In this paper, we study a class of fractional optimization problems, in which\nthe numerator of the objective is the sum of a convex function and a\ndifferentiable function with a Lipschitz continuous gradient, while the\ndenominator is a nonsmooth convex function. This model has broad applicability\nand encompasses several important optimization problems in the literature. To\naddress these problems, we propose an inexact variable metric proximal\ngradient-subgradient algorithm (iVPGSA), which, to our knowledge, is the first\ninexact proximal algorithm specifically designed for solving such type of\nfractional problems. By incorporating a variable metric proximal term and\nallowing for inexact solutions to the subproblem under a flexible error\ncriterion, the proposed algorithm is highly adaptable to a broader range of\nproblems while achieving favorable computational efficiency. Under mild\nassumptions, we establish that any accumulation point of the sequence generated\nby the iVPGSA is a critical point of the target problem. Moreover, we develop\nan improved Kurdyka-{\\L}ojasiewicz (KL)-based analysis framework to prove the\nglobal convergence of the entire sequence and characterize its convergence\nrate, \\textit{without} requiring a strict sufficient descent property. Our\nresults offer detailed insights into how the KL exponent and inexactness\ninfluence the convergence rate. The proposed analysis framework also has the\npotential to serve as a theoretical tool for studying the convergence rates of\na wide range of inexact algorithms beyond the iVPGSA. Finally, some numerical\nexperiments on the $\\ell_1/\\ell_2$ Lasso problem and the constrained\n$\\ell_1/\\ell_2$ sparse optimization problem are conducted to show the superior\nperformance of the iVPGSA in comparison to existing algorithms.\n","authors":["Lei Yang","Xiangrui Kong","Min Zhang","Yaohua Hu"],"pdf_url":"https://arxiv.org/pdf/2504.11023v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2406.04646"},{"id":"http://arxiv.org/abs/2504.10999v1","updated":"2025-04-15T09:15:58Z","published":"2025-04-15T09:15:58Z","title":"Splitting the Forward-Backward Algorithm: A Full Characterization","summary":"  We study frugal splitting algorithms with minimal lifting for solving\nmonotone inclusion problems involving sums of maximal monotone and cocoercive\noperators. Building on a foundational result by Ryu, we fully characterize all\nmethods that use only individual resolvent evaluations, direct evaluations of\ncocoercive operators, and minimal memory resources while ensuring convergence\nvia averaged fixed-point iterations. We show that all such methods are captured\nby a unified framework, which includes known schemes and enables new ones with\npromising features. Systematic numerical experiments lead us to propose three\ndesign heuristics to achieve excellent performances in practice, yielding\nsignificant gains over existing methods.\n","authors":["Anton Åkerman","Enis Chenchene","Pontus Giselsson","Emanuele Naldi"],"pdf_url":"https://arxiv.org/pdf/2504.10999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10990v1","updated":"2025-04-15T09:02:36Z","published":"2025-04-15T09:02:36Z","title":"Mathematical Analysis of the PDE Model for the Consensus-based\n  Optimization","summary":"  In this paper, we develop an analytical framework for the partial\ndifferential equation underlying the consensus-based optimization model. The\nmain challenge arises from the nonlinear, nonlocal nature of the consensus\npoint, coupled with a diffusion term that is both singular and degenerate. By\nemploying a regularization procedure in combination with a compactness\nargument, we establish the global existence and uniqueness of weak solutions in\n$L^\\infty(0,T;L^1\\cap L^\\infty(\\mathbb{R}^d))$. Furthermore, we show that the\nweak solutions exhibit improved $H^2$-regularity when the initial data is\nregular.\n","authors":["Jinhuan Wang","Keyu Li","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2504.10990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10955v1","updated":"2025-04-15T08:00:46Z","published":"2025-04-15T08:00:46Z","title":"Designing optimal subsidy schemes and recycling plans for sustainable\n  treatment of construction and demolition waste","summary":"  More than 10 billion tons of construction and demolition waste (CW) are\ngenerated globally each year, exerting a significant impact on the environment.\nIn the CW recycling process, the government and the carrier are the two primary\nstakeholders. The carrier is responsible for transporting CW from production\nsites to backfill sites or processing facilities, with a primary focus on\ntransport efficiency and revenue. Meanwhile, the government aims to minimize\npollution from the recycling system, which is influenced by transport modes,\nshipment distances, and the processing methods used for CW. This paper develops\na bi-objective, bi-level optimization model to address these challenges. The\nupper-level model is a linear programming model that optimizes the government's\nsubsidy scheme, while the lower-level model is a minimum-cost flow model that\noptimizes the carrier's recycling plan. A hybrid heuristic solution method is\nproposed to tackle the problem's complexity. A case study in Chengdu, China,\ndemonstrates the computational efficiency of the model and its small solution\ngap. With an optimized subsidy scheme and recycling plan, pollution can be\nreduced by over 29.29% through a relatively small investment in subsidies.\n","authors":["Lei Yu","Qian Ge","Ke Han","Wen Ji","Yueqi Liu"],"pdf_url":"https://arxiv.org/pdf/2504.10955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10954v1","updated":"2025-04-15T08:00:15Z","published":"2025-04-15T08:00:15Z","title":"Offset-free Nonlinear MPC with Koopman-based Surrogate Models","summary":"  In this paper, we design offset-free nonlinear Model Predictive Control (MPC)\nfor surrogate models based on Extended Dynamic Mode Decomposition (EDMD). The\nmodel used for prediction in MPC is augmented with a disturbance term, that is\nestimated by an observer. If the full information about the equilibrium of the\nreal system is not available, a reference calculator is introduced in the\nalgorithm to compute the MPC state and input references. The control algorithm\nguarantees offset-free tracking of the controlled output under the assumption\nthat the modeling errors are asymptotically constant. The effectiveness of the\nproposed approach is showcased with numerical simulations for two popular\nbenchmark systems: the van-der-Pol oscillator and the four-tanks process.\n","authors":["Irene Schimperna","Lea Bold","Karl Worthmann"],"pdf_url":"https://arxiv.org/pdf/2504.10954v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.09568v2","updated":"2025-04-15T07:51:15Z","published":"2024-10-12T15:30:17Z","title":"Second-Order Min-Max Optimization with Lazy Hessians","summary":"  This paper studies second-order methods for convex-concave minimax\noptimization. Monteiro and Svaiter (2012) proposed a method to solve the\nproblem with an optimal iteration complexity of $\\mathcal{O}(\\epsilon^{-3/2})$\nto find an $\\epsilon$-saddle point. However, it is unclear whether the\ncomputational complexity, $\\mathcal{O}((N+ d^2) d \\epsilon^{-2/3})$, can be\nimproved. In the above, we follow Doikov et al. (2023) and assume the\ncomplexity of obtaining a first-order oracle as $N$ and the complexity of\nobtaining a second-order oracle as $dN$. In this paper, we show that the\ncomputation cost can be reduced by reusing Hessian across iterations. Our\nmethods take the overall computational complexity of $ \\tilde{\\mathcal{O}}(\n(N+d^2)(d+ d^{2/3}\\epsilon^{-2/3}))$, which improves those of previous methods\nby a factor of $d^{1/3}$. Furthermore, we generalize our method to\nstrongly-convex-strongly-concave minimax problems and establish the complexity\nof $\\tilde{\\mathcal{O}}((N+d^2) (d + d^{2/3} \\kappa^{2/3}) )$ when the\ncondition number of the problem is $\\kappa$, enjoying a similar speedup upon\nthe state-of-the-art method. Numerical experiments on both real and synthetic\ndatasets also verify the efficiency of our method.\n","authors":["Lesi Chen","Chengchang Liu","Jingzhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.09568v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2411.08994v2","updated":"2025-04-15T06:36:38Z","published":"2024-11-13T19:33:49Z","title":"A characterization of positive spanning sets with ties to strongly\n  connected digraphs","summary":"  Positive spanning sets (PSSs) are families of vectors that span a given\nlinear space through non-negative linear combinations. Despite certain classes\nof PSSs being well understood, a complete characterization of PSSs remains\nelusive. In this paper, we explore a relatively understudied relationship\nbetween positive spanning sets and strongly edge-connected digraphs, in that\nthe former can be viewed as a generalization of the latter. We leverage this\nconnection to define a decomposition structure for positive spanning sets\ninspired by the ear decomposition from digraph theory.\n","authors":["Denis Cornaz","Sébastien Kerleau","Clément W. Royer"],"pdf_url":"https://arxiv.org/pdf/2411.08994v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17049v2","updated":"2025-04-15T06:07:22Z","published":"2024-05-27T11:03:48Z","title":"Verifying Properties of Binary Neural Networks Using Sparse Polynomial\n  Optimization","summary":"  This paper explores methods for verifying the properties of Binary Neural\nNetworks (BNNs), focusing on robustness against adversarial attacks. Despite\ntheir lower computational and memory needs, BNNs, like their full-precision\ncounterparts, are also sensitive to input perturbations. Established methods\nfor solving this problem are predominantly based on Satisfiability Modulo\nTheories and Mixed-Integer Linear Programming techniques, which are\ncharacterized by NP complexity and often face scalability issues.\n  We introduce an alternative approach using Semidefinite Programming\nrelaxations derived from sparse Polynomial Optimization. Our approach,\ncompatible with continuous input space, not only mitigates numerical issues\nassociated with floating-point calculations but also enhances verification\nscalability through the strategic use of tighter first-order semidefinite\nrelaxations. We demonstrate the effectiveness of our method in verifying\nrobustness against both $\\|.\\|_\\infty$ and $\\|.\\|_2$-based adversarial attacks.\n","authors":["Jianting Yang","Srećko Ðurašinović","Jean-Bernard Lasserre","Victor Magron","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2405.17049v2.pdf","comment":"25 pages, 2 figures, 8 tables"},{"id":"http://arxiv.org/abs/2504.10855v1","updated":"2025-04-15T04:34:24Z","published":"2025-04-15T04:34:24Z","title":"Virtual Contraction Approach to Decentralized Adaptive Stabilization of\n  Nonlinear Time-Delayed Networks","summary":"  In this paper, we utilize a diagonally dominant structure for the\ndecentralized stabilization of unknown nonlinear time-delayed networks.\nGeneralizing the idea of virtual contraction analysis to time-delayed systems,\nwe demonstrate that nonlinear time-delayed networks can be stabilized by\ndiagonal high-gains if the input matrices possess certain generalized\n(column/row) diagonally dominant properties. To achieve stabilization of\nunknown networks, we further propose a distributed adaptive tuning rule for\neach individual gain function, ensuring that all closed-loop trajectories\nconverge to the origin. The effectiveness of the proposed decentralized\nadaptive control is verified in a case study on epidemic spreading control in\nSIS networks with transmission delays.\n","authors":["Yu Kawano","Zhiyong Sun"],"pdf_url":"https://arxiv.org/pdf/2504.10855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08178v3","updated":"2025-04-15T03:35:43Z","published":"2025-04-11T00:20:37Z","title":"A Piecewise Lyapunov Analysis of Sub-quadratic SGD: Applications to\n  Robust and Quantile Regression","summary":"  Motivated by robust and quantile regression problems, we investigate the\nstochastic gradient descent (SGD) algorithm for minimizing an objective\nfunction $f$ that is locally strongly convex with a sub--quadratic tail. This\nsetting covers many widely used online statistical methods. We introduce a\nnovel piecewise Lyapunov function that enables us to handle functions $f$ with\nonly first-order differentiability, which includes a wide range of popular loss\nfunctions such as Huber loss. Leveraging our proposed Lyapunov function, we\nderive finite-time moment bounds under general diminishing stepsizes, as well\nas constant stepsizes. We further establish the weak convergence, central limit\ntheorem and bias characterization under constant stepsize, providing the first\ngeometrical convergence result for sub--quadratic SGD. Our results have wide\napplications, especially in online statistical methods. In particular, we\ndiscuss two applications of our results. 1) Online robust regression: We\nconsider a corrupted linear model with sub--exponential covariates and\nheavy--tailed noise. Our analysis provides convergence rates comparable to\nthose for corrupted models with Gaussian covariates and noise. 2) Online\nquantile regression: Importantly, our results relax the common assumption in\nprior work that the conditional density is continuous and provide a more\nfine-grained analysis for the moment bounds.\n","authors":["Yixuan Zhang","Dongyan Huo","Yudong Chen","Qiaomin Xie"],"pdf_url":"https://arxiv.org/pdf/2504.08178v3.pdf","comment":"ACM SIGMETRICS 2025. 40 pages, 12 figures"},{"id":"http://arxiv.org/abs/2501.16371v2","updated":"2025-04-15T03:30:52Z","published":"2025-01-22T21:19:42Z","title":"Which Optimizer Works Best for Physics-Informed Neural Networks and\n  Kolmogorov-Arnold Networks?","summary":"  Physics-Informed Neural Networks (PINNs) have revolutionized the computation\nof PDE solutions by integrating partial differential equations (PDEs) into the\nneural network's training process as soft constraints, becoming an important\ncomponent of the scientific machine learning (SciML) ecosystem. More recently,\nphysics-informed Kolmogorv-Arnold networks (PIKANs) have also shown to be\neffective and comparable in accuracy with PINNs. In their current\nimplementation, both PINNs and PIKANs are mainly optimized using first-order\nmethods like Adam, as well as quasi-Newton methods such as BFGS and its\nlow-memory variant, L-BFGS. However, these optimizers often struggle with\nhighly non-linear and non-convex loss landscapes, leading to challenges such as\nslow convergence, local minima entrapment, and (non)degenerate saddle points.\nIn this study, we investigate the performance of Self-Scaled BFGS (SSBFGS),\nSelf-Scaled Broyden (SSBroyden) methods and other advanced quasi-Newton\nschemes, including BFGS and L-BFGS with different line search strategies\napproaches. These methods dynamically rescale updates based on historical\ngradient information, thus enhancing training efficiency and accuracy. We\nsystematically compare these optimizers -- using both PINNs and PIKANs -- on\nkey challenging linear, stiff, multi-scale and non-linear PDEs, including the\nBurgers, Allen-Cahn, Kuramoto-Sivashinsky, and Ginzburg-Landau equations. Our\nfindings provide state-of-the-art results with orders-of-magnitude accuracy\nimprovements without the use of adaptive weights or any other enhancements\ntypically employed in PINNs. More broadly, our results reveal insights into the\neffectiveness of second-order optimization strategies in significantly\nimproving the convergence and accurate generalization of PINNs and PIKANs.\n","authors":["Elham Kiyani","Khemraj Shukla","Jorge F. Urbán","Jérôme Darbon","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2501.16371v2.pdf","comment":"36 pages, 27 figures"},{"id":"http://arxiv.org/abs/2504.10389v2","updated":"2025-04-15T02:49:31Z","published":"2025-04-14T16:35:20Z","title":"Diversity-Fair Online Selection","summary":"  Online selection problems frequently arise in applications such as\ncrowdsourcing and employee recruitment. Existing research typically focuses on\ncandidates with a single attribute. However, crowdsourcing tasks often require\ncontributions from individuals across various demographics. Further motivated\nby the dynamic nature of crowdsourcing and hiring, we study the diversity-fair\nonline selection problem, in which a recruiter must make real-time decisions to\nfoster workforce diversity across many dimensions. We propose two scenarios for\nthis problem. The fixed-capacity scenario, suited for short-term hiring for\ncrowdsourced workers, provides the recruiter with a fixed capacity to fill\ntemporary job vacancies. In contrast, in the unknown-capacity scenario,\nrecruiters optimize diversity across recruitment seasons with increasing\ncapacities, reflecting that the firm honors diversity consideration in a\nlong-term employee acquisition strategy. By modeling the diversity over $d$\ndimensions as a max-min fairness objective, we show that no policy can surpass\na competitive ratio of $O(1/d^{1/3})$ for either scenario, indicating that any\nachievable result inevitably decays by some polynomial factor in $d$. To this\nend, we develop bilevel hierarchical randomized policies that ensure compliance\nwith the capacity constraint. For the fixed-capacity scenario, leveraging\nmarginal information about the arriving population allows us to achieve a\ncompetitive ratio of $1/(4\\sqrt{d} \\lceil \\log_2 d \\rceil)$. For the\nunknown-capacity scenario, we establish a competitive ratio of\n$\\Omega(1/d^{3/4})$ under mild boundedness conditions. In both bilevel\nhierarchical policies, the higher level determines ex-ante selection\nprobabilities and then informs the lower level's randomized selection that\nensures no loss in efficiency. Both policies prioritize core diversity and then\nadjust for underrepresented dimensions.\n","authors":["Ming Hu","Yanzhi Li","Tongwen Wu"],"pdf_url":"https://arxiv.org/pdf/2504.10389v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10814v1","updated":"2025-04-15T02:28:55Z","published":"2025-04-15T02:28:55Z","title":"An Operator Splitting Method for Large-Scale CVaR-Constrained Quadratic\n  Programs","summary":"  We introduce a fast and scalable method for solving quadratic programs with\nconditional value-at-risk (CVaR) constraints. While these problems can be\nformulated as standard quadratic programs, the number of variables and\nconstraints grows linearly with the number of scenarios, making general-purpose\nsolvers impractical for large-scale problems. Our method combines operator\nsplitting with a specialized $O(m\\log m)$ algorithm for projecting onto CVaR\nconstraints, where $m$ is the number of scenarios. The method alternates\nbetween solving a linear system and performing parallel projections: onto CVaR\nconstraints using our specialized algorithm and onto box constraints with a\nclosed-form solution. Numerical examples from several application domains\ndemonstrate that our method outperforms general-purpose solvers by several\norders of magnitude on problems with up to millions of scenarios. Our method is\nimplemented in an open-source package called CVQP.\n","authors":["Eric Luxenberg","David Pérez-Piñeiro","Steven Diamond","Stephen Boyd"],"pdf_url":"https://arxiv.org/pdf/2504.10814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10796v1","updated":"2025-04-15T01:47:11Z","published":"2025-04-15T01:47:11Z","title":"Wasserstein Distributionally Regret Optimization","summary":"  Distributionally Robust Optimization (DRO) is a popular framework for\ndecision-making under uncertainty, but its adversarial nature can lead to\noverly conservative solutions. To address this, we study ex-ante\nDistributionally Robust Regret Optimization (DRRO), focusing on\nWasserstein-based ambiguity sets which are popular due to their links to\nregularization and machine learning. We provide a systematic analysis of\nWasserstein DRRO, paralleling known results for Wasserstein DRO. Under\nsmoothness and regularity conditions, we show that Wasserstein DRRO coincides\nwith Empirical Risk Minimization (ERM) up to first-order terms, and exactly so\nin convex quadratic settings. We revisit the Wasserstein DRRO newsvendor\nproblem, where the loss is the maximum of two linear functions of demand and\ndecision. Extending [25], we show that the regret can be computed by maximizing\ntwo one-dimensional concave functions. For more general loss functions\ninvolving the maximum of multiple linear terms in multivariate random variables\nand decision vectors, we prove that computing the regret and thus also the DRRO\npolicy is NP-hard. We then propose a convex relaxation for these more general\nWasserstein DRRO problems and demonstrate its strong empirical performance.\nFinally, we provide an upper bound on the optimality gap of our relaxation and\nshow it improves over recent alternatives.\n","authors":["Lukas-Benedikt Fiechtner","Jose Blanchet"],"pdf_url":"https://arxiv.org/pdf/2504.10796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10788v1","updated":"2025-04-15T01:16:00Z","published":"2025-04-15T01:16:00Z","title":"A novel heuristic algorithm: adaptive and various learning-based\n  algorithm","summary":"  A novel population-based heuristic algorithm called the adaptive and various\nlearning-based algorithm (AVLA) is proposed for solving general optimization\nproblems in this paper. The main idea of AVLA is inspired by the learning\nbehaviors of individuals in a group, e.g. a school class. The algorithm\nformulates the following learning behaviors: a. Elite members will learn from\neach other; b. A common member will learn from some elite member and other\ncommon members; c. Members with unsatisfied performance will reflect their\nbehavior after performance estimation; d. The whole group will reflect their\nbehavior and try to improve if the performance of the group as a whole has not\nbeen improved for a long time. AVLA adopts the success-history based parameter\nadaptation to lighten the burden of parameter adjustment. To verify the\nefficiency of the AVLA, we apply it and its no-adaptation version with other\neight well-known heuristics to 100 benchmark problems. The comparison clearly\nshows that AVLA performs as well as SHADE and the non-adaption version of AVLA\noutperforms others except AVLA and SHADE.\n","authors":["Sheng-Xue He"],"pdf_url":"https://arxiv.org/pdf/2504.10788v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2504.10770v1","updated":"2025-04-15T00:15:09Z","published":"2025-04-15T00:15:09Z","title":"Collaborative Bayesian Optimization via Wasserstein Barycenters","summary":"  Motivated by the growing need for black-box optimization and data privacy, we\nintroduce a collaborative Bayesian optimization (BO) framework that addresses\nboth of these challenges. In this framework agents work collaboratively to\noptimize a function they only have oracle access to. In order to mitigate\nagainst communication and privacy constraints, agents are not allowed to share\ntheir data but can share their Gaussian process (GP) surrogate models. To\nenable collaboration under these constraints, we construct a central model to\napproximate the objective function by leveraging the concept of Wasserstein\nbarycenters of GPs. This central model integrates the shared models without\naccessing the underlying data. A key aspect of our approach is a collaborative\nacquisition function that balances exploration and exploitation, allowing for\nthe optimization of decision variables collaboratively in each iteration. We\nprove that our proposed algorithm is asymptotically consistent and that its\nimplementation via Monte Carlo methods is numerically accurate. Through\nnumerical experiments, we demonstrate that our approach outperforms other\nbaseline collaborative frameworks and is competitive with centralized\napproaches that do not consider data privacy.\n","authors":["Donglin Zhan","Haoting Zhang","Rhonda Righter","Zeyu Zheng","James Anderson"],"pdf_url":"https://arxiv.org/pdf/2504.10770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11665v1","updated":"2025-04-15T23:31:39Z","published":"2025-04-15T23:31:39Z","title":"Optimal SVI-Weighted PSPS Decisions with Decision-Dependent Outage\n  Uncertainty","summary":"  Public Safety Power Shutoffs (PSPS) are a pre-emptive strategy to mitigate\nthe wildfires caused by power system malfunction. System operators implement\nPSPS to balance wildfire mitigation efforts through de-energization of\ntransmission lines against the risk of widespread blackouts modeled with load\nshedding.\n  Existing approaches do not incorporate decision-dependent wildfire-driven\nfailure probabilities, as modeling outage scenario probabilities requires\nincorporating high-order polynomial terms in the objective. This paper uses\ndistribution shaping to develop an efficient MILP problem representation of the\ndistributionally robust PSPS problem. Building upon the author's prior work,\nthe wildfire risk of operating a transmission line is a function of the\nprobability of a wildfire-driven outage and its subsequent expected impact in\nacres burned.\n  A day-ahead unit commitment and line de-energization PSPS framework is used\nto assess the trade-off between total cost and wildfire risk at different\nlevels of distributional robustness, parameterized by a level of distributional\ndissimilarity $\\kappa$. We perform simulations on the IEEE RTS 24-bus test\nsystem.\n","authors":["Ryan Greenough","Kohei Murakami","Jan Kleissl","Adil Khurram"],"pdf_url":"https://arxiv.org/pdf/2504.11665v1.pdf","comment":"10 pages, 7 figures, and 3 tables"},{"id":"http://arxiv.org/abs/2504.11663v1","updated":"2025-04-15T23:19:36Z","published":"2025-04-15T23:19:36Z","title":"Reachability Analysis of Nonlinear Discrete-Time Systems Using\n  Polyhedral Relaxations and Constrained Zonotopes","summary":"  This paper presents a novel algorithm for reachability analysis of nonlinear\ndiscrete-time systems. The proposed method combines constrained zonotopes (CZs)\nwith polyhedral relaxations of factorable representations of nonlinear\nfunctions to propagate CZs through nonlinear functions, which is normally done\nusing conservative linearization techniques. The new propagation method\nprovides better approximations than those resulting from linearization\nprocedures, leading to significant improvements in the computation of reachable\nsets in comparison to other CZ methods from the literature. Numerical examples\nhighlight the advantages of the proposed algorithm.\n","authors":["Brenner S. Rego","Guilherme V. Raffo","Marco H. Terra","Joseph K. Scott"],"pdf_url":"https://arxiv.org/pdf/2504.11663v1.pdf","comment":"6 pages, 2 figures. This is a preprint of the paper presented at the\n  63rd IEEE Conference on Decision and Control (CDC 2024). arXiv admin note:\n  text overlap with arXiv:2504.00130"},{"id":"http://arxiv.org/abs/2501.06931v2","updated":"2025-04-15T22:22:10Z","published":"2025-01-12T20:59:40Z","title":"Discrete lossless convexification for pointing constraints","summary":"  Discrete Lossless Convexification (DLCvx) formulates a convex relaxation for\na specific class of discrete-time non-convex optimal control problems. It\nestablishes sufficient conditions under which the solution of the relaxed\nproblem satisfies the original non-convex constraints at specified time grid\npoints. Furthermore, it provides an upper bound on the number of time grid\npoints where these sufficient conditions may not hold, and thus the original\nconstraints could be violated. This paper extends DLCvx to problems with\ncontrol pointing constraints. Additionally, it introduces a novel DLCvx\nformulation for mixed-integer optimal control problems in which the control is\neither inactive or constrained within an annular sector. This formulation\nbroadens the feasible space for problems with pointing constraints. A numerical\nexample is provided to illustrate its application.\n","authors":["Dayou Luo","Fabio Spada","Behçet Açıkmeşe"],"pdf_url":"https://arxiv.org/pdf/2501.06931v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.18368v2","updated":"2025-04-15T22:21:15Z","published":"2024-03-27T08:58:41Z","title":"The Mercer-Young Theorem for Matrix-Valued Kernels on Separable Metric\n  Spaces","summary":"  We generalize the characterization theorem going back to Mercer and Young,\nwhich states that a symmetric and continuous kernel is positive definite if and\nonly if it is integrally positive definite, to matrix-valued kernels on\nseparable metric spaces. We also demonstrate the applications of the\ngeneralized theorem to the field of convex optimization and other areas.\n","authors":["Eyal Neuman","Sturmius Tuschmann"],"pdf_url":"https://arxiv.org/pdf/2403.18368v2.pdf","comment":"17 pages, 2 figures"},{"id":"http://arxiv.org/abs/2504.11647v1","updated":"2025-04-15T22:20:54Z","published":"2025-04-15T22:20:54Z","title":"The Pontryagin Maximum Principle for Training Convolutional Neural\n  Networks","summary":"  A novel batch sequential quadratic Hamiltonian (bSQH) algorithm for training\nconvolutional neural networks (CNNs) with $L^0$-based regularization is\npresented. This methodology is based on a discrete-time Pontryagin maximum\nprinciple (PMP). It uses forward and backward sweeps together with the\nlayerwise approximate maximization of an augmented Hamiltonian function, where\nthe augmentation parameter is chosen adaptively.\n  A technique for determining this augmentation parameter is proposed, and the\nloss-reduction and convergence properties of the bSQH algorithm are analysed\ntheoretically and validated numerically. Results of numerical experiments in\nthe context of image classification with a sparsity enforcing $L^0$-based\nregularizer demonstrate the effectiveness of the proposed method in full-batch\nand mini-batch modes.\n","authors":["Sebastian Hofmann","Alfio Borzì"],"pdf_url":"https://arxiv.org/pdf/2504.11647v1.pdf","comment":"Currently under revision in 'SIAM Journal on Mathematics of Data\n  Science'"},{"id":"http://arxiv.org/abs/2504.11645v1","updated":"2025-04-15T22:13:55Z","published":"2025-04-15T22:13:55Z","title":"Achieving Tighter Finite-Time Rates for Heterogeneous Federated\n  Stochastic Approximation under Markovian Sampling","summary":"  Motivated by collaborative reinforcement learning (RL) and optimization with\ntime-correlated data, we study a generic federated stochastic approximation\nproblem involving $M$ agents, where each agent is characterized by an\nagent-specific (potentially nonlinear) local operator. The goal is for the\nagents to communicate intermittently via a server to find the root of the\naverage of the agents' local operators. The generality of our setting stems\nfrom allowing for (i) Markovian data at each agent and (ii) heterogeneity in\nthe roots of the agents' local operators. The limited recent work that has\naccounted for both these features in a federated setting fails to guarantee\nconvergence to the desired point or to show any benefit of collaboration;\nfurthermore, they rely on projection steps in their algorithms to guarantee\nbounded iterates. Our work overcomes each of these limitations. We develop a\nnovel algorithm titled \\texttt{FedHSA}, and prove that it guarantees\nconvergence to the correct point, while enjoying an $M$-fold linear speedup in\nsample-complexity due to collaboration. To our knowledge, \\emph{this is the\nfirst finite-time result of its kind}, and establishing it (without relying on\na projection step) entails a fairly intricate argument that accounts for the\ninterplay between complex temporal correlations due to Markovian sampling,\nmultiple local steps to save communication, and the drift-effects induced by\nheterogeneous local operators. Our results have implications for a broad class\nof heterogeneous federated RL problems (e.g., policy evaluation and control)\nwith function approximation, where the agents' Markov decision processes can\ndiffer in their probability transition kernels and reward functions.\n","authors":["Feng Zhu","Aritra Mitra","Robert W. Heath"],"pdf_url":"https://arxiv.org/pdf/2504.11645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11592v1","updated":"2025-04-15T20:10:21Z","published":"2025-04-15T20:10:21Z","title":"Provably Safe Control for Constrained Nonlinear Systems with Bounded\n  Input","summary":"  In real-world control applications, actuator constraints and output\nconstraints (specifically in tracking problems) are inherent and critical to\nensuring safe and reliable operation. However, generally, control strategies\noften neglect these physical limitations, leading to potential instability,\ndegraded performance, or even system failure when deployed on real-world\nsystems. This paper addresses the control design problem for a class of\nnonlinear systems under both actuator saturation and output constraints. First,\na smooth asymmetric saturation model (a more generic representative of\npractical scenarios) is proposed to model actuator saturation, which ensures\nthat the control inputs always remain confined within a predefined set to\nensure safety. Based on the proposed model, we develop a nonlinear control\nframework that guarantees output tracking while ensuring that system output\nremains confined to the predefined set. Later, we integrate this design with\nthe constrained output tracking control problem, wherein we show that the\nsystem output tracks its desired trajectory by simultaneously satisfying input\nand output constraints. The global stabilization of the tracking error is\nachieved in the presence of input constraints, while semi-global stabilization\nis achieved in the presence of both input and output constraints. Additionally,\nwe rigorously establish the boundedness of all closed-loop signals under the\nproposed design. Simulation results demonstrate the effectiveness of the\nproposed methods in handling asymmetric constraints while achieving desirable\ntracking performance.\n","authors":["Saurabh Kumar","Shashi Ranjan Kumar","Abhinav Sinha"],"pdf_url":"https://arxiv.org/pdf/2504.11592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11590v1","updated":"2025-04-15T20:09:18Z","published":"2025-04-15T20:09:18Z","title":"Approximating a matrix as the square of a skew-symmetric matrix, with\n  application to estimating angular velocity from acceleration data","summary":"  In this paper we study the problem of finding the best approximation of a\nreal square matrix by a matrix that can be represented as the square of a real,\nskew-symmetric matrix. This problem is important in the design of robust\nnumerical algorithms aimed at estimating rigid body kinematics from multiple\naccelerometer measurements. We give a constructive proof for the existence of a\nbest approximant in the Frobenius norm. We demonstrate the construction with\nsome small examples, and we showcase the practical importance of this work to\nthe problem of determining the angular velocity of a rotating rigid body from\nits acceleration measurements.\n","authors":["Yang Wan","Benjamin E. Grossman-Ponemona","Haneesh Kesari"],"pdf_url":"https://arxiv.org/pdf/2504.11590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.07830v3","updated":"2025-04-15T19:34:36Z","published":"2022-04-16T16:00:14Z","title":"Riemannian optimization using three different metrics for Hermitian PSD\n  fixed-rank constraints: an extended version","summary":"  For smooth optimization problems with a Hermitian positive semi-definite\nfixed-rank constraint, we consider three existing approaches including the\nsimple Burer--Monteiro method, and Riemannian optimization over quotient\ngeometry and the embedded geometry. These three methods can be all represented\nvia quotient geometry with three Riemannian metrics $g^i(\\cdot, \\cdot)$\n$(i=1,2,3)$. By taking the nonlinear conjugate gradient method (CG) as an\nexample, we show that CG in the factor-based Burer--Monteiro approach is\nequivalent to Riemannian CG on the quotient geometry with the Bures-Wasserstein\nmetric $g^1$. Riemannian CG on the quotient geometry with the metric $g^3$ is\nequivalent to Riemannian CG on the embedded geometry. For comparing the three\napproaches, we analyze the condition number of the Riemannian Hessian near the\nminimizer under the three different metrics. Under certain assumptions, the\ncondition number from the Bures-Wasserstein metric $g^1$ is significantly\ndifferent from the other two metrics.\n  Numerical experiments show that the Burer--Monteiro CG method has obviously\nslower asymptotic convergence rate when the minimizer is rank deficient, which\nis consistent with the condition number analysis.\n","authors":["Shixin Zheng","Wen Huang","Bart Vandereycken","Xiangxiong Zhang"],"pdf_url":"https://arxiv.org/pdf/2204.07830v3.pdf","comment":"Revised the proof in Bures-Wasserstein metric, revised some notations\n  and typos, reorder in Rayleigh quotient section for better readability"},{"id":"http://arxiv.org/abs/2504.11570v1","updated":"2025-04-15T19:25:50Z","published":"2025-04-15T19:25:50Z","title":"Traffic Adaptive Moving-window Service Patrolling for Real-time Incident\n  Management during High-impact Events","summary":"  This paper presents the Traffic Adaptive Moving-window Patrolling Algorithm\n(TAMPA), designed to improve real-time incident management during major events\nlike sports tournaments and concerts. Such events significantly stress\ntransportation networks, requiring efficient and adaptive patrol solutions.\nTAMPA integrates predictive traffic modeling and real-time complaint\nestimation, dynamically optimizing patrol deployment. Using dynamic\nprogramming, the algorithm continuously adjusts patrol strategies within short\nplanning windows, effectively balancing immediate response and efficient\nrouting. Leveraging the Dvoretzky-Kiefer-Wolfowitz inequality, TAMPA detects\nsignificant shifts in complaint patterns, triggering proactive adjustments in\npatrol routes. Theoretical analyses ensure performance remains closely aligned\nwith optimal solutions. Simulation results from an urban traffic network\ndemonstrate TAMPA's superior performance, showing improvements of approximately\n87.5\\% over stationary methods and 114.2\\% over random strategies. Future work\nincludes enhancing adaptability and incorporating digital twin technology for\nimproved predictive accuracy, particularly relevant for events like the 2026\nFIFA World Cup at MetLife Stadium.\n","authors":["Haozhe Lei","Ya-Ting Yang","Tao Li","Zilin Bian","Fan Zuo","Sundeep Rangan","Kaan Ozbay"],"pdf_url":"https://arxiv.org/pdf/2504.11570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15164v2","updated":"2025-04-15T18:55:45Z","published":"2024-08-27T16:03:16Z","title":"Approximate controllability for 2D Euler equations","summary":"  Approximate controllability of the Euler equations is investigated by means\nof a finite set of actuators. It is proven that approximate controllability\nholds if we can find a saturating subset of actuators. The notion of saturating\nset is relaxed when compared to previous literature, still being a sufficient\ncondition for approximate controllability. The result holds for general bounded\ntwo-dimensional spatial domains with smooth boundary. An example of a\nsaturating set is given in the case the spatial domain is the unit disk.\n","authors":["Sérgio S. Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2408.15164v2.pdf","comment":"This version v2 includes an example of a saturating set"},{"id":"http://arxiv.org/abs/2504.11556v1","updated":"2025-04-15T18:55:40Z","published":"2025-04-15T18:55:40Z","title":"$C_{loc}^{1,1}$ optimal pairs in the dual optimal transport problem for\n  a Lorentzian cost along displacement interpolations","summary":"  We consider the optimal transportation problem on a globally hyperbolic\nspacetime with a cost function $c$, which corresponds to the optimal\ntransportation problem on a complete Riemannian manifold where the cost\nfunction is given by the squared Riemannian distance. Building upon methods of\nweak KAM theory, we will establish the existence of $C_{loc}^{1,1}$ optimal\npairs for the dual optimal transport problem for probability measures along\ndisplacement interpolations.\n","authors":["Alec Metsch"],"pdf_url":"https://arxiv.org/pdf/2504.11556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11555v1","updated":"2025-04-15T18:53:51Z","published":"2025-04-15T18:53:51Z","title":"Sub-optimality of the Separation Principle for Quadratic Control from\n  Bilinear Observations","summary":"  We consider the problem of controlling a linear dynamical system from\nbilinear observations with minimal quadratic cost. Despite the similarity of\nthis problem to standard linear quadratic Gaussian (LQG) control, we show that\nwhen the observation model is bilinear, neither does the Separation Principle\nhold, nor is the optimal controller affine in the estimated state. Moreover,\nthe cost-to-go is non-convex in the control input. Hence, finding an analytical\nexpression for the optimal feedback controller is difficult in general. Under\ncertain settings, we show that the standard LQG controller locally maximizes\nthe cost instead of minimizing it. Furthermore, the optimal controllers\n(derived analytically) are not unique and are nonlinear in the estimated state.\nWe also introduce a notion of input-dependent observability and derive\nconditions under which the Kalman filter covariance remains bounded. We\nillustrate our theoretical results through numerical experiments in multiple\nsynthetic settings.\n","authors":["Yahya Sattar","Sunmook Choi","Yassir Jedra","Maryam Fazel","Sarah Dean"],"pdf_url":"https://arxiv.org/pdf/2504.11555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06646v2","updated":"2025-04-15T18:15:01Z","published":"2024-01-12T15:52:02Z","title":"Block Majorization Minimization with Extrapolation and Application to\n  $β$-NMF","summary":"  We propose a Block Majorization Minimization method with Extrapolation (BMMe)\nfor solving a class of multi-convex optimization problems. The extrapolation\nparameters of BMMe are updated using a novel adaptive update rule. By showing\nthat block majorization minimization can be reformulated as a block mirror\ndescent method, with the Bregman divergence adaptively updated at each\niteration, we establish subsequential convergence for BMMe. We use this method\nto design efficient algorithms to tackle nonnegative matrix factorization\nproblems with the $\\beta$-divergences ($\\beta$-NMF) for $\\beta\\in [1,2]$. These\nalgorithms, which are multiplicative updates with extrapolation, benefit from\nour novel results that offer convergence guarantees. We also empirically\nillustrate the significant acceleration of BMMe for $\\beta$-NMF through\nextensive experiments.\n","authors":["Le Thi Khanh Hien","Valentin Leplat","Nicolas Gillis"],"pdf_url":"https://arxiv.org/pdf/2401.06646v2.pdf","comment":"Code available from https://github.com/vleplat/BMMe, several\n  clarifications compared to the previous version, Accepted in SIAM J. on\n  Mathematics of Data Science"},{"id":"http://arxiv.org/abs/2504.11499v1","updated":"2025-04-15T01:29:17Z","published":"2025-04-15T01:29:17Z","title":"Adaptive and various learning-based algorithm for supply chain network\n  equilibrium problems","summary":"  This paper proposes a novel nonlinear programming model to capture the\nequilibrium state of complex supply chain networks. The model, equivalent to a\nvariational inequality model, relaxes traditional strict assumptions to\naccommodate real-world complexities, such as nonlinear, non-convex, and\nnon-smooth relationships between production, consumption, and pricing. To\nefficiently solve this challenging problem, we introduce a novel heuristic\nalgorithm, the adaptive and various learning-based algorithm (AVLA), inspired\nby group learning behaviors. AVLA simulates individual learning processes\nwithin different subgroups at various stages, employing a success history-based\nparameter adaptation mechanism to reduce manual tuning. Extensive computational\nexperiments on 29 benchmark problems and 5 supply chain networks demonstrate\nAVLA's superior performance compared to 19 state of the art algorithms. AVLA\nconsistently achieves the best results in terms of both average and best\nobjective function values, making it a powerful tool for addressing complex\nsupply chain network equilibrium problems.\n","authors":["Sheng-Xue He"],"pdf_url":"https://arxiv.org/pdf/2504.11499v1.pdf","comment":"40 pages. arXiv admin note: substantial text overlap with\n  arXiv:2504.10788"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2504.11454v1","updated":"2025-04-15T17:59:43Z","published":"2025-04-15T17:59:43Z","title":"Elucidating the Design Space of Multimodal Protein Language Models","summary":"  Multimodal protein language models (PLMs) integrate sequence and token-based\nstructural information, serving as a powerful foundation for protein modeling,\ngeneration, and design. However, the reliance on tokenizing 3D structures into\ndiscrete tokens causes substantial loss of fidelity about fine-grained\nstructural details and correlations. In this paper, we systematically elucidate\nthe design space of multimodal PLMs to overcome their limitations. We identify\ntokenization loss and inaccurate structure token predictions by the PLMs as\nmajor bottlenecks. To address these, our proposed design space covers improved\ngenerative modeling, structure-aware architectures and representation learning,\nand data exploration. Our advancements approach finer-grained supervision,\ndemonstrating that token-based multimodal PLMs can achieve robust structural\nmodeling. The effective design methods dramatically improve the structure\ngeneration diversity, and notably, folding abilities of our 650M model by\nreducing the RMSD from 5.52 to 2.36 on PDB testset, even outperforming 3B\nbaselines and on par with the specialized folding models.\n","authors":[" Cheng-Yen"," Hsieh","Xinyou Wang","Daiheng Zhang","Dongyu Xue","Fei Ye","Shujian Huang","Zaixiang Zheng","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2504.11454v1.pdf","comment":"Project Page: https://bytedance.github.io/dplm/dplm-2.1/"},{"id":"http://arxiv.org/abs/2504.11453v1","updated":"2025-04-15T17:59:05Z","published":"2025-04-15T17:59:05Z","title":"A Clean Slate for Offline Reinforcement Learning","summary":"  Progress in offline reinforcement learning (RL) has been impeded by ambiguous\nproblem definitions and entangled algorithmic designs, resulting in\ninconsistent implementations, insufficient ablations, and unfair evaluations.\nAlthough offline RL explicitly avoids environment interaction, prior methods\nfrequently employ extensive, undocumented online evaluation for hyperparameter\ntuning, complicating method comparisons. Moreover, existing reference\nimplementations differ significantly in boilerplate code, obscuring their core\nalgorithmic contributions. We address these challenges by first introducing a\nrigorous taxonomy and a transparent evaluation protocol that explicitly\nquantifies online tuning budgets. To resolve opaque algorithmic design, we\nprovide clean, minimalistic, single-file implementations of various model-free\nand model-based offline RL methods, significantly enhancing clarity and\nachieving substantial speed-ups. Leveraging these streamlined implementations,\nwe propose Unifloral, a unified algorithm that encapsulates diverse prior\napproaches within a single, comprehensive hyperparameter space, enabling\nalgorithm development in a shared hyperparameter space. Using Unifloral with\nour rigorous evaluation protocol, we develop two novel algorithms - TD3-AWR\n(model-free) and MoBRAC (model-based) - which substantially outperform\nestablished baselines. Our implementation is publicly available at\nhttps://github.com/EmptyJackson/unifloral.\n","authors":["Matthew Thomas Jackson","Uljad Berdica","Jarek Liesen","Shimon Whiteson","Jakob Nicolaus Foerster"],"pdf_url":"https://arxiv.org/pdf/2504.11453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11443v1","updated":"2025-04-15T17:55:32Z","published":"2025-04-15T17:55:32Z","title":"Early Impacts of M365 Copilot","summary":"  Advances in generative AI have rapidly expanded the potential of computers to\nperform or assist in a wide array of tasks traditionally performed by humans.\nWe analyze a large, real-world randomized experiment of over 6,000 workers at\n56 firms to present some of the earliest evidence on how these technologies are\nchanging the way knowledge workers do their jobs. We find substantial time\nsavings on common core tasks across a wide range of industries and occupations:\nworkers who make use of this technology spent half an hour less reading email\neach week and completed documents 12% faster. Despite the newness of the\ntechnology, nearly 40% of workers who were given access to the tool used it\nregularly in their work throughout the 6-month study.\n","authors":["Eleanor Wiske Dillon","Sonia Jaffe","Sida Peng","Alexia Cambon"],"pdf_url":"https://arxiv.org/pdf/2504.11443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11442v1","updated":"2025-04-15T17:55:20Z","published":"2025-04-15T17:55:20Z","title":"TextArena","summary":"  TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.\n","authors":["Leon Guertler","Bobby Cheng","Simon Yu","Bo Liu","Leshem Choshen","Cheston Tan"],"pdf_url":"https://arxiv.org/pdf/2504.11442v1.pdf","comment":"work in progress; 5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2504.11438v1","updated":"2025-04-15T17:53:18Z","published":"2025-04-15T17:53:18Z","title":"Mamba-Based Ensemble learning for White Blood Cell Classification","summary":"  White blood cell (WBC) classification assists in assessing immune health and\ndiagnosing various diseases, yet manual classification is labor-intensive and\nprone to inconsistencies. Recent advancements in deep learning have shown\npromise over traditional methods; however, challenges such as data imbalance\nand the computational demands of modern technologies, such as Transformer-based\nmodels which do not scale well with input size, limit their practical\napplication. This paper introduces a novel framework that leverages Mamba\nmodels integrated with ensemble learning to improve WBC classification. Mamba\nmodels, known for their linear complexity, provide a scalable alternative to\nTransformer-based approaches, making them suitable for deployment in\nresource-constrained environments. Additionally, we introduce a new WBC\ndataset, Chula-WBC-8, for benchmarking. Our approach not only validates the\neffectiveness of Mamba models in this domain but also demonstrates their\npotential to significantly enhance classification efficiency without\ncompromising accuracy. The source code can be found at\nhttps://github.com/LewisClifton/Mamba-WBC-Classification.\n","authors":["Lewis Clifton","Xin Tian","Duangdao Palasuwan","Phandee Watanaboonyongcharoen","Ponlapat Rojnuckarin","Nantheera Anantrasirichai"],"pdf_url":"https://arxiv.org/pdf/2504.11438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11436v1","updated":"2025-04-15T17:52:00Z","published":"2025-04-15T17:52:00Z","title":"Shifting Work Patterns with Generative AI","summary":"  We present evidence on how generative AI changes the work patterns of\nknowledge workers using data from a 6-month-long, cross-industry, randomized\nfield experiment. Half of the 6,000 workers in the study received access to a\ngenerative AI tool integrated into the applications they already used for\nemails, document creation, and meetings. We find that access to the AI tool\nduring the first year of its release primarily impacted behaviors that could be\nchanged independently and not behaviors that required coordination to change:\nworkers who used the tool spent 3 fewer hours, or 25% less time on email each\nweek (intent to treat estimate is 1.4 hours) and seemed to complete documents\nmoderately faster, but did not significantly change time spent in meetings.\n","authors":["Eleanor Wiske Dillon","Sonia Jaffe","Nicole Immorlica","Christopher T. Stanton"],"pdf_url":"https://arxiv.org/pdf/2504.11436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11433v1","updated":"2025-04-15T17:47:20Z","published":"2025-04-15T17:47:20Z","title":"Predicting Wave Dynamics using Deep Learning with Multistep Integration\n  Inspired Attention and Physics-Based Loss Decomposition","summary":"  In this paper, we present a physics-based deep learning framework for\ndata-driven prediction of wave propagation in fluid media. The proposed\napproach, termed Multistep Integration-Inspired Attention (MI2A), combines a\ndenoising-based convolutional autoencoder for reduced latent representation\nwith an attention-based recurrent neural network with long-short-term memory\ncells for time evolution of reduced coordinates. This proposed architecture\ndraws inspiration from classical linear multistep methods to enhance stability\nand long-horizon accuracy in latent-time integration. Despite the efficiency of\nhybrid neural architectures in modeling wave dynamics, autoregressive\npredictions are often prone to accumulating phase and amplitude errors over\ntime. To mitigate this issue within the MI2A framework, we introduce a novel\nloss decomposition strategy that explicitly separates the training loss\nfunction into distinct phase and amplitude components. We assess the\nperformance of MI2A against two baseline reduced-order models trained with\nstandard mean-squared error loss: a sequence-to-sequence recurrent neural\nnetwork and a variant using Luong-style attention. To demonstrate the\neffectiveness of the MI2A model, we consider three benchmark wave propagation\nproblems of increasing complexity, namely one-dimensional linear convection,\nthe nonlinear viscous Burgers equation, and the two-dimensional Saint-Venant\nshallow water system. Our results demonstrate that the MI2A framework\nsignificantly improves the accuracy and stability of long-term predictions,\naccurately preserving wave amplitude and phase characteristics. Compared to the\nstandard long-short term memory and attention-based models, MI2A-based deep\nlearning exhibits superior generalization and temporal accuracy, making it a\npromising tool for real-time wave modeling.\n","authors":["Indu Kant Deo","Rajeev K. Jaiman"],"pdf_url":"https://arxiv.org/pdf/2504.11433v1.pdf","comment":"30 pages, 14 figures"},{"id":"http://arxiv.org/abs/2504.10478v2","updated":"2025-04-15T17:46:59Z","published":"2025-04-14T17:59:07Z","title":"Weight Ensembling Improves Reasoning in Language Models","summary":"  We investigate a failure mode that arises during the training of reasoning\nmodels, where the diversity of generations begins to collapse, leading to\nsuboptimal test-time scaling. Notably, the Pass@1 rate reliably improves during\nsupervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a\nsimple intervention of interpolating the weights of the latest SFT checkpoint\nwith an early checkpoint, otherwise known as WiSE-FT, almost completely\nrecovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves\nbetter test-time scaling (Best@k, majority vote) and achieves superior results\nwith less data when tuned further by reinforcement learning. Finally, we find\nthat WiSE-FT provides complementary performance gains that cannot be achieved\nonly through diversity-inducing decoding strategies, like temperature scaling.\nWe formalize a bias-variance tradeoff of Pass@k with respect to the expectation\nand variance of Pass@1 over the test distribution. We find that WiSE-FT can\nreduce bias and variance simultaneously, while temperature scaling inherently\ntrades-off between bias and variance.\n","authors":["Xingyu Dang","Christina Baek","Kaiyue Wen","Zico Kolter","Aditi Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2504.10478v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11431v1","updated":"2025-04-15T17:41:54Z","published":"2025-04-15T17:41:54Z","title":"Masculine Defaults via Gendered Discourse in Podcasts and Large Language\n  Models","summary":"  Masculine defaults are widely recognized as a significant type of gender\nbias, but they are often unseen as they are under-researched. Masculine\ndefaults involve three key parts: (i) the cultural context, (ii) the masculine\ncharacteristics or behaviors, and (iii) the reward for, or simply acceptance\nof, those masculine characteristics or behaviors. In this work, we study\ndiscourse-based masculine defaults, and propose a twofold framework for (i) the\nlarge-scale discovery and analysis of gendered discourse words in spoken\ncontent via our Gendered Discourse Correlation Framework (GDCF); and (ii) the\nmeasurement of the gender bias associated with these gendered discourse words\nin LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus\nour study on podcasts, a popular and growing form of social media, analyzing\n15,117 podcast episodes. We analyze correlations between gender and discourse\nwords -- discovered via LDA and BERTopic -- to automatically form gendered\ndiscourse word lists. We then study the prevalence of these gendered discourse\nwords in domain-specific contexts, and find that gendered discourse-based\nmasculine defaults exist in the domains of business, technology/politics, and\nvideo games. Next, we study the representation of these gendered discourse\nwords from a state-of-the-art LLM embedding model from OpenAI, and find that\nthe masculine discourse words have a more stable and robust representation than\nthe feminine discourse words, which may result in better system performance on\ndownstream tasks for men. Hence, men are rewarded for their discourse patterns\nwith better system performance by one of the state-of-the-art language models\n-- and this embedding disparity is a representational harm and a masculine\ndefault.\n","authors":["Maria Teleki","Xiangjue Dong","Haoran Liu","James Caverlee"],"pdf_url":"https://arxiv.org/pdf/2504.11431v1.pdf","comment":"To appear in ICWSM 2025"},{"id":"http://arxiv.org/abs/2504.11426v1","updated":"2025-04-15T17:38:47Z","published":"2025-04-15T17:38:47Z","title":"A Dual-Space Framework for General Knowledge Distillation of Large\n  Language Models","summary":"  Knowledge distillation (KD) is a promising solution to compress large\nlanguage models (LLMs) by transferring their knowledge to smaller models.\nDuring this process, white-box KD methods usually minimize the distance between\nthe output distributions of the teacher model and the student model to transfer\nmore information. However, we reveal that the current white-box KD framework\nexhibits two limitations: a) bridging probability distributions from different\noutput spaces will limit the similarity between the teacher model and the\nstudent model; b) this framework cannot be applied to LLMs with different\nvocabularies. One of the root causes for these limitations is that the\ndistributions from the teacher and the student for KD are output by different\nprediction heads, which yield distributions in different output spaces and\ndimensions. Therefore, in this paper, we propose a dual-space knowledge\ndistillation (DSKD) framework that unifies the prediction heads of the teacher\nand the student models for KD. Specifically, we first introduce two projectors\nwith ideal initialization to project the teacher/student hidden states into the\nstudent/teacher representation spaces. After this, the hidden states from\ndifferent models can share the same head and unify the output spaces of the\ndistributions. Furthermore, we develop an exact token alignment (ETA) algorithm\nto align the same tokens in two differently-tokenized sequences. Based on the\nabove, our DSKD framework is a general KD framework that supports both\noff-policy and on-policy KD, and KD between any two LLMs regardless of their\nvocabularies. Extensive experiments on instruction-following, mathematical\nreasoning, and code generation benchmarks show that DSKD significantly\noutperforms existing methods based on the current white-box KD framework and\nsurpasses other cross-tokenizer KD methods for LLMs with different\nvocabularies.\n","authors":["Xue Zhang","Songming Zhang","Yunlong Liang","Fandong Meng","Yufeng Chen","Jinan Xu","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.11426v1.pdf","comment":"19 pages, 9 figures, 11 tables, under review. Code is available at:\n  https://github.com/songmzhang/DSKDv2. arXiv admin note: text overlap with\n  arXiv:2406.17328"},{"id":"http://arxiv.org/abs/2410.19494v2","updated":"2025-04-15T17:38:16Z","published":"2024-10-25T11:51:37Z","title":"Graph Linearization Methods for Reasoning on Graphs with Large Language\n  Models","summary":"  Large language models have evolved to process multiple modalities beyond\ntext, such as images and audio, which motivates us to explore how to\neffectively leverage them for graph reasoning tasks. The key question,\ntherefore, is how to transform graphs into linear sequences of tokens, a\nprocess we term \"graph linearization\", so that LLMs can handle graphs\nnaturally. We consider that graphs should be linearized meaningfully to reflect\ncertain properties of natural language text, such as local dependency and\nglobal alignment, in order to ease contemporary LLMs, trained on trillions of\ntextual tokens, better understand graphs. To achieve this, we developed several\ngraph linearization methods based on graph centrality and degeneracy. These\nmethods are further enhanced using node relabeling techniques. The experimental\nresults demonstrate the effectiveness of our methods compared to the random\nlinearization baseline. Our work introduces novel graph representations\nsuitable for LLMs, contributing to the potential integration of graph machine\nlearning with the trend of multimodal processing using a unified transformer\nmodel.\n","authors":["Christos Xypolopoulos","Guokan Shang","Xiao Fei","Giannis Nikolentzos","Hadi Abdine","Iakovos Evdaimon","Michail Chatzianastasis","Giorgos Stamou","Michalis Vazirgiannis"],"pdf_url":"https://arxiv.org/pdf/2410.19494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11415v1","updated":"2025-04-15T17:31:46Z","published":"2025-04-15T17:31:46Z","title":"Robustness and sex differences in skin cancer detection: logistic\n  regression vs CNNs","summary":"  Deep learning has been reported to achieve high performances in the detection\nof skin cancer, yet many challenges regarding the reproducibility of results\nand biases remain. This study is a replication (different data, same analysis)\nof a study on Alzheimer's disease [28] which studied robustness of logistic\nregression (LR) and convolutional neural networks (CNN) across patient sexes.\nWe explore sex bias in skin cancer detection, using the PAD-UFES-20 dataset\nwith LR trained on handcrafted features reflecting dermatological guidelines\n(ABCDE and the 7-point checklist), and a pre-trained ResNet-50 model. We\nevaluate these models in alignment with [28]: across multiple training datasets\nwith varied sex composition to determine their robustness. Our results show\nthat both the LR and the CNN were robust to the sex distributions, but the\nresults also revealed that the CNN had a significantly higher accuracy (ACC)\nand area under the receiver operating characteristics (AUROC) for male patients\nthan for female patients. We hope these findings to contribute to the growing\nfield of investigating potential bias in popular medical machine learning\nmethods. The data and relevant scripts to reproduce our results can be found in\nour Github.\n","authors":["Nikolette Pedersen","Regitze Sydendal","Andreas Wulff","Ralf Raumanns","Eike Petersen","Veronika Cheplygina"],"pdf_url":"https://arxiv.org/pdf/2504.11415v1.pdf","comment":"16 pages (excluding appendix), 2 figures (excluding appendix),\n  submitted to MIUA 2025 conference (response pending)"},{"id":"http://arxiv.org/abs/2504.11412v1","updated":"2025-04-15T17:28:15Z","published":"2025-04-15T17:28:15Z","title":"Measures of Variability for Risk-averse Policy Gradient","summary":"  Risk-averse reinforcement learning (RARL) is critical for decision-making\nunder uncertainty, which is especially valuable in high-stake applications.\nHowever, most existing works focus on risk measures, e.g., conditional\nvalue-at-risk (CVaR), while measures of variability remain underexplored. In\nthis paper, we comprehensively study nine common measures of variability,\nnamely Variance, Gini Deviation, Mean Deviation, Mean-Median Deviation,\nStandard Deviation, Inter-Quantile Range, CVaR Deviation, Semi_Variance, and\nSemi_Standard Deviation. Among them, four metrics have not been previously\nstudied in RARL. We derive policy gradient formulas for these unstudied\nmetrics, improve gradient estimation for Gini Deviation, analyze their gradient\nproperties, and incorporate them with the REINFORCE and PPO frameworks to\npenalize the dispersion of returns.\n  Our empirical study reveals that variance-based metrics lead to unstable\npolicy updates. In contrast, CVaR Deviation and Gini Deviation show consistent\nperformance across different randomness and evaluation domains, achieving high\nreturns while effectively learning risk-averse policies. Mean Deviation and\nSemi_Standard Deviation are also competitive across different scenarios. This\nwork provides a comprehensive overview of variability measures in RARL,\noffering practical insights for risk-aware decision-making and guiding future\nresearch on risk metrics and RARL algorithms.\n","authors":["Yudong Luo","Yangchen Pan","Jiaqi Tan","Pascal Poupart"],"pdf_url":"https://arxiv.org/pdf/2504.11412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08301v2","updated":"2025-04-15T17:26:49Z","published":"2024-09-10T14:47:31Z","title":"Gaussian Differentially Private Human Faces Under a Face Radial Curve\n  Representation","summary":"  In this paper we consider the problem of releasing a Gaussian Differentially\nPrivate (GDP) 3D human face. The human face is a complex structure with many\nfeatures and inherently tied to one's identity. Protecting this data, in a\nformally private way, is important yet challenging given the dimensionality of\nthe problem. We extend approximate DP techniques for functional data to the GDP\nframework. We further propose a novel representation, face radial curves, of a\n3D face as a set of functions and then utilize our proposed GDP functional data\nmechanism. To preserve the shape of the face while injecting noise we rely on\ntools from shape analysis for our novel representation of the face. We show\nthat our method preserves the shape of the average face and injects less noise\nthan traditional methods for the same privacy budget. Our mechanism consists of\ntwo primary components, the first is generally applicable to function value\nsummaries (as are commonly found in nonparametric statistics or functional data\nanalysis) while the second is general to disk-like surfaces and hence more\napplicable than just to human faces.\n","authors":["Carlos Soto","Matthew Reimherr","Aleksandra Slavkovic","Mark Shriver"],"pdf_url":"https://arxiv.org/pdf/2409.08301v2.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2409.19552v3","updated":"2025-04-15T17:22:42Z","published":"2024-09-29T04:41:10Z","title":"OmniXAS: A Universal Deep-Learning Framework for Materials X-ray\n  Absorption Spectra","summary":"  X-ray absorption spectroscopy (XAS) is a powerful characterization technique\nfor probing the local chemical environment of absorbing atoms. However,\nanalyzing XAS data presents significant challenges, often requiring extensive,\ncomputationally intensive simulations, as well as significant domain expertise.\nThese limitations hinder the development of fast, robust XAS analysis pipelines\nthat are essential in high-throughput studies and for autonomous\nexperimentation. We address these challenges with OmniXAS, a framework that\ncontains a suite of transfer learning approaches for XAS prediction, each\ncontributing to improved accuracy and efficiency, as demonstrated on K-edge\nspectra database covering eight 3d transition metals (Ti-Cu). The OmniXAS\nframework is built upon three distinct strategies. First, we use M3GNet to\nderive latent representations of the local chemical environment of absorption\nsites as input for XAS prediction, achieving up to order-of-magnitude\nimprovements over conventional featurization techniques. Second, we employ a\nhierarchical transfer learning strategy, training a universal multi-task model\nacross elements before fine-tuning for element-specific predictions. Models\nbased on this cascaded approach after element-wise fine-tuning outperform\nelement-specific models by up to 69%. Third, we implement cross-fidelity\ntransfer learning, adapting a universal model to predict spectra generated by\nsimulation of a different fidelity with a higher computational cost. This\napproach improves prediction accuracy by up to 11% over models trained on the\ntarget fidelity alone. Our approach boosts the throughput of XAS modeling by\norders of magnitude versus first-principles simulations and is extendable to\nXAS prediction for a broader range of elements. This transfer learning\nframework is generalizable to enhance deep-learning models that target other\nproperties in materials research.\n","authors":["Shubha R. Kharel","Fanchen Meng","Xiaohui Qu","Matthew R. Carbone","Deyu Lu"],"pdf_url":"https://arxiv.org/pdf/2409.19552v3.pdf","comment":"Main manuscript: 23 pages, 14 figures. Supplemental material (13\n  pages, 6 figures) available as a separate file in arXiv ancillary files\n  (additional downloadable files)"},{"id":"http://arxiv.org/abs/2504.11397v1","updated":"2025-04-15T17:13:42Z","published":"2025-04-15T17:13:42Z","title":"MLPs and KANs for data-driven learning in physical problems: A\n  performance comparison","summary":"  There is increasing interest in solving partial differential equations (PDEs)\nby casting them as machine learning problems. Recently, there has been a spike\nin exploring Kolmogorov-Arnold Networks (KANs) as an alternative to traditional\nneural networks represented by Multi-Layer Perceptrons (MLPs). While showing\npromise, their performance advantages in physics-based problems remain largely\nunexplored. Several critical questions persist: Can KANs capture complex\nphysical dynamics and under what conditions might they outperform traditional\narchitectures? In this work, we present a comparative study of KANs and MLPs\nfor learning physical systems governed by PDEs. We assess their performance\nwhen applied in deep operator networks (DeepONet) and graph network-based\nsimulators (GNS), and test them on physical problems that vary significantly in\nscale and complexity. Drawing inspiration from the Kolmogorov Representation\nTheorem, we examine the behavior of KANs and MLPs across shallow and deep\nnetwork architectures. Our results reveal that although KANs do not\nconsistently outperform MLPs when configured as deep neural networks, they\ndemonstrate superior expressiveness in shallow network settings, significantly\noutpacing MLPs in accuracy over our test cases. This suggests that KANs are a\npromising choice, offering a balance of efficiency and accuracy in applications\ninvolving physical systems.\n","authors":["Raghav Pant","Sikan Li","Xingjian Li","Hassan Iqbal","Krishna Kumar"],"pdf_url":"https://arxiv.org/pdf/2504.11397v1.pdf","comment":"30 pages, 18 figures, 8 tables"},{"id":"http://arxiv.org/abs/2504.11393v1","updated":"2025-04-15T17:02:15Z","published":"2025-04-15T17:02:15Z","title":"DataDecide: How to Predict Best Pretraining Data with Small Experiments","summary":"  Because large language models are expensive to pretrain on different\ndatasets, using smaller-scale experiments to decide on data is crucial for\nreducing costs. Which benchmarks and methods of making decisions from observed\nperformance at small scale most accurately predict the datasets that yield the\nbest large models? To empower open exploration of this question, we release\nmodels, data, and evaluations in DataDecide -- the most extensive open suite of\nmodels over differences in data and scale. We conduct controlled pretraining\nexperiments across 25 corpora with differing sources, deduplication, and\nfiltering up to 100B tokens, model sizes up to 1B parameters, and 3 random\nseeds. We find that the ranking of models at a single, small size (e.g., 150M\nparameters) is a strong baseline for predicting best models at our larger\ntarget scale (1B) (~80% of com parisons correct). No scaling law methods among\n8 baselines exceed the compute-decision frontier of single-scale predictions,\nbut DataDecide can measure improvement in future scaling laws. We also identify\nthat using continuous likelihood metrics as proxies in small experiments makes\nbenchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable\nat the target 1B scale with just 0.01% of the compute.\n","authors":["Ian Magnusson","Nguyen Tai","Ben Bogin","David Heineman","Jena D. Hwang","Luca Soldaini","Akshita Bhagia","Jiacheng Liu","Dirk Groeneveld","Oyvind Tafjord","Noah A. Smith","Pang Wei Koh","Jesse Dodge"],"pdf_url":"https://arxiv.org/pdf/2504.11393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11386v1","updated":"2025-04-15T16:57:09Z","published":"2025-04-15T16:57:09Z","title":"Trajectory Encoding Temporal Graph Networks","summary":"  Temporal Graph Networks (TGNs) have demonstrated significant success in\ndynamic graph tasks such as link prediction and node classification. Both tasks\ncomprise transductive settings, where the model predicts links among known\nnodes, and in inductive settings, where it generalises learned patterns to\npreviously unseen nodes. Existing TGN designs face a dilemma under these dual\nscenarios. Anonymous TGNs, which rely solely on temporal and structural\ninformation, offer strong inductive generalisation but struggle to distinguish\nknown nodes. In contrast, non-anonymous TGNs leverage node features to excel in\ntransductive tasks yet fail to adapt to new nodes. To address this challenge,\nwe propose Trajectory Encoding TGN (TETGN). Our approach introduces\nautomatically expandable node identifiers (IDs) as learnable temporal\npositional features and performs message passing over these IDs to capture each\nnode's historical context. By integrating this trajectory-aware module with a\nstandard TGN using multi-head attention, TETGN effectively balances\ntransductive accuracy with inductive generalisation. Experimental results on\nthree real-world datasets show that TETGN significantly outperforms strong\nbaselines on both link prediction and node classification tasks, demonstrating\nits ability to unify the advantages of anonymous and non-anonymous models for\ndynamic graph learning.\n","authors":["Jiafeng Xiong","Rizos Sakellariou"],"pdf_url":"https://arxiv.org/pdf/2504.11386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11383v1","updated":"2025-04-15T16:54:04Z","published":"2025-04-15T16:54:04Z","title":"Accelerating Multiscale Modeling with Hybrid Solvers: Coupling FEM and\n  Neural Operators with Domain Decomposition","summary":"  Numerical solvers for partial differential equations (PDEs) face challenges\nbalancing computational cost and accuracy, especially in multiscale and dynamic\nsystems. Neural operators can significantly speed up simulations; however, they\noften face challenges such as error accumulation and limited generalization in\nmultiphysics problems. This work introduces a novel hybrid framework that\nintegrates physics-informed DeepONet with FEM through domain decomposition. The\ncore innovation lies in adaptively coupling FEM and DeepONet subdomains via a\nSchwarz alternating method. This methodology strategically allocates\ncomputationally demanding regions to a pre-trained Deep Operator Network, while\nthe remaining computational domain is solved through FEM. To address dynamic\nsystems, we integrate the Newmark time-stepping scheme directly into the\nDeepONet, significantly mitigating error accumulation in long-term simulations.\nFurthermore, an adaptive subdomain evolution enables the ML-resolved region to\nexpand dynamically, capturing emerging fine-scale features without remeshing.\nThe framework's efficacy has been validated across a range of solid mechanics\nproblems, including static, quasi-static, and dynamic regimes, demonstrating\naccelerated convergence rates (up to 20% improvement compared to FE-FE\napproaches), while preserving solution fidelity with error < 1%. Our case\nstudies show that our proposed hybrid solver: (1) maintains solution continuity\nacross subdomain interfaces, (2) reduces computational costs by eliminating\nfine mesh requirements, (3) mitigates error accumulation in time-dependent\nsimulations, and (4) enables automatic adaptation to evolving physical\nphenomena. This work bridges the gap between numerical methods and AI-driven\nsurrogates, offering a scalable pathway for high-fidelity simulations in\nengineering and scientific applications.\n","authors":["Wei Wanga","Maryam Hakimzadeh","Haihui Ruan","Somdatta Goswami"],"pdf_url":"https://arxiv.org/pdf/2504.11383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11364v1","updated":"2025-04-15T16:30:02Z","published":"2025-04-15T16:30:02Z","title":"Teaching Large Language Models to Reason through Learning and Forgetting","summary":"  Leveraging inference-time search in large language models has proven\neffective in further enhancing a trained model's capability to solve complex\nmathematical and reasoning problems. However, this approach significantly\nincreases computational costs and inference time, as the model must generate\nand evaluate multiple candidate solutions to identify a viable reasoning path.\nTo address this, we propose an effective approach that integrates search\ncapabilities directly into the model by fine-tuning it using both successful\n(learning) and failed reasoning paths (forgetting) derived from diverse search\nmethods. While fine-tuning the model with these data might seem\nstraightforward, we identify a critical issue: the model's search capability\ntends to degrade rapidly if fine-tuning is performed naively. We show that this\ndegradation can be substantially mitigated by employing a smaller learning\nrate. Extensive experiments on the challenging Game-of-24 and Countdown\nmathematical reasoning benchmarks show that our approach not only outperforms\nboth standard fine-tuning and inference-time search baselines but also\nsignificantly reduces inference time by 180$\\times$.\n","authors":["Tianwei Ni","Allen Nie","Sapana Chaudhary","Yao Liu","Huzefa Rangwala","Rasool Fakoor"],"pdf_url":"https://arxiv.org/pdf/2504.11364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09722v3","updated":"2025-04-15T16:25:03Z","published":"2025-03-12T18:11:37Z","title":"The Pitfalls of Imitation Learning when Actions are Continuous","summary":"  We study the problem of imitating an expert demonstrator in a discrete-time,\ncontinuous state-and-action control system. We show that, even if the dynamics\nsatisfy a control-theoretic property called exponentially stability (i.e. the\neffects of perturbations decay exponentially quickly), and the expert is smooth\nand deterministic, any smooth, deterministic imitator policy necessarily\nsuffers error on execution that is exponentially larger, as a function of\nproblem horizon, than the error under the distribution of expert training data.\nOur negative result applies to any algorithm which learns solely from expert\ndata, including both behavior cloning and offline-RL algorithms, unless the\nalgorithm produces highly \"improper\" imitator policies--those which are\nnon-smooth, non-Markovian, or which exhibit highly state-dependent\nstochasticity--or unless the expert trajectory distribution is sufficiently\n\"spread.\" We provide experimental evidence of the benefits of these more\ncomplex policy parameterizations, explicating the benefits of today's popular\npolicy parameterizations in robot learning (e.g. action-chunking and Diffusion\nPolicies). We also establish a host of complementary negative and positive\nresults for imitation in control systems.\n","authors":["Max Simchowitz","Daniel Pfrommer","Ali Jadbabaie"],"pdf_url":"https://arxiv.org/pdf/2503.09722v3.pdf","comment":"98 pages, 2 figures, updated proof sketch"},{"id":"http://arxiv.org/abs/2504.11353v1","updated":"2025-04-15T16:23:25Z","published":"2025-04-15T16:23:25Z","title":"An Adaptive Dropout Approach for High-Dimensional Bayesian Optimization","summary":"  Bayesian optimization (BO) is a widely used algorithm for solving expensive\nblack-box optimization problems. However, its performance decreases\nsignificantly on high-dimensional problems due to the inherent\nhigh-dimensionality of the acquisition function. In the proposed algorithm, we\nadaptively dropout the variables of the acquisition function along the\niterations. By gradually reducing the dimension of the acquisition function,\nthe proposed approach has less and less difficulty to optimize the acquisition\nfunction. Numerical experiments demonstrate that AdaDropout effectively tackle\nhigh-dimensional challenges and improve solution quality where standard\nBayesian optimization methods often struggle. Moreover, it achieves superior\nresults when compared with state-of-the-art high-dimensional Bayesian\noptimization approaches. This work provides a simple yet efficient solution for\nhigh-dimensional expensive optimization.\n","authors":["Jundi Huang","Dawei Zhan"],"pdf_url":"https://arxiv.org/pdf/2504.11353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11345v1","updated":"2025-04-15T16:16:38Z","published":"2025-04-15T16:16:38Z","title":"Erzeugunsgrad, VC-Dimension and Neural Networks with rational activation\n  function","summary":"  The notion of Erzeugungsgrad was introduced by Joos Heintz in 1983 to bound\nthe number of non-empty cells occurring after a process of quantifier\nelimination. We extend this notion and the combinatorial bounds of Theorem 2 in\nHeintz (1983) using the degree for constructible sets defined in\nPardo-Sebasti\\'an (2022). We show that the Erzeugungsgrad is the key ingredient\nto connect affine Intersection Theory over algebraically closed fields and the\nVC-Theory of Computational Learning Theory for families of classifiers given by\nparameterized families of constructible sets. In particular, we prove that the\nVC-dimension and the Krull dimension are linearly related up to logarithmic\nfactors based on Intersection Theory. Using this relation, we study the density\nof correct test sequences in evasive varieties. We apply these ideas to analyze\nparameterized families of neural networks with rational activation function.\n","authors":["Luis Miguel Pardo","Daniel Sebastián"],"pdf_url":"https://arxiv.org/pdf/2504.11345v1.pdf","comment":"50 pages"},{"id":"http://arxiv.org/abs/2504.11344v1","updated":"2025-04-15T16:15:16Z","published":"2025-04-15T16:15:16Z","title":"Interpretable Hybrid-Rule Temporal Point Processes","summary":"  Temporal Point Processes (TPPs) are widely used for modeling event sequences\nin various medical domains, such as disease onset prediction, progression\nanalysis, and clinical decision support. Although TPPs effectively capture\ntemporal dynamics, their lack of interpretability remains a critical challenge.\nRecent advancements have introduced interpretable TPPs. However, these methods\nfail to incorporate numerical features, thereby limiting their ability to\ngenerate precise predictions. To address this issue, we propose Hybrid-Rule\nTemporal Point Processes (HRTPP), a novel framework that integrates temporal\nlogic rules with numerical features, improving both interpretability and\npredictive accuracy in event modeling. HRTPP comprises three key components:\nbasic intensity for intrinsic event likelihood, rule-based intensity for\nstructured temporal dependencies, and numerical feature intensity for dynamic\nprobability modulation. To effectively discover valid rules, we introduce a\ntwo-phase rule mining strategy with Bayesian optimization. To evaluate our\nmethod, we establish a multi-criteria assessment framework, incorporating rule\nvalidity, model fitting, and temporal predictive accuracy. Experimental results\non real-world medical datasets demonstrate that HRTPP outperforms\nstate-of-the-art interpretable TPPs in terms of predictive performance and\nclinical interpretability. In case studies, the rules extracted by HRTPP\nexplain the disease progression, offering valuable contributions to medical\ndiagnosis.\n","authors":["Yunyang Cao","Juekai Lin","Hongye Wang","Wenhao Li","Bo Jin"],"pdf_url":"https://arxiv.org/pdf/2504.11344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11343v1","updated":"2025-04-15T16:15:02Z","published":"2025-04-15T16:15:02Z","title":"A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce","summary":"  Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training.\n","authors":["Wei Xiong","Jiarui Yao","Yuhui Xu","Bo Pang","Lei Wang","Doyen Sahoo","Junnan Li","Nan Jiang","Tong Zhang","Caiming Xiong","Hanze Dong"],"pdf_url":"https://arxiv.org/pdf/2504.11343v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.19631v3","updated":"2025-04-15T16:11:56Z","published":"2024-07-29T01:22:04Z","title":"\"A Good Bot Always Knows Its Limitations\": Assessing Autonomous System\n  Decision-making Competencies through Factorized Machine Self-confidence","summary":"  How can intelligent machines assess their competency to complete a task? This\nquestion has come into focus for autonomous systems that algorithmically make\ndecisions under uncertainty. We argue that machine self-confidence -- a form of\nmeta-reasoning based on self-assessments of system knowledge about the state of\nthe world, itself, and ability to reason about and execute tasks -- leads to\nmany computable and useful competency indicators for such agents. This paper\npresents our body of work, so far, on this concept in the form of the\nFactorized Machine Self-confidence (FaMSeC) framework, which holistically\nconsiders several major factors driving competency in algorithmic\ndecision-making: outcome assessment, solver quality, model quality, alignment\nquality, and past experience. In FaMSeC, self-confidence indicators are derived\nvia 'problem-solving statistics' embedded in Markov decision process solvers\nand related approaches. These statistics come from evaluating probabilistic\nexceedance margins in relation to certain outcomes and associated competency\nstandards specified by an evaluator. Once designed, and evaluated, the\nstatistics can be easily incorporated into autonomous agents and serve as\nindicators of competency. We include detailed descriptions and examples for\nMarkov decision process agents, and show how outcome assessment and solver\nquality factors can be found for a range of tasking contexts through novel use\nof meta-utility functions, behavior simulations, and surrogate prediction\nmodels. Numerical evaluations are performed to demonstrate that FaMSeC\nindicators perform as desired (references to human subject studies beyond the\nscope of this paper are provided).\n","authors":["Brett W. Israelsen","Nisar R. Ahmed","Matthew Aitken","Eric W. Frew","Dale A. Lawrence","Brian M. Argrow"],"pdf_url":"https://arxiv.org/pdf/2407.19631v3.pdf","comment":"63 pages, 22 figures, version accepted to ACM THRI"},{"id":"http://arxiv.org/abs/2504.11336v1","updated":"2025-04-15T16:09:06Z","published":"2025-04-15T16:09:06Z","title":"Looking beyond the next token","summary":"  The structure of causal language model training assumes that each token can\nbe accurately predicted from the previous context. This contrasts with humans'\nnatural writing and reasoning process, where goals are typically known before\nthe exact argument or phrasings. While this mismatch has been well studied in\nthe literature, the working assumption has been that architectural changes are\nneeded to address this mismatch. We argue that rearranging and processing the\ntraining data sequences can allow models to more accurately imitate the true\ndata-generating process, and does not require any other changes to the\narchitecture or training infrastructure. We demonstrate that this technique,\nTrelawney, and the inference algorithms derived from it allow us to improve\nperformance on several key benchmarks that span planning, algorithmic\nreasoning, and story generation tasks. Finally, our method naturally enables\nthe generation of long-term goals at no additional cost. We investigate how\nusing the model's goal-generation capability can further improve planning and\nreasoning. Additionally, we believe Trelawney could potentially open doors to\nnew capabilities beyond the current language modeling paradigm.\n","authors":["Abitha Thankaraj","Yiding Jiang","J. Zico Kolter","Yonatan Bisk"],"pdf_url":"https://arxiv.org/pdf/2504.11336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11335v1","updated":"2025-04-15T16:07:54Z","published":"2025-04-15T16:07:54Z","title":"Code Reborn AI-Driven Legacy Systems Modernization from COBOL to Java","summary":"  This study investigates AI-driven modernization of legacy COBOL code into\nJava, addressing a critical challenge in aging software systems. Leveraging the\nLegacy COBOL 2024 Corpus -- 50,000 COBOL files from public and enterprise\nsources -- Java parses the code, AI suggests upgrades, and React visualizes\ngains. Achieving 93% accuracy, complexity drops 35% (from 18 to 11.7) and\ncoupling 33% (from 8 to 5.4), surpassing manual efforts (75%) and rule-based\ntools (82%). The approach offers a scalable path to rejuvenate COBOL systems,\nvital for industries like banking and insurance.\n","authors":["Gopichand Bandarupalli"],"pdf_url":"https://arxiv.org/pdf/2504.11335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11321v1","updated":"2025-04-15T16:01:39Z","published":"2025-04-15T16:01:39Z","title":"Subset-Contrastive Multi-Omics Network Embedding","summary":"  Motivation: Network-based analyses of omics data are widely used, and while\nmany of these methods have been adapted to single-cell scenarios, they often\nremain memory- and space-intensive. As a result, they are better suited to\nbatch data or smaller datasets. Furthermore, the application of network-based\nmethods in multi-omics often relies on similarity-based networks, which lack\nstructurally-discrete topologies. This limitation may reduce the effectiveness\nof graph-based methods that were initially designed for topologies with better\ndefined structures. Results: We propose Subset-Contrastive multi-Omics Network\nEmbedding (SCONE), a method that employs contrastive learning techniques on\nlarge datasets through a scalable subgraph contrastive approach. By exploiting\nthe pairwise similarity basis of many network-based omics methods, we\ntransformed this characteristic into a strength, developing an approach that\naims to achieve scalable and effective analysis. Our method demonstrates\nsynergistic omics integration for cell type clustering in single-cell data.\nAdditionally, we evaluate its performance in a bulk multi-omics integration\nscenario, where SCONE performs comparable to the state-of-the-art despite\nutilising limited views of the original data. We anticipate that our findings\nwill motivate further research into the use of subset contrastive methods for\nomics data.\n","authors":["Pedro Henrique da Costa Avelar","Min Wu","Sophia Tsoka"],"pdf_url":"https://arxiv.org/pdf/2504.11321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11320v1","updated":"2025-04-15T16:00:21Z","published":"2025-04-15T16:00:21Z","title":"Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints","summary":"  Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.\n","authors":["Ruicheng Ao","Gan Luo","David Simchi-Levi","Xinshang Wang"],"pdf_url":"https://arxiv.org/pdf/2504.11320v1.pdf","comment":"42 pages, 18 figures"},{"id":"http://arxiv.org/abs/2504.11318v1","updated":"2025-04-15T15:59:32Z","published":"2025-04-15T15:59:32Z","title":"Mildly-Interacting Fermionic Unitaries are Efficiently Learnable","summary":"  Recent work has shown that one can efficiently learn fermionic Gaussian\nunitaries, also commonly known as nearest-neighbor matchcircuits or\nnon-interacting fermionic unitaries. However, one could ask a similar question\nabout unitaries that are near Gaussian: for example, unitaries prepared with a\nsmall number of non-Gaussian circuit elements. These operators find\nsignificance in quantum chemistry and many-body physics, yet no algorithm\nexists to learn them.\n  We give the first such result by devising an algorithm which makes queries to\na $n$-mode fermionic unitary $U$ prepared by at most $O(t)$ non-Gaussian gates\nand returns a circuit approximating $U$ to diamond distance $\\varepsilon$ in\ntime $\\textrm{poly}(n,2^t,1/\\varepsilon)$. This resolves a central open\nquestion of Mele and Herasymenko under the strongest distance metric. In fact,\nour algorithm is much more general: we define a property of unitary Gaussianity\nknown as unitary Gaussian dimension and show that our algorithm can learn\n$n$-mode unitaries of Gaussian dimension at least $2n - O(t)$ in time\n$\\textrm{poly}(n,2^t,1/\\varepsilon)$. Indeed, this class subsumes unitaries\nprepared by at most $O(t)$ non-Gaussian gates but also includes several\nunitaries that require up to $2^{O(t)}$ non-Gaussian gates to construct.\n  In addition, we give a $\\textrm{poly}(n,1/\\varepsilon)$-time algorithm to\ndistinguish whether an $n$-mode unitary is of Gaussian dimension at least $k$\nor $\\varepsilon$-far from all such unitaries in Frobenius distance, promised\nthat one is the case. Along the way, we prove structural results about\nnear-Gaussian fermionic unitaries that are likely to be of independent\ninterest.\n","authors":["Vishnu Iyer"],"pdf_url":"https://arxiv.org/pdf/2504.11318v1.pdf","comment":"30 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.09347v2","updated":"2025-04-15T15:55:26Z","published":"2025-04-12T21:32:42Z","title":"Inferring Outcome Means of Exponential Family Distributions Estimated by\n  Deep Neural Networks","summary":"  While deep neural networks (DNNs) are widely used for prediction, inference\non DNN-estimated subject-specific means for categorical or exponential family\noutcomes remains underexplored. We address this by proposing a DNN estimator\nunder generalized nonparametric regression models (GNRMs) and developing a\nrigorous inference framework. Unlike existing approaches that assume\nindependence between prediction errors and inputs to establish the error bound,\na condition often violated in GNRMs, we allow for dependence and our\ntheoretical analysis demonstrates the feasibility of drawing inference under\nGNRMs. To implement inference, we consider an Ensemble Subsampling Method (ESM)\nthat leverages U-statistics and the Hoeffding decomposition to construct\nreliable confidence intervals for DNN estimates. We show that, under GNRM\nsettings, ESM enables model-free variance estimation and accounts for\nheterogeneity among individuals in the population. Through simulations under\nnonparametric logistic, Poisson, and binomial regression models, we demonstrate\nthe effectiveness and efficiency of our method. We further apply the method to\nthe electronic Intensive Care Unit (eICU) dataset, a large-scale collection of\nanonymized health records from ICU patients, to predict ICU readmission risk\nand offer patient-centric insights for clinical decision-making.\n","authors":["Xuran Meng","Yi Li"],"pdf_url":"https://arxiv.org/pdf/2504.09347v2.pdf","comment":"44 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2504.11304v1","updated":"2025-04-15T15:45:48Z","published":"2025-04-15T15:45:48Z","title":"Differentially Private Geodesic and Linear Regression","summary":"  In statistical applications it has become increasingly common to encounter\ndata structures that live on non-linear spaces such as manifolds. Classical\nlinear regression, one of the most fundamental methodologies of statistical\nlearning, captures the relationship between an independent variable and a\nresponse variable which both are assumed to live in Euclidean space. Thus,\ngeodesic regression emerged as an extension where the response variable lives\non a Riemannian manifold. The parameters of geodesic regression, as with linear\nregression, capture the relationship of sensitive data and hence one should\nconsider the privacy protection practices of said parameters. We consider\nreleasing Differentially Private (DP) parameters of geodesic regression via the\nK-Norm Gradient (KNG) mechanism for Riemannian manifolds. We derive theoretical\nbounds for the sensitivity of the parameters showing they are tied to their\nrespective Jacobi fields and hence the curvature of the space. This\ncorroborates recent findings of differential privacy for the Fr\\'echet mean. We\ndemonstrate the efficacy of our methodology on the sphere,\n$\\mbS^2\\subset\\mbR^3$ and, since it is general to Riemannian manifolds, the\nmanifold of Euclidean space which simplifies geodesic regression to a case of\nlinear regression. Our methodology is general to any Riemannian manifold and\nthus it is suitable for data in domains such as medical imaging and computer\nvision.\n","authors":["Aditya Kulkarni","Carlos Soto"],"pdf_url":"https://arxiv.org/pdf/2504.11304v1.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.11302v1","updated":"2025-04-15T15:45:14Z","published":"2025-04-15T15:45:14Z","title":"Limits of Discrete Energy of Families of Increasing Sets","summary":"  The Hausdorff dimension of a set can be detected using the Riesz energy.\nHere, we consider situations where a sequence of points, $\\{x_n\\}$, ``fills\nin'' a set $E \\subset \\mathbb{R}^d$ in an appropriate sense and investigate the\ndegree to which the discrete analog to the Riesz energy of these sets can be\nused to bound the Hausdorff dimension of $E$. We also discuss applications to\ndata science and Erd\\H{o}s/Falconer type problems.\n","authors":["Hari Nathan"],"pdf_url":"https://arxiv.org/pdf/2504.11302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11299v1","updated":"2025-04-15T15:42:49Z","published":"2025-04-15T15:42:49Z","title":"Efficient and Stable Multi-Dimensional Kolmogorov-Smirnov Distance","summary":"  We revisit extending the Kolmogorov-Smirnov distance between probability\ndistributions to the multidimensional setting and make new arguments about the\nproper way to approach this generalization. Our proposed formulation maximizes\nthe difference over orthogonal dominating rectangular ranges (d-sided\nrectangles in R^d), and is an integral probability metric. We also prove that\nthe distance between a distribution and a sample from the distribution\nconverges to 0 as the sample size grows, and bound this rate. Moreover, we show\nthat one can, up to this same approximation error, compute the distance\nefficiently in 4 or fewer dimensions; specifically the runtime is near-linear\nin the size of the sample needed for that error. With this, we derive a\ndelta-precision two-sample hypothesis test using this distance. Finally, we\nshow these metric and approximation properties do not hold for other popular\nvariants.\n","authors":["Peter Matthew Jacobs","Foad Namjoo","Jeff M. Phillips"],"pdf_url":"https://arxiv.org/pdf/2504.11299v1.pdf","comment":"21 pages, Primary: stat.CO. Secondary: cs.CG, cs.LG"},{"id":"http://arxiv.org/abs/2504.11284v1","updated":"2025-04-15T15:25:27Z","published":"2025-04-15T15:25:27Z","title":"Bipartite Ranking From Multiple Labels: On Loss Versus Label Aggregation","summary":"  Bipartite ranking is a fundamental supervised learning problem, with the goal\nof learning a ranking over instances with maximal area under the ROC curve\n(AUC) against a single binary target label. However, one may often observe\nmultiple binary target labels, e.g., from distinct human annotators. How can\none synthesize such labels into a single coherent ranking? In this work, we\nformally analyze two approaches to this problem -- loss aggregation and label\naggregation -- by characterizing their Bayes-optimal solutions. Based on this,\nwe show that while both methods can yield Pareto-optimal solutions, loss\naggregation can exhibit label dictatorship: one can inadvertently (and\nundesirably) favor one label over others. This suggests that label aggregation\ncan be preferable to loss aggregation, which we empirically verify.\n","authors":["Michal Lukasik","Lin Chen","Harikrishna Narasimhan","Aditya Krishna Menon","Wittawat Jitkrittum","Felix X. Yu","Sashank J. Reddi","Gang Fu","Mohammadhossein Bateni","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2504.11284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.24043v3","updated":"2025-04-15T15:16:00Z","published":"2025-03-31T13:07:33Z","title":"Frequency-Aware Attention-LSTM for PM$_{2.5}$ Time Series Forecasting","summary":"  To enhance the accuracy and robustness of PM$_{2.5}$ concentration\nforecasting, this paper introduces FALNet, a Frequency-Aware LSTM Network that\nintegrates frequency-domain decomposition, temporal modeling, and\nattention-based refinement. The model first applies STL and FFT to extract\ntrend, seasonal, and denoised residual components, effectively filtering out\nhigh-frequency noise. The filtered residuals are then fed into a stacked LSTM\nto capture long-term dependencies, followed by a multi-head attention mechanism\nthat dynamically focuses on key time steps. Experiments conducted on real-world\nurban air quality datasets demonstrate that FALNet consistently outperforms\nconventional models across standard metrics such as MAE, RMSE, and $R^2$. The\nmodel shows strong adaptability in capturing sharp fluctuations during\npollution peaks and non-stationary conditions. These results validate the\neffectiveness and generalizability of FALNet for real-time air pollution\nprediction, environmental risk assessment, and decision-making support.\n","authors":["Jiahui Lu","Shuang Wu","Zhenkai Qin","Guifang Yang"],"pdf_url":"https://arxiv.org/pdf/2503.24043v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07082v3","updated":"2025-04-15T15:07:30Z","published":"2024-07-09T17:55:23Z","title":"Can Learned Optimization Make Reinforcement Learning Less Difficult?","summary":"  While reinforcement learning (RL) holds great potential for decision making\nin the real world, it suffers from a number of unique difficulties which often\nneed specific consideration. In particular: it is highly non-stationary;\nsuffers from high degrees of plasticity loss; and requires exploration to\nprevent premature convergence to local optima and maximize return. In this\npaper, we consider whether learned optimization can help overcome these\nproblems. Our method, Learned Optimization for Plasticity, Exploration and\nNon-stationarity (OPEN), meta-learns an update rule whose input features and\noutput structure are informed by previously proposed solutions to these\ndifficulties. We show that our parameterization is flexible enough to enable\nmeta-learning in diverse learning contexts, including the ability to use\nstochasticity for exploration. Our experiments demonstrate that when\nmeta-trained on single and small sets of environments, OPEN outperforms or\nequals traditionally used optimizers. Furthermore, OPEN shows strong\ngeneralization characteristics across a range of environments and agent\narchitectures.\n","authors":["Alexander David Goldie","Chris Lu","Matthew Thomas Jackson","Shimon Whiteson","Jakob Nicolaus Foerster"],"pdf_url":"https://arxiv.org/pdf/2407.07082v3.pdf","comment":"Added Metadata for Neurips 2024"},{"id":"http://arxiv.org/abs/2504.11264v1","updated":"2025-04-15T15:04:39Z","published":"2025-04-15T15:04:39Z","title":"DeepSelective: Feature Gating and Representation Matching for\n  Interpretable Clinical Prediction","summary":"  The rapid accumulation of Electronic Health Records (EHRs) has transformed\nhealthcare by providing valuable data that enhance clinical predictions and\ndiagnoses. While conventional machine learning models have proven effective,\nthey often lack robust representation learning and depend heavily on\nexpert-crafted features. Although deep learning offers powerful solutions, it\nis often criticized for its lack of interpretability. To address these\nchallenges, we propose DeepSelective, a novel end to end deep learning\nframework for predicting patient prognosis using EHR data, with a strong\nemphasis on enhancing model interpretability. DeepSelective combines data\ncompression techniques with an innovative feature selection approach,\nintegrating custom-designed modules that work together to improve both accuracy\nand interpretability. Our experiments demonstrate that DeepSelective not only\nenhances predictive accuracy but also significantly improves interpretability,\nmaking it a valuable tool for clinical decision-making. The source code is\nfreely available at http://www.healthinformaticslab.org/supp/resources.php .\n","authors":["Ruochi Zhang","Qian Yang","Xiaoyang Wang","Haoran Wu","Qiong Zhou","Yu Wang","Kewei Li","Yueying Wang","Yusi Fan","Jiale Zhang","Lan Huang","Chang Liu","Fengfeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.11264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11258v1","updated":"2025-04-15T14:56:42Z","published":"2025-04-15T14:56:42Z","title":"Multi-Agent Reinforcement Learning for Greenhouse Gas Offset Credit\n  Markets","summary":"  Climate change is a major threat to the future of humanity, and its impacts\nare being intensified by excess man-made greenhouse gas emissions. One method\ngovernments can employ to control these emissions is to provide firms with\nemission limits and penalize any excess emissions above the limit. Excess\nemissions may also be offset by firms who choose to invest in carbon reducing\nand capturing projects. These projects generate offset credits which can be\nsubmitted to a regulating agency to offset a firm's excess emissions, or they\ncan be traded with other firms. In this work, we characterize the finite-agent\nNash equilibrium for offset credit markets. As computing Nash equilibria is an\nNP-hard problem, we utilize the modern reinforcement learning technique\nNash-DQN to efficiently estimate the market's Nash equilibria. We demonstrate\nnot only the validity of employing reinforcement learning methods applied to\nclimate themed financial markets, but also the significant financial savings\nemitting firms may achieve when abiding by the Nash equilibria through\nnumerical experiments.\n","authors":["Liam Welsh","Udit Grover","Sebastian Jaimungal"],"pdf_url":"https://arxiv.org/pdf/2504.11258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11255v1","updated":"2025-04-15T14:51:44Z","published":"2025-04-15T14:51:44Z","title":"Reconstructing Fine-Grained Network Data using Autoencoder Architectures\n  with Domain Knowledge Penalties","summary":"  The ability to reconstruct fine-grained network session data, including\nindividual packets, from coarse-grained feature vectors is crucial for\nimproving network security models. However, the large-scale collection and\nstorage of raw network traffic pose significant challenges, particularly for\ncapturing rare cyberattack samples. These challenges hinder the ability to\nretain comprehensive datasets for model training and future threat detection.\nTo address this, we propose a machine learning approach guided by formal\nmethods to encode and reconstruct network data. Our method employs autoencoder\nmodels with domain-informed penalties to impute PCAP session headers from\nstructured feature representations. Experimental results demonstrate that\nincorporating domain knowledge through constraint-based loss terms\nsignificantly improves reconstruction accuracy, particularly for categorical\nfeatures with session-level encodings. By enabling efficient reconstruction of\ndetailed network sessions, our approach facilitates data-efficient model\ntraining while preserving privacy and storage efficiency.\n","authors":["Mark Cheung","Sridhar Venkatesan"],"pdf_url":"https://arxiv.org/pdf/2504.11255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11250v1","updated":"2025-04-15T14:46:58Z","published":"2025-04-15T14:46:58Z","title":"A Rollout-Based Algorithm and Reward Function for Efficient Resource\n  Allocation in Business Processes","summary":"  Resource allocation plays a critical role in minimizing cycle time and\nimproving the efficiency of business processes. Recently, Deep Reinforcement\nLearning (DRL) has emerged as a powerful tool to optimize resource allocation\npolicies in business processes. In the DRL framework, an agent learns a policy\nthrough interaction with the environment, guided solely by reward signals that\nindicate the quality of its decisions. However, existing algorithms are not\nsuitable for dynamic environments such as business processes. Furthermore,\nexisting DRL-based methods rely on engineered reward functions that approximate\nthe desired objective, but a misalignment between reward and objective can lead\nto undesired decisions or suboptimal policies. To address these issues, we\npropose a rollout-based DRL algorithm and a reward function to optimize the\nobjective directly. Our algorithm iteratively improves the policy by evaluating\nexecution trajectories following different actions. Our reward function\ndirectly decomposes the objective function of minimizing the mean cycle time.\nMaximizing our reward function guarantees that the objective function is\nminimized without requiring extensive reward engineering. The results show that\nour method consistently learns the optimal policy in all six evaluated business\nprocesses, outperforming the state-of-the-art algorithm that can only learn the\noptimal policy in two of the evaluated processes.\n","authors":["Jeroen Middelhuis","Zaharah Bukhsh","Ivo Adan","Remco Dijkman"],"pdf_url":"https://arxiv.org/pdf/2504.11250v1.pdf","comment":"Pre-print submitted to the 23rd International Conference on Business\n  Process Management"},{"id":"http://arxiv.org/abs/2504.11249v1","updated":"2025-04-15T14:46:25Z","published":"2025-04-15T14:46:25Z","title":"Cryo-em images are intrinsically low dimensional","summary":"  Simulation-based inference provides a powerful framework for cryo-electron\nmicroscopy, employing neural networks in methods like CryoSBI to infer\nbiomolecular conformations via learned latent representations. This latent\nspace represents a rich opportunity, encoding valuable information about the\nphysical system and the inference process. Harnessing this potential hinges on\nunderstanding the underlying geometric structure of these representations. We\ninvestigate this structure by applying manifold learning techniques to CryoSBI\nrepresentations of hemagglutinin (simulated and experimental). We reveal that\nthese high-dimensional data inherently populate low-dimensional, smooth\nmanifolds, with simulated data effectively covering the experimental\ncounterpart. By characterizing the manifold's geometry using Diffusion Maps and\nidentifying its principal axes of variation via coordinate interpretation\nmethods, we establish a direct link between the latent structure and key\nphysical parameters. Discovering this intrinsic low-dimensionality and\ninterpretable geometric organization not only validates the CryoSBI approach\nbut enables us to learn more from the data structure and provides opportunities\nfor improving future inference strategies by exploiting this revealed manifold\ngeometry.\n","authors":["Luke Evans","Octavian-Vlad Murad","Lars Dingeldein","Pilar Cossio","Roberto Covino","Marina Meila"],"pdf_url":"https://arxiv.org/pdf/2504.11249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11247v1","updated":"2025-04-15T14:45:51Z","published":"2025-04-15T14:45:51Z","title":"Next-Future: Sample-Efficient Policy Learning for Robotic-Arm Tasks","summary":"  Hindsight Experience Replay (HER) is widely regarded as the state-of-the-art\nalgorithm for achieving sample-efficient multi-goal reinforcement learning (RL)\nin robotic manipulation tasks with binary rewards. HER facilitates learning\nfrom failed attempts by replaying trajectories with redefined goals. However,\nit relies on a heuristic-based replay method that lacks a principled framework.\nTo address this limitation, we introduce a novel replay strategy,\n\"Next-Future\", which focuses on rewarding single-step transitions. This\napproach significantly enhances sample efficiency and accuracy in learning\nmulti-goal Markov decision processes (MDPs), particularly under stringent\naccuracy requirements -- a critical aspect for performing complex and precise\nrobotic-arm tasks. We demonstrate the efficacy of our method by highlighting\nhow single-step learning enables improved value approximation within the\nmulti-goal RL framework. The performance of the proposed replay strategy is\nevaluated across eight challenging robotic manipulation tasks, using ten random\nseeds for training. Our results indicate substantial improvements in sample\nefficiency for seven out of eight tasks and higher success rates in six tasks.\nFurthermore, real-world experiments validate the practical feasibility of the\nlearned policies, demonstrating the potential of \"Next-Future\" in solving\ncomplex robotic-arm tasks.\n","authors":["Fikrican Özgür","René Zurbrügg","Suryansh Kumar"],"pdf_url":"https://arxiv.org/pdf/2504.11247v1.pdf","comment":"10 pages, 9 figures, 6 tables"},{"id":"http://arxiv.org/abs/2504.11246v1","updated":"2025-04-15T14:44:47Z","published":"2025-04-15T14:44:47Z","title":"Respiratory Inhaler Sound Event Classification Using Self-Supervised\n  Learning","summary":"  Asthma is a chronic respiratory condition that affects millions of people\nworldwide. While this condition can be managed by administering controller\nmedications through handheld inhalers, clinical studies have shown low\nadherence to the correct inhaler usage technique. Consequently, many patients\nmay not receive the full benefit of their medication. Automated classification\nof inhaler sounds has recently been studied to assess medication adherence.\nHowever, the existing classification models were typically trained using data\nfrom specific inhaler types, and their ability to generalize to sounds from\ndifferent inhalers remains unexplored. In this study, we adapted the wav2vec\n2.0 self-supervised learning model for inhaler sound classification by\npre-training and fine-tuning this model on inhaler sounds. The proposed model\nshows a balanced accuracy of 98% on a dataset collected using a dry powder\ninhaler and smartwatch device. The results also demonstrate that re-finetuning\nthis model on minimal data from a target inhaler is a promising approach to\nadapting a generic inhaler sound classification model to a different inhaler\ndevice and audio capture hardware. This is the first study in the field to\ndemonstrate the potential of smartwatches as assistive technologies for the\npersonalized monitoring of inhaler adherence using machine learning models.\n","authors":["Davoud Shariat Panah","Alessandro N Franciosi","Cormac McCarthy","Andrew Hines"],"pdf_url":"https://arxiv.org/pdf/2504.11246v1.pdf","comment":"Accepted at the IEEE EMBC 2025 Conference"},{"id":"http://arxiv.org/abs/2504.11243v1","updated":"2025-04-15T14:43:19Z","published":"2025-04-15T14:43:19Z","title":"Towards Automated Safety Requirements Derivation Using Agent-based RAG","summary":"  We study the automated derivation of safety requirements in a self-driving\nvehicle use case, leveraging LLMs in combination with agent-based\nretrieval-augmented generation. Conventional approaches that utilise\npre-trained LLMs to assist in safety analyses typically lack domain-specific\nknowledge. Existing RAG approaches address this issue, yet their performance\ndeteriorates when handling complex queries and it becomes increasingly harder\nto retrieve the most relevant information. This is particularly relevant for\nsafety-relevant applications. In this paper, we propose the use of agent-based\nRAG to derive safety requirements and show that the retrieved information is\nmore relevant to the queries. We implement an agent-based approach on a\ndocument pool of automotive standards and the Apollo case study, as a\nrepresentative example of an automated driving perception system. Our solution\nis tested on a data set of safety requirement questions and answers, extracted\nfrom the Apollo data. Evaluating a set of selected RAG metrics, we present and\ndiscuss advantages of a agent-based approach compared to default RAG methods.\n","authors":["Balahari Vignesh Balu","Florian Geissler","Francesco Carella","Joao-Vitor Zacchi","Josef Jiru","Nuria Mata","Reinhard Stolle"],"pdf_url":"https://arxiv.org/pdf/2504.11243v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2504.03624v3","updated":"2025-04-15T14:36:01Z","published":"2025-04-04T17:41:58Z","title":"Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer\n  Models","summary":"  As inference-time scaling becomes critical for enhanced reasoning\ncapabilities, it is increasingly becoming important to build models that are\nefficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid\nMamba-Transformer models designed to reduce inference cost for a given accuracy\nlevel. To achieve this goal, we replace the majority of self-attention layers\nin the common Transformer model architecture with Mamba layers that perform\nconstant computation and require constant memory per generated token. We show\nthat Nemotron-H models offer either better or on-par accuracy compared to other\nsimilarly-sized state-of-the-art open-sourced Transformer models (e.g.,\nQwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at\ninference. To further increase inference speed and reduce the memory required\nat inference time, we created Nemotron-H-47B-Base from the 56B model using a\nnew compression via pruning and distillation technique called MiniPuzzle.\nNemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20%\nfaster to infer. In addition, we introduce an FP8-based training recipe and\nshow that it can achieve on par results with BF16-based training. This recipe\nis used to train the 56B model. We are releasing Nemotron-H base model\ncheckpoints with support in Hugging Face and NeMo.\n","authors":[" NVIDIA"," :","Aaron Blakeman","Aarti Basant","Abhinav Khattar","Adithya Renduchintala","Akhiad Bercovich","Aleksander Ficek","Alexis Bjorlin","Ali Taghibakhshi","Amala Sanjay Deshmukh","Ameya Sunil Mahabaleshwarkar","Andrew Tao","Anna Shors","Ashwath Aithal","Ashwin Poojary","Ayush Dattagupta","Balaram Buddharaju","Bobby Chen","Boris Ginsburg","Boxin Wang","Brandon Norick","Brian Butterfield","Bryan Catanzaro","Carlo del Mundo","Chengyu Dong","Christine Harvey","Christopher Parisien","Dan Su","Daniel Korzekwa","Danny Yin","Daria Gitman","David Mosallanezhad","Deepak Narayanan","Denys Fridman","Dima Rekesh","Ding Ma","Dmytro Pykhtar","Dong Ahn","Duncan Riach","Dusan Stosic","Eileen Long","Elad Segal","Ellie Evans","Eric Chung","Erick Galinkin","Evelina Bakhturina","Ewa Dobrowolska","Fei Jia","Fuxiao Liu","Gargi Prasad","Gerald Shen","Guilin Liu","Guo Chen","Haifeng Qian","Helen Ngo","Hongbin Liu","Hui Li","Igor Gitman","Ilia Karmanov","Ivan Moshkov","Izik Golan","Jan Kautz","Jane Polak Scowcroft","Jared Casper","Jarno Seppanen","Jason Lu","Jason Sewall","Jiaqi Zeng","Jiaxuan You","Jimmy Zhang","Jing Zhang","Jining Huang","Jinze Xue","Jocelyn Huang","Joey Conway","John Kamalu","Jon Barker","Jonathan Cohen","Joseph Jennings","Jupinder Parmar","Karan Sapra","Kari Briski","Kateryna Chumachenko","Katherine Luna","Keshav Santhanam","Kezhi Kong","Kirthi Sivamani","Krzysztof Pawelec","Kumar Anik","Kunlun Li","Lawrence McAfee","Leon Derczynski","Lindsey Pavao","Luis Vega","Lukas Voegtle","Maciej Bala","Maer Rodrigues de Melo","Makesh Narsimhan Sreedhar","Marcin Chochowski","Markus Kliegl","Marta Stepniewska-Dziubinska","Matthieu Le","Matvei Novikov","Mehrzad Samadi","Michael Andersch","Michael Evans","Miguel Martinez","Mike Chrzanowski","Mike Ranzinger","Mikolaj Blaz","Misha Smelyanskiy","Mohamed Fawzy","Mohammad Shoeybi","Mostofa Patwary","Nayeon Lee","Nima Tajbakhsh","Ning Xu","Oleg Rybakov","Oleksii Kuchaiev","Olivier Delalleau","Osvald Nitski","Parth Chadha","Pasha Shamis","Paulius Micikevicius","Pavlo Molchanov","Peter Dykas","Philipp Fischer","Pierre-Yves Aquilanti","Piotr Bialecki","Prasoon Varshney","Pritam Gundecha","Przemek Tredak","Rabeeh Karimi","Rahul Kandu","Ran El-Yaniv","Raviraj Joshi","Roger Waleffe","Ruoxi Zhang","Sabrina Kavanaugh","Sahil Jain","Samuel Kriman","Sangkug Lym","Sanjeev Satheesh","Saurav Muralidharan","Sean Narenthiran","Selvaraj Anandaraj","Seonmyeong Bak","Sergey Kashirsky","Seungju Han","Shantanu Acharya","Shaona Ghosh","Sharath Turuvekere Sreenivas","Sharon Clay","Shelby Thomas","Shrimai Prabhumoye","Shubham Pachori","Shubham Toshniwal","Shyamala Prayaga","Siddhartha Jain","Sirshak Das","Slawek Kierat","Somshubra Majumdar","Song Han","Soumye Singhal","Sriharsha Niverty","Stefania Alborghetti","Suseella Panguluri","Swetha Bhendigeri","Syeda Nahida Akter","Szymon Migacz","Tal Shiri","Terry Kong","Timo Roman","Tomer Ronen","Trisha Saar","Tugrul Konuk","Tuomas Rintamaki","Tyler Poon","Ushnish De","Vahid Noroozi","Varun Singh","Vijay Korthikanti","Vitaly Kurin","Wasi Uddin Ahmad","Wei Du","Wei Ping","Wenliang Dai","Wonmin Byeon","Xiaowei Ren","Yao Xu","Yejin Choi","Yian Zhang","Ying Lin","Yoshi Suhara","Zhiding Yu","Zhiqi Li","Zhiyu Li","Zhongbo Zhu","Zhuolin Yang","Zijia Chen"],"pdf_url":"https://arxiv.org/pdf/2504.03624v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11229v1","updated":"2025-04-15T14:30:18Z","published":"2025-04-15T14:30:18Z","title":"The Forward-Forward Algorithm: Characterizing Training Behavior","summary":"  The Forward-Forward algorithm is an alternative learning method which\nconsists of two forward passes rather than a forward and backward pass employed\nby backpropagation. Forward-Forward networks employ layer local loss functions\nwhich are optimized based on the layer activation for each forward pass rather\nthan a single global objective function. This work explores the dynamics of\nmodel and layer accuracy changes in Forward-Forward networks as training\nprogresses in pursuit of a mechanistic understanding of their internal\nbehavior. Treatments to various system characteristics are applied to\ninvestigate changes in layer and overall model accuracy as training progresses,\nhow accuracy is impacted by layer depth, and how strongly individual layer\naccuracy is correlated with overall model accuracy. The empirical results\npresented suggest that layers deeper within Forward-Forward networks experience\na delay in accuracy improvement relative to shallower layers and that shallower\nlayer accuracy is strongly correlated with overall model accuracy.\n","authors":["Reece Adamson"],"pdf_url":"https://arxiv.org/pdf/2504.11229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03369v2","updated":"2025-04-15T14:29:02Z","published":"2024-06-05T15:24:20Z","title":"Posterior and variational inference for deep neural networks with\n  heavy-tailed weights","summary":"  We consider deep neural networks in a Bayesian framework with a prior\ndistribution sampling the network weights at random. Following a recent idea of\nAgapiou and Castillo (2023), who show that heavy-tailed prior distributions\nachieve automatic adaptation to smoothness, we introduce a simple Bayesian deep\nlearning prior based on heavy-tailed weights and ReLU activation. We show that\nthe corresponding posterior distribution achieves near-optimal minimax\ncontraction rates, simultaneously adaptive to both intrinsic dimension and\nsmoothness of the underlying function, in a variety of contexts including\nnonparametric regression, geometric data and Besov spaces. While most works so\nfar need a form of model selection built-in within the prior distribution, a\nkey aspect of our approach is that it does not require to sample\nhyperparameters to learn the architecture of the network. We also provide\nvariational Bayes counterparts of the results, that show that mean-field\nvariational approximations still benefit from near-optimal theoretical support.\n","authors":["Ismaël Castillo","Paul Egels"],"pdf_url":"https://arxiv.org/pdf/2406.03369v2.pdf","comment":"58 pages"},{"id":"http://arxiv.org/abs/2504.11227v1","updated":"2025-04-15T14:28:48Z","published":"2025-04-15T14:28:48Z","title":"VEXP: A Low-Cost RISC-V ISA Extension for Accelerated Softmax\n  Computation in Transformers","summary":"  While Transformers are dominated by Floating-Point (FP)\nMatrix-Multiplications, their aggressive acceleration through dedicated\nhardware or many-core programmable systems has shifted the performance\nbottleneck to non-linear functions like Softmax. Accelerating Softmax is\nchallenging due to its non-pointwise, non-linear nature, with exponentiation as\nthe most demanding step. To address this, we design a custom arithmetic block\nfor Bfloat16 exponentiation leveraging a novel approximation algorithm based on\nSchraudolph's method, and we integrate it into the Floating-Point Unit (FPU) of\nthe RISC-V cores of a compute cluster, through custom Instruction Set\nArchitecture (ISA) extensions, with a negligible area overhead of 1\\%. By\noptimizing the software kernels to leverage the extension, we execute Softmax\nwith 162.7$\\times$ less latency and 74.3$\\times$ less energy compared to the\nbaseline cluster, achieving an 8.2$\\times$ performance improvement and\n4.1$\\times$ higher energy efficiency for the FlashAttention-2 kernel in GPT-2\nconfiguration. Moreover, the proposed approach enables a multi-cluster system\nto efficiently execute end-to-end inference of pre-trained Transformer models,\nsuch as GPT-2, GPT-3 and ViT, achieving up to 5.8$\\times$ and 3.6$\\times$\nreduction in latency and energy consumption, respectively, without requiring\nre-training and with negligible accuracy loss.\n","authors":["Run Wang","Gamze Islamoglu","Andrea Belano","Viviane Potocnik","Francesco Conti","Angelo Garofalo","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2504.11227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16033v2","updated":"2025-04-15T14:26:43Z","published":"2024-11-25T01:42:12Z","title":"Generative AI for Brane Configurations and Coamoeba","summary":"  We introduce a generative AI model to obtain Type IIB brane configurations\nthat realize toric phases of a family of 4d N=1 supersymmetric gauge theories.\nThese 4d N=1 quiver gauge theories are worldvolume theories of a D3-brane\nprobing a toric Calabi-Yau 3-fold. The Type IIB brane configurations are given\nby the coamoeba projection of the mirror curve associated with the toric\nCalabi-Yau 3-fold. The shape of the mirror curve and its coamoeba projection,\nas well as the corresponding Type IIB brane configuration and the toric phase\nof the 4d N=1 theory, all depend on the complex structure moduli parameterizing\nthe mirror curve. We train a generative AI model, a conditional variational\nautoencoder (CVAE), that takes a choice of complex structure moduli as input\nand generates the corresponding coamoeba. This enables us not only to obtain a\nhigh-resolution representation of the entire phase space for a family of 4d N=1\ntheories corresponding to the same toric Calabi-Yau 3-fold, but also to\ncontinuously track the movements of the mirror curve and the branes wrapping\nthe curve in the corresponding Type IIB brane configurations during phase\ntransitions associated with Seiberg duality.\n","authors":["Rak-Kyeong Seong"],"pdf_url":"https://arxiv.org/pdf/2411.16033v2.pdf","comment":"21 pages, 8 figures, 1 table. v2: published version"},{"id":"http://arxiv.org/abs/2504.11216v1","updated":"2025-04-15T14:20:42Z","published":"2025-04-15T14:20:42Z","title":"Diversity-Driven Learning: Tackling Spurious Correlations and Data\n  Heterogeneity in Federated Models","summary":"  Federated Learning (FL) enables decentralized training of machine learning\nmodels on distributed data while preserving privacy. However, in real-world FL\nsettings, client data is often non-identically distributed and imbalanced,\nresulting in statistical data heterogeneity which impacts the generalization\ncapabilities of the server's model across clients, slows convergence and\nreduces performance. In this paper, we address this challenge by first\nproposing a characterization of statistical data heterogeneity by means of 6\nmetrics of global and client attribute imbalance, class imbalance, and spurious\ncorrelations. Next, we create and share 7 computer vision datasets for binary\nand multiclass image classification tasks in Federated Learning that cover a\nbroad range of statistical data heterogeneity and hence simulate real-world\nsituations. Finally, we propose FedDiverse, a novel client selection algorithm\nin FL which is designed to manage and leverage data heterogeneity across\nclients by promoting collaboration between clients with complementary data\ndistributions. Experiments on the seven proposed FL datasets demonstrate\nFedDiverse's effectiveness in enhancing the performance and robustness of a\nvariety of FL methods while having low communication and computational\noverhead.\n","authors":["Gergely D. Németh","Eros Fanì","Yeat Jeng Ng","Barbara Caputo","Miguel Ángel Lozano","Nuria Oliver","Novi Quadrianto"],"pdf_url":"https://arxiv.org/pdf/2504.11216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11212v1","updated":"2025-04-15T14:13:54Z","published":"2025-04-15T14:13:54Z","title":"SDFs from Unoriented Point Clouds using Neural Variational Heat\n  Distances","summary":"  We propose a novel variational approach for computing neural Signed Distance\nFields (SDF) from unoriented point clouds. To this end, we replace the commonly\nused eikonal equation with the heat method, carrying over to the neural domain\nwhat has long been standard practice for computing distances on discrete\nsurfaces. This yields two convex optimization problems for whose solution we\nemploy neural networks: We first compute a neural approximation of the\ngradients of the unsigned distance field through a small time step of heat flow\nwith weighted point cloud densities as initial data. Then we use it to compute\na neural approximation of the SDF. We prove that the underlying variational\nproblems are well-posed. Through numerical experiments, we demonstrate that our\nmethod provides state-of-the-art surface reconstruction and consistent SDF\ngradients. Furthermore, we show in a proof-of-concept that it is accurate\nenough for solving a PDE on the zero-level set.\n","authors":["Samuel Weidemaier","Florine Hartwig","Josua Sassen","Sergio Conti","Mirela Ben-Chen","Martin Rumpf"],"pdf_url":"https://arxiv.org/pdf/2504.11212v1.pdf","comment":"14 pages, 16 figures, 4 tables"},{"id":"http://arxiv.org/abs/2504.04383v2","updated":"2025-04-15T14:07:31Z","published":"2025-04-06T06:23:27Z","title":"Retro-Search: Exploring Untaken Paths for Deeper and Efficient Reasoning","summary":"  Large reasoning models exhibit remarkable reasoning capabilities via long,\nelaborate reasoning trajectories. Supervised fine-tuning on such reasoning\ntraces, also known as distillation, can be a cost-effective way to boost\nreasoning capabilities of student models. However, empirical observations\nreveal that these reasoning trajectories are often suboptimal, switching\nexcessively between different lines of thought, resulting in under-thinking,\nover-thinking, and even degenerate responses. We introduce Retro-Search, an\nMCTS-inspired search algorithm, for distilling higher quality reasoning paths\nfrom large reasoning models. Retro-Search retrospectively revises reasoning\npaths to discover better, yet shorter traces, which can then lead to student\nmodels with enhanced reasoning capabilities with shorter, thus faster\ninference. Our approach can enable two use cases: self-improvement, where\nmodels are fine-tuned on their own Retro-Search-ed thought traces, and\nweak-to-strong improvement, where a weaker model revises stronger model's\nthought traces via Retro-Search. For self-improving, R1-distill-7B, fine-tuned\non its own Retro-Search-ed traces, reduces the average reasoning length by\n31.2% while improving performance by 7.7% across seven math benchmarks. For\nweak-to-strong improvement, we retrospectively revise R1-671B's traces from the\nOpenThoughts dataset using R1-distill-32B as the Retro-Search-er, a model 20x\nsmaller. Qwen2.5-32B, fine-tuned on this refined data, achieves performance\ncomparable to R1-distill-32B, yielding an 11.3% reduction in reasoning length\nand a 2.4% performance improvement compared to fine-tuning on the original\nOpenThoughts data. Our work counters recently emergent viewpoints that question\nthe relevance of search algorithms in the era of large reasoning models, by\ndemonstrating that there are still opportunities for algorithmic advancements,\neven for frontier models.\n","authors":["Ximing Lu","Seungju Han","David Acuna","Hyunwoo Kim","Jaehun Jung","Shrimai Prabhumoye","Niklas Muennighoff","Mostofa Patwary","Mohammad Shoeybi","Bryan Catanzaro","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2504.04383v2.pdf","comment":"Code and data will be publicly released upon internal approval"},{"id":"http://arxiv.org/abs/2409.09441v3","updated":"2025-04-15T13:55:47Z","published":"2024-09-14T13:51:37Z","title":"PIP-Loco: A Proprioceptive Infinite Horizon Planning Framework for\n  Quadrupedal Robot Locomotion","summary":"  A core strength of Model Predictive Control (MPC) for quadrupedal locomotion\nhas been its ability to enforce constraints and provide interpretability of the\nsequence of commands over the horizon. However, despite being able to plan, MPC\nstruggles to scale with task complexity, often failing to achieve robust\nbehavior on rapidly changing surfaces. On the other hand, model-free\nReinforcement Learning (RL) methods have outperformed MPC on multiple terrains,\nshowing emergent motions but inherently lack any ability to handle constraints\nor perform planning. To address these limitations, we propose a framework that\nintegrates proprioceptive planning with RL, allowing for agile and safe\nlocomotion behaviors through the horizon. Inspired by MPC, we incorporate an\ninternal model that includes a velocity estimator and a Dreamer module. During\ntraining, the framework learns an expert policy and an internal model that are\nco-dependent, facilitating exploration for improved locomotion behaviors.\nDuring deployment, the Dreamer module solves an infinite-horizon MPC problem,\nadapting actions and velocity commands to respect the constraints. We validate\nthe robustness of our training framework through ablation studies on internal\nmodel components and demonstrate improved robustness to training noise.\nFinally, we evaluate our approach across multi-terrain scenarios in both\nsimulation and hardware.\n","authors":["Aditya Shirwatkar","Naman Saxena","Kishore Chandra","Shishir Kolathaya"],"pdf_url":"https://arxiv.org/pdf/2409.09441v3.pdf","comment":"Accepted at IEEE International Conference on Robotics and Automation\n  (ICRA) 2025"},{"id":"http://arxiv.org/abs/2504.11197v1","updated":"2025-04-15T13:53:08Z","published":"2025-04-15T13:53:08Z","title":"Efficient Distributed Retrieval-Augmented Generation for Enhancing\n  Language Model Performance","summary":"  Small language models (SLMs) support efficient deployments on\nresource-constrained edge devices, but their limited capacity compromises\ninference performance. Retrieval-augmented generation (RAG) is a promising\nsolution to enhance model performance by integrating external databases,\nwithout requiring intensive on-device model retraining. However, large-scale\npublic databases and user-specific private contextual documents are typically\nlocated on the cloud and the device separately, while existing RAG\nimplementations are primarily centralized. To bridge this gap, we propose\nDRAGON, a distributed RAG framework to enhance on-device SLMs through both\ngeneral and personal knowledge without the risk of leaking document privacy.\nSpecifically, DRAGON decomposes multi-document RAG into multiple parallel token\ngeneration processes performed independently and locally on the cloud and the\ndevice, and employs a newly designed Speculative Aggregation, a dual-side\nspeculative algorithm to avoid frequent output synchronization between the\ncloud and device. A new scheduling algorithm is further introduced to identify\nthe optimal aggregation side based on real-time network conditions. Evaluations\non real-world hardware testbed demonstrate a significant performance\nimprovement of DRAGON-up to 1.9x greater gains over standalone SLM compared to\nthe centralized RAG, substantial reduction in per-token latency, and negligible\nTime to First Token (TTFT) overhead.\n","authors":["Shangyu Liu","Zhenzhe Zheng","Xiaoyao Huang","Fan Wu","Jie Wu"],"pdf_url":"https://arxiv.org/pdf/2504.11197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11195v1","updated":"2025-04-15T13:49:31Z","published":"2025-04-15T13:49:31Z","title":"R-TPT: Improving Adversarial Robustness of Vision-Language Models\n  through Test-Time Prompt Tuning","summary":"  Vision-language models (VLMs), such as CLIP, have gained significant\npopularity as foundation models, with numerous fine-tuning methods developed to\nenhance performance on downstream tasks. However, due to their inherent\nvulnerability and the common practice of selecting from a limited set of\nopen-source models, VLMs suffer from a higher risk of adversarial attacks than\ntraditional vision models. Existing defense techniques typically rely on\nadversarial fine-tuning during training, which requires labeled data and lacks\nof flexibility for downstream tasks. To address these limitations, we propose\nrobust test-time prompt tuning (R-TPT), which mitigates the impact of\nadversarial attacks during the inference stage. We first reformulate the\nclassic marginal entropy objective by eliminating the term that introduces\nconflicts under adversarial conditions, retaining only the pointwise entropy\nminimization. Furthermore, we introduce a plug-and-play reliability-based\nweighted ensembling strategy, which aggregates useful information from reliable\naugmented views to strengthen the defense. R-TPT enhances defense against\nadversarial attacks without requiring labeled training data while offering high\nflexibility for inference tasks. Extensive experiments on widely used\nbenchmarks with various attacks demonstrate the effectiveness of R-TPT. The\ncode is available in https://github.com/TomSheng21/R-TPT.\n","authors":["Lijun Sheng","Jian Liang","Zilei Wang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2504.11195v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2304.03807v5","updated":"2025-04-15T13:44:39Z","published":"2023-04-07T18:21:30Z","title":"Privacy-Preserving CNN Training with Transfer Learning: Multiclass\n  Logistic Regression","summary":"  In this paper, we present a practical solution to implement\nprivacy-preserving CNN training based on mere Homomorphic Encryption (HE)\ntechnique. To our best knowledge, this is the first attempt successfully to\ncrack this nut and no work ever before has achieved this goal. Several\ntechniques combine to accomplish the task:: (1) with transfer learning,\nprivacy-preserving CNN training can be reduced to homomorphic neural network\ntraining, or even multiclass logistic regression (MLR) training; (2) via a\nfaster gradient variant called $\\texttt{Quadratic Gradient}$, an enhanced\ngradient method for MLR with a state-of-the-art performance in convergence\nspeed is applied in this work to achieve high performance; (3) we employ the\nthought of transformation in mathematics to transform approximating Softmax\nfunction in the encryption domain to the approximation of the Sigmoid function.\nA new type of loss function termed $\\texttt{Squared Likelihood Error}$ has been\ndeveloped alongside to align with this change.; and (4) we use a simple but\nflexible matrix-encoding method named $\\texttt{Volley Revolver}$ to manage the\ndata flow in the ciphertexts, which is the key factor to complete the whole\nhomomorphic CNN training. The complete, runnable C++ code to implement our work\ncan be found at:\n\\href{https://github.com/petitioner/HE.CNNtraining}{$\\texttt{https://github.com/petitioner/HE.CNNtraining}$}.\nWe select $\\texttt{REGNET\\_X\\_400MF}$ as our pre-trained model for transfer\nlearning. We use the first 128 MNIST training images as training data and the\nwhole MNIST testing dataset as the testing data. The client only needs to\nupload 6 ciphertexts to the cloud and it takes $\\sim 21$ mins to perform 2\niterations on a cloud with 64 vCPUs, resulting in a precision of $21.49\\%$.\n","authors":["John Chiang"],"pdf_url":"https://arxiv.org/pdf/2304.03807v5.pdf","comment":"In this work, we initiated to implement privacy-persevering CNN\n  training based on mere HE techniques by presenting a faster HE-friendly\n  algorithm"},{"id":"http://arxiv.org/abs/2308.09531v3","updated":"2025-04-15T13:34:10Z","published":"2023-08-18T13:11:23Z","title":"Privacy-Preserving 3-Layer Neural Network Training","summary":"  In this manuscript, we consider the problem of privacy-preserving training of\nneural networks in the mere homomorphic encryption setting. We combine several\nexsiting techniques available, extend some of them, and finally enable the\ntraining of 3-layer neural networks for both the regression and classification\nproblems using mere homomorphic encryption technique.\n","authors":["John Chiang"],"pdf_url":"https://arxiv.org/pdf/2308.09531v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11170v1","updated":"2025-04-15T13:17:14Z","published":"2025-04-15T13:17:14Z","title":"A Real-time Anomaly Detection Method for Robots based on a Flexible and\n  Sparse Latent Space","summary":"  The growing demand for robots to operate effectively in diverse environments\nnecessitates the need for robust real-time anomaly detection techniques during\nrobotic operations. However, deep learning-based models in robotics face\nsignificant challenges due to limited training data and highly noisy signal\nfeatures. In this paper, we present Sparse Masked Autoregressive Flow-based\nAdversarial AutoEncoders model to address these problems. This approach\nintegrates Masked Autoregressive Flow model into Adversarial AutoEncoders to\nconstruct a flexible latent space and utilize Sparse autoencoder to efficiently\nfocus on important features, even in scenarios with limited feature space. Our\nexperiments demonstrate that the proposed model achieves a 4.96% to 9.75%\nhigher area under the receiver operating characteristic curve for\npick-and-place robotic operations with randomly placed cans, compared to\nexisting state-of-the-art methods. Notably, it showed up to 19.67% better\nperformance in scenarios involving collisions with lightweight objects.\nAdditionally, unlike the existing state-of-the-art model, our model performs\ninferences within 1 millisecond, ensuring real-time anomaly detection. These\ncapabilities make our model highly applicable to machine learning-based robotic\nsafety systems in dynamic environments. The code will be made publicly\navailable after acceptance.\n","authors":["Taewook Kang","Bum-Jae You","Juyoun Park","Yisoo Lee"],"pdf_url":"https://arxiv.org/pdf/2504.11170v1.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2504.11168v1","updated":"2025-04-15T13:16:02Z","published":"2025-04-15T13:16:02Z","title":"Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails","summary":"  Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems.\n","authors":["William Hackett","Lewis Birch","Stefan Trawicki","Neeraj Suri","Peter Garraghan"],"pdf_url":"https://arxiv.org/pdf/2504.11168v1.pdf","comment":"12 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2308.09013v2","updated":"2025-04-15T13:05:54Z","published":"2023-08-17T14:37:35Z","title":"Deep-seeded Clustering for Emotion Recognition from Wearable\n  Physiological Sensors","summary":"  According to the circumplex model of affect, an emotional response could\ncharacterized by a level of pleasure (valence) and intensity (arousal). As it\nreflects on the autonomic nervous system (ANS) activity, modern wearable\nwristbands can record non-invasively and during our everyday lives peripheral\nend-points of this response. While emotion recognition from physiological\nsignals is usually achieved using supervised machine learning algorithms that\nrequire ground truth labels for training, collecting it is cumbersome and\nparticularly unfeasible in naturalistic settings, and extracting meaningful\ninsights from these signals requires domain knowledge and might be prone to\nbias. Here, we propose and test a deep-seeded clustering algorithm that\nautomatically extracts and classifies features from those physiological signals\nwith minimal supervision - combining an autoencoder (AE) for unsupervised\nfeature representation and c-means clustering for fine-grained classification.\nWe also show that the model obtains good performance results across three\ndifferent datasets frequently used in affective computing studies (accuracies\nof 80.7% on WESAD, 64.2% on Stress-Predict and 61.0% on CEAP360-VR).\n","authors":["Marta A. Conceição","Antoine Dubois","Sonja Haustein","Bruno Miranda","Carlos Lima Azevedo"],"pdf_url":"https://arxiv.org/pdf/2308.09013v2.pdf","comment":"14 pages, 15 figure, 4 tables"},{"id":"http://arxiv.org/abs/2502.14914v2","updated":"2025-04-15T12:58:38Z","published":"2025-02-19T07:55:51Z","title":"What Is a Good Caption? A Comprehensive Visual Caption Benchmark for\n  Evaluating Both Correctness and Thoroughness","summary":"  Visual captioning benchmarks have become outdated with the emergence of\nmodern multimodal large language models (MLLMs), as the brief ground-truth\nsentences and traditional metrics fail to assess detailed captions effectively.\nWhile recent benchmarks attempt to address this by focusing on keyword\nextraction or object-centric evaluation, they remain limited to vague-view or\nobject-view analyses and incomplete visual element coverage. In this paper, we\nintroduce CAPability, a comprehensive multi-view benchmark for evaluating\nvisual captioning across 12 dimensions spanning six critical views. We curate\nnearly 11K human-annotated images and videos with visual element annotations to\nevaluate the generated captions. CAPability stably assesses both the\ncorrectness and thoroughness of captions using F1-score. By converting\nannotations to QA pairs, we further introduce a heuristic metric, \\textit{know\nbut cannot tell} ($K\\bar{T}$), indicating a significant performance gap between\nQA and caption capabilities. Our work provides the first holistic analysis of\nMLLMs' captioning abilities, as we identify their strengths and weaknesses\nacross various dimensions, guiding future research to enhance specific aspects\nof capabilities.\n","authors":["Zhihang Liu","Chen-Wei Xie","Bin Wen","Feiwu Yu","Jixuan Chen","Boqiang Zhang","Nianzu Yang","Pandeng Li","Yinglu Li","Zuan Gao","Yun Zheng","Hongtao Xie"],"pdf_url":"https://arxiv.org/pdf/2502.14914v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11150v1","updated":"2025-04-15T12:53:07Z","published":"2025-04-15T12:53:07Z","title":"GC-GAT: Multimodal Vehicular Trajectory Prediction using Graph Goal\n  Conditioning and Cross-context Attention","summary":"  Predicting future trajectories of surrounding vehicles heavily relies on what\ncontextual information is given to a motion prediction model. The context\nitself can be static (lanes, regulatory elements, etc) or dynamic (traffic\nparticipants). This paper presents a lane graph-based motion prediction model\nthat first predicts graph-based goal proposals and later fuses them with cross\nattention over multiple contextual elements. We follow the famous\nencoder-interactor-decoder architecture where the encoder encodes scene context\nusing lightweight Gated Recurrent Units, the interactor applies cross-context\nattention over encoded scene features and graph goal proposals, and the decoder\nregresses multimodal trajectories via Laplacian Mixture Density Network from\nthe aggregated encodings. Using cross-attention over graph-based goal proposals\ngives robust trajectory estimates since the model learns to attend to future\ngoal-relevant scene elements for the intended agent. We evaluate our work on\nnuScenes motion prediction dataset, achieving state-of-the-art results.\n","authors":["Mahir Gulzar","Yar Muhammad","Naveed Muhammad"],"pdf_url":"https://arxiv.org/pdf/2504.11150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11130v1","updated":"2025-04-15T12:30:21Z","published":"2025-04-15T12:30:21Z","title":"Divergence of Empirical Neural Tangent Kernel in Classification Problems","summary":"  This paper demonstrates that in classification problems, fully connected\nneural networks (FCNs) and residual neural networks (ResNets) cannot be\napproximated by kernel logistic regression based on the Neural Tangent Kernel\n(NTK) under overtraining (i.e., when training time approaches infinity).\nSpecifically, when using the cross-entropy loss, regardless of how large the\nnetwork width is (as long as it is finite), the empirical NTK diverges from the\nNTK on the training samples as training time increases. To establish this\nresult, we first demonstrate the strictly positive definiteness of the NTKs for\nmulti-layer FCNs and ResNets. Then, we prove that during training, % with the\ncross-entropy loss, the neural network parameters diverge if the smallest\neigenvalue of the empirical NTK matrix (Gram matrix) with respect to training\nsamples is bounded below by a positive constant. This behavior contrasts\nsharply with the lazy training regime commonly observed in regression problems.\nConsequently, using a proof by contradiction, we show that the empirical NTK\ndoes not uniformly converge to the NTK across all times on the training samples\nas the network width increases. We validate our theoretical results through\nexperiments on both synthetic data and the MNIST classification task. This\nfinding implies that NTK theory is not applicable in this context, with\nsignificant theoretical implications for understanding neural networks in\nclassification problems.\n","authors":["Zixiong Yu","Songtao Tian","Guhan Chen"],"pdf_url":"https://arxiv.org/pdf/2504.11130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17758v2","updated":"2025-04-15T12:28:35Z","published":"2024-10-23T10:50:07Z","title":"A Neural Network Alternative to Tree-based Models","summary":"  Tabular datasets are widely used in scientific disciplines such as biology.\nWhile these disciplines have already adopted AI methods to enhance their\nfindings and analysis, they mainly use tree-based methods due to their\ninterpretability. At the same time, artificial neural networks have been shown\nto offer superior flexibility and depth for rich and complex non-tabular\nproblems, but they are falling behind tree-based models for tabular data in\nterms of performance and interpretability. Although sparsity has been shown to\nimprove the interpretability and performance of ANN models for complex\nnon-tabular datasets, enforcing sparsity structurally and formatively for\ntabular data before training the model, remains an open question. To address\nthis question, we establish a method that infuses sparsity in neural networks\nby utilising attention mechanisms to capture the features' importance in\ntabular datasets. We show that our models, Sparse TABular NET or sTAB-Net with\nattention mechanisms, are more effective than tree-based models, reaching the\nstate-of-the-art on biological datasets. They further permit the extraction of\ninsights from these datasets and achieve better performance than post-hoc\nmethods like SHAP.\n","authors":["Salvatore Raieli","Nathalie Jeanray","Stéphane Gerart","Sebastien Vachenc","Abdulrahman Altahhan"],"pdf_url":"https://arxiv.org/pdf/2410.17758v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11128v1","updated":"2025-04-15T12:25:42Z","published":"2025-04-15T12:25:42Z","title":"K-means Enhanced Density Gradient Analysis for Urban and Transport\n  Metrics Using Multi-Modal Satellite Imagery","summary":"  This paper presents a novel computational approach for evaluating urban\nmetrics through density gradient analysis using multi-modal satellite imagery,\nwith applications including public transport and other urban systems. By\ncombining optical and Synthetic Aperture Radar (SAR) data, we develop a method\nto segment urban areas, identify urban centers, and quantify density gradients.\nOur approach calculates two key metrics: the density gradient coefficient\n($\\alpha$) and the minimum effective distance (LD) at which density reaches a\ntarget threshold. We further employ machine learning techniques, specifically\nK-means clustering, to objectively identify uniform and high-variability\nregions within density gradient plots. We demonstrate that these metrics\nprovide an effective screening tool for public transport analyses by revealing\nthe underlying urban structure. Through comparative analysis of two\nrepresentative cities with contrasting urban morphologies (monocentric vs\npolycentric), we establish relationships between density gradient\ncharacteristics and public transport network topologies. Cities with clear\ndensity peaks in their gradient plots indicate distinct urban centers requiring\ndifferent transport strategies than those with more uniform density\ndistributions. This methodology offers urban planners a cost-effective,\nglobally applicable approach to preliminary public transport assessment using\nfreely available satellite data. The complete implementation, with additional\nexamples and documentation, is available in an open-source repository under the\nMIT license at https://github.com/nexri/Satellite-Imagery-Urban-Analysis.\n","authors":["P. Tomkiewicz","J. Jaworski","P. Zielonka","A. Wilinski"],"pdf_url":"https://arxiv.org/pdf/2504.11128v1.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.14719v3","updated":"2025-04-15T12:11:11Z","published":"2024-12-19T10:41:24Z","title":"Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition","summary":"  Micro-Action Recognition (MAR) has gained increasing attention due to its\ncrucial role as a form of non-verbal communication in social interactions, with\npromising potential for applications in human communication and emotion\nanalysis. However, current approaches often overlook the inherent ambiguity in\nmicro-actions, which arises from the wide category range and subtle visual\ndifferences between categories. This oversight hampers the accuracy of\nmicro-action recognition. In this paper, we propose a novel Prototypical\nCalibrating Ambiguous Network (PCAN) to unleash and mitigate the ambiguity of\nMAR. Firstly, we employ a hierarchical action-tree to identify the ambiguous\nsample, categorizing them into distinct sets of ambiguous samples of false\nnegatives and false positives, considering both body- and action-level\ncategories. Secondly, we implement an ambiguous contrastive refinement module\nto calibrate these ambiguous samples by regulating the distance between\nambiguous samples and their corresponding prototypes. This calibration process\naims to pull false negative (FN) samples closer to their respective prototypes\nand push false positive (FP) samples apart from their affiliated prototypes. In\naddition, we propose a new prototypical diversity amplification loss to\nstrengthen the model's capacity by amplifying the differences between different\nprototypes. Finally, we propose a prototype-guided rectification to rectify\nprediction by incorporating the representability of prototypes. Extensive\nexperiments conducted on the benchmark dataset demonstrate the superior\nperformance of our method compared to existing approaches. The code is\navailable at https://github.com/kunli-cs/PCAN.\n","authors":["Kun Li","Dan Guo","Guoliang Chen","Chunxiao Fan","Jingyuan Xu","Zhiliang Wu","Hehe Fan","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14719v3.pdf","comment":"Fix typos; Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2504.11118v1","updated":"2025-04-15T12:07:14Z","published":"2025-04-15T12:07:14Z","title":"Revealing Covert Attention by Analyzing Human and Reinforcement Learning\n  Agent Gameplay","summary":"  This study introduces a novel method for revealing human covert attention\npatterns using gameplay data alone, utilizing offline attention techniques from\nreinforcement learning (RL). We propose the contextualized, task-relevant (CTR)\nattention network, which generates attention maps from both human and RL agent\ngameplay in Atari environments. These maps are sparse yet retain the necessary\ninformation for the current player's decision making. We compare the\nCTR-derived attention maps with a temporally integrated overt attention (TIOA)\nmodel based on eye-tracking data, serving as a point of comparison and\ndiscussion. Visual inspection reveals distinct attention patterns: human CTR\nmaps focus on the player and rather nearby opponents, occasionally shifting\nbetween stronger focus and broader views - sometimes even attending to empty\nspace ahead. In contrast, agent maps maintain a consistent broad focus on most\nobjects, including distant ones and the player. Quantitative analysis further\ndemonstrates that human CTR maps align more closely with TIOA than agent maps\ndo. Our findings indicate that the CTR attention network can effectively reveal\nhuman covert attention patterns from gameplay alone, without the need for\nadditional data like brain activity recordings. This work contributes to\nunderstanding human-agent attention differences and enables the development of\nRL agents augmented with human covert attention.\n","authors":["Henrik Krauss","Takehisa Yairi"],"pdf_url":"https://arxiv.org/pdf/2504.11118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18370v3","updated":"2025-04-15T11:43:49Z","published":"2024-12-24T11:53:24Z","title":"Unveiling the Threat of Fraud Gangs to Graph Neural Networks:\n  Multi-Target Graph Injection Attacks Against GNN-Based Fraud Detectors","summary":"  Graph neural networks (GNNs) have emerged as an effective tool for fraud\ndetection, identifying fraudulent users, and uncovering malicious behaviors.\nHowever, attacks against GNN-based fraud detectors and their risks have rarely\nbeen studied, thereby leaving potential threats unaddressed. Recent findings\nsuggest that frauds are increasingly organized as gangs or groups. In this\nwork, we design attack scenarios where fraud gangs aim to make their fraud\nnodes misclassified as benign by camouflaging their illicit activities in\ncollusion. Based on these scenarios, we study adversarial attacks against\nGNN-based fraud detectors by simulating attacks of fraud gangs in three\nreal-world fraud cases: spam reviews, fake news, and medical insurance frauds.\nWe define these attacks as multi-target graph injection attacks and propose\nMonTi, a transformer-based Multi-target one-Time graph injection attack model.\nMonTi simultaneously generates attributes and edges of all attack nodes with a\ntransformer encoder, capturing interdependencies between attributes and edges\nmore effectively than most existing graph injection attack methods that\ngenerate these elements sequentially. Additionally, MonTi adaptively allocates\nthe degree budget for each attack node to explore diverse injection structures\ninvolving target, candidate, and attack nodes, unlike existing methods that fix\nthe degree budget across all attack nodes. Experiments show that MonTi\noutperforms the state-of-the-art graph injection attack methods on five\nreal-world graphs.\n","authors":["Jinhyeok Choi","Heehyeon Kim","Joyce Jiyoung Whang"],"pdf_url":"https://arxiv.org/pdf/2412.18370v3.pdf","comment":"19 pages, 5 figures, 12 tables, The 39th AAAI Conference on\n  Artificial Intelligence (AAAI 2025)"},{"id":"http://arxiv.org/abs/2504.11091v1","updated":"2025-04-15T11:36:27Z","published":"2025-04-15T11:36:27Z","title":"AI-guided Antibiotic Discovery Pipeline from Target Selection to\n  Compound Identification","summary":"  Antibiotic resistance presents a growing global health crisis, demanding new\ntherapeutic strategies that target novel bacterial mechanisms. Recent advances\nin protein structure prediction and machine learning-driven molecule generation\noffer a promising opportunity to accelerate drug discovery. However, practical\nguidance on selecting and integrating these models into real-world pipelines\nremains limited. In this study, we develop an end-to-end, artificial\nintelligence-guided antibiotic discovery pipeline that spans target\nidentification to compound realization. We leverage structure-based clustering\nacross predicted proteomes of multiple pathogens to identify conserved,\nessential, and non-human-homologous targets. We then systematically evaluate\nsix leading 3D-structure-aware generative models$\\unicode{x2014}$spanning\ndiffusion, autoregressive, graph neural network, and language model\narchitectures$\\unicode{x2014}$on their usability, chemical validity, and\nbiological relevance. Rigorous post-processing filters and commercial analogue\nsearches reduce over 100 000 generated compounds to a focused, synthesizable\nset. Our results highlight DeepBlock and TamGen as top performers across\ndiverse criteria, while also revealing critical trade-offs between model\ncomplexity, usability, and output quality. This work provides a comparative\nbenchmark and blueprint for deploying artificial intelligence in early-stage\nantibiotic development.\n","authors":["Maximilian G. Schuh","Joshua Hesse","Stephan A. Sieber"],"pdf_url":"https://arxiv.org/pdf/2504.11091v1.pdf","comment":"12 pages, preprint"},{"id":"http://arxiv.org/abs/2504.11089v1","updated":"2025-04-15T11:34:03Z","published":"2025-04-15T11:34:03Z","title":"InfoClus: Informative Clustering of High-dimensional Data Embeddings","summary":"  Developing an understanding of high-dimensional data can be facilitated by\nvisualizing that data using dimensionality reduction. However, the\nlow-dimensional embeddings are often difficult to interpret. To facilitate the\nexploration and interpretation of low-dimensional embeddings, we introduce a\nnew concept named partitioning with explanations. The idea is to partition the\ndata shown through the embedding into groups, each of which is given a sparse\nexplanation using the original high-dimensional attributes. We introduce an\nobjective function that quantifies how much we can learn through observing the\nexplanations of the data partitioning, using information theory, and also how\ncomplex the explanations are. Through parameterization of the complexity, we\ncan tune the solutions towards the desired granularity. We propose InfoClus,\nwhich optimizes the partitioning and explanations jointly, through greedy\nsearch constrained over a hierarchical clustering. We conduct a qualitative and\nquantitative analysis of InfoClus on three data sets. We contrast the results\non the Cytometry data with published manual analysis results, and compare with\ntwo other recent methods for explaining embeddings (RVX and VERA). These\ncomparisons highlight that InfoClus has distinct advantages over existing\nprocedures and methods. We find that InfoClus can automatically create good\nstarting points for the analysis of dimensionality-reduction-based scatter\nplots.\n","authors":["Fuyin Lai","Edith Heiter","Guillaume Bied","Jefrey Lijffijt"],"pdf_url":"https://arxiv.org/pdf/2504.11089v1.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2504.11085v1","updated":"2025-04-15T11:31:17Z","published":"2025-04-15T11:31:17Z","title":"TD-Suite: All Batteries Included Framework for Technical Debt\n  Classification","summary":"  Recognizing that technical debt is a persistent and significant challenge\nrequiring sophisticated management tools, TD-Suite offers a comprehensive\nsoftware framework specifically engineered to automate the complex task of its\nclassification within software projects. It leverages the advanced natural\nlanguage understanding of state-of-the-art transformer models to analyze\ntextual artifacts, such as developer discussions in issue reports, where subtle\nindicators of debt often lie hidden.\n  TD-Suite provides a seamless end-to-end pipeline, managing everything from\ninitial data ingestion and rigorous preprocessing to model training, thorough\nevaluation, and final inference. This allows it to support both straightforward\nbinary classification (debt or no debt) and more valuable, identifying specific\ncategories like code, design, or documentation debt, thus enabling more\ntargeted management strategies.\n  To ensure the generated models are robust and perform reliably on real-world,\noften imbalanced, datasets, TD-Suite incorporates critical training\nmethodologies: k-fold cross-validation assesses generalization capability,\nearly stopping mechanisms prevent overfitting to the training data, and class\nweighting strategies effectively address skewed data distributions. Beyond core\nfunctionality, and acknowledging the growing importance of sustainability, the\nframework integrates tracking and reporting of carbon emissions associated with\nthe computationally intensive model training process.\n  It also features a user-friendly Gradio web interface in a Docker container\nsetup, simplifying model interaction, evaluation, and inference.\n","authors":["Karthik Shivashankar","Antonio Martini"],"pdf_url":"https://arxiv.org/pdf/2504.11085v1.pdf","comment":"In submission"},{"id":"http://arxiv.org/abs/2504.11079v1","updated":"2025-04-15T11:24:43Z","published":"2025-04-15T11:24:43Z","title":"Scalability and Maintainability Challenges and Solutions in Machine\n  Learning: Systematic Literature Review","summary":"  This systematic literature review examines the critical challenges and\nsolutions related to scalability and maintainability in Machine Learning (ML)\nsystems. As ML applications become increasingly complex and widespread across\nindustries, the need to balance system scalability with long-term\nmaintainability has emerged as a significant concern. This review synthesizes\ncurrent research and practices addressing these dual challenges across the\nentire ML life-cycle, from data engineering to model deployment in production.\nWe analyzed 124 papers to identify and categorize 41 maintainability challenges\nand 13 scalability challenges, along with their corresponding solutions. Our\nfindings reveal intricate inter dependencies between scalability and\nmaintainability, where improvements in one often impact the other.\n  The review is structured around six primary research questions, examining\nmaintainability and scalability challenges in data engineering, model\nengineering, and ML system development. We explore how these challenges\nmanifest differently across various stages of the ML life-cycle.\n  This comprehensive overview offers valuable insights for both researchers and\npractitioners in the field of ML systems. It aims to guide future research\ndirections, inform best practices, and contribute to the development of more\nrobust, efficient, and sustainable ML applications across various domains.\n","authors":["Karthik Shivashankar","Ghadi S. Al Hajj","Antonio Martini"],"pdf_url":"https://arxiv.org/pdf/2504.11079v1.pdf","comment":"Minor Revision ACM Computing Survey"},{"id":"http://arxiv.org/abs/2504.11076v1","updated":"2025-04-15T11:21:37Z","published":"2025-04-15T11:21:37Z","title":"Using Time Structure to Estimate Causal Effects","summary":"  There exist several approaches for estimating causal effects in time series\nwhen latent confounding is present. Many of these approaches rely on additional\nauxiliary observed variables or time series such as instruments, negative\ncontrols or time series that satisfy the front- or backdoor criterion in\ncertain graphs. In this paper, we present a novel approach for estimating\ndirect (and via Wright's path rule total) causal effects in a time series setup\nwhich does not rely on additional auxiliary observed variables or time series.\nThis approach assumes that the underlying time series is a Structural Vector\nAutoregressive (SVAR) process and estimates direct causal effects by solving\ncertain linear equation systems made up of different covariances and model\nparameters. We state sufficient graphical criteria in terms of the so-called\nfull time graph under which these linear equations systems are uniquely\nsolvable and under which their solutions contain the to-be-identified direct\ncausal effects as components. We also state sufficient lag-based criteria under\nwhich the previously mentioned graphical conditions are satisfied and, thus,\nunder which direct causal effects are identifiable. Several numerical\nexperiments underline the correctness and applicability of our results.\n","authors":["Tom Hochsprung","Jakob Runge","Andreas Gerhardus"],"pdf_url":"https://arxiv.org/pdf/2504.11076v1.pdf","comment":"25 pages main paper, 25 pages Appendix, 50 pages in total, 3 tables,\n  7 figures"},{"id":"http://arxiv.org/abs/2504.11074v1","updated":"2025-04-15T11:16:13Z","published":"2025-04-15T11:16:13Z","title":"Dynamical errors in machine learning forecasts","summary":"  In machine learning forecasting, standard error metrics such as mean absolute\nerror (MAE) and mean squared error (MSE) quantify discrepancies between\npredictions and target values. However, these metrics do not directly evaluate\nthe physical and/or dynamical consistency of forecasts, an increasingly\ncritical concern in scientific and engineering applications.\n  Indeed, a fundamental yet often overlooked question is whether machine\nlearning forecasts preserve the dynamical behavior of the underlying system.\nAddressing this issue is essential for assessing the fidelity of machine\nlearning models and identifying potential failure modes, particularly in\napplications where maintaining correct dynamical behavior is crucial.\n  In this work, we investigate the relationship between standard forecasting\nerror metrics, such as MAE and MSE, and the dynamical properties of the\nunderlying system. To achieve this goal, we use two recently developed\ndynamical indices: the instantaneous dimension ($d$), and the inverse\npersistence ($\\theta$). Our results indicate that larger forecast errors --\ne.g., higher MSE -- tend to occur in states with higher $d$ (higher complexity)\nand higher $\\theta$ (lower persistence). To further assess dynamical\nconsistency, we propose error metrics based on the dynamical indices that\nmeasure the discrepancy of the forecasted $d$ and $\\theta$ versus their correct\nvalues. Leveraging these dynamical indices-based metrics, we analyze direct and\nrecursive forecasting strategies for three canonical datasets -- Lorenz,\nKuramoto-Sivashinsky equation, and Kolmogorov flow -- as well as a real-world\nweather forecasting task. Our findings reveal substantial distortions in\ndynamical properties in ML forecasts, especially for long forecast lead times\nor long recursive simulations, providing complementary information on ML\nforecast fidelity that can be used to improve ML models.\n","authors":["Zhou Fang","Gianmarco Mengaldo"],"pdf_url":"https://arxiv.org/pdf/2504.11074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11067v1","updated":"2025-04-15T11:02:34Z","published":"2025-04-15T11:02:34Z","title":"Morphing-based Compression for Data-centric ML Pipelines","summary":"  Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours.\n","authors":["Sebastian Baunsgaard","Matthias Boehm"],"pdf_url":"https://arxiv.org/pdf/2504.11067v1.pdf","comment":"20 pages, 28 figures, 4 tables"},{"id":"http://arxiv.org/abs/2411.13914v4","updated":"2025-04-15T10:42:55Z","published":"2024-11-21T07:57:59Z","title":"ICODE: Modeling Dynamical Systems with Extrinsic Input Information","summary":"  Learning models of dynamical systems with external inputs, which may be, for\nexample, nonsmooth or piecewise, is crucial for studying complex phenomena and\npredicting future state evolution, which is essential for applications such as\nsafety guarantees and decision-making. In this work, we introduce \\emph{Input\nConcomitant Neural ODEs (ICODEs)}, which incorporate precise real-time input\ninformation into the learning process of the models, rather than treating the\ninputs as hidden parameters to be learned. The sufficient conditions to ensure\nthe model's contraction property are provided to guarantee that system\ntrajectories of the trained model converge to a fixed point, regardless of\ninitial conditions across different training processes. We validate our method\nthrough experiments on several representative real dynamics: Single-link robot,\nDC-to-DC converter, motion dynamics of a rigid body, Rabinovich-Fabrikant\nequation, Glycolytic-glycogenolytic pathway model, and heat conduction\nequation. The experimental results demonstrate that our proposed ICODEs\nefficiently learn the ground truth systems, achieving superior prediction\nperformance under both typical and atypical inputs. This work offers a valuable\nclass of neural ODE models for understanding physical systems with explicit\nexternal input information, with potentially promising applications in fields\nsuch as physics and robotics. Our code is available online at\nhttps://github.com/EEE-ai59/ICODE.git.\n","authors":["Zhaoyi Li","Wenjie Mei","Ke Yu","Yang Bai","Shihua Li"],"pdf_url":"https://arxiv.org/pdf/2411.13914v4.pdf","comment":"To be published in IEEE Transactions on Automation Science and\n  Engineering"},{"id":"http://arxiv.org/abs/2504.11054v1","updated":"2025-04-15T10:41:11Z","published":"2025-04-15T10:41:11Z","title":"Zero-Shot Whole-Body Humanoid Control via Behavioral Foundation Models","summary":"  Unsupervised reinforcement learning (RL) aims at pre-training agents that can\nsolve a wide range of downstream tasks in complex environments. Despite recent\nadvancements, existing approaches suffer from several limitations: they may\nrequire running an RL process on each downstream task to achieve a satisfactory\nperformance, they may need access to datasets with good coverage or\nwell-curated task-specific samples, or they may pre-train policies with\nunsupervised losses that are poorly correlated with the downstream tasks of\ninterest. In this paper, we introduce a novel algorithm regularizing\nunsupervised RL towards imitating trajectories from unlabeled behavior\ndatasets. The key technical novelty of our method, called Forward-Backward\nRepresentations with Conditional-Policy Regularization, is to train\nforward-backward representations to embed the unlabeled trajectories to the\nsame latent space used to represent states, rewards, and policies, and use a\nlatent-conditional discriminator to encourage policies to ``cover'' the states\nin the unlabeled behavior dataset. As a result, we can learn policies that are\nwell aligned with the behaviors in the dataset, while retaining zero-shot\ngeneralization capabilities for reward-based and imitation tasks. We\ndemonstrate the effectiveness of this new approach in a challenging humanoid\ncontrol problem: leveraging observation-only motion capture datasets, we train\nMeta Motivo, the first humanoid behavioral foundation model that can be\nprompted to solve a variety of whole-body tasks, including motion tracking,\ngoal reaching, and reward optimization. The resulting model is capable of\nexpressing human-like behaviors and it achieves competitive performance with\ntask-specific methods while outperforming state-of-the-art unsupervised RL and\nmodel-based baselines.\n","authors":["Andrea Tirinzoni","Ahmed Touati","Jesse Farebrother","Mateusz Guzek","Anssi Kanervisto","Yingchen Xu","Alessandro Lazaric","Matteo Pirotta"],"pdf_url":"https://arxiv.org/pdf/2504.11054v1.pdf","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2504.11053v1","updated":"2025-04-15T10:40:40Z","published":"2025-04-15T10:40:40Z","title":"QualiTagger: Automating software quality detection in issue trackers","summary":"  A systems quality is a major concern for development teams when it evolve.\nUnderstanding the effects of a loss of quality in the codebase is crucial to\navoid side effects like the appearance of technical debt. Although the\nidentification of these qualities in software requirements described in natural\nlanguage has been investigated, most of the results are often not applicable in\npractice, and rely on having been validated on small datasets and limited\namount of projects. For many years, machine learning (ML) techniques have been\nproved as a valid technique to identify and tag terms described in natural\nlanguage. In order to advance previous works, in this research we use cutting\nedge models like Transformers, together with a vast dataset mined and curated\nfrom GitHub, to identify what text is usually associated with different quality\nproperties. We also study the distribution of such qualities in issue trackers\nfrom openly accessible software repositories, and we evaluate our approach both\nwith students from a software engineering course and with its application to\nrecognize security labels in industry.\n","authors":["Karthik Shivashankar","Rafael Capilla","Maren Maritsdatter Kruke","Mili Orucevic","Antonio Martini"],"pdf_url":"https://arxiv.org/pdf/2504.11053v1.pdf","comment":"IN Review ASE journal"},{"id":"http://arxiv.org/abs/2306.08929v3","updated":"2025-04-15T10:24:14Z","published":"2023-06-15T08:02:07Z","title":"Inferring Communities of Interest in Collaborative Learning-based\n  Recommender Systems","summary":"  Collaborative-learning-based recommender systems, such as those employing\nFederated Learning (FL) and Gossip Learning (GL), allow users to train models\nwhile keeping their history of liked items on their devices. While these\nmethods were seen as promising for enhancing privacy, recent research has shown\nthat collaborative learning can be vulnerable to various privacy attacks. In\nthis paper, we propose a novel attack called Community Inference Attack (CIA),\nwhich enables an adversary to identify community members based on a set of\ntarget items. What sets CIA apart is its efficiency: it operates at low\ncomputational cost by eliminating the need for training surrogate models.\nInstead, it uses a comparison-based approach, inferring sensitive information\nby comparing users' models rather than targeting any specific individual model.\nTo evaluate the effectiveness of CIA, we conduct experiments on three\nreal-world recommendation datasets using two recommendation models under both\nFederated and Gossip-like settings. The results demonstrate that CIA can be up\nto 10 times more accurate than random guessing. Additionally, we evaluate two\nmitigation strategies: Differentially Private Stochastic Gradient Descent\n(DP-SGD) and a Share less policy, which involves sharing fewer, less sensitive\nmodel parameters. Our findings suggest that the Share less strategy offers a\nbetter privacy-utility trade-off, especially in GL.\n","authors":["Yacine Belal","Sonia Ben Mokhtar","Mohamed Maouche","Anthony Simonet-Boulogne"],"pdf_url":"https://arxiv.org/pdf/2306.08929v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03506v4","updated":"2025-04-15T10:00:22Z","published":"2025-03-05T13:54:13Z","title":"Opinion: Revisiting synthetic data classifications from a privacy\n  perspective","summary":"  Synthetic data is emerging as a cost-effective solution necessary to meet the\nincreasing data demands of AI development, created either from existing\nknowledge or derived from real data. The traditional classification of\nsynthetic data types into hybrid, partial or fully synthetic datasets has\nlimited value and does not reflect the ever-increasing methods to generate\nsynthetic data. The generation method and their source jointly shape the\ncharacteristics of synthetic data, which in turn determines its practical\napplications. We make a case for an alternative approach to grouping synthetic\ndata types that better reflect privacy perspectives in order to facilitate\nregulatory guidance in the generation and processing of synthetic data. This\napproach to classification provides flexibility to new advancements like deep\ngenerative methods and offers a more practical framework for future\napplications.\n","authors":["Vibeke Binz Vallevik","Serena Elizabeth Marshall","Aleksandar Babic","Jan Franz Nygaard"],"pdf_url":"https://arxiv.org/pdf/2503.03506v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11026v1","updated":"2025-04-15T09:50:03Z","published":"2025-04-15T09:50:03Z","title":"A PyTorch-Compatible Spike Encoding Framework for Energy-Efficient\n  Neuromorphic Applications","summary":"  Spiking Neural Networks (SNNs) offer promising energy efficiency advantages,\nparticularly when processing sparse spike trains. However, their\nincompatibility with traditional datasets, which consist of batches of input\nvectors rather than spike trains, necessitates the development of efficient\nencoding methods. This paper introduces a novel, open-source PyTorch-compatible\nPython framework for spike encoding, designed for neuromorphic applications in\nmachine learning and reinforcement learning. The framework supports a range of\nencoding algorithms, including Leaky Integrate-and-Fire (LIF), Step Forward\n(SF), Pulse Width Modulation (PWM), and Ben's Spiker Algorithm (BSA), as well\nas specialized encoding strategies covering population coding and reinforcement\nlearning scenarios. Furthermore, we investigate the performance trade-offs of\neach method on embedded hardware using C/C++ implementations, considering\nenergy consumption, computation time, spike sparsity, and reconstruction\naccuracy. Our findings indicate that SF typically achieves the lowest\nreconstruction error and offers the highest energy efficiency and fastest\nencoding speed, achieving the second-best spike sparsity. At the same time,\nother methods demonstrate particular strengths depending on the signal\ncharacteristics. This framework and the accompanying empirical analysis provide\nvaluable resources for selecting optimal encoding strategies for\nenergy-efficient SNN applications.\n","authors":["Alexandru Vasilache","Jona Scholz","Vincent Schilling","Sven Nitzsche","Florian Kaelber","Johannes Korsch","Juergen Becker"],"pdf_url":"https://arxiv.org/pdf/2504.11026v1.pdf","comment":"A preliminary version of this work was accepted at the 20th\n  International Conference on Systems (ICONS 2025), May 18-22, 2025, Nice,\n  France. The conference proceedings will be published by IARIA Press (ISSN:\n  2308-4243, ISBN: 978-1-68558-278-4) and archived in the ThinkMind Digital\n  Library. The proposed Spike Encoding Framework can be accessed at\n  https://github.com/Alex-Vasilache/Spike-Encoding"},{"id":"http://arxiv.org/abs/2408.08055v2","updated":"2025-04-15T09:49:17Z","published":"2024-08-15T09:49:37Z","title":"DeNOTS: Stable Deep Neural ODEs for Time Series","summary":"  Neural ODEs are a prominent branch of methods designed to capture the\ntemporal evolution of complex time-stamped data. Their idea is to solve an ODE\nwith Neural Network-defined dynamics, which take the immediate parameters of\nthe observed system into account. However, larger integration intervals cause\ninstability, which forces most modern methods to normalize time to $[0, 1]$. We\nprovably stabilize these models by introducing an adaptive negative feedback\nmechanism. This modification allows for longer integration, which in turn\nimplies higher expressiveness, mirroring the behaviour of increasing depth in\nconventional Neural Networks.Additionally, it provides intriguing theoretical\nproperties: forgetfulness and missing-value robustness. For three open\ndatasets, our method obtains up to 20\\% improvements in downstream quality if\ncompared to existing baselines, including State Space Models and Neural~CDEs.\n","authors":["Ilya Kuleshov","Evgenia Romanenkova","Galina Boeva","Vladislav Zhuzhel","Evgeni Vorsin","Alexey Zaytsev"],"pdf_url":"https://arxiv.org/pdf/2408.08055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11022v1","updated":"2025-04-15T09:47:57Z","published":"2025-04-15T09:47:57Z","title":"Meta-learning For Few-Shot Time Series Crop Type Classification: A\n  Benchmark On The EuroCropsML Dataset","summary":"  Spatial imbalances in crop type data pose significant challenges for accurate\nclassification in remote sensing applications. Algorithms aiming at\ntransferring knowledge from data-rich to data-scarce tasks have thus surged in\npopularity. However, despite their effectiveness in previous evaluations, their\nperformance in challenging real-world applications is unclear and needs to be\nevaluated. This study benchmarks transfer learning and several meta-learning\nalgorithms, including (First-Order) Model-Agnostic Meta-Learning ((FO)-MAML),\nAlmost No Inner Loop (ANIL), and Task-Informed Meta-Learning (TIML), on the\nreal-world EuroCropsML time series dataset, which combines farmer-reported crop\ndata with Sentinel-2 satellite observations from Estonia, Latvia, and Portugal.\nOur findings indicate that MAML-based meta-learning algorithms achieve slightly\nhigher accuracy compared to simpler transfer learning methods when applied to\ncrop type classification tasks in Estonia after pre-training on data from\nLatvia. However, this improvement comes at the cost of increased computational\ndemands and training time. Moreover, we find that the transfer of knowledge\nbetween geographically disparate regions, such as Estonia and Portugal, poses\nsignificant challenges to all investigated algorithms. These insights\nunderscore the trade-offs between accuracy and computational resource\nrequirements in selecting machine learning methods for real-world crop type\nclassification tasks and highlight the difficulties of transferring knowledge\nbetween different regions of the Earth. To facilitate future research in this\ndomain, we present the first comprehensive benchmark for evaluating transfer\nand meta-learning methods for crop type classification under real-world\nconditions. The corresponding code is publicly available at\nhttps://github.com/dida-do/eurocrops-meta-learning.\n","authors":["Joana Reuss","Jan Macdonald","Simon Becker","Konrad Schultka","Lorenz Richter","Marco Körner"],"pdf_url":"https://arxiv.org/pdf/2504.11022v1.pdf","comment":"19 pages, 7 figures, 12 tables"},{"id":"http://arxiv.org/abs/2503.23001v3","updated":"2025-04-15T09:43:59Z","published":"2025-03-29T07:44:34Z","title":"Buyer-Initiated Auction Mechanism for Data Redemption in Machine\n  Unlearning","summary":"  The rapid growth of artificial intelligence (AI) has raised privacy concerns\nover user data, leading to regulations like the General Data Protection\nRegulation (GDPR) and the California Consumer Privacy Act (CCPA). With the\nessential toolbox provided by machine unlearning, AI service providers are now\nable to remove user data from their trained models as well as the training\ndatasets, so as to comply with such regulations. However, extensive data\nredemption can be costly and degrade model accuracy. To balance the cost of\nunlearning and the privacy protection, we propose a buyer-initiated auction\nmechanism for data redemption, enabling the service provider to purchase data\nfrom willing users with appropriate compensation. This approach does not\nrequire the server to have any a priori knowledge about the users' privacy\npreference, and provides an efficient solution for maximizing the social\nwelfare in the investigated problem.\n","authors":["Bin Han","Di Feng","Jie Wang","Hans D. Schotten"],"pdf_url":"https://arxiv.org/pdf/2503.23001v3.pdf","comment":"Submitted to IEEE GLOBECOM 2025"},{"id":"http://arxiv.org/abs/2504.03784v3","updated":"2025-04-15T09:29:06Z","published":"2025-04-03T16:16:35Z","title":"Robust Reinforcement Learning from Human Feedback for Large Language\n  Models Fine-Tuning","summary":"  Reinforcement learning from human feedback (RLHF) has emerged as a key\ntechnique for aligning the output of large language models (LLMs) with human\npreferences. To learn the reward function, most existing RLHF algorithms use\nthe Bradley-Terry model, which relies on assumptions about human preferences\nthat may not reflect the complexity and variability of real-world judgments. In\nthis paper, we propose a robust algorithm to enhance the performance of\nexisting approaches under such reward model misspecifications. Theoretically,\nour algorithm reduces the variance of reward and policy estimators, leading to\nimproved regret bounds. Empirical evaluations on LLM benchmark datasets\ndemonstrate that the proposed algorithm consistently outperforms existing\nmethods, with 77-81% of responses being favored over baselines on the Anthropic\nHelpful and Harmless dataset.\n","authors":["Kai Ye","Hongyi Zhou","Jin Zhu","Francesco Quinzan","Chengchung Shi"],"pdf_url":"https://arxiv.org/pdf/2504.03784v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08974v4","updated":"2025-04-15T09:15:39Z","published":"2024-07-12T04:04:54Z","title":"Topology-enhanced machine learning model (Top-ML) for anticancer peptide\n  prediction","summary":"  Recently, therapeutic peptides have demonstrated great promise for cancer\ntreatment. To explore powerful anticancer peptides, artificial intelligence\n(AI)-based approaches have been developed to systematically screen potential\ncandidates. However, the lack of efficient featurization of peptides has become\na bottleneck for these machine-learning models. In this paper, we propose a\ntopology-enhanced machine learning model (Top-ML) for anticancer peptides\nprediction. Our Top-ML employs peptide topological features derived from its\nsequence \"connection\" information characterized by vector and spectral\ndescriptors. Our Top-ML model, employing an Extra-Trees classifier, has been\nvalidated on the AntiCP 2.0 and mACPpred 2.0 benchmark datasets, achieving\nstate-of-the-art performance or results comparable to existing deep learning\nmodels, while providing greater interpretability. Our results highlight the\npotential of leveraging novel topology-based featurization to accelerate the\nidentification of anticancer peptides.\n","authors":["Joshua Zhi En Tan","JunJie Wee","Xue Gong","Kelin Xia"],"pdf_url":"https://arxiv.org/pdf/2407.08974v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03250v4","updated":"2025-04-15T09:00:37Z","published":"2024-07-03T16:29:47Z","title":"When big data actually are low-rank, or entrywise approximation of\n  certain function-generated matrices","summary":"  The article concerns low-rank approximation of matrices generated by sampling\na smooth function of two $m$-dimensional variables. We identify several\nmisconceptions surrounding a claim that, for a specific class of analytic\nfunctions, such $n \\times n$ matrices admit accurate entrywise approximation of\nrank that is independent of $m$ and grows as $\\log(n)$ -- colloquially known as\n''big-data matrices are approximately low-rank''. We provide a theoretical\nexplanation of the numerical results presented in support of this claim,\ndescribing three narrower classes of functions for which function-generated\nmatrices can be approximated within an entrywise error of order $\\varepsilon$\nwith rank $\\mathcal{O}(\\log(n) \\varepsilon^{-2} \\log(\\varepsilon^{-1}))$ that\nis independent of the dimension $m$: (i) functions of the inner product of the\ntwo variables, (ii) functions of the Euclidean distance between the variables,\nand (iii) shift-invariant positive-definite kernels. We extend our argument to\ntensor-train approximation of tensors generated with functions of the\n''higher-order inner product'' of their multiple variables. We discuss our\nresults in the context of low-rank approximation of (a) growing datasets and\n(b) attention in transformer neural networks.\n","authors":["Stanislav Budzinskiy"],"pdf_url":"https://arxiv.org/pdf/2407.03250v4.pdf","comment":"Accepted for publication in SIAM Journal on Mathematics of Data\n  Science"},{"id":"http://arxiv.org/abs/2504.10987v1","updated":"2025-04-15T08:59:03Z","published":"2025-04-15T08:59:03Z","title":"Leveraging Vertical Public-Private Split for Improved Synthetic Data\n  Generation","summary":"  Differentially Private Synthetic Data Generation (DP-SDG) is a key enabler of\nprivate and secure tabular-data sharing, producing artificial data that carries\nthrough the underlying statistical properties of the input data. This typically\ninvolves adding carefully calibrated statistical noise to guarantee individual\nprivacy, at the cost of synthetic data quality. Recent literature has explored\nscenarios where a small amount of public data is used to help enhance the\nquality of synthetic data. These methods study a horizontal public-private\npartitioning which assumes access to a small number of public rows that can be\nused for model initialization, providing a small utility gain. However,\nrealistic datasets often naturally consist of public and private attributes,\nmaking a vertical public-private partitioning relevant for practical synthetic\ndata deployments. We propose a novel framework that adapts horizontal\npublic-assisted methods into the vertical setting. We compare this framework\nagainst our alternative approach that uses conditional generation, highlighting\ninitial limitations of public-data assisted methods and proposing future\nresearch directions to address these challenges.\n","authors":["Samuel Maddock","Shripad Gade","Graham Cormode","Will Bullock"],"pdf_url":"https://arxiv.org/pdf/2504.10987v1.pdf","comment":"Accepted to the Synthetic Data x Data Access Problem (SynthData)\n  workshop @ ICLR 2025"},{"id":"http://arxiv.org/abs/2504.07157v2","updated":"2025-04-15T08:56:03Z","published":"2025-04-09T11:19:42Z","title":"GAAPO: Genetic Algorithmic Applied to Prompt Optimization","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, with their performance heavily dependent on the quality of input\nprompts. While prompt engineering has proven effective, it typically relies on\nmanual adjustments, making it time-consuming and potentially suboptimal. This\npaper introduces GAAPO (Genetic Algorithm Applied to Prompt Optimization), a\nnovel hybrid optimization framework that leverages genetic algorithm principles\nto evolve prompts through successive generations. Unlike traditional genetic\napproaches that rely solely on mutation and crossover operations, GAAPO\nintegrates multiple specialized prompt generation strategies within its\nevolutionary framework. Through extensive experimentation on diverse datasets\nincluding ETHOS, MMLU-Pro, and GPQA, our analysis reveals several important\npoint for the future development of automatic prompt optimization methods:\nimportance of the tradeoff between the population size and the number of\ngenerations, effect of selection methods on stability results, capacity of\ndifferent LLMs and especially reasoning models to be able to automatically\ngenerate prompts from similar queries... Furthermore, we provide insights into\nthe relative effectiveness of different prompt generation strategies and their\nevolution across optimization phases. These findings contribute to both the\ntheoretical understanding of prompt optimization and practical applications in\nimproving LLM performance.\n","authors":["Xavier Sécheresse","Jacques-Yves Guilbert--Ly","Antoine Villedieu de Torcy"],"pdf_url":"https://arxiv.org/pdf/2504.07157v2.pdf","comment":"26 pages, 9 figures"},{"id":"http://arxiv.org/abs/2405.18432v2","updated":"2025-04-15T08:53:58Z","published":"2024-05-28T17:59:51Z","title":"Unsupervised Model Tree Heritage Recovery","summary":"  The number of models shared online has recently skyrocketed, with over one\nmillion public models available on Hugging Face. Sharing models allows other\nusers to build on existing models, using them as initialization for\nfine-tuning, improving accuracy, and saving compute and energy. However, it\nalso raises important intellectual property issues, as fine-tuning may violate\nthe license terms of the original model or that of its training data. A Model\nTree, i.e., a tree data structure rooted at a foundation model and having\ndirected edges between a parent model and other models directly fine-tuned from\nit (children), would settle such disputes by making the model heritage\nexplicit. Unfortunately, current models are not well documented, with most\nmodel metadata (e.g., \"model cards\") not providing accurate information about\nheritage. In this paper, we introduce the task of Unsupervised Model Tree\nHeritage Recovery (Unsupervised MoTHer Recovery) for collections of neural\nnetworks. For each pair of models, this task requires: i) determining if they\nare directly related, and ii) establishing the direction of the relationship.\nOur hypothesis is that model weights encode this information, the challenge is\nto decode the underlying tree structure given the weights. We discover several\nproperties of model weights that allow us to perform this task. By using these\nproperties, we formulate the MoTHer Recovery task as finding a directed minimal\nspanning tree. In extensive experiments we demonstrate that our method\nsuccessfully reconstructs complex Model Trees.\n","authors":["Eliahu Horwitz","Asaf Shul","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2405.18432v2.pdf","comment":"ICLR 2025. Project page: https://horwitz.ai/mother"},{"id":"http://arxiv.org/abs/2504.10983v1","updated":"2025-04-15T08:46:53Z","published":"2025-04-15T08:46:53Z","title":"ProtFlow: Fast Protein Sequence Design via Flow Matching on Compressed\n  Protein Language Model Embeddings","summary":"  The design of protein sequences with desired functionalities is a fundamental\ntask in protein engineering. Deep generative methods, such as autoregressive\nmodels and diffusion models, have greatly accelerated the discovery of novel\nprotein sequences. However, these methods mainly focus on local or shallow\nresidual semantics and suffer from low inference efficiency, large modeling\nspace and high training cost. To address these challenges, we introduce\nProtFlow, a fast flow matching-based protein sequence design framework that\noperates on embeddings derived from semantically meaningful latent space of\nprotein language models. By compressing and smoothing the latent space,\nProtFlow enhances performance while training on limited computational\nresources. Leveraging reflow techniques, ProtFlow enables high-quality\nsingle-step sequence generation. Additionally, we develop a joint design\npipeline for the design scene of multichain proteins. We evaluate ProtFlow\nacross diverse protein design tasks, including general peptides and long-chain\nproteins, antimicrobial peptides, and antibodies. Experimental results\ndemonstrate that ProtFlow outperforms task-specific methods in these\napplications, underscoring its potential and broad applicability in\ncomputational protein sequence design and analysis.\n","authors":["Zitai Kong","Yiheng Zhu","Yinlong Xu","Hanjing Zhou","Mingzhe Yin","Jialu Wu","Hongxia Xu","Chang-Yu Hsieh","Tingjun Hou","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2504.10983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07612v2","updated":"2025-04-15T08:43:28Z","published":"2024-07-10T12:50:44Z","title":"Teaching Transformers Causal Reasoning through Axiomatic Training","summary":"  For text-based AI systems to interact in the real world, causal reasoning is\nan essential skill. Since active interventions are costly, we study to what\nextent a system can learn causal reasoning from symbolic demonstrations of\ncausal axioms. Specifically, we present an axiomatic training method where the\nsystem learns from multiple demonstrations of a causal axiom (or rule), rather\nthan incorporating the axiom as an inductive bias or inferring it from data\nvalues. A key question is whether the system would learn to generalize from the\naxiom demonstrations to more complex scenarios. Our results, based on applying\naxiomatic training to learn the transitivity axiom and d-separation rule,\nindicate that such generalization is possible. To avoid data contamination\nissues, we start with a 67 million parameter transformer model and train it\nfrom scratch. On both tasks, we find that a model trained on linear causal\nchains (along with some noisy variations) can generalize well to complex\ngraphs, including longer causal chains, causal chains with reversed order, and\ngraphs with branching.To handle diverse text inputs, the same method is\nextended to finetune language models. Finetuning Llama-3.1 8B model on our\naxiomatic data leads to significant gains on causal benchmarks such as\nCorr2Cause and CLEAR, in some cases providing state-of-the-art performance\nsurpassing GPT-4.\n","authors":["Aniket Vashishtha","Abhinav Kumar","Atharva Pandey","Abbavaram Gowtham Reddy","Kabir Ahuja","Vineeth N Balasubramanian","Amit Sharma"],"pdf_url":"https://arxiv.org/pdf/2407.07612v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10973v1","updated":"2025-04-15T08:34:13Z","published":"2025-04-15T08:34:13Z","title":"Early Detection of Cognitive Impairment in Elderly using a Passive\n  FPVS-EEG BCI and Machine Learning -- Extended Version","summary":"  Early dementia diagnosis requires biomarkers sensitive to both structural and\nfunctional brain changes. While structural neuroimaging biomarkers have\nprogressed significantly, objective functional biomarkers of early cognitive\ndecline remain a critical unmet need. Current cognitive assessments often rely\non behavioral responses, making them susceptible to factors like effort,\npractice effects, and educational background, thereby hindering early and\naccurate detection. This work introduces a novel approach, leveraging a\nlightweight convolutional neural network (CNN) to infer cognitive impairment\nlevels directly from electroencephalography (EEG) data. Critically, this method\nemploys a passive fast periodic visual stimulation (FPVS) paradigm, eliminating\nthe need for explicit behavioral responses or task comprehension from the\nparticipant. This passive approach provides an objective measure of working\nmemory function, independent of confounding factors inherent in active\ncognitive tasks, and offers a promising new avenue for early and unbiased\ndetection of cognitive decline.\n","authors":["Tomasz M. Rutkowski","Stanisław Narębski","Mihoko Otake-Matsuura","Tomasz Komendziński"],"pdf_url":"https://arxiv.org/pdf/2504.10973v1.pdf","comment":"4 pages, 4 figures, exteded version of an abstract accepted for a\n  poster presentation at the 47th Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society (EMBC2025), Copenhagen, Denmark,\n  July 14-17, 2025"},{"id":"http://arxiv.org/abs/2402.04620v6","updated":"2025-04-15T08:32:15Z","published":"2024-02-07T07:07:02Z","title":"CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract\n  Patients","summary":"  The healthcare landscape is evolving, with patients seeking reliable\ninformation about their health conditions and available treatment options.\nDespite the abundance of information sources, the digital age overwhelms\nindividuals with excess, often inaccurate information. Patients primarily trust\nmedical professionals, highlighting the need for expert-endorsed health\ninformation. However, increased patient loads on experts has led to reduced\ncommunication time, impacting information sharing. To address this gap, we\ndeveloped CataractBot. CataractBot answers cataract surgery related questions\ninstantly using an LLM to query a curated knowledge base, and provides\nexpert-verified responses asynchronously. It has multimodal and multilingual\ncapabilities. In an in-the-wild deployment study with 49 patients and\nattendants, 4 doctors, and 2 patient coordinators, CataractBot demonstrated\npotential, providing anytime accessibility, saving time, accommodating diverse\nliteracy levels, alleviating power differences, and adding a privacy layer\nbetween patients and doctors. Users reported that their trust in the system was\nestablished through expert verification. Broadly, our results could inform\nfuture work on expert-mediated LLM bots.\n","authors":["Pragnya Ramjee","Bhuvan Sachdeva","Satvik Golechha","Shreyas Kulkarni","Geeta Fulari","Kaushik Murali","Mohit Jain"],"pdf_url":"https://arxiv.org/pdf/2402.04620v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04222v2","updated":"2025-04-15T08:30:04Z","published":"2025-04-05T16:18:33Z","title":"TrafficLLM: Enhancing Large Language Models for Network Traffic Analysis\n  with Generic Traffic Representation","summary":"  Machine learning (ML) powered network traffic analysis has been widely used\nfor the purpose of threat detection. Unfortunately, their generalization across\ndifferent tasks and unseen data is very limited. Large language models (LLMs),\nknown for their strong generalization capabilities, have shown promising\nperformance in various domains. However, their application to the traffic\nanalysis domain is limited due to significantly different characteristics of\nnetwork traffic. To address the issue, in this paper, we propose TrafficLLM,\nwhich introduces a dual-stage fine-tuning framework to learn generic traffic\nrepresentation from heterogeneous raw traffic data. The framework uses\ntraffic-domain tokenization, dual-stage tuning pipeline, and extensible\nadaptation to help LLM release generalization ability on dynamic traffic\nanalysis tasks, such that it enables traffic detection and traffic generation\nacross a wide range of downstream tasks. We evaluate TrafficLLM across 10\ndistinct scenarios and 229 types of traffic. TrafficLLM achieves F1-scores of\n0.9875 and 0.9483, with up to 80.12% and 33.92% better performance than\nexisting detection and generation methods. It also shows strong generalization\non unseen traffic with an 18.6% performance improvement. We further evaluate\nTrafficLLM in real-world scenarios. The results confirm that TrafficLLM is easy\nto scale and achieves accurate detection performance on enterprise traffic.\n","authors":["Tianyu Cui","Xinjie Lin","Sijia Li","Miao Chen","Qilei Yin","Qi Li","Ke Xu"],"pdf_url":"https://arxiv.org/pdf/2504.04222v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03751v2","updated":"2025-04-15T08:28:27Z","published":"2024-06-06T05:27:33Z","title":"Adaptive Multi-Scale Decomposition Framework for Time Series Forecasting","summary":"  Transformer-based and MLP-based methods have emerged as leading approaches in\ntime series forecasting (TSF). While Transformer-based methods excel in\ncapturing long-range dependencies, they suffer from high computational\ncomplexities and tend to overfit. Conversely, MLP-based methods offer\ncomputational efficiency and adeptness in modeling temporal dynamics, but they\nstruggle with capturing complex temporal patterns effectively. To address these\nchallenges, we propose a novel MLP-based Adaptive Multi-Scale Decomposition\n(AMD) framework for TSF. Our framework decomposes time series into distinct\ntemporal patterns at multiple scales, leveraging the Multi-Scale Decomposable\nMixing (MDM) block to dissect and aggregate these patterns in a residual\nmanner. Complemented by the Dual Dependency Interaction (DDI) block and the\nAdaptive Multi-predictor Synthesis (AMS) block, our approach effectively models\nboth temporal and channel dependencies and utilizes autocorrelation to refine\nmulti-scale data integration. Comprehensive experiments demonstrate that our\nAMD framework not only overcomes the limitations of existing methods but also\nconsistently achieves state-of-the-art performance in both long-term and\nshort-term forecasting tasks across various datasets, showcasing superior\nefficiency. Code is available at https://github.com/TROUBADOUR000/AMD\n","authors":["Yifan Hu","Peiyuan Liu","Peng Zhu","Dawei Cheng","Tao Dai"],"pdf_url":"https://arxiv.org/pdf/2406.03751v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08306v4","updated":"2025-04-15T08:24:40Z","published":"2025-03-11T11:16:47Z","title":"Reasoning in visual navigation of end-to-end trained agents: a dynamical\n  systems approach","summary":"  Progress in Embodied AI has made it possible for end-to-end-trained agents to\nnavigate in photo-realistic environments with high-level reasoning and\nzero-shot or language-conditioned behavior, but benchmarks are still dominated\nby simulation. In this work, we focus on the fine-grained behavior of\nfast-moving real robots and present a large-scale experimental study involving\n\\numepisodes{} navigation episodes in a real environment with a physical robot,\nwhere we analyze the type of reasoning emerging from end-to-end training. In\nparticular, we study the presence of realistic dynamics which the agent learned\nfor open-loop forecasting, and their interplay with sensing. We analyze the way\nthe agent uses latent memory to hold elements of the scene structure and\ninformation gathered during exploration. We probe the planning capabilities of\nthe agent, and find in its memory evidence for somewhat precise plans over a\nlimited horizon. Furthermore, we show in a post-hoc analysis that the value\nfunction learned by the agent relates to long-term planning. Put together, our\nexperiments paint a new picture on how using tools from computer vision and\nsequential decision making have led to new capabilities in robotics and\ncontrol. An interactive tool is available at\neurope.naverlabs.com/research/publications/reasoning-in-visual-navigation-of-end-to-end-trained-agents.\n","authors":["Steeven Janny","Hervé Poirier","Leonid Antsfeld","Guillaume Bono","Gianluca Monaci","Boris Chidlovskii","Francesco Giuliari","Alessio Del Bue","Christian Wolf"],"pdf_url":"https://arxiv.org/pdf/2503.08306v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14122v2","updated":"2025-04-15T08:15:08Z","published":"2025-01-23T22:36:06Z","title":"Reinforcement Learning Platform for Adversarial Black-box Attacks with\n  Custom Distortion Filters","summary":"  We present a Reinforcement Learning Platform for Adversarial Black-box\nuntargeted and targeted attacks, RLAB, that allows users to select from various\ndistortion filters to create adversarial examples. The platform uses a\nReinforcement Learning agent to add minimum distortion to input images while\nstill causing misclassification by the target model. The agent uses a novel\ndual-action method to explore the input image at each step to identify\nsensitive regions for adding distortions while removing noises that have less\nimpact on the target model. This dual action leads to faster and more efficient\nconvergence of the attack. The platform can also be used to measure the\nrobustness of image classification models against specific distortion types.\nAlso, retraining the model with adversarial samples significantly improved\nrobustness when evaluated on benchmark datasets. The proposed platform\noutperforms state-of-the-art methods in terms of the average number of queries\nrequired to cause misclassification. This advances trustworthiness with a\npositive social impact.\n","authors":["Soumyendu Sarkar","Ashwin Ramesh Babu","Sajad Mousavi","Vineet Gundecha","Sahand Ghorbanpour","Avisek Naug","Ricardo Luna Gutierrez","Antonio Guillen"],"pdf_url":"https://arxiv.org/pdf/2501.14122v2.pdf","comment":"Accepted at the 2025 AAAI Conference on Artificial Intelligence\n  Proceedings"},{"id":"http://arxiv.org/abs/2409.10570v2","updated":"2025-04-15T08:07:58Z","published":"2024-09-14T08:08:55Z","title":"Protecting Copyright of Medical Pre-trained Language Models:\n  Training-Free Backdoor Model Watermarking","summary":"  With the advancement of intelligent healthcare, medical pre-trained language\nmodels (Med-PLMs) have emerged and demonstrated significant effectiveness in\ndownstream medical tasks. While these models are valuable assets, they are\nvulnerable to misuse and theft, requiring copyright protection. However,\nexisting watermarking methods for pre-trained language models (PLMs) cannot be\ndirectly applied to Med-PLMs due to domain-task mismatch and inefficient\nwatermark embedding. To fill this gap, we propose the first training-free\nbackdoor model watermarking for Med-PLMs. Our method employs low-frequency\nwords as triggers, embedding the watermark by replacing their embeddings in the\nmodel's word embedding layer with those of specific medical terms. The\nwatermarked Med-PLMs produce the same output for triggers as for the\ncorresponding specified medical terms. We leverage this unique mapping to\ndesign tailored watermark extraction schemes for different downstream tasks,\nthereby addressing the challenge of domain-task mismatch in previous methods.\nExperiments demonstrate superior effectiveness of our watermarking method\nacross medical downstream tasks. Moreover, the method exhibits robustness\nagainst model extraction, pruning, fusion-based backdoor removal attacks, while\nmaintaining high efficiency with 10-second watermark embedding.\n","authors":["Cong Kong","Rui Xu","Weixi Chen","Jiawei Chen","Zhaoxia Yin"],"pdf_url":"https://arxiv.org/pdf/2409.10570v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2504.10959v1","updated":"2025-04-15T08:05:27Z","published":"2025-04-15T08:05:27Z","title":"Learning-Based User Association for MmWave Vehicular Networks With\n  Kernelized Contextual Bandits","summary":"  Vehicles require timely channel conditions to determine the base station (BS)\nto communicate with, but it is costly to estimate the fast-fading mmWave\nchannels frequently. Without additional channel estimations, the proposed\nDistributed Kernelized Upper Confidence Bound (DK-UCB) algorithm estimates the\ncurrent instantaneous transmission rates utilizing past contexts, such as the\nvehicle's location and velocity, along with past instantaneous transmission\nrates. To capture the nonlinear mapping from a context to the instantaneous\ntransmission rate, DK-UCB maps a context into the reproducing kernel Hilbert\nspace (RKHS) where a linear mapping becomes observable. To improve estimation\naccuracy, we propose a novel kernel function in RKHS which incorporates the\npropagation characteristics of the mmWave signals. Moreover, DK-UCB encourages\na vehicle to share necessary information when it has conducted significant\nexplorations, which speeds up the learning process while maintaining affordable\ncommunication costs.\n","authors":["Xiaoyang He","Xiaoxia Huang"],"pdf_url":"https://arxiv.org/pdf/2504.10959v1.pdf","comment":"Accepted by IEEE WCNC 2025"},{"id":"http://arxiv.org/abs/2504.10958v1","updated":"2025-04-15T08:05:16Z","published":"2025-04-15T08:05:16Z","title":"Recognition of Geometrical Shapes by Dictionary Learning","summary":"  Dictionary learning is a versatile method to produce an overcomplete set of\nvectors, called atoms, to represent a given input with only a few atoms. In the\nliterature, it has been used primarily for tasks that explore its powerful\nrepresentation capabilities, such as for image reconstruction. In this work, we\npresent a first approach to make dictionary learning work for shape\nrecognition, considering specifically geometrical shapes. As we demonstrate,\nthe choice of the underlying optimization method has a significant impact on\nrecognition quality. Experimental results confirm that dictionary learning may\nbe an interesting method for shape recognition tasks.\n","authors":["Alexander Köhler","Michael Breuß"],"pdf_url":"https://arxiv.org/pdf/2504.10958v1.pdf","comment":"6 pages, 4 figures, ACDSA 2025 conference"},{"id":"http://arxiv.org/abs/2504.10957v1","updated":"2025-04-15T08:04:39Z","published":"2025-04-15T08:04:39Z","title":"When is Task Vector Provably Effective for Model Editing? A\n  Generalization Analysis of Nonlinear Transformers","summary":"  Task arithmetic refers to editing the pre-trained model by adding a weighted\nsum of task vectors, each of which is the weight update from the pre-trained\nmodel to fine-tuned models for certain tasks. This approach recently gained\nattention as a computationally efficient inference method for model editing,\ne.g., multi-task learning, forgetting, and out-of-domain generalization\ncapabilities. However, the theoretical understanding of why task vectors can\nexecute various conceptual operations remains limited, due to the highly\nnon-convexity of training Transformer-based models. To the best of our\nknowledge, this paper provides the first theoretical characterization of the\ngeneralization guarantees of task vector methods on nonlinear Transformers. We\nconsider a conceptual learning setting, where each task is a binary\nclassification problem based on a discriminative pattern. We theoretically\nprove the effectiveness of task addition in simultaneously learning a set of\nirrelevant or aligned tasks, as well as the success of task negation in\nunlearning one task from irrelevant or contradictory tasks. Moreover, we prove\nthe proper selection of linear coefficients for task arithmetic to achieve\nguaranteed generalization to out-of-domain tasks. All of our theoretical\nresults hold for both dense-weight parameters and their low-rank\napproximations. Although established in a conceptual setting, our theoretical\nfindings were validated on a practical machine unlearning task using the large\nlanguage model Phi-1.5 (1.3B).\n","authors":["Hongkang Li","Yihua Zhang","Shuai Zhang","Meng Wang","Sijia Liu","Pin-Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2504.10957v1.pdf","comment":"Published at ICLR 2025 as an oral paper"},{"id":"http://arxiv.org/abs/2504.08780v2","updated":"2025-04-15T07:59:02Z","published":"2025-04-05T06:16:43Z","title":"How Relevance Emerges: Interpreting LoRA Fine-Tuning in Reranking LLMs","summary":"  We conduct a behavioral exploration of LoRA fine-tuned LLMs for Passage\nReranking to understand how relevance signals are learned and deployed by Large\nLanguage Models. By fine-tuning Mistral-7B, LLaMA3.1-8B, and Pythia-6.9B on MS\nMARCO under diverse LoRA configurations, we investigate how relevance modeling\nevolves across checkpoints, the impact of LoRA rank (1, 2, 8, 32), and the\nrelative importance of updated MHA vs. MLP components. Our ablations reveal\nwhich layers and projections within LoRA transformations are most critical for\nreranking accuracy. These findings offer fresh explanations into LoRA's\nadaptation mechanisms, setting the stage for deeper mechanistic studies in\nInformation Retrieval. All models used in this study have been shared.\n","authors":["Atharva Nijasure","Tanya Chowdhury","James Allan"],"pdf_url":"https://arxiv.org/pdf/2504.08780v2.pdf","comment":"Extended Abstract"},{"id":"http://arxiv.org/abs/2409.13213v3","updated":"2025-04-15T07:56:42Z","published":"2024-09-20T04:50:49Z","title":"MalMixer: Few-Shot Malware Classification with Retrieval-Augmented\n  Semi-Supervised Learning","summary":"  Recent growth and proliferation of malware has tested practitioners' ability\nto promptly classify new samples according to malware families. In contrast to\nlabor-intensive reverse engineering efforts, machine learning approaches have\ndemonstrated increased speed and accuracy. However, most existing deep-learning\nmalware family classifiers must be calibrated using a large number of samples\nthat are painstakingly manually analyzed before training. Furthermore, as novel\nmalware samples arise that are beyond the scope of the training set, additional\nreverse engineering effort must be employed to update the training set. The\nsheer volume of new samples found in the wild creates substantial pressure on\npractitioners' ability to reverse engineer enough malware to adequately train\nmodern classifiers. In this paper, we present MalMixer, a malware family\nclassifier using semi-supervised learning that achieves high accuracy with\nsparse training data. We present a novel domain-knowledge-aware technique for\naugmenting malware feature representations, enhancing few-shot performance of\nsemi-supervised malware family classification. We show that MalMixer achieves\nstate-of-the-art performance in few-shot malware family classification\nsettings. Our research confirms the feasibility and effectiveness of\nlightweight, domain-knowledge-aware feature augmentation methods and highlights\nthe capabilities of similar semi-supervised classifiers in addressing malware\nclassification issues.\n","authors":["Jiliang Li","Yifan Zhang","Yu Huang","Kevin Leach"],"pdf_url":"https://arxiv.org/pdf/2409.13213v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14936v2","updated":"2025-04-15T07:53:50Z","published":"2025-03-19T06:44:29Z","title":"Enhancing Code LLM Training with Programmer Attention","summary":"  Human attention provides valuable yet underexploited signals for code LLM\ntraining, offering a perspective beyond purely machine-driven attention.\nDespite the complexity and cost of collecting eye-tracking data, there has also\nbeen limited progress in systematically using these signals for code LLM\ntraining. To address both issues, we propose a cohesive pipeline spanning\naugmentation and reward-based fine-tuning. Specifically, we introduce (1) an\neye-tracking path augmentation method to expand programmer attention datasets,\n(2) a pattern abstraction step that refines raw fixations into learnable\nattention motifs, and (3) a reward-guided strategy for integrating these\ninsights directly into a CodeT5 supervised fine-tuning process. Our experiments\nyield +7.16 in CodeBLEU on the CodeXGlue benchmark for code summarization,\nunderscoring how uniting human and machine attention can boost code\nintelligence. We hope this work encourages broader exploration of human-centric\nmethods in next-generation AI4SE.\n","authors":["Yifan Zhang","Chen Huang","Zachary Karas","Dung Thuy Nguyen","Kevin Leach","Yu Huang"],"pdf_url":"https://arxiv.org/pdf/2503.14936v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09568v2","updated":"2025-04-15T07:51:15Z","published":"2024-10-12T15:30:17Z","title":"Second-Order Min-Max Optimization with Lazy Hessians","summary":"  This paper studies second-order methods for convex-concave minimax\noptimization. Monteiro and Svaiter (2012) proposed a method to solve the\nproblem with an optimal iteration complexity of $\\mathcal{O}(\\epsilon^{-3/2})$\nto find an $\\epsilon$-saddle point. However, it is unclear whether the\ncomputational complexity, $\\mathcal{O}((N+ d^2) d \\epsilon^{-2/3})$, can be\nimproved. In the above, we follow Doikov et al. (2023) and assume the\ncomplexity of obtaining a first-order oracle as $N$ and the complexity of\nobtaining a second-order oracle as $dN$. In this paper, we show that the\ncomputation cost can be reduced by reusing Hessian across iterations. Our\nmethods take the overall computational complexity of $ \\tilde{\\mathcal{O}}(\n(N+d^2)(d+ d^{2/3}\\epsilon^{-2/3}))$, which improves those of previous methods\nby a factor of $d^{1/3}$. Furthermore, we generalize our method to\nstrongly-convex-strongly-concave minimax problems and establish the complexity\nof $\\tilde{\\mathcal{O}}((N+d^2) (d + d^{2/3} \\kappa^{2/3}) )$ when the\ncondition number of the problem is $\\kappa$, enjoying a similar speedup upon\nthe state-of-the-art method. Numerical experiments on both real and synthetic\ndatasets also verify the efficiency of our method.\n","authors":["Lesi Chen","Chengchang Liu","Jingzhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.09568v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2503.05447v2","updated":"2025-04-15T07:51:10Z","published":"2025-03-07T14:17:45Z","title":"Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts","summary":"  Linear Sequence Modeling (LSM) like linear attention, state space models and\nlinear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant\narchitectural improvements. In this paper, we introduce Linear-MoE, a\nproduction-level system for modeling and training large-scale models that\nintegrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules\nfor linear-complexity sequence modeling and MoE layers for sparsely activation,\naiming to offer high performance with efficient training. The Linear-MoE system\ncomprises: 1) Modeling subsystem, which provides a unified framework supporting\nall instances of LSM. and 2) Training subsystem, which facilitates efficient\ntraining by incorporating various advanced parallelism technologies,\nparticularly Sequence Parallelism designed for Linear-MoE models. Additionally,\nwe explore hybrid models that combine Linear-MoE layers with standard\nTransformer-MoE layers with its Sequence Parallelism to further enhance model\nflexibility and performance. Evaluations on two model series, A0.3B-2B and\nA1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining\ncompetitive performance on various benchmarks, showcasing its potential as a\nnext-generation foundational model architecture. Code:\nhttps://github.com/OpenSparseLLMs/Linear-MoE.\n","authors":["Weigao Sun","Disen Lan","Tong Zhu","Xiaoye Qu","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2503.05447v2.pdf","comment":"Technical report, 17 pages"},{"id":"http://arxiv.org/abs/2503.03523v2","updated":"2025-04-15T07:49:44Z","published":"2025-03-05T14:07:29Z","title":"O-RAN xApps Conflict Management using Graph Convolutional Networks","summary":"  The lack of a unified mechanism to coordinate and prioritize the actions of\ndifferent applications can create three types of conflicts (direct, indirect,\nand implicit). Conflict management in O-RAN refers to the process of\nidentifying and resolving conflicts between network applications. In our paper,\nwe introduce a novel data-driven GCN-based method called GRAPH-based\nIntelligent xApp Conflict Prediction and Analysis (GRAPHICA) based on Graph\nConvolutional Network (GCN). It predicts three types of conflicts (direct,\nindirect, and implicit) and pinpoints the root causes (xApps). GRAPHICA\ncaptures the complex and hidden dependencies among the xApps, controlled\nparameters, and KPIs in O-RAN to predict possible conflicts. Then, it\nidentifies the root causes (xApps) contributing to the predicted conflicts. The\nproposed method was tested on highly imbalanced synthesized datasets where\nconflict instances range from 40% to 10%. The model is tested in a setting that\nsimulates real-world scenarios where conflicts are rare to assess its\nperformance. Experimental results demonstrate a high F1-score over 98% for the\nsynthesized datasets with different levels of class imbalance.\n","authors":["Maryam Al Shami","Jun Yan","Emmanuel Thepie Fapi"],"pdf_url":"https://arxiv.org/pdf/2503.03523v2.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2504.08217v2","updated":"2025-04-15T07:45:49Z","published":"2025-04-11T02:50:38Z","title":"DrivAer Transformer: A high-precision and fast prediction method for\n  vehicle aerodynamic drag coefficient based on the DrivAerNet++ dataset","summary":"  At the current stage, deep learning-based methods have demonstrated excellent\ncapabilities in evaluating aerodynamic performance, significantly reducing the\ntime and cost required for traditional computational fluid dynamics (CFD)\nsimulations. However, when faced with the task of processing extremely complex\nthree-dimensional (3D) vehicle models, the lack of large-scale datasets and\ntraining resources, coupled with the inherent diversity and complexity of the\ngeometry of different vehicle models, means that the prediction accuracy and\nversatility of these networks are still not up to the level required for\ncurrent production. In view of the remarkable success of Transformer models in\nthe field of natural language processing and their strong potential in the\nfield of image processing, this study innovatively proposes a point cloud\nlearning framework called DrivAer Transformer (DAT). The DAT structure uses the\nDrivAerNet++ dataset, which contains high-fidelity CFD data of\nindustrial-standard 3D vehicle shapes. enabling accurate estimation of air drag\ndirectly from 3D meshes, thus avoiding the limitations of traditional methods\nsuch as 2D image rendering or signed distance fields (SDF). DAT enables fast\nand accurate drag prediction, driving the evolution of the aerodynamic\nevaluation process and laying the critical foundation for introducing a\ndata-driven approach to automotive design. The framework is expected to\naccelerate the vehicle design process and improve development efficiency.\n","authors":["Jiaqi He","Xiangwen Luo","Yiping Wang"],"pdf_url":"https://arxiv.org/pdf/2504.08217v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.10436v3","updated":"2025-04-15T07:37:10Z","published":"2025-02-09T14:24:16Z","title":"MERGE$^3$: Efficient Evolutionary Merging on Consumer-grade GPUs","summary":"  Evolutionary model merging enables the creation of high-performing multi-task\nmodels but remains computationally prohibitive for consumer hardware. We\nintroduce MERGE$^3$, an efficient framework that makes evolutionary merging\nfeasible on a single GPU by reducing fitness computation costs 50$\\times$ while\npreserving performance. MERGE$^3$ achieves this by Extracting a reduced dataset\nfor evaluation, Estimating model abilities using Item Response Theory (IRT),\nand Evolving optimal merges via IRT-based performance estimators. Our method\nenables state-of-the-art multilingual and cross-lingual merging, transferring\nknowledge across languages with significantly lower computational overhead. We\nprovide theoretical guarantees and an open-source library, democratizing\nhigh-quality model merging.\n","authors":["Tommaso Mencattini","Adrian Robert Minut","Donato Crisostomi","Andrea Santilli","Emanuele Rodolà"],"pdf_url":"https://arxiv.org/pdf/2502.10436v3.pdf","comment":"19 pages, 13 figures"},{"id":"http://arxiv.org/abs/2504.10936v1","updated":"2025-04-15T07:32:35Z","published":"2025-04-15T07:32:35Z","title":"Can LLMs Leverage Observational Data? Towards Data-Driven Causal\n  Discovery with LLMs","summary":"  Causal discovery traditionally relies on statistical methods applied to\nobservational data, often requiring large datasets and assumptions about\nunderlying causal structures. Recent advancements in Large Language Models\n(LLMs) have introduced new possibilities for causal discovery by providing\ndomain expert knowledge. However, it remains unclear whether LLMs can\neffectively process observational data for causal discovery. In this work, we\nexplore the potential of LLMs for data-driven causal discovery by integrating\nobservational data for LLM-based reasoning. Specifically, we examine whether\nLLMs can effectively utilize observational data through two prompting\nstrategies: pairwise prompting and breadth first search (BFS)-based prompting.\nIn both approaches, we incorporate the observational data directly into the\nprompt to assess LLMs' ability to infer causal relationships from such data.\nExperiments on benchmark datasets show that incorporating observational data\nenhances causal discovery, boosting F1 scores by up to 0.11 point using both\npairwise and BFS LLM-based prompting, while outperforming traditional\nstatistical causal discovery baseline by up to 0.52 points. Our findings\nhighlight the potential and limitations of LLMs for data-driven causal\ndiscovery, demonstrating their ability to move beyond textual metadata and\neffectively interpret and utilize observational data for more informed causal\nreasoning. Our studies lays the groundwork for future advancements toward fully\nLLM-driven causal discovery.\n","authors":["Yuni Susanti","Michael Färber"],"pdf_url":"https://arxiv.org/pdf/2504.10936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10932v1","updated":"2025-04-15T07:19:54Z","published":"2025-04-15T07:19:54Z","title":"Multi-scale DeepOnet (Mscale-DeepOnet) for Mitigating Spectral Bias in\n  Learning High Frequency Operators of Oscillatory Functions","summary":"  In this paper, a multi-scale DeepOnet (Mscale-DeepOnet) is proposed to reduce\nthe spectral bias of the DeepOnet in learning high-frequency mapping between\nhighly oscillatory functions, with an application to the nonlinear mapping\nbetween the coefficient of the Helmholtz equation and its solution. The\nMscale-DeepOnet introduces the multiscale neural network in the branch and\ntrunk networks of the original DeepOnet, the resulting Mscale-DeepOnet is shown\nto be able to capture various high-frequency components of the mapping itself\nand its image. Numerical results demonstrate the substantial improvement of the\nMscale-DeepOnet for the problem of wave scattering in the high-frequency regime\nover the normal DeepOnet with a similar number of network parameters.\n","authors":["Bo Wang","Lizuo Liu","Wei Cai"],"pdf_url":"https://arxiv.org/pdf/2504.10932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10925v1","updated":"2025-04-15T07:12:00Z","published":"2025-04-15T07:12:00Z","title":"Transfer Learning for Temporal Link Prediction","summary":"  Link prediction on graphs has applications spanning from recommender systems\nto drug discovery. Temporal link prediction (TLP) refers to predicting future\nlinks in a temporally evolving graph and adds additional complexity related to\nthe dynamic nature of graphs. State-of-the-art TLP models incorporate memory\nmodules alongside graph neural networks to learn both the temporal mechanisms\nof incoming nodes and the evolving graph topology. However, memory modules only\nstore information about nodes seen at train time, and hence such models cannot\nbe directly transferred to entirely new graphs at test time and deployment. In\nthis work, we study a new transfer learning task for temporal link prediction,\nand develop transfer-effective methods for memory-laden models. Specifically,\nmotivated by work showing the informativeness of structural signals for the TLP\ntask, we augment a structural mapping module to the existing TLP model\narchitectures, which learns a mapping from graph structural (topological)\nfeatures to memory embeddings. Our work paves the way for a memory-free\nfoundation model for TLP.\n","authors":["Ayan Chatterjee","Barbara Ikica","Babak Ravandi","John Palowitch"],"pdf_url":"https://arxiv.org/pdf/2504.10925v1.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.10923v1","updated":"2025-04-15T07:09:54Z","published":"2025-04-15T07:09:54Z","title":"Fast-Powerformer: A Memory-Efficient Transformer for Accurate Mid-Term\n  Wind Power Forecasting","summary":"  Wind power forecasting (WPF), as a significant research topic within\nrenewable energy, plays a crucial role in enhancing the security, stability,\nand economic operation of power grids. However, due to the high stochasticity\nof meteorological factors (e.g., wind speed) and significant fluctuations in\nwind power output, mid-term wind power forecasting faces a dual challenge of\nmaintaining high accuracy and computational efficiency. To address these\nissues, this paper proposes an efficient and lightweight mid-term wind power\nforecasting model, termed Fast-Powerformer. The proposed model is built upon\nthe Reformer architecture, incorporating structural enhancements such as a\nlightweight Long Short-Term Memory (LSTM) embedding module, an input\ntransposition mechanism, and a Frequency Enhanced Channel Attention Mechanism\n(FECAM). These improvements enable the model to strengthen temporal feature\nextraction, optimize dependency modeling across variables, significantly reduce\ncomputational complexity, and enhance sensitivity to periodic patterns and\ndominant frequency components. Experimental results conducted on multiple\nreal-world wind farm datasets demonstrate that the proposed Fast-Powerformer\nachieves superior prediction accuracy and operational efficiency compared to\nmainstream forecasting approaches. Furthermore, the model exhibits fast\ninference speed and low memory consumption, highlighting its considerable\npractical value for real-world deployment scenarios.\n","authors":["Mingyi Zhu","Zhaoxin Li","Qiao Lin","Li Ding"],"pdf_url":"https://arxiv.org/pdf/2504.10923v1.pdf","comment":"Mingyi Zhu is the first author. Li Ding is the corresponding author"},{"id":"http://arxiv.org/abs/2410.14081v2","updated":"2025-04-15T07:02:50Z","published":"2024-10-17T23:13:32Z","title":"Reward-free World Models for Online Imitation Learning","summary":"  Imitation learning (IL) enables agents to acquire skills directly from expert\ndemonstrations, providing a compelling alternative to reinforcement learning.\nHowever, prior online IL approaches struggle with complex tasks characterized\nby high-dimensional inputs and complex dynamics. In this work, we propose a\nnovel approach to online imitation learning that leverages reward-free world\nmodels. Our method learns environmental dynamics entirely in latent spaces\nwithout reconstruction, enabling efficient and accurate modeling. We adopt the\ninverse soft-Q learning objective, reformulating the optimization process in\nthe Q-policy space to mitigate the instability associated with traditional\noptimization in the reward-policy space. By employing a learned latent dynamics\nmodel and planning for control, our approach consistently achieves stable,\nexpert-level performance in tasks with high-dimensional observation or action\nspaces and intricate dynamics. We evaluate our method on a diverse set of\nbenchmarks, including DMControl, MyoSuite, and ManiSkill2, demonstrating\nsuperior empirical performance compared to existing approaches.\n","authors":["Shangzhe Li","Zhiao Huang","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2410.14081v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10917v1","updated":"2025-04-15T06:57:26Z","published":"2025-04-15T06:57:26Z","title":"Towards A Universal Graph Structural Encoder","summary":"  Recent advancements in large-scale pre-training have shown the potential to\nlearn generalizable representations for downstream tasks. In the graph domain,\nhowever, capturing and transferring structural information across different\ngraph domains remains challenging, primarily due to the inherent differences in\ntopological patterns across various contexts. Additionally, most existing\nmodels struggle to capture the complexity of rich graph structures, leading to\ninadequate exploration of the embedding space. To address these challenges, we\npropose GFSE, a universal graph structural encoder designed to capture\ntransferable structural patterns across diverse domains such as molecular\ngraphs, social networks, and citation networks. GFSE is the first cross-domain\ngraph structural encoder pre-trained with multiple self-supervised learning\nobjectives. Built on a Graph Transformer, GFSE incorporates attention\nmechanisms informed by graph inductive bias, enabling it to encode intricate\nmulti-level and fine-grained topological features. The pre-trained GFSE\nproduces generic and theoretically expressive positional and structural\nencoding for graphs, which can be seamlessly integrated with various downstream\ngraph feature encoders, including graph neural networks for vectorized features\nand Large Language Models for text-attributed graphs. Comprehensive experiments\non synthetic and real-world datasets demonstrate GFSE's capability to\nsignificantly enhance the model's performance while requiring substantially\nless task-specific fine-tuning. Notably, GFSE achieves state-of-the-art\nperformance in 81.6% evaluated cases, spanning diverse graph models and\ndatasets, highlighting its potential as a powerful and versatile encoder for\ngraph-structured data.\n","authors":["Jialin Chen","Haolan Zuo","Haoyu Peter Wang","Siqi Miao","Pan Li","Rex Ying"],"pdf_url":"https://arxiv.org/pdf/2504.10917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10902v1","updated":"2025-04-15T06:23:24Z","published":"2025-04-15T06:23:24Z","title":"Leveraging Submodule Linearity Enhances Task Arithmetic Performance in\n  LLMs","summary":"  Task arithmetic is a straightforward yet highly effective strategy for model\nmerging, enabling the resultant model to exhibit multi-task capabilities.\nRecent research indicates that models demonstrating linearity enhance the\nperformance of task arithmetic. In contrast to existing methods that rely on\nthe global linearization of the model, we argue that this linearity already\nexists within the model's submodules. In particular, we present a statistical\nanalysis and show that submodules (e.g., layers, self-attentions, and MLPs)\nexhibit significantly higher linearity than the overall model. Based on these\nfindings, we propose an innovative model merging strategy that independently\nmerges these submodules. Especially, we derive a closed-form solution for\noptimal merging weights grounded in the linear properties of these submodules.\nExperimental results demonstrate that our method consistently outperforms the\nstandard task arithmetic approach and other established baselines across\ndifferent model scales and various tasks. This result highlights the benefits\nof leveraging the linearity of submodules and provides a new perspective for\nexploring solutions for effective and practical multi-task model merging.\n","authors":["Rui Dai","Sile Hu","Xu Shen","Yonggang Zhang","Xinmei Tian","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2504.10902v1.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2504.10900v1","updated":"2025-04-15T06:23:00Z","published":"2025-04-15T06:23:00Z","title":"Bridging Distribution Gaps in Time Series Foundation Model Pretraining\n  with Prototype-Guided Normalization","summary":"  Foundation models have achieved remarkable success across diverse\nmachine-learning domains through large-scale pretraining on large, diverse\ndatasets. However, pretraining on such datasets introduces significant\nchallenges due to substantial mismatches in data distributions, a problem\nparticularly pronounced with time series data. In this paper, we tackle this\nissue by proposing a domain-aware adaptive normalization strategy within the\nTransformer architecture. Specifically, we replace the traditional LayerNorm\nwith a prototype-guided dynamic normalization mechanism (ProtoNorm), where\nlearned prototypes encapsulate distinct data distributions, and\nsample-to-prototype affinity determines the appropriate normalization layer.\nThis mechanism effectively captures the heterogeneity of time series\ncharacteristics, aligning pretrained representations with downstream tasks.\nThrough comprehensive empirical evaluation, we demonstrate that our method\nsignificantly outperforms conventional pretraining techniques across both\nclassification and forecasting tasks, while effectively mitigating the adverse\neffects of distribution shifts during pretraining. Incorporating ProtoNorm is\nas simple as replacing a single line of code. Extensive experiments on diverse\nreal-world time series benchmarks validate the robustness and generalizability\nof our approach, advancing the development of more versatile time series\nfoundation models.\n","authors":["Peiliang Gong","Emadeldeen Eldele","Min Wu","Zhenghua Chen","Xiaoli Li","Daoqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.10900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07570v2","updated":"2025-04-15T06:22:26Z","published":"2025-03-10T17:40:05Z","title":"Split-n-Chain: Privacy-Preserving Multi-Node Split Learning with\n  Blockchain-Based Auditability","summary":"  Deep learning, when integrated with a large amount of training data, has the\npotential to outperform machine learning in terms of high accuracy. Recently,\nprivacy-preserving deep learning has drawn significant attention of the\nresearch community. Different privacy notions in deep learning include privacy\nof data provided by data-owners and privacy of parameters and/or\nhyperparameters of the underlying neural network. Federated learning is a\npopular privacy-preserving execution environment where data-owners participate\nin learning the parameters collectively without leaking their respective data\nto other participants. However, federated learning suffers from certain\nsecurity/privacy issues. In this paper, we propose Split-n-Chain, a variant of\nsplit learning where the layers of the network are split among several\ndistributed nodes. Split-n-Chain achieves several privacy properties:\ndata-owners need not share their training data with other nodes, and no nodes\nhave access to the parameters and hyperparameters of the neural network (except\nthat of the respective layers they hold). Moreover, Split-n-Chain uses\nblockchain to audit the computation done by different nodes. Our experimental\nresults show that: Split-n-Chain is efficient, in terms of time required to\nexecute different phases, and the training loss trend is similar to that for\nthe same neural network when implemented in a monolithic fashion.\n","authors":["Mukesh Sahani","Binanda Sengupta"],"pdf_url":"https://arxiv.org/pdf/2503.07570v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17049v2","updated":"2025-04-15T06:07:22Z","published":"2024-05-27T11:03:48Z","title":"Verifying Properties of Binary Neural Networks Using Sparse Polynomial\n  Optimization","summary":"  This paper explores methods for verifying the properties of Binary Neural\nNetworks (BNNs), focusing on robustness against adversarial attacks. Despite\ntheir lower computational and memory needs, BNNs, like their full-precision\ncounterparts, are also sensitive to input perturbations. Established methods\nfor solving this problem are predominantly based on Satisfiability Modulo\nTheories and Mixed-Integer Linear Programming techniques, which are\ncharacterized by NP complexity and often face scalability issues.\n  We introduce an alternative approach using Semidefinite Programming\nrelaxations derived from sparse Polynomial Optimization. Our approach,\ncompatible with continuous input space, not only mitigates numerical issues\nassociated with floating-point calculations but also enhances verification\nscalability through the strategic use of tighter first-order semidefinite\nrelaxations. We demonstrate the effectiveness of our method in verifying\nrobustness against both $\\|.\\|_\\infty$ and $\\|.\\|_2$-based adversarial attacks.\n","authors":["Jianting Yang","Srećko Ðurašinović","Jean-Bernard Lasserre","Victor Magron","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2405.17049v2.pdf","comment":"25 pages, 2 figures, 8 tables"},{"id":"http://arxiv.org/abs/2502.01860v3","updated":"2025-04-15T05:34:27Z","published":"2025-02-03T22:19:28Z","title":"SE Arena: An Interactive Platform for Evaluating Foundation Models in\n  Software Engineering","summary":"  Foundation models (FMs), particularly large language models (LLMs), have\nshown significant promise in various software engineering (SE) tasks, including\ncode generation, debugging, and requirement refinement. Despite these advances,\nexisting evaluation frameworks are insufficient for assessing model performance\nin iterative, context-rich workflows characteristic of SE activities. To\naddress this limitation, we introduce SE Arena, an interactive platform\ndesigned to evaluate SE-focused chatbots. SE Arena provides a transparent,\nopen-source leaderboard, supports multi-round conversational workflows, and\nenables end-to-end model comparisons. The platform introduces novel metrics,\nincluding the consistency score that measures model consistency through\nself-play matches. Moreover, SE Arena incorporates a new feature called\nRepoChat, which automatically injects repository-related context (e.g., issues,\ncommits, pull requests) into the conversation, further aligning evaluations\nwith real-world development processes. This paper outlines the design and\ncapabilities of SE Arena, emphasizing its potential to advance the evaluation\nand practical application of FMs in software engineering.\n","authors":["Zhimin Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.01860v3.pdf","comment":"Check the arena at\n  https://huggingface.co/spaces/SE-Arena/Software-Engineering-Arena"},{"id":"http://arxiv.org/abs/2504.03699v3","updated":"2025-04-15T05:26:26Z","published":"2025-03-25T05:32:43Z","title":"Reinforcing Clinical Decision Support through Multi-Agent Systems and\n  Ethical AI Governance","summary":"  Recent advances in the data-driven medicine approach, which integrates\nethically managed and explainable artificial intelligence into clinical\ndecision support systems (CDSS), are critical to ensure reliable and effective\npatient care. This paper focuses on comparing novel agent system designs that\nuse modular agents to analyze laboratory results, vital signs, and clinical\ncontext, and to predict and validate results. We implement our agent system\nwith the eICU database, including running lab analysis, vitals-only\ninterpreters, and contextual reasoners agents first, then sharing the memory\ninto the integration agent, prediction agent, transparency agent, and a\nvalidation agent. Our results suggest that the multi-agent system (MAS)\nperformed better than the single-agent system (SAS) with mortality prediction\naccuracy (59%, 56%) and the mean error for length of stay (LOS)(4.37 days, 5.82\ndays), respectively. However, the transparency score for the SAS (86.21) is\nslightly better than the transparency score for MAS (85.5). Finally, this study\nsuggests that our agent-based framework not only improves process transparency\nand prediction accuracy but also strengthens trustworthy AI-assisted decision\nsupport in an intensive care setting.\n","authors":["Ying-Jung Chen","Ahmad Albarqawi","Chi-Sheng Chen"],"pdf_url":"https://arxiv.org/pdf/2504.03699v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10878v1","updated":"2025-04-15T05:11:40Z","published":"2025-04-15T05:11:40Z","title":"Large Language Model-Informed Feature Discovery Improves Prediction and\n  Interpretation of Credibility Perceptions of Visual Content","summary":"  In today's visually dominated social media landscape, predicting the\nperceived credibility of visual content and understanding what drives human\njudgment are crucial for countering misinformation. However, these tasks are\nchallenging due to the diversity and richness of visual features. We introduce\na Large Language Model (LLM)-informed feature discovery framework that\nleverages multimodal LLMs, such as GPT-4o, to evaluate content credibility and\nexplain its reasoning. We extract and quantify interpretable features using\ntargeted prompts and integrate them into machine learning models to improve\ncredibility predictions. We tested this approach on 4,191 visual social media\nposts across eight topics in science, health, and politics, using credibility\nratings from 5,355 crowdsourced workers. Our method outperformed zero-shot\nGPT-based predictions by 13 percent in R2, and revealed key features like\ninformation concreteness and image format. We discuss the implications for\nmisinformation mitigation, visual credibility, and the role of LLMs in social\nscience.\n","authors":["Yilang Peng","Sijia Qian","Yingdan Lu","Cuihua Shen"],"pdf_url":"https://arxiv.org/pdf/2504.10878v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2309.06774v2","updated":"2025-04-15T05:09:21Z","published":"2023-09-13T07:49:28Z","title":"Fundamental Limits of Deep Learning-Based Binary Classifiers Trained\n  with Hinge Loss","summary":"  Although deep learning (DL) has led to several breakthroughs in many\ndisciplines, the fundamental understanding on why and how DL is empirically\nsuccessful remains elusive. To attack this fundamental problem and unravel the\nmysteries behind DL's empirical successes, significant innovations toward a\nunified theory of DL have been made. Although these innovations encompass\nnearly fundamental advances in optimization, generalization, and approximation,\nno work has quantified the testing performance of a DL-based algorithm employed\nto solve a pattern classification problem. To overcome this fundamental\nchallenge in part, this paper exposes the fundamental testing performance\nlimits of DL-based binary classifiers trained with hinge loss. For binary\nclassifiers that are based on deep rectified linear unit (ReLU) feedforward\nneural networks (FNNs) and deep FNNs with ReLU and Tanh activation, we derive\ntheir respective novel asymptotic testing performance limits, which are\nvalidated by extensive computer experiments.\n","authors":["Tilahun M. Getu","Georges Kaddoum","M. Bennis"],"pdf_url":"https://arxiv.org/pdf/2309.06774v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.09746v2","updated":"2025-04-15T04:59:42Z","published":"2023-06-16T10:25:43Z","title":"Finite-Time Analysis of Temporal Difference Learning with Experience\n  Replay","summary":"  Temporal-difference (TD) learning is widely regarded as one of the most\npopular algorithms in reinforcement learning (RL). Despite its widespread use,\nit has only been recently that researchers have begun to actively study its\nfinite time behavior, including the finite time bound on mean squared error and\nsample complexity. On the empirical side, experience replay has been a key\ningredient in the success of deep RL algorithms, but its theoretical effects on\nRL have yet to be fully understood. In this paper, we present a simple\ndecomposition of the Markovian noise terms and provide finite-time error bounds\nfor TD-learning with experience replay. Specifically, under the Markovian\nobservation model, we demonstrate that for both the averaged iterate and final\niterate cases, the error term induced by a constant step-size can be\neffectively controlled by the size of the replay buffer and the mini-batch\nsampled from the experience replay buffer.\n","authors":["Han-Dong Lim","Donghwan Lee"],"pdf_url":"https://arxiv.org/pdf/2306.09746v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10865v1","updated":"2025-04-15T04:56:33Z","published":"2025-04-15T04:56:33Z","title":"Understanding the theoretical properties of projected Bellman equation,\n  linear Q-learning, and approximate value iteration","summary":"  In this paper, we study the theoretical properties of the projected Bellman\nequation (PBE) and two algorithms to solve this equation: linear Q-learning and\napproximate value iteration (AVI). We consider two sufficient conditions for\nthe existence of a solution to PBE : strictly negatively row dominating\ndiagonal (SNRDD) assumption and a condition motivated by the convergence of\nAVI. The SNRDD assumption also ensures the convergence of linear Q-learning,\nand its relationship with the convergence of AVI is examined. Lastly, several\ninteresting observations on the solution of PBE are provided when using\n$\\epsilon$-greedy policy.\n","authors":["Han-Dong Lim","Donghwan Lee"],"pdf_url":"https://arxiv.org/pdf/2504.10865v1.pdf","comment":"Initial submission"},{"id":"http://arxiv.org/abs/2410.19892v2","updated":"2025-04-15T04:41:17Z","published":"2024-10-25T13:56:13Z","title":"Air Quality Prediction with Physics-Guided Dual Neural ODEs in Open\n  Systems","summary":"  Air pollution significantly threatens human health and ecosystems,\nnecessitating effective air quality prediction to inform public policy.\nTraditional approaches are generally categorized into physics-based and\ndata-driven models. Physics-based models usually struggle with high\ncomputational demands and closed-system assumptions, while data-driven models\nmay overlook essential physical dynamics, confusing the capturing of\nspatiotemporal correlations. Although some physics-guided approaches combine\nthe strengths of both models, they often face a mismatch between explicit\nphysical equations and implicit learned representations. To address these\nchallenges, we propose Air-DualODE, a novel physics-guided approach that\nintegrates dual branches of Neural ODEs for air quality prediction. The first\nbranch applies open-system physical equations to capture spatiotemporal\ndependencies for learning physics dynamics, while the second branch identifies\nthe dependencies not addressed by the first in a fully data-driven way. These\ndual representations are temporally aligned and fused to enhance prediction\naccuracy. Our experimental results demonstrate that Air-DualODE achieves\nstate-of-the-art performance in predicting pollutant concentrations across\nvarious spatial scales, thereby offering a promising solution for real-world\nair quality challenges.\n","authors":["Jindong Tian","Yuxuan Liang","Ronghui Xu","Peng Chen","Chenjuan Guo","Aoying Zhou","Lujia Pan","Zhongwen Rao","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2410.19892v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19821v2","updated":"2025-04-15T04:24:18Z","published":"2025-03-25T16:32:03Z","title":"IgCraft: A versatile sequence generation framework for antibody\n  discovery and engineering","summary":"  Designing antibody sequences to better resemble those observed in natural\nhuman repertoires is a key challenge in biologics development. We introduce\nIgCraft: a multi-purpose model for paired human antibody sequence generation,\nbuilt on Bayesian Flow Networks. IgCraft presents one of the first unified\ngenerative modeling frameworks capable of addressing multiple antibody sequence\ndesign tasks with a single model, including unconditional sampling, sequence\ninpainting, inverse folding, and CDR motif scaffolding. Our approach achieves\ncompetitive results across the full spectrum of these tasks while constraining\ngeneration to the space of human antibody sequences, exhibiting particular\nstrengths in CDR motif scaffolding (grafting) where we achieve state-of-the-art\nperformance in terms of humanness and preservation of structural properties. By\nintegrating previously separate tasks into a single scalable generative model,\nIgCraft provides a versatile platform for sampling human antibody sequences\nunder a variety of contexts relevant to antibody discovery and engineering.\nModel code and weights are publicly available at\nhttps://github.com/mgreenig/IgCraft.\n","authors":["Matthew Greenig","Haowen Zhao","Vladimir Radenkovic","Aubin Ramon","Pietro Sormanni"],"pdf_url":"https://arxiv.org/pdf/2503.19821v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10851v1","updated":"2025-04-15T04:19:04Z","published":"2025-04-15T04:19:04Z","title":"ICAFS: Inter-Client-Aware Feature Selection for Vertical Federated\n  Learning","summary":"  Vertical federated learning (VFL) enables a paradigm for vertically\npartitioned data across clients to collaboratively train machine learning\nmodels. Feature selection (FS) plays a crucial role in Vertical Federated\nLearning (VFL) due to the unique nature that data are distributed across\nmultiple clients. In VFL, different clients possess distinct subsets of\nfeatures for overlapping data samples, making the process of identifying and\nselecting the most relevant features a complex yet essential task. Previous FS\nefforts have primarily revolved around intra-client feature selection,\noverlooking vital feature interaction across clients, leading to subpar model\noutcomes. We introduce ICAFS, a novel multi-stage ensemble approach for\neffective FS in VFL by considering inter-client interactions. By employing\nconditional feature synthesis alongside multiple learnable feature selectors,\nICAFS facilitates ensemble FS over these selectors using synthetic embeddings.\nThis method bypasses the limitations of private gradient sharing and allows for\nmodel training using real data with refined embeddings. Experiments on multiple\nreal-world datasets demonstrate that ICAFS surpasses current state-of-the-art\nmethods in prediction accuracy.\n","authors":["Ruochen Jin","Boning Tong","Shu Yang","Bojian Hou","Li Shen"],"pdf_url":"https://arxiv.org/pdf/2504.10851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10850v1","updated":"2025-04-15T04:17:37Z","published":"2025-04-15T04:17:37Z","title":"How to Enhance Downstream Adversarial Robustness (almost) without\n  Touching the Pre-Trained Foundation Model?","summary":"  With the rise of powerful foundation models, a pre-training-fine-tuning\nparadigm becomes increasingly popular these days: A foundation model is\npre-trained using a huge amount of data from various sources, and then the\ndownstream users only need to fine-tune and adapt it to specific downstream\ntasks. However, due to the high computation complexity of adversarial training,\nit is not feasible to fine-tune the foundation model to improve its robustness\non the downstream task. Observing the above challenge, we want to improve the\ndownstream robustness without updating/accessing the weights in the foundation\nmodel. Inspired from existing literature in robustness inheritance (Kim et al.,\n2020), through theoretical investigation, we identify a close relationship\nbetween robust contrastive learning with the adversarial robustness of\nsupervised learning. To further validate and utilize this theoretical insight,\nwe design a simple-yet-effective robust auto-encoder as a data pre-processing\nmethod before feeding the data into the foundation model. The proposed approach\nhas zero access to the foundation model when training the robust auto-encoder.\nExtensive experiments demonstrate the effectiveness of the proposed method in\nimproving the robustness of downstream tasks, verifying the connection between\nthe feature robustness (implied by small adversarial contrastive loss) and the\nrobustness of the downstream task.\n","authors":["Meiqi Liu","Zhuoqun Huang","Yue Xing"],"pdf_url":"https://arxiv.org/pdf/2504.10850v1.pdf","comment":"22 pages, 2 figures, 12 tables. Include 10 pages of appendices"},{"id":"http://arxiv.org/abs/2302.05508v2","updated":"2025-04-15T04:08:13Z","published":"2023-02-10T20:54:10Z","title":"FairPy: A Toolkit for Evaluation of Prediction Biases and their\n  Mitigation in Large Language Models","summary":"  Recent studies have demonstrated that large pretrained language models (LLMs)\nsuch as BERT and GPT-2 exhibit biases in token prediction, often inherited from\nthe data distributions present in their training corpora. In response, a number\nof mathematical frameworks have been proposed to quantify, identify, and\nmitigate these the likelihood of biased token predictions. In this paper, we\npresent a comprehensive survey of such techniques tailored towards widely used\nLLMs such as BERT, GPT-2, etc. We additionally introduce Fairpy, a modular and\nextensible toolkit that provides plug-and-play interfaces for integrating these\nmathematical tools, enabling users to evaluate both pretrained and custom\nlanguage models. Fairpy supports the implementation of existing debiasing\nalgorithms. The toolkit is open-source and publicly available at:\n\\href{https://github.com/HrishikeshVish/Fairpy}{https://github.com/HrishikeshVish/Fairpy}\n","authors":["Hrishikesh Viswanath","Tianyi Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.05508v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14629v2","updated":"2025-04-15T04:04:00Z","published":"2023-10-23T07:02:30Z","title":"Making informed decisions in cutting tool maintenance in milling: A\n  KNN-based model agnostic approach","summary":"  Tool Condition Monitoring (TCM) is vital for maintaining productivity and\nproduct quality in machining. This study leverages machine learning to analyze\nreal-time force signals collected from experiments under various tool wear\nconditions. Statistical analysis and feature selection using decision trees\nwere followed by classification using a K-Nearest Neighbors (KNN) algorithm,\nwith hyperparameter tuning to enhance performance. While machine learning has\nbeen widely applied in TCM, interpretability remains limited. This work\nintroduces a KNN-based white-box model that enhances transparency in\ndecision-making by revealing how features influence classification. The model\nnot only detects tool wear but also provides insights into the reasoning behind\neach decision, enabling manufacturers to make informed maintenance choices.\n","authors":["Revati M. Wahul","Aditya M. Rahalkar","Om M. Khare","Abhishek D. Patange","Rohan N. Soman"],"pdf_url":"https://arxiv.org/pdf/2310.14629v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03026v3","updated":"2025-04-15T03:45:30Z","published":"2023-10-04T17:59:49Z","title":"LanguageMPC: Large Language Models as Decision Makers for Autonomous\n  Driving","summary":"  Existing learning-based autonomous driving (AD) systems face challenges in\ncomprehending high-level information, generalizing to rare events, and\nproviding interpretability. To address these problems, this work employs Large\nLanguage Models (LLMs) as a decision-making component for complex AD scenarios\nthat require human commonsense understanding. We devise cognitive pathways to\nenable comprehensive reasoning with LLMs, and develop algorithms for\ntranslating LLM decisions into actionable driving commands. Through this\napproach, LLM decisions are seamlessly integrated with low-level controllers by\nguided parameter matrix adaptation. Extensive experiments demonstrate that our\nproposed method not only consistently surpasses baseline approaches in\nsingle-vehicle tasks, but also helps handle complex driving behaviors even\nmulti-vehicle coordination, thanks to the commonsense reasoning capabilities of\nLLMs. This paper presents an initial step toward leveraging LLMs as effective\ndecision-makers for intricate AD scenarios in terms of safety, efficiency,\ngeneralizability, and interoperability. We aspire for it to serve as\ninspiration for future research in this field. Project page:\nhttps://sites.google.com/view/llm-mpc\n","authors":["Hao Sha","Yao Mu","Yuxuan Jiang","Li Chen","Chenfeng Xu","Ping Luo","Shengbo Eben Li","Masayoshi Tomizuka","Wei Zhan","Mingyu Ding"],"pdf_url":"https://arxiv.org/pdf/2310.03026v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07031v2","updated":"2025-04-15T03:43:54Z","published":"2024-03-11T04:19:05Z","title":"Cramming Contextual Bandits for On-policy Statistical Evaluation","summary":"  We introduce the cram method as a general statistical framework for\nevaluating the final learned policy from a multi-armed contextual bandit\nalgorithm, using the dataset generated by the same bandit algorithm. The\nproposed on-policy evaluation methodology differs from most existing methods\nthat focus on off-policy performance evaluation of contextual bandit\nalgorithms. Cramming utilizes an entire bandit sequence through a single pass\nof data, leading to both statistically and computationally efficient\nevaluation. We prove that if a bandit algorithm satisfies a certain stability\ncondition, the resulting crammed evaluation estimator is consistent and\nasymptotically normal under mild regularity conditions. Furthermore, we show\nthat this stability condition holds for commonly used linear contextual bandit\nalgorithms, including epsilon-greedy, Thompson Sampling, and Upper Confidence\nBound algorithms. Using both synthetic and publicly available datasets, we\ncompare the empirical performance of cramming with the state-of-the-art\nmethods. The results demonstrate that the proposed cram method reduces the\nevaluation standard error by approximately 40% relative to off-policy\nevaluation methods while preserving unbiasedness and valid confidence interval\ncoverage.\n","authors":["Zeyang Jia","Kosuke Imai","Michael Lingzhi Li"],"pdf_url":"https://arxiv.org/pdf/2403.07031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14414v2","updated":"2025-04-15T03:41:17Z","published":"2024-07-19T15:40:59Z","title":"System-1.x: Learning to Balance Fast and Slow Planning with Language\n  Models","summary":"  Language models can be used to solve long-horizon planning problems in two\ndistinct modes: a fast 'System-1' mode, directly generating plans without any\nexplicit search or backtracking, and a slow 'System-2' mode, planning\nstep-by-step by explicitly searching over possible actions. While System-2 is\ntypically more effective, it is also more computationally expensive, making it\ninfeasible for long plans or large action spaces. Moreover, isolated System-1\nor 2 ignores the user's end goals, failing to provide ways to control the\nmodel's behavior. To this end, we propose the System-1.x Planner, a\ncontrollable planning framework with LLMs that is capable of generating hybrid\nplans and balancing between the two planning modes based on the difficulty of\nthe problem at hand. System-1.x consists of (i) a controller, (ii) a System-1\nPlanner, and (iii) a System-2 Planner. Based on a user-specified hybridization\nfactor (x) governing the mixture between System-1 and 2, the controller\ndecomposes a problem into sub-goals, and classifies them as easy or hard to be\nsolved by either System-1 or 2, respectively. We fine-tune all three components\non top of a single base LLM, requiring only search traces as supervision.\nExperiments with two diverse planning tasks -- Maze Navigation and Blocksworld\n-- show that our System-1.x Planner outperforms a System-1 Planner, a System-2\nPlanner trained to approximate A* search, and also a symbolic planner (A*). We\ndemonstrate the following key properties of our planner: (1) controllability:\nincreasing the hybridization factor (e.g., System-1.75 vs 1.5) performs more\nsearch, improving performance, (2) flexibility: by building a neuro-symbolic\nvariant with a neural System-1 and a symbolic System-2, we can use existing\nsymbolic methods, and (3) generalizability: by being able to learn from\ndifferent search algorithms, our method is robust to the choice of search\nalgorithm.\n","authors":["Swarnadeep Saha","Archiki Prasad","Justin Chih-Yao Chen","Peter Hase","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2407.14414v2.pdf","comment":"ICLR 2025 (Camera-Ready)"},{"id":"http://arxiv.org/abs/2504.08178v3","updated":"2025-04-15T03:35:43Z","published":"2025-04-11T00:20:37Z","title":"A Piecewise Lyapunov Analysis of Sub-quadratic SGD: Applications to\n  Robust and Quantile Regression","summary":"  Motivated by robust and quantile regression problems, we investigate the\nstochastic gradient descent (SGD) algorithm for minimizing an objective\nfunction $f$ that is locally strongly convex with a sub--quadratic tail. This\nsetting covers many widely used online statistical methods. We introduce a\nnovel piecewise Lyapunov function that enables us to handle functions $f$ with\nonly first-order differentiability, which includes a wide range of popular loss\nfunctions such as Huber loss. Leveraging our proposed Lyapunov function, we\nderive finite-time moment bounds under general diminishing stepsizes, as well\nas constant stepsizes. We further establish the weak convergence, central limit\ntheorem and bias characterization under constant stepsize, providing the first\ngeometrical convergence result for sub--quadratic SGD. Our results have wide\napplications, especially in online statistical methods. In particular, we\ndiscuss two applications of our results. 1) Online robust regression: We\nconsider a corrupted linear model with sub--exponential covariates and\nheavy--tailed noise. Our analysis provides convergence rates comparable to\nthose for corrupted models with Gaussian covariates and noise. 2) Online\nquantile regression: Importantly, our results relax the common assumption in\nprior work that the conditional density is continuous and provide a more\nfine-grained analysis for the moment bounds.\n","authors":["Yixuan Zhang","Dongyan Huo","Yudong Chen","Qiaomin Xie"],"pdf_url":"https://arxiv.org/pdf/2504.08178v3.pdf","comment":"ACM SIGMETRICS 2025. 40 pages, 12 figures"},{"id":"http://arxiv.org/abs/2501.16371v2","updated":"2025-04-15T03:30:52Z","published":"2025-01-22T21:19:42Z","title":"Which Optimizer Works Best for Physics-Informed Neural Networks and\n  Kolmogorov-Arnold Networks?","summary":"  Physics-Informed Neural Networks (PINNs) have revolutionized the computation\nof PDE solutions by integrating partial differential equations (PDEs) into the\nneural network's training process as soft constraints, becoming an important\ncomponent of the scientific machine learning (SciML) ecosystem. More recently,\nphysics-informed Kolmogorv-Arnold networks (PIKANs) have also shown to be\neffective and comparable in accuracy with PINNs. In their current\nimplementation, both PINNs and PIKANs are mainly optimized using first-order\nmethods like Adam, as well as quasi-Newton methods such as BFGS and its\nlow-memory variant, L-BFGS. However, these optimizers often struggle with\nhighly non-linear and non-convex loss landscapes, leading to challenges such as\nslow convergence, local minima entrapment, and (non)degenerate saddle points.\nIn this study, we investigate the performance of Self-Scaled BFGS (SSBFGS),\nSelf-Scaled Broyden (SSBroyden) methods and other advanced quasi-Newton\nschemes, including BFGS and L-BFGS with different line search strategies\napproaches. These methods dynamically rescale updates based on historical\ngradient information, thus enhancing training efficiency and accuracy. We\nsystematically compare these optimizers -- using both PINNs and PIKANs -- on\nkey challenging linear, stiff, multi-scale and non-linear PDEs, including the\nBurgers, Allen-Cahn, Kuramoto-Sivashinsky, and Ginzburg-Landau equations. Our\nfindings provide state-of-the-art results with orders-of-magnitude accuracy\nimprovements without the use of adaptive weights or any other enhancements\ntypically employed in PINNs. More broadly, our results reveal insights into the\neffectiveness of second-order optimization strategies in significantly\nimproving the convergence and accurate generalization of PINNs and PIKANs.\n","authors":["Elham Kiyani","Khemraj Shukla","Jorge F. Urbán","Jérôme Darbon","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2501.16371v2.pdf","comment":"36 pages, 27 figures"},{"id":"http://arxiv.org/abs/2408.12902v2","updated":"2025-04-15T03:28:22Z","published":"2024-08-23T08:10:13Z","title":"IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model\n  with Multimodal Capabilities","summary":"  In the field of multimodal large language models (MLLMs), common methods\ntypically involve unfreezing the language model during training to foster\nprofound visual understanding. However, the fine-tuning of such models with\nvision-language data often leads to a diminution of their natural language\nprocessing (NLP) capabilities. To avoid this performance degradation, a\nstraightforward solution is to freeze the language model while developing\nmultimodal competencies. Unfortunately, previous works have not attained\nsatisfactory outcomes. Building on the strategy of freezing the language model,\nwe conduct thorough structural exploration and introduce the Inner-Adaptor\nArchitecture (IAA). Specifically, the architecture incorporates multiple\nmultimodal adaptors at varying depths within the large language model to\nfacilitate direct interaction with the inherently text-oriented transformer\nlayers, thereby enabling the frozen language model to acquire multimodal\ncapabilities. Unlike previous approaches of freezing language models that\nrequire large-scale aligned data, our proposed architecture is able to achieve\nsuperior performance on small-scale datasets. We conduct extensive experiments\nto improve the general multimodal capabilities and visual grounding abilities\nof the MLLM. Our approach remarkably outperforms previous state-of-the-art\nmethods across various vision-language benchmarks without sacrificing\nperformance on NLP tasks. Code and models are available at\nhttps://github.com/360CVGroup/Inner-Adaptor-Architecture.\n","authors":["Bin Wang","Chunyu Xie","Dawei Leng","Yuhui Yin"],"pdf_url":"https://arxiv.org/pdf/2408.12902v2.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2402.05406v3","updated":"2025-04-15T03:24:22Z","published":"2024-02-08T04:48:26Z","title":"Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes","summary":"  Structured pruning is a promising approach to create smaller, faster LLMs.\nHowever, existing methods typically rely on backward passes, which can inflate\nmemory requirements and compute costs. In this work we introduce Bonsai, a\ngradient-free structured pruning method that eliminates the need for\nbackpropagation, significantly reducing memory requirements and compute costs\nwhile achieving state-of-the-art pruning performance. Bonsai uses\nforward-pass-only perturbative pruning to enable efficient compression of large\nmodels on a broader range of hardware configurations. Unlike existing\nstructured pruning approaches, Bonsai not only achieves better compression with\nfewer resources, but also produces models that are twice as fast as those\ngenerated by semi-structured pruning. As a concrete demonstration, we use\nBonsai to prune an 8B LLaMA-3 model to 50% sparsity on a single A6000 GPU -- a\ntask infeasible with backprop-based methods, which require 2-3x memory. Our\nresults show that removing backprop as a requirement not only enables pruning\nlarger models on constrained hardware but can also lead to state-of-the-art\nefficiency and performance.\n","authors":["Lucio Dery","Steven Kolawole","Jean-François Kagy","Virginia Smith","Graham Neubig","Ameet Talwalkar"],"pdf_url":"https://arxiv.org/pdf/2402.05406v3.pdf","comment":"19 pages, 6 fiigures, 16 tables"},{"id":"http://arxiv.org/abs/2504.10833v1","updated":"2025-04-15T03:24:13Z","published":"2025-04-15T03:24:13Z","title":"Towards Spatially-Aware and Optimally Faithful Concept-Based\n  Explanations","summary":"  Post-hoc, unsupervised concept-based explanation methods (U-CBEMs) are a\npromising tool for generating semantic explanations of the decision-making\nprocesses in deep neural networks, having applications in both model\nimprovement and understanding. It is vital that the explanation is accurate, or\nfaithful, to the model, yet we identify several limitations of prior\nfaithfulness metrics that inhibit an accurate evaluation; most notably, prior\nmetrics involve only the set of concepts present, ignoring how they may be\nspatially distributed. We address these limitations with Surrogate Faithfulness\n(SF), an evaluation method that introduces a spatially-aware surrogate and two\nnovel faithfulness metrics. Using SF, we produce Optimally Faithful (OF)\nexplanations, where concepts are found that maximize faithfulness. Our\nexperiments show that (1) adding spatial-awareness to prior U-CBEMs increases\nfaithfulness in all cases; (2) OF produces significantly more faithful\nexplanations than prior U-CBEMs (30% or higher improvement in error); (3) OF's\nlearned concepts generalize well to out-of-domain data and are more robust to\nadversarial examples, where prior U-CBEMs struggle.\n","authors":["Shubham Kumar","Dwip Dalal","Narendra Ahuja"],"pdf_url":"https://arxiv.org/pdf/2504.10833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.12921v3","updated":"2025-04-15T03:14:54Z","published":"2023-04-24T03:09:25Z","title":"AwesomeMeta+: A Mixed-Prototyping Meta-Learning System Supporting AI\n  Application Design Anywhere","summary":"  Meta-learning, also known as ``learning to learn'', enables models to acquire\ngreat generalization abilities by learning from various tasks. Recent\nadvancements have made these models applicable across various fields without\ndata constraints, offering new opportunities for general artificial\nintelligence. However, applying these models can be challenging due to their\noften task-specific, standalone nature and the technical barriers involved. To\naddress this challenge, we develop AwesomeMeta+, a prototyping and learning\nsystem designed to standardize the key components of meta-learning within the\ncontext of systems engineering. It standardizes different components of\nmeta-learning and uses a building block metaphor to assist in model\nconstruction. By employing a modular, building-block approach, AwesomeMeta+\nfacilitates the construction of meta-learning models that can be adapted and\noptimized for specific application needs in real-world systems. The system is\ndeveloped to support the full lifecycle of meta-learning system engineering,\nfrom design to deployment, by enabling users to assemble compatible algorithmic\nmodules. We evaluate AwesomeMeta+ through feedback from 50 researchers and a\nseries of machine-based tests and user studies. The results demonstrate that\nAwesomeMeta+ enhances users' understanding of meta-learning principles,\naccelerates system engineering processes, and provides valuable decision-making\nsupport for efficient deployment of meta-learning systems in complex\napplication scenarios.\n","authors":["Jingyao Wang","Yuxuan Yang","Wenwen Qiang","Changwen Zheng","Fuchun Sun"],"pdf_url":"https://arxiv.org/pdf/2304.12921v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.03160v3","updated":"2025-04-15T02:57:20Z","published":"2025-04-04T04:41:28Z","title":"DeepResearcher: Scaling Deep Research via Reinforcement Learning in\n  Real-world Environments","summary":"  Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.\n","authors":["Yuxiang Zheng","Dayuan Fu","Xiangkun Hu","Xiaojie Cai","Lyumanshan Ye","Pengrui Lu","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2504.03160v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10821v1","updated":"2025-04-15T02:48:52Z","published":"2025-04-15T02:48:52Z","title":"Progressive Rock Music Classification","summary":"  This study investigates the classification of progressive rock music, a genre\ncharacterized by complex compositions and diverse instrumentation, distinct\nfrom other musical styles. Addressing this Music Information Retrieval (MIR)\ntask, we extracted comprehensive audio features, including spectrograms,\nMel-Frequency Cepstral Coefficients (MFCCs), chromagrams, and beat positions\nfrom song snippets using the Librosa library. A winner-take-all voting strategy\nwas employed to aggregate snippet-level predictions into final song\nclassifications. We conducted a comparative analysis of various machine\nlearning techniques. Ensemble methods, encompassing Bagging (Random Forest,\nExtraTrees, Bagging Classifier) and Boosting (XGBoost, Gradient Boosting), were\nexplored, utilizing Principal Component Analysis (PCA) for dimensionality\nreduction to manage computational constraints with high-dimensional feature\nsets. Additionally, deep learning approaches were investigated, including the\ndevelopment of custom 1D Convolutional Neural Network (1D CNN) architectures\n(named \"Zuck\" and \"Satya\") featuring specific layer configurations,\nnormalization, and activation functions. Furthermore, we fine-tuned a\nstate-of-the-art Audio Spectrogram Transformer (AST) model, leveraging its\nattention-based mechanisms for audio classification. Performance evaluation on\nvalidation and test sets revealed varying effectiveness across models, with\nensemble methods like Extra Trees achieving test accuracies up to 76.38%. This\nresearch provides insights into the application and relative performance of\ndiverse machine learning paradigms for the nuanced task of progressive rock\ngenre classification.\n","authors":["Arpan Nagar","Joseph Bensabat","Jokent Gaza","Moinak Dey"],"pdf_url":"https://arxiv.org/pdf/2504.10821v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2410.10762v4","updated":"2025-04-15T02:44:55Z","published":"2024-10-14T17:40:40Z","title":"AFlow: Automating Agentic Workflow Generation","summary":"  Large language models (LLMs) have demonstrated remarkable potential in\nsolving complex tasks across diverse domains, typically by employing agentic\nworkflows that follow detailed instructions and operational sequences. However,\nconstructing these workflows requires significant human effort, limiting\nscalability and generalizability. Recent research has sought to automate the\ngeneration and optimization of these workflows, but existing methods still rely\non initial manual setup and fall short of achieving fully automated and\neffective workflow generation. To address this challenge, we reformulate\nworkflow optimization as a search problem over code-represented workflows,\nwhere LLM-invoking nodes are connected by edges. We introduce AFlow, an\nautomated framework that efficiently explores this space using Monte Carlo Tree\nSearch, iteratively refining workflows through code modification,\ntree-structured experience, and execution feedback. Empirical evaluations\nacross six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7%\naverage improvement over state-of-the-art baselines. Furthermore, AFlow enables\nsmaller models to outperform GPT-4o on specific tasks at 4.55% of its inference\ncost in dollars. The code is available at\nhttps://github.com/FoundationAgents/AFlow.\n","authors":["Jiayi Zhang","Jinyu Xiang","Zhaoyang Yu","Fengwei Teng","Xionghui Chen","Jiaqi Chen","Mingchen Zhuge","Xin Cheng","Sirui Hong","Jinlin Wang","Bingnan Zheng","Bang Liu","Yuyu Luo","Chenglin Wu"],"pdf_url":"https://arxiv.org/pdf/2410.10762v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07154v2","updated":"2025-04-15T02:44:15Z","published":"2025-02-11T00:33:31Z","title":"Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting\n  Confidence Improves Mathematical Reasoning","summary":"  Recent progress in large language models (LLMs) highlights the power of\nscaling test-time compute to achieve strong performance on complex tasks, such\nas mathematical reasoning and code generation. This raises a critical question:\nhow should model training be modified to optimize performance under a\nsubsequent test-time compute strategy and budget? To explore this, we focus on\npass@N, a simple test-time strategy that searches for a correct answer in $N$\nindependent samples. We show, surprisingly, that training with cross-entropy\n(CE) loss can be ${\\it misaligned}$ with pass@N in that pass@N accuracy ${\\it\ndecreases}$ with longer training. We explain the origins of this misalignment\nin terms of model overconfidence induced by CE, and experimentally verify our\nprediction of overconfidence as an impediment to scaling test-time compute via\npass@N. Furthermore we suggest a principled, modified training loss that is\nbetter aligned to pass@N by limiting model confidence and rescuing pass@N test\nperformance. Our algorithm demonstrates improved mathematical reasoning on MATH\nand MiniF2F benchmarks under several scenarios: (1) providing answers to math\nquestions; and (2) proving theorems by searching over proof trees of varying\nshapes. Overall our work underscores the importance of co-designing two\ntraditionally separate phases of LLM development: training-time protocols and\ntest-time search and reasoning strategies.\n","authors":["Feng Chen","Allan Raventos","Nan Cheng","Surya Ganguli","Shaul Druckmann"],"pdf_url":"https://arxiv.org/pdf/2502.07154v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10817v1","updated":"2025-04-15T02:38:00Z","published":"2025-04-15T02:38:00Z","title":"FHBench: Towards Efficient and Personalized Federated Learning for\n  Multimodal Healthcare","summary":"  Federated Learning (FL) has emerged as an effective solution for\nmulti-institutional collaborations without sharing patient data, offering a\nrange of methods tailored for diverse applications. However, real-world medical\ndatasets are often multimodal, and computational resources are limited, posing\nsignificant challenges for existing FL approaches. Recognizing these\nlimitations, we developed the Federated Healthcare Benchmark(FHBench), a\nbenchmark specifically designed from datasets derived from real-world\nhealthcare applications. FHBench encompasses critical diagnostic tasks across\ndomains such as the nervous, cardiovascular, and respiratory systems and\ngeneral pathology, providing comprehensive support for multimodal healthcare\nevaluations and filling a significant gap in existing benchmarks. Building on\nFHBench, we introduced Efficient Personalized Federated Learning with Adaptive\nLoRA(EPFL), a personalized FL framework that demonstrates superior efficiency\nand effectiveness across various healthcare modalities. Our results highlight\nthe robustness of FHBench as a benchmarking tool and the potential of EPFL as\nan innovative approach to advancing healthcare-focused FL, addressing key\nlimitations of existing methods.\n","authors":["Penghao Wang","Qian Chen","Teng Zhang","Yingwei Zhang","Wang Lu","Yiqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2504.10817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17182v3","updated":"2025-04-15T02:32:00Z","published":"2024-07-24T11:34:24Z","title":"A DeepONet for inverting the Neumann-to-Dirichlet Operator in Electrical\n  Impedance Tomography: An approximation theoretic perspective and numerical\n  results","summary":"  In this work, we consider the non-invasive medical imaging modality of\nElectrical Impedance Tomography, where the problem is to recover the\nconductivity in a medium from a set of data that arises out of a\ncurrent-to-voltage map (Neumann-to-Dirichlet operator) defined on the boundary\nof the medium. We formulate this inverse problem as an operator-learning\nproblem where the goal is to learn the implicitly defined operator-to-function\nmap between the space of Neumann-to-Dirichlet operators to the space of\nadmissible conductivities. Subsequently, we use an operator-learning\narchitecture, popularly called DeepONets, to learn this operator-to-function\nmap. Thus far, most of the operator learning architectures have been\nimplemented to learn operators between function spaces. In this work, we\ngeneralize the earlier works and use a DeepONet to actually {learn an\noperator-to-function} map. We provide a Universal Approximation Theorem type\nresult which guarantees that this implicitly defined operator-to-function map\nbetween the space of Neumann-to-Dirichlet operator to the space of conductivity\nfunction can be approximated to an arbitrary degree using such a DeepONet.\nFurthermore, we provide a computational implementation of our proposed approach\nand compare it against a standard baseline. We show that the proposed approach\nachieves good reconstructions and outperforms the baseline method in our\nexperiments.\n","authors":["Anuj Abhishek","Thilo Strauss"],"pdf_url":"https://arxiv.org/pdf/2407.17182v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09210v2","updated":"2025-04-15T02:22:16Z","published":"2025-04-12T13:32:11Z","title":"FairACE: Achieving Degree Fairness in Graph Neural Networks via\n  Contrastive and Adversarial Group-Balanced Training","summary":"  Fairness has been a significant challenge in graph neural networks (GNNs)\nsince degree biases often result in un-equal prediction performance among nodes\nwith varying degrees. Existing GNN models focus on prediction accuracy,\nfrequently overlooking fairness across different degree groups. To addressthis\nissue, we propose a novel GNN framework, namely Fairness- Aware Asymmetric\nContrastive Ensemble (FairACE), which inte-grates asymmetric contrastive\nlearning with adversarial training to improve degree fairness. FairACE captures\none-hop local neighborhood information and two-hop monophily similarity to\ncreate fairer node representations and employs a degree fairness regulator to\nbalance performance between high-degree and low-degree nodes. During model\ntraining, a novel group-balanced fairness loss is proposed to minimize\nclassification disparities across degree groups. In addition, we also propose a\nnovel fairness metric, the Accuracy Distribution Gap (ADG), which can\nquantitatively assess and ensure equitable performance across different\ndegree-based node groups. Experimental results on both synthetic and real-world\ndatasets demonstrate that FairACE significantly improves degree fairness\nmetrics while maintaining competitive accuracy in comparison to the\nstate-of-the-art GNN models.\n","authors":["Jiaxin Liu","Xiaoqian Jiang","Xiang Li","Bohan Zhang","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.09210v2.pdf","comment":null}],"Computation and Language":[{"id":"http://arxiv.org/abs/2504.11456v1","updated":"2025-04-15T17:59:51Z","published":"2025-04-15T17:59:51Z","title":"DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and\n  Verifiable Mathematical Dataset for Advancing Reasoning","summary":"  The capacity for complex mathematical reasoning is a key benchmark for\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\nshows promise, progress is significantly hindered by the lack of large-scale\ntraining data that is sufficiently challenging, possesses verifiable answer\nformats suitable for RL, and is free from contamination with evaluation\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\nlarge-scale dataset comprising approximately 103K mathematical problems,\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\nis curated through a rigorous pipeline involving source analysis, stringent\ndecontamination against numerous benchmarks, and filtering for high difficulty\n(primarily Levels 5-9), significantly exceeding existing open resources in\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\nRL, and three distinct R1-generated solutions suitable for diverse training\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\nmathematical topics, DeepMath-103K promotes the development of generalizable\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\nsignificant improvements on challenging mathematical benchmarks, validating its\neffectiveness. We release DeepMath-103K publicly to facilitate community\nprogress in building more capable AI reasoning systems:\nhttps://github.com/zwhe99/DeepMath.\n","authors":["Zhiwei He","Tian Liang","Jiahao Xu","Qiuzhi Liu","Xingyu Chen","Yue Wang","Linfeng Song","Dian Yu","Zhenwen Liang","Wenxuan Wang","Zhuosheng Zhang","Rui Wang","Zhaopeng Tu","Haitao Mi","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2504.11456v1.pdf","comment":"WIP"},{"id":"http://arxiv.org/abs/2504.11442v1","updated":"2025-04-15T17:55:20Z","published":"2025-04-15T17:55:20Z","title":"TextArena","summary":"  TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.\n","authors":["Leon Guertler","Bobby Cheng","Simon Yu","Bo Liu","Leshem Choshen","Cheston Tan"],"pdf_url":"https://arxiv.org/pdf/2504.11442v1.pdf","comment":"work in progress; 5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2504.11441v1","updated":"2025-04-15T17:54:59Z","published":"2025-04-15T17:54:59Z","title":"TADACap: Time-series Adaptive Domain-Aware Captioning","summary":"  While image captioning has gained significant attention, the potential of\ncaptioning time-series images, prevalent in areas like finance and healthcare,\nremains largely untapped. Existing time-series captioning methods typically\noffer generic, domain-agnostic descriptions of time-series shapes and struggle\nto adapt to new domains without substantial retraining. To address these\nlimitations, we introduce TADACap, a retrieval-based framework to generate\ndomain-aware captions for time-series images, capable of adapting to new\ndomains without retraining. Building on TADACap, we propose a novel retrieval\nstrategy that retrieves diverse image-caption pairs from a target domain\ndatabase, namely TADACap-diverse. We benchmarked TADACap-diverse against\nstate-of-the-art methods and ablation variants. TADACap-diverse demonstrates\ncomparable semantic accuracy while requiring significantly less annotation\neffort.\n","authors":["Elizabeth Fons","Rachneet Kaur","Zhen Zeng","Soham Palande","Tucker Balch","Svitlana Vyetrenko","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2504.11441v1.pdf","comment":"Accepted to ICAIF 2024"},{"id":"http://arxiv.org/abs/2504.11431v1","updated":"2025-04-15T17:41:54Z","published":"2025-04-15T17:41:54Z","title":"Masculine Defaults via Gendered Discourse in Podcasts and Large Language\n  Models","summary":"  Masculine defaults are widely recognized as a significant type of gender\nbias, but they are often unseen as they are under-researched. Masculine\ndefaults involve three key parts: (i) the cultural context, (ii) the masculine\ncharacteristics or behaviors, and (iii) the reward for, or simply acceptance\nof, those masculine characteristics or behaviors. In this work, we study\ndiscourse-based masculine defaults, and propose a twofold framework for (i) the\nlarge-scale discovery and analysis of gendered discourse words in spoken\ncontent via our Gendered Discourse Correlation Framework (GDCF); and (ii) the\nmeasurement of the gender bias associated with these gendered discourse words\nin LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus\nour study on podcasts, a popular and growing form of social media, analyzing\n15,117 podcast episodes. We analyze correlations between gender and discourse\nwords -- discovered via LDA and BERTopic -- to automatically form gendered\ndiscourse word lists. We then study the prevalence of these gendered discourse\nwords in domain-specific contexts, and find that gendered discourse-based\nmasculine defaults exist in the domains of business, technology/politics, and\nvideo games. Next, we study the representation of these gendered discourse\nwords from a state-of-the-art LLM embedding model from OpenAI, and find that\nthe masculine discourse words have a more stable and robust representation than\nthe feminine discourse words, which may result in better system performance on\ndownstream tasks for men. Hence, men are rewarded for their discourse patterns\nwith better system performance by one of the state-of-the-art language models\n-- and this embedding disparity is a representational harm and a masculine\ndefault.\n","authors":["Maria Teleki","Xiangjue Dong","Haoran Liu","James Caverlee"],"pdf_url":"https://arxiv.org/pdf/2504.11431v1.pdf","comment":"To appear in ICWSM 2025"},{"id":"http://arxiv.org/abs/2504.11426v1","updated":"2025-04-15T17:38:47Z","published":"2025-04-15T17:38:47Z","title":"A Dual-Space Framework for General Knowledge Distillation of Large\n  Language Models","summary":"  Knowledge distillation (KD) is a promising solution to compress large\nlanguage models (LLMs) by transferring their knowledge to smaller models.\nDuring this process, white-box KD methods usually minimize the distance between\nthe output distributions of the teacher model and the student model to transfer\nmore information. However, we reveal that the current white-box KD framework\nexhibits two limitations: a) bridging probability distributions from different\noutput spaces will limit the similarity between the teacher model and the\nstudent model; b) this framework cannot be applied to LLMs with different\nvocabularies. One of the root causes for these limitations is that the\ndistributions from the teacher and the student for KD are output by different\nprediction heads, which yield distributions in different output spaces and\ndimensions. Therefore, in this paper, we propose a dual-space knowledge\ndistillation (DSKD) framework that unifies the prediction heads of the teacher\nand the student models for KD. Specifically, we first introduce two projectors\nwith ideal initialization to project the teacher/student hidden states into the\nstudent/teacher representation spaces. After this, the hidden states from\ndifferent models can share the same head and unify the output spaces of the\ndistributions. Furthermore, we develop an exact token alignment (ETA) algorithm\nto align the same tokens in two differently-tokenized sequences. Based on the\nabove, our DSKD framework is a general KD framework that supports both\noff-policy and on-policy KD, and KD between any two LLMs regardless of their\nvocabularies. Extensive experiments on instruction-following, mathematical\nreasoning, and code generation benchmarks show that DSKD significantly\noutperforms existing methods based on the current white-box KD framework and\nsurpasses other cross-tokenizer KD methods for LLMs with different\nvocabularies.\n","authors":["Xue Zhang","Songming Zhang","Yunlong Liang","Fandong Meng","Yufeng Chen","Jinan Xu","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.11426v1.pdf","comment":"19 pages, 9 figures, 11 tables, under review. Code is available at:\n  https://github.com/songmzhang/DSKDv2. arXiv admin note: text overlap with\n  arXiv:2406.17328"},{"id":"http://arxiv.org/abs/2410.19494v2","updated":"2025-04-15T17:38:16Z","published":"2024-10-25T11:51:37Z","title":"Graph Linearization Methods for Reasoning on Graphs with Large Language\n  Models","summary":"  Large language models have evolved to process multiple modalities beyond\ntext, such as images and audio, which motivates us to explore how to\neffectively leverage them for graph reasoning tasks. The key question,\ntherefore, is how to transform graphs into linear sequences of tokens, a\nprocess we term \"graph linearization\", so that LLMs can handle graphs\nnaturally. We consider that graphs should be linearized meaningfully to reflect\ncertain properties of natural language text, such as local dependency and\nglobal alignment, in order to ease contemporary LLMs, trained on trillions of\ntextual tokens, better understand graphs. To achieve this, we developed several\ngraph linearization methods based on graph centrality and degeneracy. These\nmethods are further enhanced using node relabeling techniques. The experimental\nresults demonstrate the effectiveness of our methods compared to the random\nlinearization baseline. Our work introduces novel graph representations\nsuitable for LLMs, contributing to the potential integration of graph machine\nlearning with the trend of multimodal processing using a unified transformer\nmodel.\n","authors":["Christos Xypolopoulos","Guokan Shang","Xiao Fei","Giannis Nikolentzos","Hadi Abdine","Iakovos Evdaimon","Michail Chatzianastasis","Giorgos Stamou","Michalis Vazirgiannis"],"pdf_url":"https://arxiv.org/pdf/2410.19494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11420v1","updated":"2025-04-15T17:35:56Z","published":"2025-04-15T17:35:56Z","title":"Reinforcing Compositional Retrieval: Retrieving Step-by-Step for\n  Composing Informative Contexts","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet they often rely on external context to handle complex\ntasks. While retrieval-augmented frameworks traditionally focus on selecting\ntop-ranked documents in a single pass, many real-world scenarios demand\ncompositional retrieval, where multiple sources must be combined in a\ncoordinated manner. In this work, we propose a tri-encoder sequential retriever\nthat models this process as a Markov Decision Process (MDP), decomposing the\nprobability of retrieving a set of elements into a sequence of conditional\nprobabilities and allowing each retrieval step to be conditioned on previously\nselected examples. We train the retriever in two stages: first, we efficiently\nconstruct supervised sequential data for initial policy training; we then\nrefine the policy to align with the LLM's preferences using a reward grounded\nin the structural correspondence of generated programs. Experimental results\nshow that our method consistently and significantly outperforms baselines,\nunderscoring the importance of explicitly modeling inter-example dependencies.\nThese findings highlight the potential of compositional retrieval for tasks\nrequiring multiple pieces of evidence or examples.\n","authors":["Quanyu Long","Jianda Chen","Zhengyuan Liu","Nancy F. Chen","Wenya Wang","Sinno Jialin Pan"],"pdf_url":"https://arxiv.org/pdf/2504.11420v1.pdf","comment":"19 pages, 8 figures"},{"id":"http://arxiv.org/abs/2504.11409v1","updated":"2025-04-15T17:26:29Z","published":"2025-04-15T17:26:29Z","title":"Efficient Hybrid Language Model Compression through Group-Aware SSM\n  Pruning","summary":"  Hybrid LLM architectures that combine Attention and State Space Models (SSMs)\nachieve state-of-the-art accuracy and runtime performance. Recent work has\ndemonstrated that applying compression and distillation to Attention-only\nmodels yields smaller, more accurate models at a fraction of the training cost.\nIn this work, we explore the effectiveness of compressing Hybrid architectures.\nWe introduce a novel group-aware pruning strategy that preserves the structural\nintegrity of SSM blocks and their sequence modeling capabilities. Furthermore,\nwe demonstrate the necessity of such SSM pruning to achieve improved accuracy\nand inference speed compared to traditional approaches. Our compression recipe\ncombines SSM, FFN, embedding dimension, and layer pruning, followed by\nknowledge distillation-based retraining, similar to the MINITRON technique.\nUsing this approach, we compress the Nemotron-H 8B Hybrid model down to 4B\nparameters with up to 40x fewer training tokens. The resulting model surpasses\nthe accuracy of similarly-sized models while achieving 2x faster inference,\nsignificantly advancing the Pareto frontier.\n","authors":["Ali Taghibakhshi","Sharath Turuvekere Sreenivas","Saurav Muralidharan","Marcin Chochowski","Yashaswi Karnati","Raviraj Joshi","Ameya Sunil Mahabaleshwarkar","Zijia Chen","Yoshi Suhara","Oluwatobi Olabiyi","Daniel Korzekwa","Mostofa Patwary","Mohammad Shoeybi","Jan Kautz","Bryan Catanzaro","Ashwath Aithal","Nima Tajbakhsh","Pavlo Molchanov"],"pdf_url":"https://arxiv.org/pdf/2504.11409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10127v2","updated":"2025-04-15T17:13:46Z","published":"2025-04-14T11:35:02Z","title":"Breaking the Data Barrier -- Building GUI Agents Through Task\n  Generalization","summary":"  Graphical User Interface (GUI) agents offer cross-platform solutions for\nautomating complex digital tasks, with significant potential to transform\nproductivity workflows. However, their performance is often constrained by the\nscarcity of high-quality trajectory data. To address this limitation, we\npropose training Vision Language Models (VLMs) on data-rich,\nreasoning-intensive tasks during a dedicated mid-training stage, and then\nexamine how incorporating these tasks facilitates generalization to GUI\nplanning scenarios. Specifically, we explore a range of tasks with readily\navailable instruction-tuning data, including GUI perception, multimodal\nreasoning, and textual reasoning. Through extensive experiments across 11\nmid-training tasks, we demonstrate that: (1) Task generalization proves highly\neffective, yielding substantial improvements across most settings. For\ninstance, multimodal mathematical reasoning enhances performance on\nAndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data\nsignificantly boosts GUI web agent performance, achieving a 5.6% improvement on\nWebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal\ngeneralization from text-based to visual domains; (2) Contrary to prior\nassumptions, GUI perception data - previously considered closely aligned with\nGUI agent tasks and widely utilized for training - has a comparatively limited\nimpact on final performance; (3) Building on these insights, we identify the\nmost effective mid-training tasks and curate optimized mixture datasets,\nresulting in absolute performance gains of 8.0% on WebArena and 12.2% on\nAndroidWorld. Our work provides valuable insights into cross-domain knowledge\ntransfer for GUI agents and offers a practical approach to addressing data\nscarcity challenges in this emerging field. The code, data and models will be\navailable at https://github.com/hkust-nlp/GUIMid.\n","authors":["Junlei Zhang","Zichen Ding","Chang Ma","Zijie Chen","Qiushi Sun","Zhenzhong Lan","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2504.10127v2.pdf","comment":"24 pages, 11 figures"},{"id":"http://arxiv.org/abs/2504.11393v1","updated":"2025-04-15T17:02:15Z","published":"2025-04-15T17:02:15Z","title":"DataDecide: How to Predict Best Pretraining Data with Small Experiments","summary":"  Because large language models are expensive to pretrain on different\ndatasets, using smaller-scale experiments to decide on data is crucial for\nreducing costs. Which benchmarks and methods of making decisions from observed\nperformance at small scale most accurately predict the datasets that yield the\nbest large models? To empower open exploration of this question, we release\nmodels, data, and evaluations in DataDecide -- the most extensive open suite of\nmodels over differences in data and scale. We conduct controlled pretraining\nexperiments across 25 corpora with differing sources, deduplication, and\nfiltering up to 100B tokens, model sizes up to 1B parameters, and 3 random\nseeds. We find that the ranking of models at a single, small size (e.g., 150M\nparameters) is a strong baseline for predicting best models at our larger\ntarget scale (1B) (~80% of com parisons correct). No scaling law methods among\n8 baselines exceed the compute-decision frontier of single-scale predictions,\nbut DataDecide can measure improvement in future scaling laws. We also identify\nthat using continuous likelihood metrics as proxies in small experiments makes\nbenchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable\nat the target 1B scale with just 0.01% of the compute.\n","authors":["Ian Magnusson","Nguyen Tai","Ben Bogin","David Heineman","Jena D. Hwang","Luca Soldaini","Akshita Bhagia","Jiacheng Liu","Dirk Groeneveld","Oyvind Tafjord","Noah A. Smith","Pang Wei Koh","Jesse Dodge"],"pdf_url":"https://arxiv.org/pdf/2504.11393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11381v1","updated":"2025-04-15T16:53:31Z","published":"2025-04-15T16:53:31Z","title":"RankAlign: A Ranking View of the Generator-Validator Gap in Large\n  Language Models","summary":"  Although large language models (LLMs) have become generally more capable and\naccurate across many tasks, some fundamental sources of unreliability remain in\ntheir behavior. One key limitation is their inconsistency at reporting the the\nsame information when prompts are changed. In this paper, we consider the\ndiscrepancy between a model's generated answer and their own verification of\nthat answer, the generator-validator gap. We define this gap in a more\nstringent way than prior work: we expect correlation of scores from a generator\nand a validator over the entire set of candidate answers. We show that\naccording to this measure, a large gap exists in various settings, including\nquestion answering, lexical semantics tasks, and next-word prediction. We then\npropose RankAlign, a ranking-based training method, and show that it\nsignificantly closes the gap by 31.8% on average, surpassing all baseline\nmethods. Moreover, this approach generalizes well to out-of-domain tasks and\nlexical items.\n","authors":["Juan Diego Rodriguez","Wenxuan Ding","Katrin Erk","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2504.11381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11373v1","updated":"2025-04-15T16:37:32Z","published":"2025-04-15T16:37:32Z","title":"Cancer-Myth: Evaluating AI Chatbot on Patient Questions with False\n  Presuppositions","summary":"  Cancer patients are increasingly turning to large language models (LLMs) as a\nnew form of internet search for medical information, making it critical to\nassess how well these models handle complex, personalized questions. However,\ncurrent medical benchmarks focus on medical exams or consumer-searched\nquestions and do not evaluate LLMs on real patient questions with detailed\nclinical contexts. In this paper, we first evaluate LLMs on cancer-related\nquestions drawn from real patients, reviewed by three hematology oncology\nphysicians. While responses are generally accurate, with GPT-4-Turbo scoring\n4.13 out of 5, the models frequently fail to recognize or address false\npresuppositions in the questions-posing risks to safe medical decision-making.\nTo study this limitation systematically, we introduce Cancer-Myth, an\nexpert-verified adversarial dataset of 585 cancer-related questions with false\npresuppositions. On this benchmark, no frontier LLM -- including GPT-4o,\nGemini-1.Pro, and Claude-3.5-Sonnet -- corrects these false presuppositions\nmore than 30% of the time. Even advanced medical agentic methods do not prevent\nLLMs from ignoring false presuppositions. These findings expose a critical gap\nin the clinical reliability of LLMs and underscore the need for more robust\nsafeguards in medical AI systems.\n","authors":["Wang Bill Zhu","Tianqi Chen","Ching Ying Lin","Jade Law","Mazen Jizzini","Jorge J. Nieva","Ruishan Liu","Robin Jia"],"pdf_url":"https://arxiv.org/pdf/2504.11373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11369v1","updated":"2025-04-15T16:36:14Z","published":"2025-04-15T16:36:14Z","title":"OpenTuringBench: An Open-Model-based Benchmark and Framework for\n  Machine-Generated Text Detection and Attribution","summary":"  Open Large Language Models (OLLMs) are increasingly leveraged in generative\nAI applications, posing new challenges for detecting their outputs. We propose\nOpenTuringBench, a new benchmark based on OLLMs, designed to train and evaluate\nmachine-generated text detectors on the Turing Test and Authorship Attribution\nproblems. OpenTuringBench focuses on a representative set of OLLMs, and\nfeatures a number of challenging evaluation tasks, including\nhuman/machine-manipulated texts, out-of-domain texts, and texts from previously\nunseen models. We also provide OTBDetector, a contrastive learning framework to\ndetect and attribute OLLM-based machine-generated texts. Results highlight the\nrelevance and varying degrees of difficulty of the OpenTuringBench tasks, with\nour detector achieving remarkable capabilities across the various tasks and\noutperforming most existing detectors. Resources are available on the\nOpenTuringBench Hugging Face repository at\nhttps://huggingface.co/datasets/MLNTeam-Unical/OpenTuringBench\n","authors":["Lucio La Cava","Andrea Tagarelli"],"pdf_url":"https://arxiv.org/pdf/2504.11369v1.pdf","comment":"Under review with ARR"},{"id":"http://arxiv.org/abs/2504.11367v1","updated":"2025-04-15T16:32:09Z","published":"2025-04-15T16:32:09Z","title":"Network Alignment","summary":"  Complex networks are frequently employed to model physical or virtual complex\nsystems. When certain entities exist across multiple systems simultaneously,\nunveiling their corresponding relationships across the networks becomes\ncrucial. This problem, known as network alignment, holds significant\nimportance. It enhances our understanding of complex system structures and\nbehaviours, facilitates the validation and extension of theoretical physics\nresearch about studying complex systems, and fosters diverse practical\napplications across various fields. However, due to variations in the\nstructure, characteristics, and properties of complex networks across different\nfields, the study of network alignment is often isolated within each domain,\nwith even the terminologies and concepts lacking uniformity. This review\ncomprehensively summarizes the latest advancements in network alignment\nresearch, focusing on analyzing network alignment characteristics and progress\nin various domains such as social network analysis, bioinformatics,\ncomputational linguistics and privacy protection. It provides a detailed\nanalysis of various methods' implementation principles, processes, and\nperformance differences, including structure consistency-based methods, network\nembedding-based methods, and graph neural network-based (GNN-based) methods.\nAdditionally, the methods for network alignment under different conditions,\nsuch as in attributed networks, heterogeneous networks, directed networks, and\ndynamic networks, are presented. Furthermore, the challenges and the open\nissues for future studies are also discussed.\n","authors":["Rui Tang","Ziyun Yong","Shuyu Jiang","Xingshu Chen","Yaofang Liu","Yi-Cheng Zhang","Gui-Quan Sun","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2504.11367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11364v1","updated":"2025-04-15T16:30:02Z","published":"2025-04-15T16:30:02Z","title":"Teaching Large Language Models to Reason through Learning and Forgetting","summary":"  Leveraging inference-time search in large language models has proven\neffective in further enhancing a trained model's capability to solve complex\nmathematical and reasoning problems. However, this approach significantly\nincreases computational costs and inference time, as the model must generate\nand evaluate multiple candidate solutions to identify a viable reasoning path.\nTo address this, we propose an effective approach that integrates search\ncapabilities directly into the model by fine-tuning it using both successful\n(learning) and failed reasoning paths (forgetting) derived from diverse search\nmethods. While fine-tuning the model with these data might seem\nstraightforward, we identify a critical issue: the model's search capability\ntends to degrade rapidly if fine-tuning is performed naively. We show that this\ndegradation can be substantially mitigated by employing a smaller learning\nrate. Extensive experiments on the challenging Game-of-24 and Countdown\nmathematical reasoning benchmarks show that our approach not only outperforms\nboth standard fine-tuning and inference-time search baselines but also\nsignificantly reduces inference time by 180$\\times$.\n","authors":["Tianwei Ni","Allen Nie","Sapana Chaudhary","Yao Liu","Huzefa Rangwala","Rasool Fakoor"],"pdf_url":"https://arxiv.org/pdf/2504.11364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11343v1","updated":"2025-04-15T16:15:02Z","published":"2025-04-15T16:15:02Z","title":"A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce","summary":"  Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training.\n","authors":["Wei Xiong","Jiarui Yao","Yuhui Xu","Bo Pang","Lei Wang","Doyen Sahoo","Junnan Li","Nan Jiang","Tong Zhang","Caiming Xiong","Hanze Dong"],"pdf_url":"https://arxiv.org/pdf/2504.11343v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.11337v1","updated":"2025-04-15T16:09:19Z","published":"2025-04-15T16:09:19Z","title":"REWARD CONSISTENCY: Improving Multi-Objective Alignment from a\n  Data-Centric Perspective","summary":"  Multi-objective preference alignment in language models often encounters a\nchallenging trade-off: optimizing for one human preference (e.g., helpfulness)\nfrequently compromises others (e.g., harmlessness) due to the inherent\nconflicts between competing objectives. While prior work mainly focuses on\nalgorithmic solutions, we explore a novel data-driven approach to uncover the\ntypes of data that can effectively mitigate these conflicts. Specifically, we\npropose the concept of Reward Consistency (RC), which identifies samples that\nalign with multiple preference objectives, thereby reducing conflicts during\ntraining. Through gradient-based analysis, we demonstrate that RC-compliant\nsamples inherently constrain performance degradation during multi-objective\noptimization. Building on these insights, we further develop Reward Consistency\nSampling, a framework that automatically constructs preference datasets that\neffectively mitigate conflicts during multi-objective alignment. Our generated\ndata achieves an average improvement of 13.37% in both the harmless rate and\nhelpfulness win rate when optimizing harmlessness and helpfulness, and can\nconsistently resolve conflicts in varying multi-objective scenarios.\n","authors":["Zhihao Xu","Yongqi Tong","Xin Zhang","Jun Zhou","Xiting Wang"],"pdf_url":"https://arxiv.org/pdf/2504.11337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11336v1","updated":"2025-04-15T16:09:06Z","published":"2025-04-15T16:09:06Z","title":"Looking beyond the next token","summary":"  The structure of causal language model training assumes that each token can\nbe accurately predicted from the previous context. This contrasts with humans'\nnatural writing and reasoning process, where goals are typically known before\nthe exact argument or phrasings. While this mismatch has been well studied in\nthe literature, the working assumption has been that architectural changes are\nneeded to address this mismatch. We argue that rearranging and processing the\ntraining data sequences can allow models to more accurately imitate the true\ndata-generating process, and does not require any other changes to the\narchitecture or training infrastructure. We demonstrate that this technique,\nTrelawney, and the inference algorithms derived from it allow us to improve\nperformance on several key benchmarks that span planning, algorithmic\nreasoning, and story generation tasks. Finally, our method naturally enables\nthe generation of long-term goals at no additional cost. We investigate how\nusing the model's goal-generation capability can further improve planning and\nreasoning. Additionally, we believe Trelawney could potentially open doors to\nnew capabilities beyond the current language modeling paradigm.\n","authors":["Abitha Thankaraj","Yiding Jiang","J. Zico Kolter","Yonatan Bisk"],"pdf_url":"https://arxiv.org/pdf/2504.11336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11331v1","updated":"2025-04-15T16:05:09Z","published":"2025-04-15T16:05:09Z","title":"Dependency Structure Augmented Contextual Scoping Framework for\n  Multimodal Aspect-Based Sentiment Analysis","summary":"  Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract\nfine-grained information from image-text pairs to identify aspect terms and\ndetermine their sentiment polarity. However, existing approaches often fall\nshort in simultaneously addressing three core challenges: Sentiment Cue\nPerception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise\nElimination (SNE). To overcome these limitations, we propose DASCO\n(\\textbf{D}ependency Structure \\textbf{A}ugmented \\textbf{Sco}ping Framework),\na fine-grained scope-oriented framework that enhances aspect-level sentiment\nreasoning by leveraging dependency parsing trees. First, we designed a\nmulti-task pretraining strategy for MABSA on our base model, combining\naspect-oriented enhancement, image-text matching, and aspect-level\nsentiment-sensitive cognition. This improved the model's perception of aspect\nterms and sentiment cues while achieving effective image-text alignment,\naddressing key challenges like SCP and MIM. Furthermore, we incorporate\ndependency trees as syntactic branch combining with semantic branch, guiding\nthe model to selectively attend to critical contextual elements within a\ntarget-specific scope while effectively filtering out irrelevant noise for\naddressing SNE problem. Extensive experiments on two benchmark datasets across\nthree subtasks demonstrate that DASCO achieves state-of-the-art performance in\nMABSA, with notable gains in JMASA (+3.1\\% F1 and +5.4\\% precision on\nTwitter2015).\n","authors":["Hao Liu","Lijun He","Jiaxi Liang","Zhihan Ren","Fan Li"],"pdf_url":"https://arxiv.org/pdf/2504.11331v1.pdf","comment":"submitted to ACM MM2025"},{"id":"http://arxiv.org/abs/2503.18172v3","updated":"2025-04-15T15:48:57Z","published":"2025-03-23T18:56:33Z","title":"Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language\n  Models on Misleading Chart Question Answering","summary":"  Misleading chart visualizations, which intentionally manipulate data\nrepresentations to support specific claims, can distort perceptions and lead to\nincorrect conclusions. Despite decades of research, misleading visualizations\nremain a widespread and pressing issue. Recent advances in multimodal large\nlanguage models (MLLMs) have demonstrated strong chart comprehension\ncapabilities, yet no existing work has systematically evaluated their ability\nto detect and interpret misleading charts. This paper introduces the Misleading\nChart Question Answering (Misleading ChartQA) Benchmark, a large-scale\nmultimodal dataset designed to assess MLLMs in identifying and reasoning about\nmisleading charts. It contains over 3,000 curated examples, covering 21 types\nof misleaders and 10 chart types. Each example includes standardized chart\ncode, CSV data, and multiple-choice questions with labeled explanations,\nvalidated through multi-round MLLM checks and exhausted expert human review. We\nbenchmark 16 state-of-the-art MLLMs on our dataset, revealing their limitations\nin identifying visually deceptive practices. We also propose a novel pipeline\nthat detects and localizes misleaders, enhancing MLLMs' accuracy in misleading\nchart interpretation. Our work establishes a foundation for advancing\nMLLM-driven misleading chart comprehension. We publicly release the sample\ndataset to support further research in this critical area.\n","authors":["Zixin Chen","Sicheng Song","Kashun Shum","Yanna Lin","Rui Sheng","Huamin Qu"],"pdf_url":"https://arxiv.org/pdf/2503.18172v3.pdf","comment":"31 pages in total. Under Review"},{"id":"http://arxiv.org/abs/2412.12144v2","updated":"2025-04-15T15:42:40Z","published":"2024-12-10T09:13:32Z","title":"Automatic Item Generation for Personality Situational Judgment Tests\n  with Large Language Models","summary":"  Personality assessment, particularly through situational judgment tests\n(SJTs), is a vital tool for psychological research, talent selection, and\neducational evaluation. This study explores the potential of GPT-4, a\nstate-of-the-art large language model (LLM), to automate the generation of\npersonality situational judgment tests (PSJTs) in Chinese. Traditional SJT\ndevelopment is labor-intensive and prone to biases, while GPT-4 offers a\nscalable, efficient alternative. Two studies were conducted: Study 1 evaluated\nthe impact of prompt design and temperature settings on content validity,\nfinding that optimized prompts with a temperature of 1.0 produced creative and\naccurate items. Study 2 assessed the psychometric properties of GPT-4-generated\nPSJTs, revealing that they demonstrated satisfactory reliability and validity,\nsurpassing the performance of manually developed tests in measuring the Big\nFive personality traits. This research highlights GPT-4's effectiveness in\ndeveloping high-quality PSJTs, providing a scalable and innovative method for\npsychometric test development. These findings expand the possibilities of\nautomatic item generation and the application of LLMs in psychology, and offer\npractical implications for streamlining test development processes in\nresource-limited settings.\n","authors":["Chang-Jin Li","Jiyuan Zhang","Yun Tang","Jian Li"],"pdf_url":"https://arxiv.org/pdf/2412.12144v2.pdf","comment":"Submitted to Computers in Human Behavior Reports. 54 pages (main\n  text), 12 pages (appendix), and 5 figures"},{"id":"http://arxiv.org/abs/2401.09727v2","updated":"2025-04-15T15:38:22Z","published":"2024-01-18T05:06:39Z","title":"Lateral Phishing With Large Language Models: A Large Organization\n  Comparative Study","summary":"  The emergence of Large Language Models (LLMs) has heightened the threat of\nphishing emails by enabling the generation of highly targeted, personalized,\nand automated attacks. Traditionally, many phishing emails have been\ncharacterized by typos, errors, and poor language. These errors can be\nmitigated by LLMs, potentially lowering the barrier for attackers. Despite\nthis, there is a lack of large-scale studies comparing the effectiveness of\nLLM-generated lateral phishing emails to those crafted by humans. Current\nliterature does not adequately address the comparative effectiveness of LLM and\nhuman-generated lateral phishing emails in a real-world, large-scale\norganizational setting, especially considering the potential for LLMs to\ngenerate more convincing and error-free phishing content. To address this gap,\nwe conducted a pioneering study within a large university, targeting its\nworkforce of approximately 9,000 individuals including faculty, staff,\nadministrators, and student workers. Our results indicate that LLM-generated\nlateral phishing emails are as effective as those written by communications\nprofessionals, emphasizing the critical threat posed by LLMs in leading\nphishing campaigns. We break down the results of the overall phishing\nexperiment, comparing vulnerability between departments and job roles.\nFurthermore, to gather qualitative data, we administered a detailed\nquestionnaire, revealing insights into the reasons and motivations behind\nvulnerable employee's actions. This study contributes to the understanding of\ncyber security threats in educational institutions and provides a comprehensive\ncomparison of LLM and human-generated phishing emails' effectiveness,\nconsidering the potential for LLMs to generate more convincing content. The\nfindings highlight the need for enhanced user education and system defenses to\nmitigate the growing threat of AI-powered phishing attacks.\n","authors":["Mazal Bethany","Athanasios Galiopoulos","Emet Bethany","Mohammad Bahrami Karkevandi","Nicole Beebe","Nishant Vishwamitra","Peyman Najafirad"],"pdf_url":"https://arxiv.org/pdf/2401.09727v2.pdf","comment":"Accepted for publication in IEEE Access. This version includes\n  revisions following peer review"},{"id":"http://arxiv.org/abs/2504.11290v1","updated":"2025-04-15T15:30:22Z","published":"2025-04-15T15:30:22Z","title":"Automated Python Translation","summary":"  Python is one of the most commonly used programming languages in industry and\neducation. Its English keywords and built-in functions/modules allow it to come\nclose to pseudo-code in terms of its readability and ease of writing. However,\nthose who do not speak English may not experience these advantages. In fact,\nthey may even be hindered in their ability to understand Python code, as the\nEnglish nature of its terms creates an additional layer of overhead. To that\nend, we introduce the task of automatically translating Python's natural\nmodality (keywords, error types, identifiers, etc.) into other human languages.\nThis presents a unique challenge, considering the abbreviated nature of these\nforms, as well as potential untranslatability of advanced\nmathematical/programming concepts across languages. We therefore create an\nautomated pipeline to translate Python into other human languages, comparing\nstrategies using machine translation and large language models. We then use\nthis pipeline to acquire translations from five common Python libraries\n(pytorch, pandas, tensorflow, numpy, and random) in seven languages, and do a\nquality test on a subset of these terms in French, Greek, and Bengali. We hope\nthis will provide a clearer path forward towards creating a universal Python,\naccessible to anyone regardless of nationality or language background.\n","authors":["Joshua Otten","Antonios Anastasopoulos","Kevin Moran"],"pdf_url":"https://arxiv.org/pdf/2504.11290v1.pdf","comment":"15 pages, 4 figures, 17 tables"},{"id":"http://arxiv.org/abs/2404.01663v6","updated":"2025-04-15T15:28:28Z","published":"2024-04-02T06:07:35Z","title":"CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small\n  Language Models","summary":"  Open large language models (LLMs) have significantly advanced the field of\nnatural language processing, showcasing impressive performance across various\ntasks.Despite the significant advancements in LLMs, their effective operation\nstill relies heavily on human input to accurately guide the dialogue flow, with\nagent tuning being a crucial optimization technique that involves human\nadjustments to the model for better response to such guidance.Addressing this\ndependency, our work introduces the TinyAgent model, trained on a meticulously\ncurated high-quality dataset. We also present the Collaborative Multi-Agent\nTuning (CMAT) framework, an innovative system designed to augment language\nagent capabilities through adaptive weight updates based on environmental\nfeedback. This framework fosters collaborative learning and real-time\nadaptation among multiple intelligent agents, enhancing their context-awareness\nand long-term memory. In this research, we propose a new communication agent\nframework that integrates multi-agent systems with environmental feedback\nmechanisms, offering a scalable method to explore cooperative behaviors.\nNotably, our TinyAgent-7B model exhibits performance on par with GPT-3.5,\ndespite having fewer parameters, signifying a substantial improvement in the\nefficiency and effectiveness of LLMs.\n","authors":["Xuechen Liang","Yangfan He","Meiling Tao","Yinghui Xia","Jianhui Wang","Tianyu Shi","Jun Wang","JingSong Yang"],"pdf_url":"https://arxiv.org/pdf/2404.01663v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17807v5","updated":"2025-04-15T15:28:20Z","published":"2024-06-23T11:58:26Z","title":"Enhancing Commentary Strategies for Imperfect Information Card Games: A\n  Study of Large Language Models in Guandan Commentary","summary":"  Recent advancements in large language models (LLMs) have unlocked the\npotential for generating high-quality game commentary. However, producing\ninsightful and engaging commentary for complex games with incomplete\ninformation remains a significant challenge. In this paper, we introduce a\nnovel commentary method that combine Reinforcement Learning (RL) and LLMs,\ntailored specifically for the Chinese card game \\textit{Guandan}. Our system\nleverages RL to generate intricate card-playing scenarios and employs LLMs to\ngenerate corresponding commentary text, effectively emulating the strategic\nanalysis and narrative prowess of professional commentators. The framework\ncomprises a state commentary guide, a Theory of Mind (ToM)-based strategy\nanalyzer, and a style retrieval module, which seamlessly collaborate to deliver\ndetailed and context-relevant game commentary in the Chinese language\nenvironment. We empower LLMs with ToM capabilities and refine both retrieval\nand information filtering mechanisms. This facilitates the generation of\npersonalized commentary content. Our experimental results showcase the\nsubstantial enhancement in performance achieved by the proposed commentary\nframework when applied to open-source LLMs, surpassing the performance of GPT-4\nacross multiple evaluation metrics.\n","authors":["Meiling Tao","Xuechen Liang","Xinyuan Song","Yangfan He","Yiling Tao","Jianhui Wang","Sun Li Tianyu Shi"],"pdf_url":"https://arxiv.org/pdf/2406.17807v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11281v1","updated":"2025-04-15T15:21:09Z","published":"2025-04-15T15:21:09Z","title":"The Obvious Invisible Threat: LLM-Powered GUI Agents' Vulnerability to\n  Fine-Print Injections","summary":"  A Large Language Model (LLM) powered GUI agent is a specialized autonomous\nsystem that performs tasks on the user's behalf according to high-level\ninstructions. It does so by perceiving and interpreting the graphical user\ninterfaces (GUIs) of relevant apps, often visually, inferring necessary\nsequences of actions, and then interacting with GUIs by executing the actions\nsuch as clicking, typing, and tapping. To complete real-world tasks, such as\nfilling forms or booking services, GUI agents often need to process and act on\nsensitive user data. However, this autonomy introduces new privacy and security\nrisks. Adversaries can inject malicious content into the GUIs that alters agent\nbehaviors or induces unintended disclosures of private information. These\nattacks often exploit the discrepancy between visual saliency for agents and\nhuman users, or the agent's limited ability to detect violations of contextual\nintegrity in task automation. In this paper, we characterized six types of such\nattacks, and conducted an experimental study to test these attacks with six\nstate-of-the-art GUI agents, 234 adversarial webpages, and 39 human\nparticipants. Our findings suggest that GUI agents are highly vulnerable,\nparticularly to contextually embedded threats. Moreover, human users are also\nsusceptible to many of these attacks, indicating that simple human oversight\nmay not reliably prevent failures. This misalignment highlights the need for\nprivacy-aware agent design. We propose practical defense strategies to inform\nthe development of safer and more reliable GUI agents.\n","authors":["Chaoran Chen","Zhiping Zhang","Bingcan Guo","Shang Ma","Ibrahim Khalilov","Simret A Gebreegziabher","Yanfang Ye","Ziang Xiao","Yaxing Yao","Tianshi Li","Toby Jia-Jun Li"],"pdf_url":"https://arxiv.org/pdf/2504.11281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11277v1","updated":"2025-04-15T15:16:45Z","published":"2025-04-15T15:16:45Z","title":"From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning\n  Method for LLMs","summary":"  Large language models (LLMs) exhibit excellent performance in natural\nlanguage processing (NLP), but remain highly sensitive to the quality of input\nqueries, especially when these queries contain misleading or inaccurate\ninformation. Existing methods focus on correcting the output, but they often\noverlook the potential of improving the ability of LLMs to detect and correct\nmisleading content in the input itself. In this paper, we propose a novel\nthree-stage fine-tuning method that enhances the ability of LLMs to detect and\ncorrect misleading information in the input, further improving response\naccuracy and reducing hallucinations. Specifically, the three stages include\n(1) training LLMs to identify misleading information, (2) training LLMs to\ncorrect the misleading information using built-in or external knowledge, and\n(3) training LLMs to generate accurate answers based on the corrected queries.\nTo evaluate our method, we conducted experiments on three datasets for the\nhallucination detection task and the question answering (QA) task, as well as\ntwo datasets containing misleading information that we constructed. The\nexperimental results demonstrate that our method significantly improves the\naccuracy and factuality of LLM responses, while also enhancing the ability to\ndetect hallucinations and reducing the generation of hallucinations in the\noutput, particularly when the query contains misleading information. We will\npublicly release our code upon acceptance.\n","authors":["Guocong Li","Weize Liu","Yihang Wu","Ping Wang","Shuaihan Huang","Hongxia Xu","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2504.11277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10356v2","updated":"2025-04-15T15:02:53Z","published":"2025-04-14T16:05:59Z","title":"MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31\n  languages","summary":"  We present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs\ncovering 31 languages. MultiLoKo consists of three partitions: a main partition\nconsisting of 500 questions per language, separately sourced to be locally\nrelevant to the specific language, and two translated partitions, containing\nhuman-authored translations from 30 non-English languages to English and vice\nversa. For comparison, we also release corresponding machine-authored\ntranslations. The data is equally distributed over two splits: a dev split and\na blind, out-of-distribution test split. MultiLoKo can be used to study a\nvariety of questions regarding the multilinguality of LLMs as well as\nmeta-questions about multilingual benchmark creation. We compute MultiLoKo\nscores for 11 base and chat models marketed to be multilingual and study their\naverage performance, their performance parity across languages, how much their\nability to answer questions depends on the question language, and which\nlanguages are most difficult. None of the models we studied performs well on\nMultiLoKo, as indicated by low average scores as well as large differences\nbetween the best and worst scoring languages. Furthermore, we find a\nsubstantial effect of the question language, indicating sub-optimal knowledge\ntransfer between languages. Lastly, we find that using local vs\nEnglish-translated data can result in differences more than 20 points for the\nbest performing models, drastically change the estimated difficulty of some\nlanguages. For using machines instead of human translations, we find a weaker\neffect on ordering of language difficulty, a larger difference in model\nrankings, and a substantial drop in estimated performance for all models.\n","authors":["Dieuwke Hupkes","Nikolay Bogoychev"],"pdf_url":"https://arxiv.org/pdf/2504.10356v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11257v1","updated":"2025-04-15T14:56:21Z","published":"2025-04-15T14:56:21Z","title":"UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction\n  Synthesis","summary":"  Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation.In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://colmon46.github.io/i2e-bench-leaderboard/\n","authors":["Xinyi Liu","Xiaoyi Zhang","Ziyun Zhang","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2504.11257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11243v1","updated":"2025-04-15T14:43:19Z","published":"2025-04-15T14:43:19Z","title":"Towards Automated Safety Requirements Derivation Using Agent-based RAG","summary":"  We study the automated derivation of safety requirements in a self-driving\nvehicle use case, leveraging LLMs in combination with agent-based\nretrieval-augmented generation. Conventional approaches that utilise\npre-trained LLMs to assist in safety analyses typically lack domain-specific\nknowledge. Existing RAG approaches address this issue, yet their performance\ndeteriorates when handling complex queries and it becomes increasingly harder\nto retrieve the most relevant information. This is particularly relevant for\nsafety-relevant applications. In this paper, we propose the use of agent-based\nRAG to derive safety requirements and show that the retrieved information is\nmore relevant to the queries. We implement an agent-based approach on a\ndocument pool of automotive standards and the Apollo case study, as a\nrepresentative example of an automated driving perception system. Our solution\nis tested on a data set of safety requirement questions and answers, extracted\nfrom the Apollo data. Evaluating a set of selected RAG metrics, we present and\ndiscuss advantages of a agent-based approach compared to default RAG methods.\n","authors":["Balahari Vignesh Balu","Florian Geissler","Francesco Carella","Joao-Vitor Zacchi","Josef Jiru","Nuria Mata","Reinhard Stolle"],"pdf_url":"https://arxiv.org/pdf/2504.11243v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2504.10458v2","updated":"2025-04-15T14:42:43Z","published":"2025-04-14T17:45:54Z","title":"GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI\n  Agents","summary":"  Existing efforts in building Graphical User Interface (GUI) agents largely\nrely on the training paradigm of supervised fine-tuning on Large\nVision-Language Models (LVLMs). However, this approach not only demands\nextensive amounts of training data but also struggles to effectively understand\nGUI screenshots and generalize to unseen interfaces. The issue significantly\nlimits its application in real-world scenarios, especially for high-level\ntasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models\n(e.g., DeepSeek-R1), which efficiently enhances the problem-solving\ncapabilities of large language models in real-world settings, we propose \\name,\nthe first reinforcement learning framework designed to enhance the GUI\ncapabilities of LVLMs in high-level real-world task scenarios, through unified\naction space rule modeling. By leveraging a small amount of carefully curated\nhigh-quality data across multiple platforms (including Windows, Linux, MacOS,\nAndroid, and Web) and employing policy optimization algorithms such as Group\nRelative Policy Optimization (GRPO) to update the model, \\name achieves\nsuperior performance using only 0.02\\% of the data (3K vs. 13M) compared to\nprevious state-of-the-art methods like OS-Atlas across eight benchmarks\nspanning three different platforms (mobile, desktop, and web). These results\ndemonstrate the immense potential of reinforcement learning based on unified\naction space rule modeling in improving the execution capabilities of LVLMs for\nreal-world GUI agent tasks.\n","authors":["Xiaobo Xia","Run Luo"],"pdf_url":"https://arxiv.org/pdf/2504.10458v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07995v2","updated":"2025-04-15T14:41:45Z","published":"2025-04-08T19:16:43Z","title":"SafeChat: A Framework for Building Trustworthy Collaborative Assistants\n  and a Case Study of its Usefulness","summary":"  Collaborative assistants, or chatbots, are data-driven decision support\nsystems that enable natural interaction for task completion. While they can\nmeet critical needs in modern society, concerns about their reliability and\ntrustworthiness persist. In particular, Large Language Model (LLM)-based\nchatbots like ChatGPT, Gemini, and DeepSeek are becoming more accessible.\nHowever, such chatbots have limitations, including their inability to explain\nresponse generation, the risk of generating problematic content, the lack of\nstandardized testing for reliability, and the need for deep AI expertise and\nextended development times. These issues make chatbots unsuitable for\ntrust-sensitive applications like elections or healthcare. To address these\nconcerns, we introduce SafeChat, a general architecture for building safe and\ntrustworthy chatbots, with a focus on information retrieval use cases. Key\nfeatures of SafeChat include: (a) safety, with a domain-agnostic design where\nresponses are grounded and traceable to approved sources (provenance), and\n'do-not-respond' strategies to prevent harmful answers; (b) usability, with\nautomatic extractive summarization of long responses, traceable to their\nsources, and automated trust assessments to communicate expected chatbot\nbehavior, such as sentiment; and (c) fast, scalable development, including a\nCSV-driven workflow, automated testing, and integration with various devices.\nWe implemented SafeChat in an executable framework using the open-source\nchatbot platform Rasa. A case study demonstrates its application in building\nElectionBot-SC, a chatbot designed to safely disseminate official election\ninformation. SafeChat is being used in many domains, validating its potential,\nand is available at: https://github.com/ai4society/trustworthy-chatbot.\n","authors":["Biplav Srivastava","Kausik Lakkaraju","Nitin Gupta","Vansh Nagpal","Bharath C. Muppasani","Sara E. Jones"],"pdf_url":"https://arxiv.org/pdf/2504.07995v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11239v1","updated":"2025-04-15T14:40:29Z","published":"2025-04-15T14:40:29Z","title":"Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling\n  Reasoning Benchmark for LLMs","summary":"  Reasoning is the fundamental capability of large language models (LLMs). Due\nto the rapid progress of LLMs, there are two main issues of current benchmarks:\ni) these benchmarks can be crushed in a short time (less than 1 year), and ii)\nthese benchmarks may be easily hacked. To handle these issues, we propose the\never-scalingness for building the benchmarks which are uncrushable, unhackable,\nauto-verifiable and general. This paper presents Nondeterministic\nPolynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark\nfor LLMs. Specifically, the NPPC has three main modules: i) npgym, which\nprovides a unified interface of 25 well-known NP-complete problems and can\ngenerate any number of instances with any levels of complexities, ii) npsolver:\nwhich provides a unified interface to evaluate the problem instances with both\nonline and offline models via APIs and local deployments, respectively, and\niii) npeval: which provides the comprehensive and ready-to-use tools to analyze\nthe performances of LLMs over different problems, the number of tokens, the aha\nmoments, the reasoning errors and the solution errors. Extensive experiments\nover widely-used LLMs demonstrate: i) NPPC can successfully decrease the\nperformances of advanced LLMs' performances to below 10%, demonstrating that\nNPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the\nmost powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and\no1/o3-mini in most NP-complete problems considered, and iii) the numbers of\ntokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and\nDeepSeek-R1, are observed first to increase and then decrease when the problem\ninstances become more and more difficult. We believe that NPPC is the first\never-scaling reasoning benchmark, serving as the uncrushable and unhackable\ntestbed for LLMs toward artificial general intelligence (AGI).\n","authors":["Chang Yang","Ruiyu Wang","Junzhe Jiang","Qi Jiang","Qinggang Zhang","Yanchen Deng","Shuxin Li","Shuyue Hu","Bo Li","Florian T. Pokorny","Xiao Huang","Xinrun Wang"],"pdf_url":"https://arxiv.org/pdf/2504.11239v1.pdf","comment":"Preliminary work, 10 pages for main text"},{"id":"http://arxiv.org/abs/2412.07355v3","updated":"2025-04-15T14:38:13Z","published":"2024-12-10T09:48:07Z","title":"Towards Predictive Communication with Brain-Computer Interfaces\n  integrating Large Language Models","summary":"  This perspective article aims at providing an outline of the state of the art\nand future developments towards the integration of cutting-edge predictive\nlanguage models with BCI. A synthetic overview of early and more recent\nlinguistic models, from natural language processing (NLP) models to recent LLM,\nthat to a varying extent improved predictive writing systems, is first\nprovided. Second, a summary of previous BCI implementations integrating\nlanguage models is presented. The few preliminary studies investigating the\npossible combination of LLM with BCI spellers to efficiently support fast\ncommunication and control are then described. Finally, current challenges and\nlimitations towards the full integration of LLM with BCI systems are discussed.\nRecent investigations suggest that the combination of LLM with BCI might\ndrastically improve human-computer interaction in patients with motor or\nlanguage disorders as well as in healthy individuals. In particular, the\npretrained autoregressive transformer models, such as GPT, that capitalize from\nparallelization, learning through pre-training and fine-tuning, promise a\nsubstantial improvement of BCI for communication with respect to previous\nsystems incorporating simpler language models. Indeed, among various models,\nthe GPT-2 was shown to represent an excellent candidate for its integration\ninto BCI although testing was only perfomed on simulated conversations and not\non real BCI scenarios. Prospectively, the full integration of LLM with advanced\nBCI systems might lead to a big leap forward towards fast, efficient and\nuser-adaptive neurotechnology.\n","authors":["Andrea Caria"],"pdf_url":"https://arxiv.org/pdf/2412.07355v3.pdf","comment":"needs major revision"},{"id":"http://arxiv.org/abs/2504.03624v3","updated":"2025-04-15T14:36:01Z","published":"2025-04-04T17:41:58Z","title":"Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer\n  Models","summary":"  As inference-time scaling becomes critical for enhanced reasoning\ncapabilities, it is increasingly becoming important to build models that are\nefficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid\nMamba-Transformer models designed to reduce inference cost for a given accuracy\nlevel. To achieve this goal, we replace the majority of self-attention layers\nin the common Transformer model architecture with Mamba layers that perform\nconstant computation and require constant memory per generated token. We show\nthat Nemotron-H models offer either better or on-par accuracy compared to other\nsimilarly-sized state-of-the-art open-sourced Transformer models (e.g.,\nQwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at\ninference. To further increase inference speed and reduce the memory required\nat inference time, we created Nemotron-H-47B-Base from the 56B model using a\nnew compression via pruning and distillation technique called MiniPuzzle.\nNemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20%\nfaster to infer. In addition, we introduce an FP8-based training recipe and\nshow that it can achieve on par results with BF16-based training. This recipe\nis used to train the 56B model. We are releasing Nemotron-H base model\ncheckpoints with support in Hugging Face and NeMo.\n","authors":[" NVIDIA"," :","Aaron Blakeman","Aarti Basant","Abhinav Khattar","Adithya Renduchintala","Akhiad Bercovich","Aleksander Ficek","Alexis Bjorlin","Ali Taghibakhshi","Amala Sanjay Deshmukh","Ameya Sunil Mahabaleshwarkar","Andrew Tao","Anna Shors","Ashwath Aithal","Ashwin Poojary","Ayush Dattagupta","Balaram Buddharaju","Bobby Chen","Boris Ginsburg","Boxin Wang","Brandon Norick","Brian Butterfield","Bryan Catanzaro","Carlo del Mundo","Chengyu Dong","Christine Harvey","Christopher Parisien","Dan Su","Daniel Korzekwa","Danny Yin","Daria Gitman","David Mosallanezhad","Deepak Narayanan","Denys Fridman","Dima Rekesh","Ding Ma","Dmytro Pykhtar","Dong Ahn","Duncan Riach","Dusan Stosic","Eileen Long","Elad Segal","Ellie Evans","Eric Chung","Erick Galinkin","Evelina Bakhturina","Ewa Dobrowolska","Fei Jia","Fuxiao Liu","Gargi Prasad","Gerald Shen","Guilin Liu","Guo Chen","Haifeng Qian","Helen Ngo","Hongbin Liu","Hui Li","Igor Gitman","Ilia Karmanov","Ivan Moshkov","Izik Golan","Jan Kautz","Jane Polak Scowcroft","Jared Casper","Jarno Seppanen","Jason Lu","Jason Sewall","Jiaqi Zeng","Jiaxuan You","Jimmy Zhang","Jing Zhang","Jining Huang","Jinze Xue","Jocelyn Huang","Joey Conway","John Kamalu","Jon Barker","Jonathan Cohen","Joseph Jennings","Jupinder Parmar","Karan Sapra","Kari Briski","Kateryna Chumachenko","Katherine Luna","Keshav Santhanam","Kezhi Kong","Kirthi Sivamani","Krzysztof Pawelec","Kumar Anik","Kunlun Li","Lawrence McAfee","Leon Derczynski","Lindsey Pavao","Luis Vega","Lukas Voegtle","Maciej Bala","Maer Rodrigues de Melo","Makesh Narsimhan Sreedhar","Marcin Chochowski","Markus Kliegl","Marta Stepniewska-Dziubinska","Matthieu Le","Matvei Novikov","Mehrzad Samadi","Michael Andersch","Michael Evans","Miguel Martinez","Mike Chrzanowski","Mike Ranzinger","Mikolaj Blaz","Misha Smelyanskiy","Mohamed Fawzy","Mohammad Shoeybi","Mostofa Patwary","Nayeon Lee","Nima Tajbakhsh","Ning Xu","Oleg Rybakov","Oleksii Kuchaiev","Olivier Delalleau","Osvald Nitski","Parth Chadha","Pasha Shamis","Paulius Micikevicius","Pavlo Molchanov","Peter Dykas","Philipp Fischer","Pierre-Yves Aquilanti","Piotr Bialecki","Prasoon Varshney","Pritam Gundecha","Przemek Tredak","Rabeeh Karimi","Rahul Kandu","Ran El-Yaniv","Raviraj Joshi","Roger Waleffe","Ruoxi Zhang","Sabrina Kavanaugh","Sahil Jain","Samuel Kriman","Sangkug Lym","Sanjeev Satheesh","Saurav Muralidharan","Sean Narenthiran","Selvaraj Anandaraj","Seonmyeong Bak","Sergey Kashirsky","Seungju Han","Shantanu Acharya","Shaona Ghosh","Sharath Turuvekere Sreenivas","Sharon Clay","Shelby Thomas","Shrimai Prabhumoye","Shubham Pachori","Shubham Toshniwal","Shyamala Prayaga","Siddhartha Jain","Sirshak Das","Slawek Kierat","Somshubra Majumdar","Song Han","Soumye Singhal","Sriharsha Niverty","Stefania Alborghetti","Suseella Panguluri","Swetha Bhendigeri","Syeda Nahida Akter","Szymon Migacz","Tal Shiri","Terry Kong","Timo Roman","Tomer Ronen","Trisha Saar","Tugrul Konuk","Tuomas Rintamaki","Tyler Poon","Ushnish De","Vahid Noroozi","Varun Singh","Vijay Korthikanti","Vitaly Kurin","Wasi Uddin Ahmad","Wei Du","Wei Ping","Wenliang Dai","Wonmin Byeon","Xiaowei Ren","Yao Xu","Yejin Choi","Yian Zhang","Ying Lin","Yoshi Suhara","Zhiding Yu","Zhiqi Li","Zhiyu Li","Zhongbo Zhu","Zhuolin Yang","Zijia Chen"],"pdf_url":"https://arxiv.org/pdf/2504.03624v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04383v2","updated":"2025-04-15T14:07:31Z","published":"2025-04-06T06:23:27Z","title":"Retro-Search: Exploring Untaken Paths for Deeper and Efficient Reasoning","summary":"  Large reasoning models exhibit remarkable reasoning capabilities via long,\nelaborate reasoning trajectories. Supervised fine-tuning on such reasoning\ntraces, also known as distillation, can be a cost-effective way to boost\nreasoning capabilities of student models. However, empirical observations\nreveal that these reasoning trajectories are often suboptimal, switching\nexcessively between different lines of thought, resulting in under-thinking,\nover-thinking, and even degenerate responses. We introduce Retro-Search, an\nMCTS-inspired search algorithm, for distilling higher quality reasoning paths\nfrom large reasoning models. Retro-Search retrospectively revises reasoning\npaths to discover better, yet shorter traces, which can then lead to student\nmodels with enhanced reasoning capabilities with shorter, thus faster\ninference. Our approach can enable two use cases: self-improvement, where\nmodels are fine-tuned on their own Retro-Search-ed thought traces, and\nweak-to-strong improvement, where a weaker model revises stronger model's\nthought traces via Retro-Search. For self-improving, R1-distill-7B, fine-tuned\non its own Retro-Search-ed traces, reduces the average reasoning length by\n31.2% while improving performance by 7.7% across seven math benchmarks. For\nweak-to-strong improvement, we retrospectively revise R1-671B's traces from the\nOpenThoughts dataset using R1-distill-32B as the Retro-Search-er, a model 20x\nsmaller. Qwen2.5-32B, fine-tuned on this refined data, achieves performance\ncomparable to R1-distill-32B, yielding an 11.3% reduction in reasoning length\nand a 2.4% performance improvement compared to fine-tuning on the original\nOpenThoughts data. Our work counters recently emergent viewpoints that question\nthe relevance of search algorithms in the era of large reasoning models, by\ndemonstrating that there are still opportunities for algorithmic advancements,\neven for frontier models.\n","authors":["Ximing Lu","Seungju Han","David Acuna","Hyunwoo Kim","Jaehun Jung","Shrimai Prabhumoye","Niklas Muennighoff","Mostofa Patwary","Mohammad Shoeybi","Bryan Catanzaro","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2504.04383v2.pdf","comment":"Code and data will be publicly released upon internal approval"},{"id":"http://arxiv.org/abs/2504.08619v3","updated":"2025-04-15T14:06:21Z","published":"2025-04-11T15:24:23Z","title":"Analyzing 16,193 LLM Papers for Fun and Profits","summary":"  Large Language Models (LLMs) are reshaping the landscape of computer science\nresearch, driving significant shifts in research priorities across diverse\nconferences and fields. This study provides a comprehensive analysis of the\npublication trend of LLM-related papers in 77 top-tier computer science\nconferences over the past six years (2019-2024). We approach this analysis from\nfour distinct perspectives: (1) We investigate how LLM research is driving\ntopic shifts within major conferences. (2) We adopt a topic modeling approach\nto identify various areas of LLM-related topic growth and reveal the topics of\nconcern at different conferences. (3) We explore distinct contribution patterns\nof academic and industrial institutions. (4) We study the influence of national\norigins on LLM development trajectories. Synthesizing the findings from these\ndiverse analytical angles, we derive ten key insights that illuminate the\ndynamics and evolution of the LLM research ecosystem.\n","authors":["Zhiqiu Xia","Lang Zhu","Bingzhe Li","Feng Chen","Qiannan Li","Chunhua Liao","Feiyi Wang","Hang Liu"],"pdf_url":"https://arxiv.org/pdf/2504.08619v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11190v1","updated":"2025-04-15T13:47:55Z","published":"2025-04-15T13:47:55Z","title":"Enhancing multimodal analogical reasoning with Logic Augmented\n  Generation","summary":"  Recent advances in Large Language Models have demonstrated their capabilities\nacross a variety of tasks. However, automatically extracting implicit knowledge\nfrom natural language remains a significant challenge, as machines lack active\nexperience with the physical world. Given this scenario, semantic knowledge\ngraphs can serve as conceptual spaces that guide the automated text generation\nreasoning process to achieve more efficient and explainable results. In this\npaper, we apply a logic-augmented generation (LAG) framework that leverages the\nexplicit representation of a text through a semantic knowledge graph and\napplies it in combination with prompt heuristics to elicit implicit analogical\nconnections. This method generates extended knowledge graph triples\nrepresenting implicit meaning, enabling systems to reason on unlabeled\nmultimodal data regardless of the domain. We validate our work through three\nmetaphor detection and understanding tasks across four datasets, as they\nrequire deep analogical reasoning capabilities. The results show that this\nintegrated approach surpasses current baselines, performs better than humans in\nunderstanding visual metaphors, and enables more explainable reasoning\nprocesses, though still has inherent limitations in metaphor understanding,\nespecially for domain-specific metaphors. Furthermore, we propose a thorough\nerror analysis, discussing issues with metaphorical annotations and current\nevaluation methods.\n","authors":["Anna Sofia Lippolis","Andrea Giovanni Nuzzolese","Aldo Gangemi"],"pdf_url":"https://arxiv.org/pdf/2504.11190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11186v1","updated":"2025-04-15T13:42:34Z","published":"2025-04-15T13:42:34Z","title":"Benchmarking Next-Generation Reasoning-Focused Large Language Models in\n  Ophthalmology: A Head-to-Head Evaluation on 5,888 Items","summary":"  Recent advances in reasoning-focused large language models (LLMs) mark a\nshift from general LLMs toward models designed for complex decision-making, a\ncrucial aspect in medicine. However, their performance in specialized domains\nlike ophthalmology remains underexplored. This study comprehensively evaluated\nand compared the accuracy and reasoning capabilities of four newly developed\nreasoning-focused LLMs, namely DeepSeek-R1, OpenAI o1, o3-mini, and Gemini 2.0\nFlash-Thinking. Each model was assessed using 5,888 multiple-choice\nophthalmology exam questions from the MedMCQA dataset in zero-shot setting.\nQuantitative evaluation included accuracy, Macro-F1, and five text-generation\nmetrics (ROUGE-L, METEOR, BERTScore, BARTScore, and AlignScore), computed\nagainst ground-truth reasonings. Average inference time was recorded for a\nsubset of 100 randomly selected questions. Additionally, two board-certified\nophthalmologists qualitatively assessed clarity, completeness, and reasoning\nstructure of responses to differential diagnosis questions.O1 (0.902) and\nDeepSeek-R1 (0.888) achieved the highest accuracy, with o1 also leading in\nMacro-F1 (0.900). The performance of models across the text-generation metrics\nvaried: O3-mini excelled in ROUGE-L (0.151), o1 in METEOR (0.232), DeepSeek-R1\nand o3-mini tied for BERTScore (0.673), DeepSeek-R1 (-4.105) and Gemini 2.0\nFlash-Thinking (-4.127) performed best in BARTScore, while o3-mini (0.181) and\no1 (0.176) led AlignScore. Inference time across the models varied, with\nDeepSeek-R1 being slowest (40.4 seconds) and Gemini 2.0 Flash-Thinking fastest\n(6.7 seconds). Qualitative evaluation revealed that DeepSeek-R1 and Gemini 2.0\nFlash-Thinking tended to provide detailed and comprehensive intermediate\nreasoning, whereas o1 and o3-mini displayed concise and summarized\njustifications.\n","authors":["Minjie Zou","Sahana Srinivasan","Thaddaeus Wai Soon Lo","Ke Zou","Gabriel Dawei Yang","Xuguang Ai","Hyunjae Kim","Maxwell Singer","Fares Antaki","Kelvin Li","Robert Chang","Marcus Tan","David Ziyou Chen","Dianbo Liu","Qingyu Chen","Yih Chung Tham"],"pdf_url":"https://arxiv.org/pdf/2504.11186v1.pdf","comment":"83 pages, 6 figures, 3 tables, 9 supplementary figures, 7\n  supplementary tables"},{"id":"http://arxiv.org/abs/2504.11183v1","updated":"2025-04-15T13:40:22Z","published":"2025-04-15T13:40:22Z","title":"Bias Beyond English: Evaluating Social Bias and Debiasing Methods in a\n  Low-Resource Setting","summary":"  Social bias in language models can potentially exacerbate social\ninequalities. Despite it having garnered wide attention, most research focuses\non English data. In a low-resource scenario, the models often perform worse due\nto insufficient training data. This study aims to leverage high-resource\nlanguage corpora to evaluate bias and experiment with debiasing methods in\nlow-resource languages. We evaluated the performance of recent multilingual\nmodels in five languages: English (\\textsc{eng}), Chinese (\\textsc{zho}),\nRussian (\\textsc{rus}), Indonesian (\\textsc{ind}) and Thai (\\textsc{tha}), and\nanalyzed four bias dimensions: \\textit{gender}, \\textit{religion},\n\\textit{nationality}, and \\textit{race-color}. By constructing multilingual\nbias evaluation datasets, this study allows fair comparisons between models\nacross languages. We have further investigated three debiasing\nmethods-\\texttt{CDA}, \\texttt{Dropout}, \\texttt{SenDeb}-and demonstrated that\ndebiasing methods from high-resource languages can be effectively transferred\nto low-resource ones, providing actionable insights for fairness research in\nmultilingual NLP.\n","authors":["Ej Zhou","Weiming Lu"],"pdf_url":"https://arxiv.org/pdf/2504.11183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06857v5","updated":"2025-04-15T13:38:08Z","published":"2024-09-10T20:45:43Z","title":"What is the Role of Small Models in the LLM Era: A Survey","summary":"  Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models\n","authors":["Lihu Chen","Gaël Varoquaux"],"pdf_url":"https://arxiv.org/pdf/2409.06857v5.pdf","comment":"a survey paper of small models"},{"id":"http://arxiv.org/abs/2504.10342v2","updated":"2025-04-15T13:34:53Z","published":"2025-04-14T15:50:39Z","title":"VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain\n  Knowledge","summary":"  Current multimodal benchmarks often conflate reasoning with domain-specific\nknowledge, making it difficult to isolate and evaluate general reasoning\nabilities in non-expert settings. To address this, we introduce VisualPuzzles,\na benchmark that targets visual reasoning while deliberately minimizing\nreliance on specialized knowledge. VisualPuzzles consists of diverse questions\nspanning five categories: algorithmic, analogical, deductive, inductive, and\nspatial reasoning. One major source of our questions is manually translated\nlogical reasoning questions from the Chinese Civil Service Examination.\nExperiments show that VisualPuzzles requires significantly less intensive\ndomain-specific knowledge and more complex reasoning compared to benchmarks\nlike MMMU, enabling us to better evaluate genuine multimodal reasoning.\nEvaluations show that state-of-the-art multimodal large language models\nconsistently lag behind human performance on VisualPuzzles, and that strong\nperformance on knowledge-intensive benchmarks does not necessarily translate to\nsuccess on reasoning-focused, knowledge-light tasks. Additionally, reasoning\nenhancements such as scaling up inference compute (with \"thinking\" modes) yield\ninconsistent gains across models and task types, and we observe no clear\ncorrelation between model size and performance. We also found that models\nexhibit different reasoning and answering patterns on VisualPuzzles compared to\nbenchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer\nlens through which to evaluate reasoning capabilities beyond factual recall and\ndomain knowledge.\n","authors":["Yueqi Song","Tianyue Ou","Yibo Kong","Zecheng Li","Graham Neubig","Xiang Yue"],"pdf_url":"https://arxiv.org/pdf/2504.10342v2.pdf","comment":"56 pages, 43 figures"},{"id":"http://arxiv.org/abs/2504.11169v1","updated":"2025-04-15T13:16:46Z","published":"2025-04-15T13:16:46Z","title":"MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media\n  Videos","summary":"  Sexism is generally defined as prejudice and discrimination based on sex or\ngender, affecting every sector of society, from social institutions to\nrelationships and individual behavior. Social media platforms amplify the\nimpact of sexism by conveying discriminatory content not only through text but\nalso across multiple modalities, highlighting the critical need for a\nmultimodal approach to the analysis of sexism online. With the rise of social\nmedia platforms where users share short videos, sexism is increasingly\nspreading through video content. Automatically detecting sexism in videos is a\nchallenging task, as it requires analyzing the combination of verbal, audio,\nand visual elements to identify sexist content. In this study, (1) we introduce\nMuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of\n$\\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose\nan innovative annotation framework for analyzing the contribution of textual\nand multimodal labels in the classification of sexist and non-sexist content;\nand (3) we evaluate a range of large language models (LLMs) and multimodal LLMs\non the task of sexism detection. We find that visual information plays a key\nrole in labeling sexist content for both humans and models. Models effectively\ndetect explicit sexism; however, they struggle with implicit cases, such as\nstereotypes, instances where annotators also show low agreement. This\nhighlights the inherent difficulty of the task, as identifying implicit sexism\ndepends on the social and cultural context.\n","authors":["Laura De Grazia","Pol Pastells","Mauro Vázquez Chas","Desmond Elliott","Danae Sánchez Villegas","Mireia Farrús","Mariona Taulé"],"pdf_url":"https://arxiv.org/pdf/2504.11169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14914v2","updated":"2025-04-15T12:58:38Z","published":"2025-02-19T07:55:51Z","title":"What Is a Good Caption? A Comprehensive Visual Caption Benchmark for\n  Evaluating Both Correctness and Thoroughness","summary":"  Visual captioning benchmarks have become outdated with the emergence of\nmodern multimodal large language models (MLLMs), as the brief ground-truth\nsentences and traditional metrics fail to assess detailed captions effectively.\nWhile recent benchmarks attempt to address this by focusing on keyword\nextraction or object-centric evaluation, they remain limited to vague-view or\nobject-view analyses and incomplete visual element coverage. In this paper, we\nintroduce CAPability, a comprehensive multi-view benchmark for evaluating\nvisual captioning across 12 dimensions spanning six critical views. We curate\nnearly 11K human-annotated images and videos with visual element annotations to\nevaluate the generated captions. CAPability stably assesses both the\ncorrectness and thoroughness of captions using F1-score. By converting\nannotations to QA pairs, we further introduce a heuristic metric, \\textit{know\nbut cannot tell} ($K\\bar{T}$), indicating a significant performance gap between\nQA and caption capabilities. Our work provides the first holistic analysis of\nMLLMs' captioning abilities, as we identify their strengths and weaknesses\nacross various dimensions, guiding future research to enhance specific aspects\nof capabilities.\n","authors":["Zhihang Liu","Chen-Wei Xie","Bin Wen","Feiwu Yu","Jixuan Chen","Boqiang Zhang","Nianzu Yang","Pandeng Li","Yinglu Li","Zuan Gao","Yun Zheng","Hongtao Xie"],"pdf_url":"https://arxiv.org/pdf/2502.14914v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10329v2","updated":"2025-04-15T12:26:05Z","published":"2024-11-15T16:29:02Z","title":"Safe Text-to-Image Generation: Simply Sanitize the Prompt Embedding","summary":"  In recent years, text-to-image (T2I) generation models have made significant\nprogress in generating high-quality images that align with text descriptions.\nHowever, these models also face the risk of unsafe generation, potentially\nproducing harmful content that violates usage policies, such as explicit\nmaterial. Existing safe generation methods typically focus on suppressing\ninappropriate content by erasing undesired concepts from visual\nrepresentations, while neglecting to sanitize the textual representation.\nAlthough these methods help mitigate the risk of misuse to some extent, their\nrobustness remains insufficient when dealing with adversarial attacks.\n  Given that semantic consistency between input text and output image is a core\nrequirement of T2I models, we identify that textual representations are likely\nthe primary source of unsafe generation. To this end, we propose Embedding\nSanitizer (ES), which enhances the safety of T2I models by sanitizing\ninappropriate concepts in prompt embeddings. To our knowledge, ES is the first\ninterpretable safe generation framework that assigns a score to each token in\nthe prompt to indicate its potential harmfulness. In addition, ES adopts a\nplug-and-play modular design, offering compatibility for seamless integration\nwith various T2I models and other safeguards. Evaluations on five prompt\nbenchmarks show that ES outperforms eleven existing safeguard baselines,\nachieving state-of-the-art robustness while maintaining high-quality image\ngeneration.\n","authors":["Huming Qiu","Guanxu Chen","Mi Zhang","Xiaohan Zhang","Xiaoyu You","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2411.10329v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11108v1","updated":"2025-04-15T11:55:24Z","published":"2025-04-15T11:55:24Z","title":"Benchmarking Vision Language Models on German Factual Data","summary":"  Similar to LLMs, the development of vision language models is mainly driven\nby English datasets and models trained in English and Chinese language, whereas\nsupport for other languages, even those considered high-resource languages such\nas German, remains significantly weaker. In this work we present an analysis of\nopen-weight VLMs on factual knowledge in the German and English language. We\ndisentangle the image-related aspects from the textual ones by analyzing\naccu-racy with jury-as-a-judge in both prompt languages and images from German\nand international contexts. We found that for celebrities and sights, VLMs\nstruggle because they are lacking visual cognition of German image contents.\nFor animals and plants, the tested models can often correctly identify the\nimage contents ac-cording to the scientific name or English common name but\nfail in German lan-guage. Cars and supermarket products were identified equally\nwell in English and German images across both prompt languages.\n","authors":["René Peinl","Vincent Tischler"],"pdf_url":"https://arxiv.org/pdf/2504.11108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11104v1","updated":"2025-04-15T11:52:20Z","published":"2025-04-15T11:52:20Z","title":"Using LLMs as prompt modifier to avoid biases in AI image generators","summary":"  This study examines how Large Language Models (LLMs) can reduce biases in\ntext-to-image generation systems by modifying user prompts. We define bias as a\nmodel's unfair deviation from population statistics given neutral prompts. Our\nexperiments with Stable Diffusion XL, 3.5 and Flux demonstrate that\nLLM-modified prompts significantly increase image diversity and reduce bias\nwithout the need to change the image generators themselves. While occasionally\nproducing results that diverge from original user intent for elaborate prompts,\nthis approach generally provides more varied interpretations of underspecified\nrequests rather than superficial variations. The method works particularly well\nfor less advanced image generators, though limitations persist for certain\ncontexts like disability representation. All prompts and generated images are\navailable at https://iisys-hof.github.io/llm-prompt-img-gen/\n","authors":["René Peinl"],"pdf_url":"https://arxiv.org/pdf/2504.11104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09378v2","updated":"2025-04-15T11:49:34Z","published":"2025-04-13T00:01:22Z","title":"Can you map it to English? The Role of Cross-Lingual Alignment in\n  Multilingual Performance of LLMs","summary":"  Large language models (LLMs) pre-trained predominantly on English text\nexhibit surprising multilingual capabilities, yet the mechanisms driving\ncross-lingual generalization remain poorly understood. This work investigates\nhow the alignment of representations for text written in different languages\ncorrelates with LLM performance on natural language understanding tasks and\ntranslation tasks, both at the language and the instance level. For this\npurpose, we introduce cross-lingual alignment metrics such as the\nDiscriminative Alignment Index (DALI) to quantify the alignment at an instance\nlevel for discriminative tasks. Through experiments on three natural language\nunderstanding tasks (Belebele, XStoryCloze, XCOPA), and machine translation, we\nfind that while cross-lingual alignment metrics strongly correlate with task\naccuracy at the language level, the sample-level alignment often fails to\ndistinguish correct from incorrect predictions, exposing alignment as a\nnecessary but insufficient condition for success.\n","authors":["Kartik Ravisankar","Hyojung Han","Marine Carpuat"],"pdf_url":"https://arxiv.org/pdf/2504.09378v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10419v2","updated":"2025-04-15T11:41:36Z","published":"2025-04-14T17:06:59Z","title":"Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large\n  Language Models with CheckboxQA","summary":"  Checkboxes are critical in real-world document processing where the presence\nor absence of ticks directly informs data extraction and decision-making\nprocesses. Yet, despite the strong performance of Large Vision and Language\nModels across a wide range of tasks, they struggle with interpreting checkable\ncontent. This challenge becomes particularly pressing in industries where a\nsingle overlooked checkbox may lead to costly regulatory or contractual\noversights. To address this gap, we introduce the CheckboxQA dataset, a\ntargeted resource designed to evaluate and improve model performance on\ncheckbox-related tasks. It reveals the limitations of current models and serves\nas a valuable tool for advancing document comprehension systems, with\nsignificant implications for applications in sectors such as legal tech and\nfinance.\n  The dataset is publicly available at:\nhttps://github.com/Snowflake-Labs/CheckboxQA\n","authors":["Michał Turski","Mateusz Chiliński","Łukasz Borchmann"],"pdf_url":"https://arxiv.org/pdf/2504.10419v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11082v1","updated":"2025-04-15T11:28:02Z","published":"2025-04-15T11:28:02Z","title":"DeepMLF: Multimodal language model with learnable tokens for deep fusion\n  in sentiment analysis","summary":"  While multimodal fusion has been extensively studied in Multimodal Sentiment\nAnalysis (MSA), the role of fusion depth and multimodal capacity allocation\nremains underexplored. In this work, we position fusion depth, scalability, and\ndedicated multimodal capacity as primary factors for effective fusion. We\nintroduce DeepMLF, a novel multimodal language model (LM) with learnable tokens\ntailored toward deep fusion. DeepMLF leverages an audiovisual encoder and a\npretrained decoder LM augmented with multimodal information across its layers.\nWe append learnable tokens to the LM that: 1) capture modality interactions in\na controlled fashion and 2) preserve independent information flow for each\nmodality. These fusion tokens gather linguistic information via causal\nself-attention in LM Blocks and integrate with audiovisual information through\ncross-attention MM Blocks. Serving as dedicated multimodal capacity, this\ndesign enables progressive fusion across multiple layers, providing depth in\nthe fusion process. Our training recipe combines modality-specific losses and\nlanguage modelling loss, with the decoder LM tasked to predict ground truth\npolarity. Across three MSA benchmarks with varying dataset characteristics,\nDeepMLF achieves state-of-the-art performance. Our results confirm that deeper\nfusion leads to better performance, with optimal fusion depths (5-7) exceeding\nthose of existing approaches. Additionally, our analysis on the number of\nfusion tokens reveals that small token sets ($\\sim$20) achieve optimal\nperformance. We examine the importance of representation learning order (fusion\ncurriculum) through audiovisual encoder initialization experiments. Our\nablation studies demonstrate the superiority of the proposed fusion design and\ngating while providing a holistic examination of DeepMLF's scalability to LLMs,\nand the impact of each training objective and embedding regularization.\n","authors":["Efthymios Georgiou","Vassilis Katsouros","Yannis Avrithis","Alexandros Potamianos"],"pdf_url":"https://arxiv.org/pdf/2504.11082v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2412.09353v2","updated":"2025-04-15T10:14:34Z","published":"2024-12-12T15:22:03Z","title":"Causal Graphical Models for Vision-Language Compositional Understanding","summary":"  Recent work has empirically shown that Vision-Language Models (VLMs) struggle\nto fully understand the compositional properties of the human language, usually\nmodeling an image caption as a \"bag of words\". As a result, they perform poorly\non compositional tasks, which require a deeper understanding of the different\nentities of a sentence (subject, verb, etc.) jointly with their mutual\nrelationships in order to be solved. In this paper, we model the dependency\nrelations among textual and visual tokens using a Causal Graphical Model (CGM),\nbuilt using a dependency parser, and we train a decoder conditioned by the VLM\nvisual encoder. Differently from standard autoregressive or parallel\npredictions, our decoder's generative process is partially-ordered following\nthe CGM structure. This structure encourages the decoder to learn only the main\ncausal dependencies in a sentence discarding spurious correlations. Using\nextensive experiments on five compositional benchmarks, we show that our method\nsignificantly outperforms all the state-of-the-art compositional approaches by\na large margin, and it also improves over methods trained using much larger\ndatasets.\n","authors":["Fiorenzo Parascandolo","Nicholas Moratelli","Enver Sangineto","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2412.09353v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2504.11042v1","updated":"2025-04-15T10:07:33Z","published":"2025-04-15T10:07:33Z","title":"LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews","summary":"  Peer review is a cornerstone of quality control in scientific publishing.\nWith the increasing workload, the unintended use of `quick' heuristics,\nreferred to as lazy thinking, has emerged as a recurring issue compromising\nreview quality. Automated methods to detect such heuristics can help improve\nthe peer-reviewing process. However, there is limited NLP research on this\nissue, and no real-world dataset exists to support the development of detection\ntools. This work introduces LazyReview, a dataset of peer-review sentences\nannotated with fine-grained lazy thinking categories. Our analysis reveals that\nLarge Language Models (LLMs) struggle to detect these instances in a zero-shot\nsetting. However, instruction-based fine-tuning on our dataset significantly\nboosts performance by 10-20 performance points, highlighting the importance of\nhigh-quality training data. Furthermore, a controlled experiment demonstrates\nthat reviews revised with lazy thinking feedback are more comprehensive and\nactionable than those written without such feedback. We will release our\ndataset and the enhanced guidelines that can be used to train junior reviewers\nin the community. (Code available here:\nhttps://github.com/UKPLab/arxiv2025-lazy-review)\n","authors":["Sukannya Purkayastha","Zhuang Li","Anne Lauscher","Lizhen Qu","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2504.11042v1.pdf","comment":"29 pages, 18 Figures, 15 Tables"},{"id":"http://arxiv.org/abs/2504.11004v1","updated":"2025-04-15T09:20:45Z","published":"2025-04-15T09:20:45Z","title":"Dynamic Compressing Prompts for Efficient Inference of Large Language\n  Models","summary":"  Large Language Models (LLMs) have shown outstanding performance across a\nvariety of tasks, partly due to advanced prompting techniques. However, these\ntechniques often require lengthy prompts, which increase computational costs\nand can hinder performance because of the limited context windows of LLMs.\nWhile prompt compression is a straightforward solution, existing methods\nconfront the challenges of retaining essential information, adapting to context\nchanges, and remaining effective across different tasks. To tackle these\nissues, we propose a task-agnostic method called Dynamic Compressing Prompts\n(LLM-DCP). Our method reduces the number of prompt tokens while aiming to\npreserve the performance as much as possible. We model prompt compression as a\nMarkov Decision Process (MDP), enabling the DCP-Agent to sequentially remove\nredundant tokens by adapting to dynamic contexts and retaining crucial content.\nWe develop a reward function for training the DCP-Agent that balances the\ncompression rate, the quality of the LLM output, and the retention of key\ninformation. This allows for prompt token reduction without needing an external\nblack-box LLM. Inspired by the progressive difficulty adjustment in curriculum\nlearning, we introduce a Hierarchical Prompt Compression (HPC) training\nstrategy that gradually increases the compression difficulty, enabling the\nDCP-Agent to learn an effective compression method that maintains information\nintegrity. Experiments demonstrate that our method outperforms state-of-the-art\ntechniques, especially at higher compression rates. The code for our approach\nwill be available at https://github.com/Fhujinwu/DCP.\n","authors":["Jinwu Hu","Wei Zhang","Yufeng Wang","Yu Hu","Bin Xiao","Mingkui Tan","Qing Du"],"pdf_url":"https://arxiv.org/pdf/2504.11004v1.pdf","comment":"Under review (submited in 2024.11)"},{"id":"http://arxiv.org/abs/2504.11001v1","updated":"2025-04-15T09:18:21Z","published":"2025-04-15T09:18:21Z","title":"ReZero: Enhancing LLM search ability by trying one-more-time","summary":"  Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM)\nperformance on knowledge-intensive tasks but depends heavily on initial search\nquery quality. Current methods, often using Reinforcement Learning (RL),\ntypically focus on query formulation or reasoning over results, without\nexplicitly encouraging persistence after a failed search. We introduce ReZero\n(Retry-Zero), a novel RL framework that directly rewards the act of retrying a\nsearch query following an initial unsuccessful attempt. This incentivizes the\nLLM to explore alternative queries rather than prematurely halting. ReZero\ndemonstrates significant improvement, achieving 46.88% accuracy compared to a\n25% baseline. By rewarding persistence, ReZero enhances LLM robustness in\ncomplex information-seeking scenarios where initial queries may prove\ninsufficient.\n","authors":["Alan Dao","Thinh Le"],"pdf_url":"https://arxiv.org/pdf/2504.11001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15726v2","updated":"2025-04-15T08:55:13Z","published":"2024-12-20T09:49:02Z","title":"Fine-tuning Whisper on Low-Resource Languages for Real-World\n  Applications","summary":"  This paper presents a new approach to fine-tuning OpenAI's Whisper model for\nlow-resource languages by introducing a novel data generation method that\nconverts sentence-level data into a long-form corpus, using Swiss German as a\ncase study. Non-sentence-level data, which could improve the performance of\nlong-form audio, is difficult to obtain and often restricted by copyright laws.\nOur method bridges this gap by transforming more accessible sentence-level data\ninto a format that preserves the model's ability to handle long-form audio and\nperform segmentation without requiring non-sentence-level data. Our data\ngeneration process improves performance in several real-world applications and\nleads to the development of a new state-of-the-art speech-to-text (STT) model\nfor Swiss German. We compare our model with a non-fine-tuned Whisper and our\nprevious state-of-the-art Swiss German STT models, where our new model achieves\nhigher BLEU scores. Our results also indicate that the proposed method is\nadaptable to other low-resource languages, supported by written guidance and\ncode that allows the creation of fine-tuned Whisper models, which keep\nsegmentation capabilities and allow the transcription of longer audio files\nusing only sentence-level data with high quality.\n","authors":["Vincenzo Timmel","Claudio Paonessa","Reza Kakooee","Manfred Vogel","Daniel Perruchoud"],"pdf_url":"https://arxiv.org/pdf/2412.15726v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05747v2","updated":"2025-04-15T08:51:05Z","published":"2025-04-08T07:24:51Z","title":"SEA-LION: Southeast Asian Languages in One Network","summary":"  Recently, Large Language Models (LLMs) have dominated much of the artificial\nintelligence scene with their ability to process and generate natural\nlanguages. However, the majority of LLM research and development remains\nEnglish-centric, leaving low-resource languages such as those in the Southeast\nAsian (SEA) region under-represented. To address this representation gap, we\nintroduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge\nmultilingual LLMs designed for SEA languages. The SEA-LION family of LLMs\nsupports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese,\nMalay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages\nlarge-scale multilingual continued pre-training with a comprehensive\npost-training regime involving multiple stages of instruction fine-tuning,\nalignment, and model merging. Evaluation results on multilingual benchmarks\nindicate that our models achieve state-of-the-art performance across LLMs\nsupporting SEA languages. We open-source the models to benefit the wider SEA\ncommunity.\n","authors":["Raymond Ng","Thanh Ngan Nguyen","Yuli Huang","Ngee Chia Tai","Wai Yi Leong","Wei Qi Leong","Xianbin Yong","Jian Gang Ngui","Yosephine Susanto","Nicholas Cheng","Hamsawardhini Rengarajan","Peerat Limkonchotiwat","Adithya Venkatadri Hulagadri","Kok Wai Teng","Yeo Yeow Tong","Bryan Siow","Wei Yi Teo","Wayne Lau","Choon Meng Tan","Brandon Ong","Zhi Hao Ong","Jann Railey Montalan","Adwin Chan","Sajeban Antonyrex","Ren Lee","Esther Choa","David Ong Tat-Wee","Bing Jie Darius Liu","William Chandra Tjhi","Erik Cambria","Leslie Teo"],"pdf_url":"https://arxiv.org/pdf/2504.05747v2.pdf","comment":"We released our model at\n  https://huggingface.co/collections/aisingapore/sea-lionv3-672589a39cdadd6a5b199581"},{"id":"http://arxiv.org/abs/2504.03786v3","updated":"2025-04-15T08:51:02Z","published":"2025-04-03T17:43:45Z","title":"Do \"New Snow Tablets\" Contain Snow? Large Language Models Over-Rely on\n  Names to Identify Ingredients of Chinese Drugs","summary":"  Traditional Chinese Medicine (TCM) has seen increasing adoption in\nhealthcare, with specialized Large Language Models (LLMs) emerging to support\nclinical applications. A fundamental requirement for these models is accurate\nidentification of TCM drug ingredients. In this paper, we evaluate how general\nand TCM-specialized LLMs perform when identifying ingredients of Chinese drugs.\nOur systematic analysis reveals consistent failure patterns: models often\ninterpret drug names literally, overuse common herbs regardless of relevance,\nand exhibit erratic behaviors when faced with unfamiliar formulations. LLMs\nalso fail to understand the verification task. These findings demonstrate that\ncurrent LLMs rely primarily on drug names rather than possessing systematic\npharmacological knowledge. To address these limitations, we propose a Retrieval\nAugmented Generation (RAG) approach focused on ingredient names. Experiments\nacross 220 TCM formulations show our method significantly improves accuracy\nfrom approximately 50% to 82% in ingredient verification tasks. Our work\nhighlights critical weaknesses in current TCM-specific LLMs and offers a\npractical solution for enhancing their clinical reliability.\n","authors":["Sifan Li","Yujun Cai","Bryan Hooi","Nanyun Peng","Yiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2504.03786v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10982v1","updated":"2025-04-15T08:46:39Z","published":"2025-04-15T08:46:39Z","title":"Exploring the Role of KG-Based RAG in Japanese Medical Question\n  Answering with Small-Scale LLMs","summary":"  Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages.\n","authors":["Yingjian Chen","Feiyang Li","Xingyu Song","Tianxiao Li","Issey Sudeka","Irene Li"],"pdf_url":"https://arxiv.org/pdf/2504.10982v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2407.07612v2","updated":"2025-04-15T08:43:28Z","published":"2024-07-10T12:50:44Z","title":"Teaching Transformers Causal Reasoning through Axiomatic Training","summary":"  For text-based AI systems to interact in the real world, causal reasoning is\nan essential skill. Since active interventions are costly, we study to what\nextent a system can learn causal reasoning from symbolic demonstrations of\ncausal axioms. Specifically, we present an axiomatic training method where the\nsystem learns from multiple demonstrations of a causal axiom (or rule), rather\nthan incorporating the axiom as an inductive bias or inferring it from data\nvalues. A key question is whether the system would learn to generalize from the\naxiom demonstrations to more complex scenarios. Our results, based on applying\naxiomatic training to learn the transitivity axiom and d-separation rule,\nindicate that such generalization is possible. To avoid data contamination\nissues, we start with a 67 million parameter transformer model and train it\nfrom scratch. On both tasks, we find that a model trained on linear causal\nchains (along with some noisy variations) can generalize well to complex\ngraphs, including longer causal chains, causal chains with reversed order, and\ngraphs with branching.To handle diverse text inputs, the same method is\nextended to finetune language models. Finetuning Llama-3.1 8B model on our\naxiomatic data leads to significant gains on causal benchmarks such as\nCorr2Cause and CLEAR, in some cases providing state-of-the-art performance\nsurpassing GPT-4.\n","authors":["Aniket Vashishtha","Abhinav Kumar","Atharva Pandey","Abbavaram Gowtham Reddy","Kabir Ahuja","Vineeth N Balasubramanian","Amit Sharma"],"pdf_url":"https://arxiv.org/pdf/2407.07612v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15055v2","updated":"2025-04-15T08:14:25Z","published":"2025-03-19T09:46:54Z","title":"ELTEX: A Framework for Domain-Driven Synthetic Data Generation","summary":"  We introduce Efficient LLM Token Extraction (ELTEX), a framework addressing\nthe critical challenge of LLM domain specialization by systematically\nextracting and integrating domain indicators throughout synthetic data\ngeneration. Unlike approaches relying on implicit knowledge transfer, ELTEX\nexplicitly leverages domain signals to maintain specialized knowledge\nintegrity. In our cybersecurity case study, ELTEX-enhanced data enables a\nfine-tuned Gemma-2B model to achieve performance competitive with GPT-4o on\nblockchain cyberattack classification while reducing computational\nrequirements. Our Google Sheets implementation makes ELTEX accessible to\nnon-technical users. Our contributions include: (1) the ELTEX framework; (2)\nGoogle Sheets Add-on implementation; (3) empirical validation showing how ELTEX\nbridges performance gaps between small and large models; and (4) a synthetic\ndataset of 11,448 texts for blockchain cyberattack detection.\n","authors":["Arina Razmyslovich","Kseniia Murasheva","Sofia Sedlova","Julien Capitaine","Eugene Dmitriev"],"pdf_url":"https://arxiv.org/pdf/2503.15055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.22303v2","updated":"2025-04-15T08:10:39Z","published":"2025-03-28T10:26:49Z","title":"Preference-based Learning with Retrieval Augmented Generation for\n  Conversational Question Answering","summary":"  Conversational Question Answering (ConvQA) involves multiple subtasks, i) to\nunderstand incomplete questions in their context, ii) to retrieve relevant\ninformation, and iii) to generate answers. This work presents PRAISE, a\npipeline-based approach for ConvQA that trains LLM adapters for each of the\nthree subtasks. As labeled training data for individual subtasks is unavailable\nin practice, PRAISE learns from its own generations using the final answering\nperformance as feedback signal without human intervention and treats\nintermediate information, like relevant evidence, as weakly labeled data. We\napply Direct Preference Optimization by contrasting successful and unsuccessful\nsamples for each subtask. In our experiments, we show the effectiveness of this\ntraining paradigm: PRAISE shows improvements per subtask and achieves new\nstate-of-the-art performance on a popular ConvQA benchmark, by gaining 15.5\npercentage points increase in precision over baselines.\n","authors":["Magdalena Kaiser","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2503.22303v2.pdf","comment":"WWW 2025 Short Paper, 5 pages"},{"id":"http://arxiv.org/abs/2504.09421v2","updated":"2025-04-15T07:52:40Z","published":"2025-04-13T04:00:40Z","title":"ClinicalGPT-R1: Pushing reasoning capability of generalist disease\n  diagnosis with large language model","summary":"  Recent advances in reasoning with large language models (LLMs)has shown\nremarkable reasoning capabilities in domains such as mathematics and coding,\nyet their application to clinical diagnosis remains underexplored. Here, we\nintroduce ClinicalGPT-R1, a reasoning enhanced generalist large language model\nfor disease diagnosis. Trained on a dataset of 20,000 real-world clinical\nrecords, ClinicalGPT-R1 leverages diverse training strategies to enhance\ndiagnostic reasoning. To benchmark performance, we curated MedBench-Hard, a\nchallenging dataset spanning seven major medical specialties and representative\ndiseases. Experimental results demonstrate that ClinicalGPT-R1 outperforms\nGPT-4o in Chinese diagnostic tasks and achieves comparable performance to GPT-4\nin English settings. This comparative study effectively validates the superior\nperformance of ClinicalGPT-R1 in disease diagnosis tasks. Resources are\navailable at https://github.com/medfound/medfound.\n","authors":["Wuyang Lan","Wenzheng Wang","Changwei Ji","Guoxing Yang","Yongbo Zhang","Xiaohong Liu","Song Wu","Guangyu Wang"],"pdf_url":"https://arxiv.org/pdf/2504.09421v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.05447v2","updated":"2025-04-15T07:51:10Z","published":"2025-03-07T14:17:45Z","title":"Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts","summary":"  Linear Sequence Modeling (LSM) like linear attention, state space models and\nlinear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant\narchitectural improvements. In this paper, we introduce Linear-MoE, a\nproduction-level system for modeling and training large-scale models that\nintegrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules\nfor linear-complexity sequence modeling and MoE layers for sparsely activation,\naiming to offer high performance with efficient training. The Linear-MoE system\ncomprises: 1) Modeling subsystem, which provides a unified framework supporting\nall instances of LSM. and 2) Training subsystem, which facilitates efficient\ntraining by incorporating various advanced parallelism technologies,\nparticularly Sequence Parallelism designed for Linear-MoE models. Additionally,\nwe explore hybrid models that combine Linear-MoE layers with standard\nTransformer-MoE layers with its Sequence Parallelism to further enhance model\nflexibility and performance. Evaluations on two model series, A0.3B-2B and\nA1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining\ncompetitive performance on various benchmarks, showcasing its potential as a\nnext-generation foundational model architecture. Code:\nhttps://github.com/OpenSparseLLMs/Linear-MoE.\n","authors":["Weigao Sun","Disen Lan","Tong Zhu","Xiaoye Qu","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2503.05447v2.pdf","comment":"Technical report, 17 pages"},{"id":"http://arxiv.org/abs/2504.10906v1","updated":"2025-04-15T06:35:27Z","published":"2025-04-15T06:35:27Z","title":"Understanding LLMs' Cross-Lingual Context Retrieval: How Good It Is And\n  Where It Comes From","summary":"  The ability of cross-lingual context retrieval is a fundamental aspect of\ncross-lingual alignment of large language models (LLMs), where the model\nextracts context information in one language based on requests in another\nlanguage. Despite its importance in real-life applications, this ability has\nnot been adequately investigated for state-of-the-art models. In this paper, we\nevaluate the cross-lingual context retrieval ability of over 40 LLMs across 12\nlanguages to understand the source of this ability, using cross-lingual machine\nreading comprehension (xMRC) as a representative scenario. Our results show\nthat several small, post-trained open LLMs show strong cross-lingual context\nretrieval ability, comparable to closed-source LLMs such as GPT-4o, and their\nestimated oracle performances greatly improve after post-training. Our\ninterpretability analysis shows that the cross-lingual context retrieval\nprocess can be divided into two main phases: question encoding and answer\nretrieval, which are formed in pre-training and post-training, respectively.\nThe phasing stability correlates with xMRC performance, and the xMRC bottleneck\nlies at the last model layers in the second phase, where the effect of\npost-training can be evidently observed. Our results also indicate that\nlarger-scale pretraining cannot improve the xMRC performance. Instead, larger\nLLMs need further multilingual post-training to fully unlock their\ncross-lingual context retrieval potential. Our code and is available at\nhttps://github.com/NJUNLP/Cross-Lingual-Context-Retrieval\n","authors":["Changjiang Gao","Hankun Lin","Shujian Huang","Xin Huang","Xue Han","Junlan Feng","Chao Deng","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2504.10906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10903v1","updated":"2025-04-15T06:28:00Z","published":"2025-04-15T06:28:00Z","title":"Efficient Reasoning Models: A Survey","summary":"  Reasoning models have demonstrated remarkable progress in solving complex and\nlogic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to\narriving at a final answer. Yet, the emergence of this \"slow-thinking\"\nparadigm, with numerous tokens generated in sequence, inevitably introduces\nsubstantial computational overhead. To this end, it highlights an urgent need\nfor effective acceleration. This survey aims to provide a comprehensive\noverview of recent advances in efficient reasoning. It categorizes existing\nworks into three key directions: (1) shorter - compressing lengthy CoTs into\nconcise yet effective reasoning chains; (2) smaller - developing compact\nlanguage models with strong reasoning capabilities through techniques such as\nknowledge distillation, other model compression techniques, and reinforcement\nlearning; and (3) faster - designing efficient decoding strategies to\naccelerate inference. A curated collection of papers discussed in this survey\nis available in our GitHub repository.\n","authors":["Sicheng Feng","Gongfan Fang","Xinyin Ma","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2504.10903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20252v2","updated":"2025-04-15T06:19:06Z","published":"2024-05-30T17:05:45Z","title":"Towards Hierarchical Multi-Agent Workflows for Zero-Shot Prompt\n  Optimization","summary":"  Large language models (LLMs) have shown great progress in responding to user\nquestions, allowing for a multitude of diverse applications. Yet, the quality\nof LLM outputs heavily depends on the prompt design, where a good prompt might\nenable the LLM to answer a very challenging question correctly. Therefore,\nrecent works have developed many strategies for improving the prompt, including\nboth manual crafting and in-domain optimization. However, their efficacy in\nunrestricted scenarios remains questionable, as the former depends on human\ndesign for specific questions and the latter usually generalizes poorly to\nunseen scenarios. To address these problems, we give LLMs the freedom to design\nthe best prompts according to themselves. Specifically, we include a hierarchy\nof LLMs, first constructing a prompt with precise instructions and accurate\nwording in a hierarchical manner, and then using this prompt to generate the\nfinal answer to the user query. We term this pipeline Hierarchical Multi-Agent\nWorkflow, or HMAW. In contrast with prior works, HMAW imposes no human\nrestriction and requires no training, and is completely task-agnostic while\ncapable of adjusting to the nuances of the underlying task. Through both\nquantitative and qualitative experiments across multiple benchmarks, we verify\nthat despite its simplicity, the proposed approach can create detailed and\nsuitable prompts, further boosting the performance of current LLMs.\n","authors":["Yuchi Liu","Jaskirat Singh","Gaowen Liu","Ali Payani","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2405.20252v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10893v1","updated":"2025-04-15T06:06:50Z","published":"2025-04-15T06:06:50Z","title":"ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search","summary":"  Large language models (LLMs) have demonstrated impressive capabilities and\nare receiving increasing attention to enhance their reasoning through scaling\ntest--time compute. However, their application in open--ended,\nknowledge--intensive, complex reasoning scenarios is still limited.\nReasoning--oriented methods struggle to generalize to open--ended scenarios due\nto implicit assumptions of complete world knowledge. Meanwhile,\nknowledge--augmented reasoning (KAR) methods fail to address two core\nchallenges: 1) error propagation, where errors in early steps cascade through\nthe chain, and 2) verification bottleneck, where the explore--exploit tradeoff\narises in multi--branch decision processes. To overcome these limitations, we\nintroduce ARise, a novel framework that integrates risk assessment of\nintermediate reasoning states with dynamic retrieval--augmented generation\n(RAG) within a Monte Carlo tree search paradigm. This approach enables\neffective construction and optimization of reasoning plans across multiple\nmaintained hypothesis branches. Experimental results show that ARise\nsignificantly outperforms the state--of--the--art KAR methods by up to 23.10%,\nand the latest RAG-equipped large reasoning models by up to 25.37%.\n","authors":["Yize Zhang","Tianshu Wang","Sirui Chen","Kun Wang","Xingyu Zeng","Hongyu Lin","Xianpei Han","Le Sun","Chaochao Lu"],"pdf_url":"https://arxiv.org/pdf/2504.10893v1.pdf","comment":"Project homepage: https://opencausalab.github.io/ARise"},{"id":"http://arxiv.org/abs/2504.05154v2","updated":"2025-04-15T05:34:48Z","published":"2025-04-07T14:57:06Z","title":"CARE: Aligning Language Models for Regional Cultural Awareness","summary":"  Existing language models (LMs) often exhibit a Western-centric bias and\nstruggle to represent diverse cultural knowledge. Previous attempts to address\nthis rely on synthetic data and express cultural knowledge only in English. In\nthis work, we study whether a small amount of human-written, multilingual\ncultural preference data can improve LMs across various model families and\nsizes. We first introduce CARE, a multilingual resource of 24.1k responses with\nhuman preferences on 2,580 questions about Chinese and Arab cultures, all\ncarefully annotated by native speakers and offering more balanced coverage.\nUsing CARE, we demonstrate that cultural alignment improves existing LMs beyond\ngeneric resources without compromising general capabilities. Moreover, we\nevaluate the cultural awareness of LMs, native speakers, and retrieved web\ncontent when queried in different languages. Our experiment reveals regional\ndisparities among LMs, which may also be reflected in the documentation gap:\nnative speakers often take everyday cultural commonsense and social norms for\ngranted, while non-natives are more likely to actively seek out and document\nthem. CARE is publicly available at https://github.com/Guochry/CARE (we plan to\nadd Japanese data in the near future).\n","authors":["Geyang Guo","Tarek Naous","Hiromi Wakaki","Yukiko Nishimura","Yuki Mitsufuji","Alan Ritter","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2504.05154v2.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2504.10886v1","updated":"2025-04-15T05:29:51Z","published":"2025-04-15T05:29:51Z","title":"Exploring Persona-dependent LLM Alignment for the Moral Machine\n  Experiment","summary":"  Deploying large language models (LLMs) with agency in real-world applications\nraises critical questions about how these models will behave. In particular,\nhow will their decisions align with humans when faced with moral dilemmas? This\nstudy examines the alignment between LLM-driven decisions and human judgment in\nvarious contexts of the moral machine experiment, including personas reflecting\ndifferent sociodemographics. We find that the moral decisions of LLMs vary\nsubstantially by persona, showing greater shifts in moral decisions for\ncritical tasks than humans. Our data also indicate an interesting partisan\nsorting phenomenon, where political persona predominates the direction and\ndegree of LLM decisions. We discuss the ethical implications and risks\nassociated with deploying these models in applications that involve moral\ndecisions.\n","authors":["Jiseon Kim","Jea Kwon","Luiz Felipe Vecchietti","Alice Oh","Meeyoung Cha"],"pdf_url":"https://arxiv.org/pdf/2504.10886v1.pdf","comment":"Accepted to ICLR 2025 Workshop - BiAlign (Bidirectional Human-AI\n  Alignment)"},{"id":"http://arxiv.org/abs/2504.10861v1","updated":"2025-04-15T04:48:18Z","published":"2025-04-15T04:48:18Z","title":"Ai2 Scholar QA: Organized Literature Synthesis with Attribution","summary":"  Retrieval-augmented generation is increasingly effective in answering\nscientific questions from literature, but many state-of-the-art systems are\nexpensive and closed-source. We introduce Ai2 Scholar QA, a free online\nscientific question answering application. To facilitate research, we make our\nentire pipeline public: as a customizable open-source Python package and\ninteractive web app, along with paper indexes accessible through public APIs\nand downloadable datasets. We describe our system in detail and present\nexperiments analyzing its key design decisions. In an evaluation on a recent\nscientific QA benchmark, we find that Ai2 Scholar QA outperforms competing\nsystems.\n","authors":["Amanpreet Singh","Joseph Chee Chang","Chloe Anastasiades","Dany Haddad","Aakanksha Naik","Amber Tanaka","Angele Zamarron","Cecile Nguyen","Jena D. Hwang","Jason Dunkleberger","Matt Latzke","Smita Rao","Jaron Lochner","Rob Evans","Rodney Kinney","Daniel S. Weld","Doug Downey","Sergey Feldman"],"pdf_url":"https://arxiv.org/pdf/2504.10861v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2302.05508v2","updated":"2025-04-15T04:08:13Z","published":"2023-02-10T20:54:10Z","title":"FairPy: A Toolkit for Evaluation of Prediction Biases and their\n  Mitigation in Large Language Models","summary":"  Recent studies have demonstrated that large pretrained language models (LLMs)\nsuch as BERT and GPT-2 exhibit biases in token prediction, often inherited from\nthe data distributions present in their training corpora. In response, a number\nof mathematical frameworks have been proposed to quantify, identify, and\nmitigate these the likelihood of biased token predictions. In this paper, we\npresent a comprehensive survey of such techniques tailored towards widely used\nLLMs such as BERT, GPT-2, etc. We additionally introduce Fairpy, a modular and\nextensible toolkit that provides plug-and-play interfaces for integrating these\nmathematical tools, enabling users to evaluate both pretrained and custom\nlanguage models. Fairpy supports the implementation of existing debiasing\nalgorithms. The toolkit is open-source and publicly available at:\n\\href{https://github.com/HrishikeshVish/Fairpy}{https://github.com/HrishikeshVish/Fairpy}\n","authors":["Hrishikesh Viswanath","Tianyi Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.05508v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10845v1","updated":"2025-04-15T04:06:27Z","published":"2025-04-15T04:06:27Z","title":"Moving Beyond Next-Token Prediction: Transformers are Context-Sensitive\n  Language Generators","summary":"  Large Language Models (LLMs), powered by Transformers, have demonstrated\nhuman-like intelligence capabilities, yet their underlying mechanisms remain\npoorly understood. This paper presents a novel framework for interpreting LLMs\nas probabilistic left context-sensitive languages (CSLs) generators. We\nhypothesize that Transformers can be effectively decomposed into three\nfundamental components: context windows, attention mechanisms, and\nautoregressive generation frameworks. This decomposition allows for the\ndevelopment of more flexible and interpretable computational models, moving\nbeyond the traditional view of attention and autoregression as inseparable\nprocesses. We argue that next-token predictions can be understood as\nprobabilistic, dynamic approximations of left CSL production rules, providing\nan intuitive explanation for how simple token predictions can yield human-like\nintelligence outputs. Given that all CSLs are left context-sensitive\n(Penttonen, 1974), we conclude that Transformers stochastically approximate\nCSLs, which are widely recognized as models of human-like intelligence. This\ninterpretation bridges the gap between Formal Language Theory and the observed\ngenerative power of Transformers, laying a foundation for future advancements\nin generative AI theory and applications. Our novel perspective on Transformer\narchitectures will foster a deeper understanding of LLMs and their future\npotentials.\n","authors":["Phill Kyu Rhee"],"pdf_url":"https://arxiv.org/pdf/2504.10845v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.04350v3","updated":"2025-04-15T03:59:54Z","published":"2024-10-06T04:03:00Z","title":"TIS-DPO: Token-level Importance Sampling for Direct Preference\n  Optimization With Estimated Weights","summary":"  Direct Preference Optimization (DPO) has been widely adopted for preference\nalignment of Large Language Models (LLMs) due to its simplicity and\neffectiveness. However, DPO is derived as a bandit problem in which the whole\nresponse is treated as a single arm, ignoring the importance differences\nbetween tokens, which may affect optimization efficiency and make it difficult\nto achieve optimal results. In this work, we propose that the optimal data for\nDPO has equal expected rewards for each token in winning and losing responses,\nas there is no difference in token importance. However, since the optimal\ndataset is unavailable in practice, we propose using the original dataset for\nimportance sampling to achieve unbiased optimization. Accordingly, we propose a\ntoken-level importance sampling DPO objective named TIS-DPO that assigns\nimportance weights to each token based on its reward. Inspired by previous\nworks, we estimate the token importance weights using the difference in\nprediction probabilities from a pair of contrastive LLMs. We explore three\nmethods to construct these contrastive LLMs: (1) guiding the original LLM with\ncontrastive prompts, (2) training two separate LLMs using winning and losing\nresponses, and (3) performing forward and reverse DPO training with winning and\nlosing responses. Experiments show that TIS-DPO significantly outperforms\nvarious baseline methods on harmlessness and helpfulness alignment and\nsummarization tasks. We also visualize the estimated weights, demonstrating\ntheir ability to identify key token positions.\n","authors":["Aiwei Liu","Haoping Bai","Zhiyun Lu","Yanchao Sun","Xiang Kong","Simon Wang","Jiulong Shan","Albin Madappally Jose","Xiaojiang Liu","Lijie Wen","Philip S. Yu","Meng Cao"],"pdf_url":"https://arxiv.org/pdf/2410.04350v3.pdf","comment":"Published in ICLR 2025, code in https://github.com/exlaw/TIS-DPO"},{"id":"http://arxiv.org/abs/2310.03026v3","updated":"2025-04-15T03:45:30Z","published":"2023-10-04T17:59:49Z","title":"LanguageMPC: Large Language Models as Decision Makers for Autonomous\n  Driving","summary":"  Existing learning-based autonomous driving (AD) systems face challenges in\ncomprehending high-level information, generalizing to rare events, and\nproviding interpretability. To address these problems, this work employs Large\nLanguage Models (LLMs) as a decision-making component for complex AD scenarios\nthat require human commonsense understanding. We devise cognitive pathways to\nenable comprehensive reasoning with LLMs, and develop algorithms for\ntranslating LLM decisions into actionable driving commands. Through this\napproach, LLM decisions are seamlessly integrated with low-level controllers by\nguided parameter matrix adaptation. Extensive experiments demonstrate that our\nproposed method not only consistently surpasses baseline approaches in\nsingle-vehicle tasks, but also helps handle complex driving behaviors even\nmulti-vehicle coordination, thanks to the commonsense reasoning capabilities of\nLLMs. This paper presents an initial step toward leveraging LLMs as effective\ndecision-makers for intricate AD scenarios in terms of safety, efficiency,\ngeneralizability, and interoperability. We aspire for it to serve as\ninspiration for future research in this field. Project page:\nhttps://sites.google.com/view/llm-mpc\n","authors":["Hao Sha","Yao Mu","Yuxuan Jiang","Li Chen","Chenfeng Xu","Ping Luo","Shengbo Eben Li","Masayoshi Tomizuka","Wei Zhan","Mingyu Ding"],"pdf_url":"https://arxiv.org/pdf/2310.03026v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14414v2","updated":"2025-04-15T03:41:17Z","published":"2024-07-19T15:40:59Z","title":"System-1.x: Learning to Balance Fast and Slow Planning with Language\n  Models","summary":"  Language models can be used to solve long-horizon planning problems in two\ndistinct modes: a fast 'System-1' mode, directly generating plans without any\nexplicit search or backtracking, and a slow 'System-2' mode, planning\nstep-by-step by explicitly searching over possible actions. While System-2 is\ntypically more effective, it is also more computationally expensive, making it\ninfeasible for long plans or large action spaces. Moreover, isolated System-1\nor 2 ignores the user's end goals, failing to provide ways to control the\nmodel's behavior. To this end, we propose the System-1.x Planner, a\ncontrollable planning framework with LLMs that is capable of generating hybrid\nplans and balancing between the two planning modes based on the difficulty of\nthe problem at hand. System-1.x consists of (i) a controller, (ii) a System-1\nPlanner, and (iii) a System-2 Planner. Based on a user-specified hybridization\nfactor (x) governing the mixture between System-1 and 2, the controller\ndecomposes a problem into sub-goals, and classifies them as easy or hard to be\nsolved by either System-1 or 2, respectively. We fine-tune all three components\non top of a single base LLM, requiring only search traces as supervision.\nExperiments with two diverse planning tasks -- Maze Navigation and Blocksworld\n-- show that our System-1.x Planner outperforms a System-1 Planner, a System-2\nPlanner trained to approximate A* search, and also a symbolic planner (A*). We\ndemonstrate the following key properties of our planner: (1) controllability:\nincreasing the hybridization factor (e.g., System-1.75 vs 1.5) performs more\nsearch, improving performance, (2) flexibility: by building a neuro-symbolic\nvariant with a neural System-1 and a symbolic System-2, we can use existing\nsymbolic methods, and (3) generalizability: by being able to learn from\ndifferent search algorithms, our method is robust to the choice of search\nalgorithm.\n","authors":["Swarnadeep Saha","Archiki Prasad","Justin Chih-Yao Chen","Peter Hase","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2407.14414v2.pdf","comment":"ICLR 2025 (Camera-Ready)"},{"id":"http://arxiv.org/abs/2504.05732v2","updated":"2025-04-15T03:28:58Z","published":"2025-04-08T07:03:48Z","title":"LLM$\\times$MapReduce-V2: Entropy-Driven Convolutional Test-Time Scaling\n  for Generating Long-Form Articles from Extremely Long Resources","summary":"  Long-form generation is crucial for a wide range of practical applications,\ntypically categorized into short-to-long and long-to-long generation. While\nshort-to-long generations have received considerable attention, generating long\ntexts from extremely long resources remains relatively underexplored. The\nprimary challenge in long-to-long generation lies in effectively integrating\nand analyzing relevant information from extensive inputs, which remains\ndifficult for current large language models (LLMs). In this paper, we propose\nLLM$\\times$MapReduce-V2, a novel test-time scaling strategy designed to enhance\nthe ability of LLMs to process extremely long inputs. Drawing inspiration from\nconvolutional neural networks, which iteratively integrate local features into\nhigher-level global representations, LLM$\\times$MapReduce-V2 utilizes stacked\nconvolutional scaling layers to progressively expand the understanding of input\nmaterials. Both quantitative and qualitative experimental results demonstrate\nthat our approach substantially enhances the ability of LLMs to process long\ninputs and generate coherent, informative long-form articles, outperforming\nseveral representative baselines. Both LLM$\\times$MapReduce-V2 and SurveyEval\nare publicly available at https://github.com/thunlp/LLMxMapReduce .\n","authors":["Haoyu Wang","Yujia Fu","Zhu Zhang","Shuo Wang","Zirui Ren","Xiaorong Wang","Zhili Li","Chaoqun He","Bo An","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2504.05732v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12902v2","updated":"2025-04-15T03:28:22Z","published":"2024-08-23T08:10:13Z","title":"IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model\n  with Multimodal Capabilities","summary":"  In the field of multimodal large language models (MLLMs), common methods\ntypically involve unfreezing the language model during training to foster\nprofound visual understanding. However, the fine-tuning of such models with\nvision-language data often leads to a diminution of their natural language\nprocessing (NLP) capabilities. To avoid this performance degradation, a\nstraightforward solution is to freeze the language model while developing\nmultimodal competencies. Unfortunately, previous works have not attained\nsatisfactory outcomes. Building on the strategy of freezing the language model,\nwe conduct thorough structural exploration and introduce the Inner-Adaptor\nArchitecture (IAA). Specifically, the architecture incorporates multiple\nmultimodal adaptors at varying depths within the large language model to\nfacilitate direct interaction with the inherently text-oriented transformer\nlayers, thereby enabling the frozen language model to acquire multimodal\ncapabilities. Unlike previous approaches of freezing language models that\nrequire large-scale aligned data, our proposed architecture is able to achieve\nsuperior performance on small-scale datasets. We conduct extensive experiments\nto improve the general multimodal capabilities and visual grounding abilities\nof the MLLM. Our approach remarkably outperforms previous state-of-the-art\nmethods across various vision-language benchmarks without sacrificing\nperformance on NLP tasks. Code and models are available at\nhttps://github.com/360CVGroup/Inner-Adaptor-Architecture.\n","authors":["Bin Wang","Chunyu Xie","Dawei Leng","Yuhui Yin"],"pdf_url":"https://arxiv.org/pdf/2408.12902v2.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2402.05406v3","updated":"2025-04-15T03:24:22Z","published":"2024-02-08T04:48:26Z","title":"Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes","summary":"  Structured pruning is a promising approach to create smaller, faster LLMs.\nHowever, existing methods typically rely on backward passes, which can inflate\nmemory requirements and compute costs. In this work we introduce Bonsai, a\ngradient-free structured pruning method that eliminates the need for\nbackpropagation, significantly reducing memory requirements and compute costs\nwhile achieving state-of-the-art pruning performance. Bonsai uses\nforward-pass-only perturbative pruning to enable efficient compression of large\nmodels on a broader range of hardware configurations. Unlike existing\nstructured pruning approaches, Bonsai not only achieves better compression with\nfewer resources, but also produces models that are twice as fast as those\ngenerated by semi-structured pruning. As a concrete demonstration, we use\nBonsai to prune an 8B LLaMA-3 model to 50% sparsity on a single A6000 GPU -- a\ntask infeasible with backprop-based methods, which require 2-3x memory. Our\nresults show that removing backprop as a requirement not only enables pruning\nlarger models on constrained hardware but can also lead to state-of-the-art\nefficiency and performance.\n","authors":["Lucio Dery","Steven Kolawole","Jean-François Kagy","Virginia Smith","Graham Neubig","Ameet Talwalkar"],"pdf_url":"https://arxiv.org/pdf/2402.05406v3.pdf","comment":"19 pages, 6 fiigures, 16 tables"},{"id":"http://arxiv.org/abs/2504.08300v3","updated":"2025-04-15T03:02:35Z","published":"2025-04-11T07:04:44Z","title":"Large language models could be rote learners","summary":"  Multiple-choice question (MCQ) benchmarks are widely used for evaluating\nLarge Language Models (LLMs), yet their reliability is undermined by benchmark\ncontamination. In this study, we reframe contamination as an inherent aspect of\nlearning and seek to disentangle genuine capability acquisition from\nsuperficial memorization in LLM evaluation. First, by analyzing model\nperformance under different memorization conditions, we uncover a\ncounterintuitive trend: LLMs perform worse on memorized MCQs than on\nnon-memorized ones, indicating the coexistence of two distinct learning\nphenomena, i.e., rote memorization and genuine capability learning. To\ndisentangle them, we propose TrinEval, a novel evaluation framework that\nreformulates MCQs into an alternative trinity format, reducing memorization\nwhile preserving knowledge assessment. Experiments validate TrinEval's\neffectiveness in reformulation, and its evaluation reveals that common LLMs may\nmemorize by rote 20.5% of knowledge points (in MMLU on average).\n","authors":["Yuyang Xu","Renjun Hu","Haochao Ying","Jian Wu","Xing Shi","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2504.08300v3.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2504.03160v3","updated":"2025-04-15T02:57:20Z","published":"2025-04-04T04:41:28Z","title":"DeepResearcher: Scaling Deep Research via Reinforcement Learning in\n  Real-world Environments","summary":"  Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.\n","authors":["Yuxiang Zheng","Dayuan Fu","Xiangkun Hu","Xiaojie Cai","Lyumanshan Ye","Pengrui Lu","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2504.03160v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10823v1","updated":"2025-04-15T02:54:16Z","published":"2025-04-15T02:54:16Z","title":"CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from\n  Multiple Perspectives","summary":"  Navigating high-stakes dilemmas involving conflicting values is challenging\neven for humans, let alone for AI. Yet prior work in evaluating the reasoning\ncapabilities of large language models (LLMs) in such situations has been\nlimited to everyday scenarios. To close this gap, this work first introduces\nCLASH (Character perspective-based LLM Assessments in Situations with\nHigh-stakes), a meticulously curated dataset consisting of 345 high-impact\ndilemmas along with 3,795 individual perspectives of diverse values. In\nparticular, we design CLASH in a way to support the study of critical aspects\nof value-based decision-making processes which are missing from prior work,\nincluding understanding decision ambivalence and psychological discomfort as\nwell as capturing the temporal shifts of values in characters' perspectives. By\nbenchmarking 10 open and closed frontier models, we uncover several key\nfindings. (1) Even the strongest models, such as GPT-4o and Claude-Sonnet,\nachieve less than 50% accuracy in identifying situations where the decision\nshould be ambivalent, while they perform significantly better in clear-cut\nscenarios. (2) While LLMs reasonably predict psychological discomfort as marked\nby human, they inadequately comprehend perspectives involving value shifts,\nindicating a need for LLMs to reason over complex values. (3) Our experiments\nalso reveal a significant correlation between LLMs' value preferences and their\nsteerability towards a given value. (4) Finally, LLMs exhibit greater\nsteerability when engaged in value reasoning from a third-party perspective,\ncompared to a first-person setup, though certain value pairs benefit uniquely\nfrom the first-person framing.\n","authors":["Ayoung Lee","Ryan Sungmo Kwon","Peter Railton","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2504.10823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10762v4","updated":"2025-04-15T02:44:55Z","published":"2024-10-14T17:40:40Z","title":"AFlow: Automating Agentic Workflow Generation","summary":"  Large language models (LLMs) have demonstrated remarkable potential in\nsolving complex tasks across diverse domains, typically by employing agentic\nworkflows that follow detailed instructions and operational sequences. However,\nconstructing these workflows requires significant human effort, limiting\nscalability and generalizability. Recent research has sought to automate the\ngeneration and optimization of these workflows, but existing methods still rely\non initial manual setup and fall short of achieving fully automated and\neffective workflow generation. To address this challenge, we reformulate\nworkflow optimization as a search problem over code-represented workflows,\nwhere LLM-invoking nodes are connected by edges. We introduce AFlow, an\nautomated framework that efficiently explores this space using Monte Carlo Tree\nSearch, iteratively refining workflows through code modification,\ntree-structured experience, and execution feedback. Empirical evaluations\nacross six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7%\naverage improvement over state-of-the-art baselines. Furthermore, AFlow enables\nsmaller models to outperform GPT-4o on specific tasks at 4.55% of its inference\ncost in dollars. The code is available at\nhttps://github.com/FoundationAgents/AFlow.\n","authors":["Jiayi Zhang","Jinyu Xiang","Zhaoyang Yu","Fengwei Teng","Xionghui Chen","Jiaqi Chen","Mingchen Zhuge","Xin Cheng","Sirui Hong","Jinlin Wang","Bingnan Zheng","Bang Liu","Yuyu Luo","Chenglin Wu"],"pdf_url":"https://arxiv.org/pdf/2410.10762v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10816v1","updated":"2025-04-15T02:31:34Z","published":"2025-04-15T02:31:34Z","title":"CSPLADE: Learned Sparse Retrieval with Causal Language Models","summary":"  In recent years, dense retrieval has been the focus of information retrieval\n(IR) research. While effective, dense retrieval produces uninterpretable dense\nvectors, and suffers from the drawback of large index size. Learned sparse\nretrieval (LSR) has emerged as promising alternative, achieving competitive\nretrieval performance while also being able to leverage the classical inverted\nindex data structure for efficient retrieval. However, limited works have\nexplored scaling LSR beyond BERT scale. In this work, we identify two\nchallenges in training large language models (LLM) for LSR: (1) training\ninstability during the early stage of contrastive training; (2) suboptimal\nperformance due to pre-trained LLM's unidirectional attention. To address these\nchallenges, we propose two corresponding techniques: (1) a lightweight\nadaptation training phase to eliminate training instability; (2) two model\nvariants to enable bidirectional information. With these techniques, we are\nable to train LSR models with 8B scale LLM, and achieve competitive retrieval\nperformance with reduced index size. Furthermore, we are among the first to\nanalyze the performance-efficiency tradeoff of LLM-based LSR model through the\nlens of model quantization. Our findings provide insights into adapting LLMs\nfor efficient retrieval modeling.\n","authors":["Zhichao Xu","Aosong Feng","Yijun Tian","Haibo Ding","Lin Leee Cheong"],"pdf_url":"https://arxiv.org/pdf/2504.10816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10797v1","updated":"2025-04-15T01:47:39Z","published":"2025-04-15T01:47:39Z","title":"Name of Thrones: Evaluating How LLMs Rank Student Names, Race, and\n  Gender in Status Hierarchies","summary":"  Across cultures, names tell a lot about their bearers as they carry deep\npersonal and cultural significance. Names also serve as powerful signals of\ngender, race, and status in the social hierarchy - a pecking order in which\nindividual positions shape others' expectations on their perceived competence\nand worth. With the widespread adoption of LLMs and as names are often an input\nfor LLMs, it is crucial to evaluate whether LLMs may sort people into status\npositions based on first and last names and, if so, whether it is in an unfair,\nbiased fashion. While prior work has primarily investigated biases in first\nnames, little attention has been paid to last names and even less to the\ncombined effects of first and last names. In this study, we conduct a\nlarge-scale analysis of name variations across 5 ethnicities to examine how AI\nexhibits name biases. Our study investigates three key characteristics of\ninequality and finds that LLMs reflect and reinforce status hierarchies based\non names that signal gender and ethnicity as they encode differential\nexpectations of competence, leadership, and economic potential. Contrary to the\ncommon assumption that AI tends to favor Whites, we show that East and, in some\ncontexts, South Asian names receive higher rankings. We also disaggregate\nAsians, a population projected to be the largest immigrant group in the U.S. by\n2055. Our results challenge the monolithic Asian model minority assumption,\nillustrating a more complex and stratified model of bias. Gender moderates\nbiases, with girls facing unfair disadvantages in certain racial groups.\nAdditionally, spanning cultural categories by adopting Western first names\nimproves AI-perceived status for East and Southeast Asian students,\nparticularly for girls. Our findings underscore the importance of\nintersectional and more nuanced understandings of race, gender, and mixed\nidentities in the evaluation of LLMs.\n","authors":["Annabella Sakunkoo","Jonathan Sakunkoo"],"pdf_url":"https://arxiv.org/pdf/2504.10797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10792v1","updated":"2025-04-15T01:26:14Z","published":"2025-04-15T01:26:14Z","title":"GUM-SAGE: A Novel Dataset and Approach for Graded Entity Salience\n  Prediction","summary":"  Determining and ranking the most salient entities in a text is critical for\nuser-facing systems, especially as users increasingly rely on models to\ninterpret long documents they only partially read. Graded entity salience\naddresses this need by assigning entities scores that reflect their relative\nimportance in a text. Existing approaches fall into two main categories:\nsubjective judgments of salience, which allow for gradient scoring but lack\nconsistency, and summarization-based methods, which define salience as\nmention-worthiness in a summary, promoting explainability but limiting outputs\nto binary labels (entities are either summary-worthy or not). In this paper, we\nintroduce a novel approach for graded entity salience that combines the\nstrengths of both approaches. Using an English dataset spanning 12 spoken and\nwritten genres, we collect 5 summaries per document and calculate each entity's\nsalience score based on its presence across these summaries. Our approach shows\nstronger correlation with scores based on human summaries and alignments, and\noutperforms existing techniques, including LLMs. We release our data and code\nat https://github.com/jl908069/gum_sum_salience to support further research on\ngraded salient entity extraction.\n","authors":["Jessica Lin","Amir Zeldes"],"pdf_url":"https://arxiv.org/pdf/2504.10792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05862v2","updated":"2025-04-15T01:14:43Z","published":"2025-04-08T09:41:03Z","title":"Are Generative AI Agents Effective Personalized Financial Advisors?","summary":"  Large language model-based agents are becoming increasingly popular as a\nlow-cost mechanism to provide personalized, conversational advice, and have\ndemonstrated impressive capabilities in relatively simple scenarios, such as\nmovie recommendations. But how do these agents perform in complex high-stakes\ndomains, where domain expertise is essential and mistakes carry substantial\nrisk? This paper investigates the effectiveness of LLM-advisors in the finance\ndomain, focusing on three distinct challenges: (1) eliciting user preferences\nwhen users themselves may be unsure of their needs, (2) providing personalized\nguidance for diverse investment preferences, and (3) leveraging advisor\npersonality to build relationships and foster trust. Via a lab-based user study\nwith 64 participants, we show that LLM-advisors often match human advisor\nperformance when eliciting preferences, although they can struggle to resolve\nconflicting user needs. When providing personalized advice, the LLM was able to\npositively influence user behavior, but demonstrated clear failure modes. Our\nresults show that accurate preference elicitation is key, otherwise, the\nLLM-advisor has little impact, or can even direct the investor toward\nunsuitable assets. More worryingly, users appear insensitive to the quality of\nadvice being given, or worse these can have an inverse relationship. Indeed,\nusers reported a preference for and increased satisfaction as well as emotional\ntrust with LLMs adopting an extroverted persona, even though those agents\nprovided worse advice.\n","authors":["Takehiro Takayanagi","Kiyoshi Izumi","Javier Sanz-Cruzado","Richard McCreadie","Iadh Ounis"],"pdf_url":"https://arxiv.org/pdf/2504.05862v2.pdf","comment":"Accepted for presentation at SIGIR 2025"},{"id":"http://arxiv.org/abs/2504.10768v1","updated":"2025-04-15T00:08:13Z","published":"2025-04-15T00:08:13Z","title":"The Art of Audience Engagement: LLM-Based Thin-Slicing of Scientific\n  Talks","summary":"  This paper examines the thin-slicing approach - the ability to make accurate\njudgments based on minimal information - in the context of scientific\npresentations. Drawing on research from nonverbal communication and personality\npsychology, we show that brief excerpts (thin slices) reliably predict overall\npresentation quality. Using a novel corpus of over one hundred real-life\nscience talks, we employ Large Language Models (LLMs) to evaluate transcripts\nof full presentations and their thin slices. By correlating LLM-based\nevaluations of short excerpts with full-talk assessments, we determine how much\ninformation is needed for accurate predictions. Our results demonstrate that\nLLM-based evaluations align closely with human ratings, proving their validity,\nreliability, and efficiency. Critically, even very short excerpts (less than 10\npercent of a talk) strongly predict overall evaluations. This suggests that the\nfirst moments of a presentation convey relevant information that is used in\nquality evaluations and can shape lasting impressions. The findings are robust\nacross different LLMs and prompting strategies. This work extends thin-slicing\nresearch to public speaking and connects theories of impression formation to\nLLMs and current research on AI communication. We discuss implications for\ncommunication and social cognition research on message reception. Lastly, we\nsuggest an LLM-based thin-slicing framework as a scalable feedback tool to\nenhance human communication.\n","authors":["Ralf Schmälzle","Sue Lim","Yuetong Du","Gary Bente"],"pdf_url":"https://arxiv.org/pdf/2504.10768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09394v2","updated":"2025-04-15T23:18:26Z","published":"2025-04-13T01:36:47Z","title":"Evaluation Under Imperfect Benchmarks and Ratings: A Case Study in Text\n  Simplification","summary":"  Despite the successes of language models, their evaluation remains a daunting\nchallenge for new and existing tasks. We consider the task of text\nsimplification, commonly used to improve information accessibility, where\nevaluation faces two major challenges. First, the data in existing benchmarks\nmight not reflect the capabilities of current language models on the task,\noften containing disfluent, incoherent, or simplistic examples. Second,\nexisting human ratings associated with the benchmarks often contain a high\ndegree of disagreement, resulting in inconsistent ratings; nevertheless,\nexisting metrics still have to show higher correlations with these imperfect\nratings. As a result, evaluation for the task is not reliable and does not\nreflect expected trends (e.g., more powerful models being assigned higher\nscores). We address these challenges for the task of text simplification\nthrough three contributions. First, we introduce SynthSimpliEval, a synthetic\nbenchmark for text simplification featuring simplified sentences generated by\nmodels of varying sizes. Through a pilot study, we show that human ratings on\nour benchmark exhibit high inter-annotator agreement and reflect the expected\ntrend: larger models produce higher-quality simplifications. Second, we show\nthat auto-evaluation with a panel of LLM judges (LLMs-as-a-jury) often suffices\nto obtain consistent ratings for the evaluation of text simplification. Third,\nwe demonstrate that existing learnable metrics for text simplification benefit\nfrom training on our LLMs-as-a-jury-rated synthetic data, closing the gap with\npure LLMs-as-a-jury for evaluation. Overall, through our case study on text\nsimplification, we show that a reliable evaluation requires higher quality test\ndata, which could be obtained through synthetic data and LLMs-as-a-jury\nratings.\n","authors":["Joseph Liu","Yoonsoo Nam","Xinyue Cui","Swabha Swayamdipta"],"pdf_url":"https://arxiv.org/pdf/2504.09394v2.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2504.06166v2","updated":"2025-04-15T23:10:49Z","published":"2025-04-08T16:05:25Z","title":"Assessing how hyperparameters impact Large Language Models' sarcasm\n  detection performance","summary":"  Sarcasm detection is challenging for both humans and machines. This work\nexplores how model characteristics impact sarcasm detection in OpenAI's GPT,\nand Meta's Llama-2 models, given their strong natural language understanding,\nand popularity. We evaluate fine-tuned and zero-shot models across various\nsizes, releases, and hyperparameters. Experiments were conducted on the\npolitical and balanced (pol-bal) portion of the popular Self-Annotated Reddit\nCorpus (SARC2.0) sarcasm dataset. Fine-tuned performance improves monotonically\nwith model size within a model family, while hyperparameter tuning also impacts\nperformance. In the fine-tuning scenario, full precision Llama-2-13b achieves\nstate-of-the-art accuracy and $F_1$-score, both measured at 0.83, comparable to\naverage human performance. In the zero-shot setting, one GPT-4 model achieves\ncompetitive performance to prior attempts, yielding an accuracy of 0.70 and an\n$F_1$-score of 0.75. Furthermore, a model's performance may increase or decline\nwith each release, highlighting the need to reassess performance after each\nrelease.\n","authors":["Montgomery Gole","Andriy Miranskyy"],"pdf_url":"https://arxiv.org/pdf/2504.06166v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2312.04642"},{"id":"http://arxiv.org/abs/2410.03529v2","updated":"2025-04-15T22:26:58Z","published":"2024-10-04T15:50:10Z","title":"No Need to Talk: Asynchronous Mixture of Language Models","summary":"  We introduce SMALLTALK LM, an innovative method for training a mixture of\nlanguage models in an almost asynchronous manner. Each model of the mixture\nspecializes in distinct parts of the data distribution, without the need for\nhigh-bandwidth communication between the nodes training each model. At\ninference, a lightweight router directs a given sequence to a single expert,\naccording to a short prefix. This inference scheme naturally uses a fraction of\nthe parameters from the overall mixture model. Unlike prior works on\nasynchronous LLM training, our routing method does not rely on full corpus\nclustering or access to metadata, making it more suitable for real-world\napplications. Our experiments on language modeling demonstrate that SMALLTALK\nLM achieves significantly lower perplexity than dense model baselines for the\nsame total training FLOPs and an almost identical inference cost. Finally, in\nour downstream evaluations we outperform the dense baseline on 75% of the\ntasks.\n","authors":["Anastasiia Filippova","Angelos Katharopoulos","David Grangier","Ronan Collobert"],"pdf_url":"https://arxiv.org/pdf/2410.03529v2.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2405.13640v2","updated":"2025-04-15T21:48:58Z","published":"2024-05-22T13:39:33Z","title":"Knowledge Graph Reasoning with Self-supervised Reinforcement Learning","summary":"  Reinforcement learning (RL) is an effective method of finding reasoning\npathways in incomplete knowledge graphs (KGs). To overcome the challenges of a\nlarge action space, a self-supervised pre-training method is proposed to warm\nup the policy network before the RL training stage. To alleviate the\ndistributional mismatch issue in general self-supervised RL (SSRL), in our\nsupervised learning (SL) stage, the agent selects actions based on the policy\nnetwork and learns from generated labels; this self-generation of labels is the\nintuition behind the name self-supervised. With this training framework, the\ninformation density of our SL objective is increased and the agent is prevented\nfrom getting stuck with the early rewarded paths. Our self-supervised RL (SSRL)\nmethod improves the performance of RL by pairing it with the wide coverage\nachieved by SL during pretraining, since the breadth of the SL objective makes\nit infeasible to train an agent with that alone. We show that our SSRL model\nmeets or exceeds current state-of-the-art results on all Hits@k and mean\nreciprocal rank (MRR) metrics on four large benchmark KG datasets. This SSRL\nmethod can be used as a plug-in for any RL architecture for a KGR task. We\nadopt two RL architectures, i.e., MINERVA and MultiHopKG as our baseline RL\nmodels and experimentally show that our SSRL model consistently outperforms\nboth baselines on all of these four KG reasoning tasks. Full code for the paper\navailable at\nhttps://github.com/owenonline/Knowledge-Graph-Reasoning-with-Self-supervised-Reinforcement-Learning.\n","authors":["Ying Ma","Owen Burns","Mingqiu Wang","Gang Li","Nan Du","Laurent El Shafey","Liqiang Wang","Izhak Shafran","Hagen Soltau"],"pdf_url":"https://arxiv.org/pdf/2405.13640v2.pdf","comment":"17 pages, 11 figures"},{"id":"http://arxiv.org/abs/2501.12537v2","updated":"2025-04-15T21:43:35Z","published":"2025-01-21T23:01:21Z","title":"Enhancing Privacy in the Early Detection of Sexual Predators Through\n  Federated Learning and Differential Privacy","summary":"  The increased screen time and isolation caused by the COVID-19 pandemic have\nled to a significant surge in cases of online grooming, which is the use of\nstrategies by predators to lure children into sexual exploitation. Previous\nefforts to detect grooming in industry and academia have involved accessing and\nmonitoring private conversations through centrally-trained models or sending\nprivate conversations to a global server. In this work, we implement a\nprivacy-preserving pipeline for the early detection of sexual predators. We\nleverage federated learning and differential privacy in order to create safer\nonline spaces for children while respecting their privacy. We investigate\nvarious privacy-preserving implementations and discuss their benefits and\nshortcomings. Our extensive evaluation using real-world data proves that\nprivacy and utility can coexist with only a slight reduction in utility.\n","authors":["Khaoula Chehbouni","Martine De Cock","Gilles Caporossi","Afaf Taik","Reihaneh Rabbany","Golnoosh Farnadi"],"pdf_url":"https://arxiv.org/pdf/2501.12537v2.pdf","comment":"Accepted to AAAI-Social Impact Track - Oral"},{"id":"http://arxiv.org/abs/2504.11626v1","updated":"2025-04-15T21:35:09Z","published":"2025-04-15T21:35:09Z","title":"Improving Instruct Models for Free: A Study on Partial Adaptation","summary":"  Instruct models, obtained from various instruction tuning or post-training\nsteps, are commonly deemed superior and more usable than their base\ncounterpart. While the model gains instruction following ability, instruction\ntuning may lead to forgetting the knowledge from pre-training or it may\nencourage the model being overly conversational or verbose. This, in turn, can\nlead to degradation of in-context few-shot learning performance. In this work,\nwe study the performance trajectory between base and instruct models by scaling\ndown the strength of instruction-tuning via the partial adaption method. We\nshow that, across several model families and model sizes, reducing the strength\nof instruction-tuning results in material improvement on a few-shot in-context\nlearning benchmark covering a variety of classic natural language tasks. This\ncomes at the cost of losing some degree of instruction following ability as\nmeasured by AlpacaEval. Our study shines light on the potential trade-off\nbetween in-context learning and instruction following abilities that is worth\nconsidering in practice.\n","authors":["Ozan İrsoy","Pengxiang Cheng","Jennifer L. Chen","Daniel Preoţiuc-Pietro","Shiyue Zhang","Duccio Pappadopulo"],"pdf_url":"https://arxiv.org/pdf/2504.11626v1.pdf","comment":"Author ordering chosen at random"},{"id":"http://arxiv.org/abs/2503.07630v2","updated":"2025-04-15T21:21:55Z","published":"2025-03-04T19:08:39Z","title":"FourierNAT: A Fourier-Mixing-Based Non-Autoregressive Transformer for\n  Parallel Sequence Generation","summary":"  We present FourierNAT, a novel non-autoregressive Transformer (NAT)\narchitecture that employs Fourier-based mixing in the decoder to generate\noutput sequences in parallel. While traditional NAT approaches often face\nchallenges with capturing global dependencies, our method leverages a discrete\nFourier transform to mix token embeddings across the entire sequence dimension,\ncoupled with learned frequency-domain gating. This allows the model to\nefficiently propagate context without explicit autoregressive steps.\nEmpirically, FourierNAT achieves competitive results against leading NAT\nbaselines on standard benchmarks like WMT machine translation and CNN/DailyMail\nsummarization, providing significant speed advantages over autoregressive\nTransformers. We further demonstrate that learned frequency-domain parameters\nallow the model to adaptively focus on long-range or short-range dependencies,\npartially mitigating the well-known coherence gaps in one-pass NAT generation.\nOverall, FourierNAT highlights the potential of integrating spectral-domain\noperations to accelerate and improve parallel text generation. This approach\ncan potentially provide great computational and time savings in inference tasks\nLLMs.\n","authors":["Andrew Kiruluta","Eric Lundy","Andreas Lemos"],"pdf_url":"https://arxiv.org/pdf/2503.07630v2.pdf","comment":"11 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.16682v2","updated":"2025-04-15T21:11:11Z","published":"2025-02-23T18:56:56Z","title":"Automatic Input Rewriting Improves Translation with Large Language\n  Models","summary":"  Can we improve machine translation (MT) with LLMs by rewriting their inputs\nautomatically? Users commonly rely on the intuition that well-written text is\neasier to translate when using off-the-shelf MT systems. LLMs can rewrite text\nin many ways but in the context of MT, these capabilities have been primarily\nexploited to rewrite outputs via post-editing. We present an empirical study of\n21 input rewriting methods with 3 open-weight LLMs for translating from English\ninto 6 target languages. We show that text simplification is the most effective\nMT-agnostic rewrite strategy and that it can be improved further when using\nquality estimation to assess translatability. Human evaluation further confirms\nthat simplified rewrites and their MT outputs both largely preserve the\noriginal meaning of the source and MT. These results suggest LLM-assisted input\nrewriting as a promising direction for improving translations.\n","authors":["Dayeon Ki","Marine Carpuat"],"pdf_url":"https://arxiv.org/pdf/2502.16682v2.pdf","comment":"27 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.01568v3","updated":"2025-04-15T20:46:47Z","published":"2025-02-03T17:50:37Z","title":"Visual Theory of Mind Enables the Invention of Proto-Writing","summary":"  Symbolic writing systems are graphical semiotic codes that are ubiquitous in\nmodern society but are otherwise absent in the animal kingdom. Anthropological\nevidence suggests that the earliest forms of some writing systems originally\nconsisted of iconic pictographs, which signify their referent via visual\nresemblance. While previous studies have examined the emergence and,\nseparately, the evolution of pictographic systems through a computational lens,\nmost employ non-naturalistic methodologies that make it difficult to draw clear\nanalogies to human and animal cognition. We develop a multi-agent reinforcement\nlearning testbed for emergent communication called a Signification Game, and\nformulate a model of inferential communication that enables agents to leverage\nvisual theory of mind to communicate actions using pictographs. Our model,\nwhich is situated within a broader formalism for animal communication, sheds\nlight on the cognitive and cultural processes underlying the emergence of\nproto-writing.\n","authors":["Benjamin A. Spiegel","Lucas Gelfond","George Konidaris"],"pdf_url":"https://arxiv.org/pdf/2502.01568v3.pdf","comment":"Accepted to CogSci 2025, published here with permission from\n  organizers"},{"id":"http://arxiv.org/abs/2503.00444v2","updated":"2025-04-15T20:10:50Z","published":"2025-03-01T10:47:45Z","title":"Figurative Archive: an open dataset and web-based application for the\n  study of metaphor","summary":"  Research on metaphor has steadily increased over the last decades, as this\nphenomenon opens a window into a range of linguistic and cognitive processes.\nAt the same time, the demand for rigorously constructed and extensively normed\nexperimental materials increased as well. Here, we present the Figurative\nArchive, an open database of 997 metaphors in Italian enriched with rating and\ncorpus-based measures (from familiarity to concreteness), derived by collecting\nstimuli used across 11 studies. It includes both everyday and literary\nmetaphors, varying in structure and semantic domains, and is validated based on\ncorrelations between familiarity and other measures. The archive has several\naspects of novelty: it is increased in size compared to previous resources; it\nincludes a measure of inclusiveness, to comply with recommendations for\nnon-discriminatory language use; it is displayed in a web-based interface, with\nfeatures for a customized consultation. We provide guidelines for using the\narchive as a source of material for studies investigating metaphor processing\nand the relationships between metaphor features in humans and computational\nmodels.\n","authors":["Maddalena Bressler","Veronica Mangiaterra","Paolo Canal","Federico Frau","Fabrizio Luciani","Biagio Scalingi","Chiara Barattieri di San Pietro","Chiara Battaglini","Chiara Pompei","Fortunata Romeo","Luca Bischetti","Valentina Bambini"],"pdf_url":"https://arxiv.org/pdf/2503.00444v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11582v1","updated":"2025-04-15T19:57:42Z","published":"2025-04-15T19:57:42Z","title":"AskQE: Question Answering as Automatic Evaluation for Machine\n  Translation","summary":"  How can a monolingual English speaker determine whether an automatic\ntranslation in French is good enough to be shared? Existing MT error detection\nand quality estimation (QE) techniques do not address this practical scenario.\nWe introduce AskQE, a question generation and answering framework designed to\ndetect critical MT errors and provide actionable feedback, helping users decide\nwhether to accept or reject MT outputs even without the knowledge of the target\nlanguage. Using ContraTICO, a dataset of contrastive synthetic MT errors in the\nCOVID-19 domain, we explore design choices for AskQE and develop an optimized\nversion relying on LLaMA-3 70B and entailed facts to guide question generation.\nWe evaluate the resulting system on the BioMQM dataset of naturally occurring\nMT errors, where AskQE has higher Kendall's Tau correlation and decision\naccuracy with human ratings compared to other QE metrics.\n","authors":["Dayeon Ki","Kevin Duh","Marine Carpuat"],"pdf_url":"https://arxiv.org/pdf/2504.11582v1.pdf","comment":"38 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.16707v2","updated":"2025-04-15T19:33:50Z","published":"2024-11-21T19:01:07Z","title":"Enhancing LLMs for Power System Simulations: A Feedback-driven\n  Multi-agent Framework","summary":"  The integration of experimental technologies with large language models\n(LLMs) is transforming scientific research. It positions AI as a versatile\nresearch assistant rather than a mere problem-solving tool. In the field of\npower systems, however, managing simulations -- one of the essential\nexperimental technologies -- remains a challenge for LLMs due to their limited\ndomain-specific knowledge, restricted reasoning capabilities, and imprecise\nhandling of simulation parameters. To address these limitations, this paper\nproposes a feedback-driven, multi-agent framework. It incorporates three\nproposed modules: an enhanced retrieval-augmented generation (RAG) module, an\nimproved reasoning module, and a dynamic environmental acting module with an\nerror-feedback mechanism. Validated on 69 diverse tasks from Daline and\nMATPOWER, this framework achieves success rates of 93.13% and 96.85%,\nrespectively. It significantly outperforms ChatGPT 4o, o1-preview, and the\nfine-tuned GPT-4o, which all achieved a success rate lower than 30% on complex\ntasks. Additionally, the proposed framework also supports rapid, cost-effective\ntask execution, completing each simulation in approximately 30 seconds at an\naverage cost of 0.014 USD for tokens. Overall, this adaptable framework lays a\nfoundation for developing intelligent LLM-based assistants for human\nresearchers, facilitating power system research and beyond.\n","authors":["Mengshuo Jia","Zeyu Cui","Gabriela Hug"],"pdf_url":"https://arxiv.org/pdf/2411.16707v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2504.11571v1","updated":"2025-04-15T19:26:59Z","published":"2025-04-15T19:26:59Z","title":"GraphicBench: A Planning Benchmark for Graphic Design with Language\n  Agents","summary":"  Large Language Model (LLM)-powered agents have unlocked new possibilities for\nautomating human tasks. While prior work has focused on well-defined tasks with\nspecified goals, the capabilities of agents in creative design tasks with\nopen-ended goals remain underexplored. We introduce GraphicBench, a new\nplanning benchmark for graphic design that covers 1,079 user queries and input\nimages across four design types. We further present GraphicTown, an LLM agent\nframework with three design experts and 46 actions (tools) to choose from for\nexecuting each step of the planned workflows in web environments. Experiments\nwith six LLMs demonstrate their ability to generate workflows that integrate\nboth explicit design constraints from user queries and implicit commonsense\nconstraints. However, these workflows often do not lead to successful execution\noutcomes, primarily due to challenges in: (1) reasoning about spatial\nrelationships, (2) coordinating global dependencies across experts, and (3)\nretrieving the most appropriate action per step. We envision GraphicBench as a\nchallenging yet valuable testbed for advancing LLM-agent planning and execution\nin creative design tasks.\n","authors":["Dayeon Ki","Tianyi Zhou","Marine Carpuat","Gang Wu","Puneet Mathur","Viswanathan Swaminathan"],"pdf_url":"https://arxiv.org/pdf/2504.11571v1.pdf","comment":"41 pages, 11 figures"},{"id":"http://arxiv.org/abs/2409.19022v2","updated":"2025-04-15T19:05:24Z","published":"2024-09-25T14:47:03Z","title":"Application of AI-based Models for Online Fraud Detection and Analysis","summary":"  Fraud is a prevalent offence that extends beyond financial loss, causing\npsychological and physical harm to victims. The advancements in online\ncommunication technologies alowed for online fraud to thrive in this vast\nnetwork, with fraudsters increasingly using these channels for deception. With\nthe progression of technologies like AI, there is a growing concern that fraud\nwill scale up, using sophisticated methods, like deep-fakes in phishing\ncampaigns, all generated by language generation models like ChatGPT. However,\nthe application of AI in detecting and analyzing online fraud remains\nunderstudied. We conduct a Systematic Literature Review on AI and NLP\ntechniques for online fraud detection. The review adhered the PRISMA-ScR\nprotocol, with eligibility criteria including relevance to online fraud, use of\ntext data, and AI methodologies. We screened 2,457 academic records, 350 met\nour eligibility criteria, and included 223. We report the state-of-the-art NLP\ntechniques for analysing various online fraud categories; the training data\nsources; the NLP algorithms and models built; and the performance metrics\nemployed for model evaluation. We find that current research on online fraud is\ndivided into various scam activitiesand identify 16 different frauds that\nresearchers focus on. This SLR enhances the academic understanding of AI-based\ndetection methods for online fraud and offers insights for policymakers, law\nenforcement, and businesses on safeguarding against such activities. We\nconclude that focusing on specific scams lacks generalization, as multiple\nmodels are required for different fraud types. The evolving nature of scams\nlimits the effectiveness of models trained on outdated data. We also identify\nissues in data limitations, training bias reporting, and selective presentation\nof metrics in model performance reporting, which can lead to potential biases\nin model evaluation.\n","authors":["Antonis Papasavva","Shane Johnson","Ed Lowther","Samantha Lundrigan","Enrico Mariconti","Anna Markovska","Nilufer Tuptuk"],"pdf_url":"https://arxiv.org/pdf/2409.19022v2.pdf","comment":"Manuscript accepted in Crime Science Journal. Please cite accordingly"},{"id":"http://arxiv.org/abs/2411.09018v4","updated":"2025-04-15T18:59:54Z","published":"2024-11-13T20:50:04Z","title":"Bridging the Visual Gap: Fine-Tuning Multimodal Models with\n  Knowledge-Adapted Captions","summary":"  Recent research increasingly focuses on training vision-language models\n(VLMs) with long, detailed image captions. However, small-scale VLMs often\nstruggle to balance the richness of these captions with the risk of\nhallucinating content during fine-tuning. In this paper, we explore how well\nVLMs adapt to such captions. To quantify caption quality, we propose Decomposed\nNLI (DNLI), an evaluation framework that breaks down generated captions into\nindividual propositions, assessing each in isolation. This fine-grained\nanalysis reveals a critical balance between capturing descriptive details and\npreventing hallucinations. Our findings show that simply reducing caption\ncomplexity or employing standard data curation techniques does not effectively\nresolve this issue. To tackle this challenge, we introduce Knowledge Adapted\n(KnowAda) fine-tuning, a data-centric approach that automatically adapts\ntraining data with the model's existing knowledge and visual understanding.\nKnowAda minimizes hallucinations while preserving high descriptiveness. We\nvalidate this approach across several small-scale VLMs (up to 7B parameters)\nand dense caption datasets, demonstrating that KnowAda effectively balances\nhallucination reduction and descriptiveness. Our results show that KnowAda\noutperforms various baselines in both automatic metrics and human evaluations.\nWe will release our code and models.\n","authors":["Moran Yanuka","Assaf Ben Kish","Yonatan Bitton","Idan Szpektor","Raja Giryes"],"pdf_url":"https://arxiv.org/pdf/2411.09018v4.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2504.11536v1","updated":"2025-04-15T18:10:22Z","published":"2025-04-15T18:10:22Z","title":"ReTool: Reinforcement Learning for Strategic Tool Use in LLMs","summary":"  While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems.\n","authors":["Jiazhan Feng","Shijue Huang","Xingwei Qu","Ge Zhang","Yujia Qin","Baoquan Zhong","Chengquan Jiang","Jinxin Chi","Wanjun Zhong"],"pdf_url":"https://arxiv.org/pdf/2504.11536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11524v1","updated":"2025-04-15T18:00:00Z","published":"2025-04-15T18:00:00Z","title":"HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis\n  Generation","summary":"  There is growing interest in hypothesis generation with large language models\n(LLMs). However, fundamental questions remain: what makes a good hypothesis,\nand how can we systematically evaluate methods for hypothesis generation? To\naddress this, we introduce HypoBench, a novel benchmark designed to evaluate\nLLMs and hypothesis generation methods across multiple aspects, including\npractical utility, generalizability, and hypothesis discovery rate. HypoBench\nincludes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets.\nWe evaluate four state-of-the-art LLMs combined with six existing\nhypothesis-generation methods. Overall, our results suggest that existing\nmethods are capable of discovering valid and novel patterns in the data.\nHowever, the results from synthetic datasets indicate that there is still\nsignificant room for improvement, as current hypothesis generation methods do\nnot fully uncover all relevant or meaningful patterns. Specifically, in\nsynthetic settings, as task difficulty increases, performance significantly\ndrops, with best models and methods only recovering 38.8% of the ground-truth\nhypotheses. These findings highlight challenges in hypothesis generation and\ndemonstrate that HypoBench serves as a valuable resource for improving AI\nsystems designed to assist scientific discovery.\n","authors":["Haokun Liu","Sicong Huang","Jingyu Hu","Yangqiaoyu Zhou","Chenhao Tan"],"pdf_url":"https://arxiv.org/pdf/2504.11524v1.pdf","comment":"29 pages, 6 figures, website link:\n  https://chicagohai.github.io/HypoBench/"},{"id":"http://arxiv.org/abs/2504.11515v1","updated":"2025-04-15T14:26:12Z","published":"2025-04-15T14:26:12Z","title":"Graph-Driven Multimodal Feature Learning Framework for Apparent\n  Personality Assessment","summary":"  Predicting personality traits automatically has become a challenging problem\nin computer vision. This paper introduces an innovative multimodal feature\nlearning framework for personality analysis in short video clips. For visual\nprocessing, we construct a facial graph and design a Geo-based two-stream\nnetwork incorporating an attention mechanism, leveraging both Graph\nConvolutional Networks (GCN) and Convolutional Neural Networks (CNN) to capture\nstatic facial expressions. Additionally, ResNet18 and VGGFace networks are\nemployed to extract global scene and facial appearance features at the frame\nlevel. To capture dynamic temporal information, we integrate a BiGRU with a\ntemporal attention module for extracting salient frame representations. To\nenhance the model's robustness, we incorporate the VGGish CNN for audio-based\nfeatures and XLM-Roberta for text-based features. Finally, a multimodal channel\nattention mechanism is introduced to integrate different modalities, and a\nMulti-Layer Perceptron (MLP) regression model is used to predict personality\ntraits. Experimental results confirm that our proposed framework surpasses\nexisting state-of-the-art approaches in performance.\n","authors":["Kangsheng Wang","Chengwei Ye","Huanzhen Zhang","Linuo Xu","Shuyan Liu"],"pdf_url":"https://arxiv.org/pdf/2504.11515v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2504.11457v1","updated":"2025-04-15T17:59:54Z","published":"2025-04-15T17:59:54Z","title":"Aligning Generative Denoising with Discriminative Objectives Unleashes\n  Diffusion for Visual Perception","summary":"  With the success of image generation, generative diffusion models are\nincreasingly adopted for discriminative tasks, as pixel generation provides a\nunified perception interface. However, directly repurposing the generative\ndenoising process for discriminative objectives reveals critical gaps rarely\naddressed previously. Generative models tolerate intermediate sampling errors\nif the final distribution remains plausible, but discriminative tasks require\nrigorous accuracy throughout, as evidenced in challenging multi-modal tasks\nlike referring image segmentation. Motivated by this gap, we analyze and\nenhance alignment between generative diffusion processes and perception tasks,\nfocusing on how perception quality evolves during denoising. We find: (1)\nearlier denoising steps contribute disproportionately to perception quality,\nprompting us to propose tailored learning objectives reflecting varying\ntimestep contributions; (2) later denoising steps show unexpected perception\ndegradation, highlighting sensitivity to training-denoising distribution\nshifts, addressed by our diffusion-tailored data augmentation; and (3)\ngenerative processes uniquely enable interactivity, serving as controllable\nuser interfaces adaptable to correctional prompts in multi-round interactions.\nOur insights significantly improve diffusion-based perception models without\narchitectural changes, achieving state-of-the-art performance on depth\nestimation, referring image segmentation, and generalist perception tasks. Code\navailable at https://github.com/ziqipang/ADDP.\n","authors":["Ziqi Pang","Xin Xu","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2504.11457v1.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2504.11455v1","updated":"2025-04-15T17:59:46Z","published":"2025-04-15T17:59:46Z","title":"SimpleAR: Pushing the Frontier of Autoregressive Visual Generation\n  through Pretraining, SFT, and RL","summary":"  This work presents SimpleAR, a vanilla autoregressive visual generation\nframework without complex architecure modifications. Through careful\nexploration of training and inference optimization, we demonstrate that: 1)\nwith only 0.5B parameters, our model can generate 1024x1024 resolution images\nwith high fidelity, and achieve competitive results on challenging\ntext-to-image benchmarks, e.g., 0.59 on GenEval and 79.66 on DPG; 2) both\nsupervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO)\ntraining could lead to significant improvements on generation aesthectics and\nprompt alignment; and 3) when optimized with inference acceleraton techniques\nlike vLLM, the time for SimpleAR to generate an 1024x1024 image could be\nreduced to around 14 seconds. By sharing these findings and open-sourcing the\ncode, we hope to reveal the potential of autoregressive visual generation and\nencourage more participation in this research field. Code is available at\nhttps://github.com/wdrink/SimpleAR.\n","authors":["Junke Wang","Zhi Tian","Xun Wang","Xinyu Zhang","Weilin Huang","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2504.11455v1.pdf","comment":"technical report, work in progress"},{"id":"http://arxiv.org/abs/2504.11451v1","updated":"2025-04-15T17:58:16Z","published":"2025-04-15T17:58:16Z","title":"PARTFIELD: Learning 3D Feature Fields for Part Segmentation and Beyond","summary":"  We propose PartField, a feedforward approach for learning part-based 3D\nfeatures, which captures the general concept of parts and their hierarchy\nwithout relying on predefined templates or text-based names, and can be applied\nto open-world 3D shapes across various modalities. PartField requires only a 3D\nfeedforward pass at inference time, significantly improving runtime and\nrobustness compared to prior approaches. Our model is trained by distilling 2D\nand 3D part proposals from a mix of labeled datasets and image segmentations on\nlarge unsupervised datasets, via a contrastive learning formulation. It\nproduces a continuous feature field which can be clustered to yield a\nhierarchical part decomposition. Comparisons show that PartField is up to 20%\nmore accurate and often orders of magnitude faster than other recent\nclass-agnostic part-segmentation methods. Beyond single-shape part\ndecomposition, consistency in the learned field emerges across shapes, enabling\ntasks such as co-segmentation and correspondence, which we demonstrate in\nseveral applications of these general-purpose, hierarchical, and consistent 3D\nfeature fields. Check our Webpage!\nhttps://research.nvidia.com/labs/toronto-ai/partfield-release/\n","authors":["Minghua Liu","Mikaela Angelina Uy","Donglai Xiang","Hao Su","Sanja Fidler","Nicholas Sharp","Jun Gao"],"pdf_url":"https://arxiv.org/pdf/2504.11451v1.pdf","comment":"https://research.nvidia.com/labs/toronto-ai/partfield-release/"},{"id":"http://arxiv.org/abs/2504.11447v1","updated":"2025-04-15T17:57:13Z","published":"2025-04-15T17:57:13Z","title":"Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion","summary":"  The application of diffusion models in 3D LiDAR scene completion is limited\ndue to diffusion's slow sampling speed. Score distillation accelerates\ndiffusion sampling but with performance degradation, while post-training with\ndirect policy optimization (DPO) boosts performance using preference data. This\npaper proposes Distillation-DPO, a novel diffusion distillation framework for\nLiDAR scene completion with preference aligment. First, the student model\ngenerates paired completion scenes with different initial noises. Second, using\nLiDAR scene evaluation metrics as preference, we construct winning and losing\nsample pairs. Such construction is reasonable, since most LiDAR scene metrics\nare informative but non-differentiable to be optimized directly. Third,\nDistillation-DPO optimizes the student model by exploiting the difference in\nscore functions between the teacher and student models on the paired completion\nscenes. Such procedure is repeated until convergence. Extensive experiments\ndemonstrate that, compared to state-of-the-art LiDAR scene completion diffusion\nmodels, Distillation-DPO achieves higher-quality scene completion while\naccelerating the completion speed by more than 5-fold. Our method is the first\nto explore adopting preference learning in distillation to the best of our\nknowledge and provide insights into preference-aligned distillation. Our code\nis public available on https://github.com/happyw1nd/DistillationDPO.\n","authors":["An Zhaol","Shengyuan Zhang","Ling Yang","Zejian Li","Jiale Wu","Haoran Xu","AnYang Wei","Perry Pengyun GU Lingyun Sun"],"pdf_url":"https://arxiv.org/pdf/2504.11447v1.pdf","comment":"Our code is public available on\n  https://github.com/happyw1nd/DistillationDPO"},{"id":"http://arxiv.org/abs/2404.03632v3","updated":"2025-04-15T17:56:35Z","published":"2024-04-04T17:53:33Z","title":"Reference-Based 3D-Aware Image Editing with Triplanes","summary":"  Generative Adversarial Networks (GANs) have emerged as powerful tools for\nhigh-quality image generation and real image editing by manipulating their\nlatent spaces. Recent advancements in GANs include 3D-aware models such as\nEG3D, which feature efficient triplane-based architectures capable of\nreconstructing 3D geometry from single images. However, limited attention has\nbeen given to providing an integrated framework for 3D-aware, high-quality,\nreference-based image editing. This study addresses this gap by exploring and\ndemonstrating the effectiveness of the triplane space for advanced\nreference-based edits. Our novel approach integrates encoding, automatic\nlocalization, spatial disentanglement of triplane features, and fusion learning\nto achieve the desired edits. We demonstrate how our approach excels across\ndiverse domains, including human faces, 360-degree heads, animal faces,\npartially stylized edits like cartoon faces, full-body clothing edits, and\nedits on class-agnostic samples. Our method shows state-of-the-art performance\nover relevant latent direction, text, and image-guided 2D and 3D-aware\ndiffusion and GAN methods, both qualitatively and quantitatively.\n","authors":["Bahri Batuhan Bilecen","Yigit Yalin","Ning Yu","Aysegul Dundar"],"pdf_url":"https://arxiv.org/pdf/2404.03632v3.pdf","comment":"CVPR 2025 Highlight. Includes supplementary material"},{"id":"http://arxiv.org/abs/2504.11441v1","updated":"2025-04-15T17:54:59Z","published":"2025-04-15T17:54:59Z","title":"TADACap: Time-series Adaptive Domain-Aware Captioning","summary":"  While image captioning has gained significant attention, the potential of\ncaptioning time-series images, prevalent in areas like finance and healthcare,\nremains largely untapped. Existing time-series captioning methods typically\noffer generic, domain-agnostic descriptions of time-series shapes and struggle\nto adapt to new domains without substantial retraining. To address these\nlimitations, we introduce TADACap, a retrieval-based framework to generate\ndomain-aware captions for time-series images, capable of adapting to new\ndomains without retraining. Building on TADACap, we propose a novel retrieval\nstrategy that retrieves diverse image-caption pairs from a target domain\ndatabase, namely TADACap-diverse. We benchmarked TADACap-diverse against\nstate-of-the-art methods and ablation variants. TADACap-diverse demonstrates\ncomparable semantic accuracy while requiring significantly less annotation\neffort.\n","authors":["Elizabeth Fons","Rachneet Kaur","Zhen Zeng","Soham Palande","Tucker Balch","Svitlana Vyetrenko","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2504.11441v1.pdf","comment":"Accepted to ICAIF 2024"},{"id":"http://arxiv.org/abs/2504.11438v1","updated":"2025-04-15T17:53:18Z","published":"2025-04-15T17:53:18Z","title":"Mamba-Based Ensemble learning for White Blood Cell Classification","summary":"  White blood cell (WBC) classification assists in assessing immune health and\ndiagnosing various diseases, yet manual classification is labor-intensive and\nprone to inconsistencies. Recent advancements in deep learning have shown\npromise over traditional methods; however, challenges such as data imbalance\nand the computational demands of modern technologies, such as Transformer-based\nmodels which do not scale well with input size, limit their practical\napplication. This paper introduces a novel framework that leverages Mamba\nmodels integrated with ensemble learning to improve WBC classification. Mamba\nmodels, known for their linear complexity, provide a scalable alternative to\nTransformer-based approaches, making them suitable for deployment in\nresource-constrained environments. Additionally, we introduce a new WBC\ndataset, Chula-WBC-8, for benchmarking. Our approach not only validates the\neffectiveness of Mamba models in this domain but also demonstrates their\npotential to significantly enhance classification efficiency without\ncompromising accuracy. The source code can be found at\nhttps://github.com/LewisClifton/Mamba-WBC-Classification.\n","authors":["Lewis Clifton","Xin Tian","Duangdao Palasuwan","Phandee Watanaboonyongcharoen","Ponlapat Rojnuckarin","Nantheera Anantrasirichai"],"pdf_url":"https://arxiv.org/pdf/2504.11438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11434v1","updated":"2025-04-15T17:51:35Z","published":"2025-04-15T17:51:35Z","title":"Enhancing Out-of-Distribution Detection with Extended Logit\n  Normalization","summary":"  Out-of-distribution (OOD) detection is essential for the safe deployment of\nmachine learning models. Recent advances have explored improved classification\nlosses and representation learning strategies to enhance OOD detection.\nHowever, these methods are often tailored to specific post-hoc detection\ntechniques, limiting their generalizability. In this work, we identify a\ncritical issue in Logit Normalization (LogitNorm), which inhibits its\neffectiveness in improving certain post-hoc OOD detection methods. To address\nthis, we propose Extended Logit Normalization ($\\textbf{ELogitNorm}$), a novel\nhyperparameter-free formulation that significantly benefits a wide range of\npost-hoc detection methods. By incorporating feature distance-awareness to\nLogitNorm, $\\textbf{ELogitNorm}$ shows more robust OOD separability and\nin-distribution (ID) confidence calibration than its predecessor. Extensive\nexperiments across standard benchmarks demonstrate that our approach\noutperforms state-of-the-art training-time methods in OOD detection while\nmaintaining strong ID classification accuracy.\n","authors":["Yifan Ding","Xixi Liu","Jonas Unger","Gabriel Eilertsen"],"pdf_url":"https://arxiv.org/pdf/2504.11434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10479v2","updated":"2025-04-15T17:50:27Z","published":"2025-04-14T17:59:25Z","title":"InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models","summary":"  We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.\n","authors":["Jinguo Zhu","Weiyun Wang","Zhe Chen","Zhaoyang Liu","Shenglong Ye","Lixin Gu","Yuchen Duan","Hao Tian","Weijie Su","Jie Shao","Zhangwei Gao","Erfei Cui","Yue Cao","Yangzhou Liu","Xingguang Wei","Hongjie Zhang","Haomin Wang","Weiye Xu","Hao Li","Jiahao Wang","Dengnian Chen","Songze Li","Yinan He","Tan Jiang","Jiapeng Luo","Yi Wang","Conghui He","Botian Shi","Xingcheng Zhang","Wenqi Shao","Junjun He","Yingtong Xiong","Wenwen Qu","Peng Sun","Penglong Jiao","Han Lv","Lijun Wu","Kaipeng Zhang","Huipeng Deng","Jiaye Ge","Kai Chen","Limin Wang","Min Dou","Lewei Lu","Xizhou Zhu","Tong Lu","Dahua Lin","Yu Qiao","Jifeng Dai","Wenhai Wang"],"pdf_url":"https://arxiv.org/pdf/2504.10479v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2504.11427v1","updated":"2025-04-15T17:39:07Z","published":"2025-04-15T17:39:07Z","title":"NormalCrafter: Learning Temporally Consistent Normals from Video\n  Diffusion Priors","summary":"  Surface normal estimation serves as a cornerstone for a spectrum of computer\nvision applications. While numerous efforts have been devoted to static image\nscenarios, ensuring temporal coherence in video-based normal estimation remains\na formidable challenge. Instead of merely augmenting existing methods with\ntemporal components, we present NormalCrafter to leverage the inherent temporal\npriors of video diffusion models. To secure high-fidelity normal estimation\nacross sequences, we propose Semantic Feature Regularization (SFR), which\naligns diffusion features with semantic cues, encouraging the model to\nconcentrate on the intrinsic semantics of the scene. Moreover, we introduce a\ntwo-stage training protocol that leverages both latent and pixel space learning\nto preserve spatial accuracy while maintaining long temporal context. Extensive\nevaluations demonstrate the efficacy of our method, showcasing a superior\nperformance in generating temporally consistent normal sequences with intricate\ndetails from diverse videos.\n","authors":["Yanrui Bin","Wenbo Hu","Haoyuan Wang","Xinya Chen","Bing Wang"],"pdf_url":"https://arxiv.org/pdf/2504.11427v1.pdf","comment":"9 pages, 6 figures, Project Page: https://normalcrafter.github.io/"},{"id":"http://arxiv.org/abs/2504.11423v1","updated":"2025-04-15T17:37:50Z","published":"2025-04-15T17:37:50Z","title":"ADT: Tuning Diffusion Models with Adversarial Supervision","summary":"  Diffusion models have achieved outstanding image generation by reversing a\nforward noising process to approximate true data distributions. During\ntraining, these models predict diffusion scores from noised versions of true\nsamples in a single forward pass, while inference requires iterative denoising\nstarting from white noise. This training-inference divergences hinder the\nalignment between inference and training data distributions, due to potential\nprediction biases and cumulative error accumulation. To address this problem,\nwe propose an intuitive but effective fine-tuning framework, called Adversarial\nDiffusion Tuning (ADT), by stimulating the inference process during\noptimization and aligning the final outputs with training data by adversarial\nsupervision. Specifically, to achieve robust adversarial training, ADT features\na siamese-network discriminator with a fixed pre-trained backbone and\nlightweight trainable parameters, incorporates an image-to-image sampling\nstrategy to smooth discriminative difficulties, and preserves the original\ndiffusion loss to prevent discriminator hacking. In addition, we carefully\nconstrain the backward-flowing path for back-propagating gradients along the\ninference path without incurring memory overload or gradient explosion.\nFinally, extensive experiments on Stable Diffusion models (v1.5, XL, and v3),\ndemonstrate that ADT significantly improves both distribution alignment and\nimage quality.\n","authors":["Dazhong Shen","Guanglu Song","Yi Zhang","Bingqi Ma","Lujundong Li","Dongzhi Jiang","Zhuofan Zong","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2504.11423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11418v1","updated":"2025-04-15T17:34:56Z","published":"2025-04-15T17:34:56Z","title":"Leveraging Point Transformers for Detecting Anatomical Landmarks in\n  Digital Dentistry","summary":"  The increasing availability of intraoral scanning devices has heightened\ntheir importance in modern clinical orthodontics. Clinicians utilize advanced\nComputer-Aided Design techniques to create patient-specific treatment plans\nthat include laboriously identifying crucial landmarks such as cusps,\nmesial-distal locations, facial axis points, and tooth-gingiva boundaries.\nDetecting such landmarks automatically presents challenges, including limited\ndataset sizes, significant anatomical variability among subjects, and the\ngeometric nature of the data. We present our experiments from the 3DTeethLand\nGrand Challenge at MICCAI 2024. Our method leverages recent advancements in\npoint cloud learning through transformer architectures. We designed a Point\nTransformer v3 inspired module to capture meaningful geometric and anatomical\nfeatures, which are processed by a lightweight decoder to predict per-point\ndistances, further processed by graph-based non-minima suppression. We report\npromising results and discuss insights on learned feature interpretability.\n","authors":["Tibor Kubík","Oldřich Kodym","Petr Šilling","Kateřina Trávníčková","Tomáš Mojžiš","Jan Matula"],"pdf_url":"https://arxiv.org/pdf/2504.11418v1.pdf","comment":"10 pages + references, 3 figures, MICCAI2024 3DTeethland Challenge\n  submission"},{"id":"http://arxiv.org/abs/2504.11416v1","updated":"2025-04-15T17:31:48Z","published":"2025-04-15T17:31:48Z","title":"Deep Learning-based Bathymetry Retrieval without In-situ Depths using\n  Remote Sensing Imagery and SfM-MVS DSMs with Data Gaps","summary":"  Accurate, detailed, and high-frequent bathymetry is crucial for shallow\nseabed areas facing intense climatological and anthropogenic pressures. Current\nmethods utilizing airborne or satellite optical imagery to derive bathymetry\nprimarily rely on either SfM-MVS with refraction correction or Spectrally\nDerived Bathymetry (SDB). However, SDB methods often require extensive manual\nfieldwork or costly reference data, while SfM-MVS approaches face challenges\neven after refraction correction. These include depth data gaps and noise in\nenvironments with homogeneous visual textures, which hinder the creation of\naccurate and complete Digital Surface Models (DSMs) of the seabed. To address\nthese challenges, this work introduces a methodology that combines the\nhigh-fidelity 3D reconstruction capabilities of the SfM-MVS methods with\nstate-of-the-art refraction correction techniques, along with the spectral\nanalysis capabilities of a new deep learning-based method for bathymetry\nprediction. This integration enables a synergistic approach where SfM-MVS\nderived DSMs with data gaps are used as training data to generate complete\nbathymetric maps. In this context, we propose Swin-BathyUNet that combines\nU-Net with Swin Transformer self-attention layers and a cross-attention\nmechanism, specifically tailored for SDB. Swin-BathyUNet is designed to improve\nbathymetric accuracy by capturing long-range spatial relationships and can also\nfunction as a standalone solution for standard SDB with various training depth\ndata, independent of the SfM-MVS output. Experimental results in two completely\ndifferent test sites in the Mediterranean and Baltic Seas demonstrate the\neffectiveness of the proposed approach through extensive experiments that\ndemonstrate improvements in bathymetric accuracy, detail, coverage, and noise\nreduction in the predicted DSM. The code is available at\nhttps://github.com/pagraf/Swin-BathyUNet.\n","authors":["Panagiotis Agrafiotis","Begüm Demir"],"pdf_url":"https://arxiv.org/pdf/2504.11416v1.pdf","comment":"Accepted for publication in ISPRS Journal of Photogrammetry and\n  Remote Sensing"},{"id":"http://arxiv.org/abs/2504.11415v1","updated":"2025-04-15T17:31:46Z","published":"2025-04-15T17:31:46Z","title":"Robustness and sex differences in skin cancer detection: logistic\n  regression vs CNNs","summary":"  Deep learning has been reported to achieve high performances in the detection\nof skin cancer, yet many challenges regarding the reproducibility of results\nand biases remain. This study is a replication (different data, same analysis)\nof a study on Alzheimer's disease [28] which studied robustness of logistic\nregression (LR) and convolutional neural networks (CNN) across patient sexes.\nWe explore sex bias in skin cancer detection, using the PAD-UFES-20 dataset\nwith LR trained on handcrafted features reflecting dermatological guidelines\n(ABCDE and the 7-point checklist), and a pre-trained ResNet-50 model. We\nevaluate these models in alignment with [28]: across multiple training datasets\nwith varied sex composition to determine their robustness. Our results show\nthat both the LR and the CNN were robust to the sex distributions, but the\nresults also revealed that the CNN had a significantly higher accuracy (ACC)\nand area under the receiver operating characteristics (AUROC) for male patients\nthan for female patients. We hope these findings to contribute to the growing\nfield of investigating potential bias in popular medical machine learning\nmethods. The data and relevant scripts to reproduce our results can be found in\nour Github.\n","authors":["Nikolette Pedersen","Regitze Sydendal","Andreas Wulff","Ralf Raumanns","Eike Petersen","Veronika Cheplygina"],"pdf_url":"https://arxiv.org/pdf/2504.11415v1.pdf","comment":"16 pages (excluding appendix), 2 figures (excluding appendix),\n  submitted to MIUA 2025 conference (response pending)"},{"id":"http://arxiv.org/abs/2409.08301v2","updated":"2025-04-15T17:26:49Z","published":"2024-09-10T14:47:31Z","title":"Gaussian Differentially Private Human Faces Under a Face Radial Curve\n  Representation","summary":"  In this paper we consider the problem of releasing a Gaussian Differentially\nPrivate (GDP) 3D human face. The human face is a complex structure with many\nfeatures and inherently tied to one's identity. Protecting this data, in a\nformally private way, is important yet challenging given the dimensionality of\nthe problem. We extend approximate DP techniques for functional data to the GDP\nframework. We further propose a novel representation, face radial curves, of a\n3D face as a set of functions and then utilize our proposed GDP functional data\nmechanism. To preserve the shape of the face while injecting noise we rely on\ntools from shape analysis for our novel representation of the face. We show\nthat our method preserves the shape of the average face and injects less noise\nthan traditional methods for the same privacy budget. Our mechanism consists of\ntwo primary components, the first is generally applicable to function value\nsummaries (as are commonly found in nonparametric statistics or functional data\nanalysis) while the second is general to disk-like surfaces and hence more\napplicable than just to human faces.\n","authors":["Carlos Soto","Matthew Reimherr","Aleksandra Slavkovic","Mark Shriver"],"pdf_url":"https://arxiv.org/pdf/2409.08301v2.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2504.11406v1","updated":"2025-04-15T17:22:24Z","published":"2025-04-15T17:22:24Z","title":"Multi-level Cellular Automata for FLIM networks","summary":"  The necessity of abundant annotated data and complex network architectures\npresents a significant challenge in deep-learning Salient Object Detection\n(deep SOD) and across the broader deep-learning landscape. This challenge is\nparticularly acute in medical applications in developing countries with limited\ncomputational resources. Combining modern and classical techniques offers a\npath to maintaining competitive performance while enabling practical\napplications. Feature Learning from Image Markers (FLIM) methodology empowers\nexperts to design convolutional encoders through user-drawn markers, with\nfilters learned directly from these annotations. Recent findings demonstrate\nthat coupling a FLIM encoder with an adaptive decoder creates a flyweight\nnetwork suitable for SOD, requiring significantly fewer parameters than\nlightweight models and eliminating the need for backpropagation. Cellular\nAutomata (CA) methods have proven successful in data-scarce scenarios but\nrequire proper initialization -- typically through user input, priors, or\nrandomness. We propose a practical intersection of these approaches: using FLIM\nnetworks to initialize CA states with expert knowledge without requiring user\ninteraction for each image. By decoding features from each level of a FLIM\nnetwork, we can initialize multiple CAs simultaneously, creating a multi-level\nframework. Our method leverages the hierarchical knowledge encoded across\ndifferent network layers, merging multiple saliency maps into a high-quality\nfinal output that functions as a CA ensemble. Benchmarks across two challenging\nmedical datasets demonstrate the competitiveness of our multi-level CA approach\ncompared to established models in the deep SOD literature.\n","authors":["Felipe Crispim Salvagnini","Jancarlo F. Gomes","Cid A. N. Santos","Silvio Jamil F. Guimarães","Alexandre X. Falcão"],"pdf_url":"https://arxiv.org/pdf/2504.11406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07491v2","updated":"2025-04-15T17:14:37Z","published":"2025-04-10T06:48:26Z","title":"Kimi-VL Technical Report","summary":"  We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE)\nvision-language model (VLM) that offers advanced multimodal reasoning,\nlong-context understanding, and strong agent capabilities - all while\nactivating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL\ndemonstrates strong performance across challenging domains: as a\ngeneral-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld),\nmatching flagship models. Furthermore, it exhibits remarkable capabilities\nacross diverse challenging vision language tasks, including college-level image\nand video comprehension, OCR, mathematical reasoning, and multi-image\nunderstanding. In comparative evaluations, it effectively competes with\ncutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and\nGemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also\nadvances in processing long contexts and perceiving clearly. With a 128K\nextended context window, Kimi-VL can process diverse long inputs, achieving\nimpressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its\nnative-resolution vision encoder, MoonViT, further allows it to see and\nunderstand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and\n34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common\ntasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant:\nKimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised\nfine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong\nlong-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8\non MathVision, and 71.3 on MathVista while maintaining the compact 2.8B\nactivated LLM parameters, setting a new standard for efficient multimodal\nthinking models. Code and models are publicly accessible at\nhttps://github.com/MoonshotAI/Kimi-VL.\n","authors":[" Kimi Team","Angang Du","Bohong Yin","Bowei Xing","Bowen Qu","Bowen Wang","Cheng Chen","Chenlin Zhang","Chenzhuang Du","Chu Wei","Congcong Wang","Dehao Zhang","Dikang Du","Dongliang Wang","Enming Yuan","Enzhe Lu","Fang Li","Flood Sung","Guangda Wei","Guokun Lai","Han Zhu","Hao Ding","Hao Hu","Hao Yang","Hao Zhang","Haoning Wu","Haotian Yao","Haoyu Lu","Heng Wang","Hongcheng Gao","Huabin Zheng","Jiaming Li","Jianlin Su","Jianzhou Wang","Jiaqi Deng","Jiezhong Qiu","Jin Xie","Jinhong Wang","Jingyuan Liu","Junjie Yan","Kun Ouyang","Liang Chen","Lin Sui","Longhui Yu","Mengfan Dong","Mengnan Dong","Nuo Xu","Pengyu Cheng","Qizheng Gu","Runjie Zhou","Shaowei Liu","Sihan Cao","Tao Yu","Tianhui Song","Tongtong Bai","Wei Song","Weiran He","Weixiao Huang","Weixin Xu","Xiaokun Yuan","Xingcheng Yao","Xingzhe Wu","Xinxing Zu","Xinyu Zhou","Xinyuan Wang","Y. Charles","Yan Zhong","Yang Li","Yangyang Hu","Yanru Chen","Yejie Wang","Yibo Liu","Yibo Miao","Yidao Qin","Yimin Chen","Yiping Bao","Yiqin Wang","Yongsheng Kang","Yuanxin Liu","Yulun Du","Yuxin Wu","Yuzhi Wang","Yuzi Yan","Zaida Zhou","Zhaowei Li","Zhejun Jiang","Zheng Zhang","Zhilin Yang","Zhiqi Huang","Zihao Huang","Zijia Zhao","Ziwei Chen","Zongyu Lin"],"pdf_url":"https://arxiv.org/pdf/2504.07491v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10127v2","updated":"2025-04-15T17:13:46Z","published":"2025-04-14T11:35:02Z","title":"Breaking the Data Barrier -- Building GUI Agents Through Task\n  Generalization","summary":"  Graphical User Interface (GUI) agents offer cross-platform solutions for\nautomating complex digital tasks, with significant potential to transform\nproductivity workflows. However, their performance is often constrained by the\nscarcity of high-quality trajectory data. To address this limitation, we\npropose training Vision Language Models (VLMs) on data-rich,\nreasoning-intensive tasks during a dedicated mid-training stage, and then\nexamine how incorporating these tasks facilitates generalization to GUI\nplanning scenarios. Specifically, we explore a range of tasks with readily\navailable instruction-tuning data, including GUI perception, multimodal\nreasoning, and textual reasoning. Through extensive experiments across 11\nmid-training tasks, we demonstrate that: (1) Task generalization proves highly\neffective, yielding substantial improvements across most settings. For\ninstance, multimodal mathematical reasoning enhances performance on\nAndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data\nsignificantly boosts GUI web agent performance, achieving a 5.6% improvement on\nWebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal\ngeneralization from text-based to visual domains; (2) Contrary to prior\nassumptions, GUI perception data - previously considered closely aligned with\nGUI agent tasks and widely utilized for training - has a comparatively limited\nimpact on final performance; (3) Building on these insights, we identify the\nmost effective mid-training tasks and curate optimized mixture datasets,\nresulting in absolute performance gains of 8.0% on WebArena and 12.2% on\nAndroidWorld. Our work provides valuable insights into cross-domain knowledge\ntransfer for GUI agents and offers a practical approach to addressing data\nscarcity challenges in this emerging field. The code, data and models will be\navailable at https://github.com/hkust-nlp/GUIMid.\n","authors":["Junlei Zhang","Zichen Ding","Chang Ma","Zijie Chen","Qiushi Sun","Zhenzhong Lan","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2504.10127v2.pdf","comment":"24 pages, 11 figures"},{"id":"http://arxiv.org/abs/2504.11389v1","updated":"2025-04-15T16:58:15Z","published":"2025-04-15T16:58:15Z","title":"VideoPanda: Video Panoramic Diffusion with Multi-view Attention","summary":"  High resolution panoramic video content is paramount for immersive\nexperiences in Virtual Reality, but is non-trivial to collect as it requires\nspecialized equipment and intricate camera setups. In this work, we introduce\nVideoPanda, a novel approach for synthesizing 360$^\\circ$ videos conditioned on\ntext or single-view video data. VideoPanda leverages multi-view attention\nlayers to augment a video diffusion model, enabling it to generate consistent\nmulti-view videos that can be combined into immersive panoramic content.\nVideoPanda is trained jointly using two conditions: text-only and single-view\nvideo, and supports autoregressive generation of long-videos. To overcome the\ncomputational burden of multi-view video generation, we randomly subsample the\nduration and camera views used during training and show that the model is able\nto gracefully generalize to generating more frames during inference. Extensive\nevaluations on both real-world and synthetic video datasets demonstrate that\nVideoPanda generates more realistic and coherent 360$^\\circ$ panoramas across\nall input conditions compared to existing methods. Visit the project website at\nhttps://research-staging.nvidia.com/labs/toronto-ai/VideoPanda/ for results.\n","authors":["Kevin Xie","Amirmojtaba Sabour","Jiahui Huang","Despoina Paschalidou","Greg Klar","Umar Iqbal","Sanja Fidler","Xiaohui Zeng"],"pdf_url":"https://arxiv.org/pdf/2504.11389v1.pdf","comment":"Project website at\n  https://research-staging.nvidia.com/labs/toronto-ai/VideoPanda/"},{"id":"http://arxiv.org/abs/2503.21958v2","updated":"2025-04-15T16:57:59Z","published":"2025-03-27T20:14:45Z","title":"SC-NeRF: NeRF-based Point Cloud Reconstruction using a Stationary Camera\n  for Agricultural Applications","summary":"  This paper presents a NeRF-based framework for point cloud (PCD)\nreconstruction, specifically designed for indoor high-throughput plant\nphenotyping facilities. Traditional NeRF-based reconstruction methods require\ncameras to move around stationary objects, but this approach is impractical for\nhigh-throughput environments where objects are rapidly imaged while moving on\nconveyors or rotating pedestals. To address this limitation, we develop a\nvariant of NeRF-based PCD reconstruction that uses a single stationary camera\nto capture images as the object rotates on a pedestal. Our workflow comprises\nCOLMAP-based pose estimation, a straightforward pose transformation to simulate\ncamera movement, and subsequent standard NeRF training. A defined Region of\nInterest (ROI) excludes irrelevant scene data, enabling the generation of\nhigh-resolution point clouds (10M points). Experimental results demonstrate\nexcellent reconstruction fidelity, with precision-recall analyses yielding an\nF-score close to 100.00 across all evaluated plant objects. Although pose\nestimation remains computationally intensive with a stationary camera setup,\noverall training and reconstruction times are competitive, validating the\nmethod's feasibility for practical high-throughput indoor phenotyping\napplications. Our findings indicate that high-quality NeRF-based 3D\nreconstructions are achievable using a stationary camera, eliminating the need\nfor complex camera motion or costly imaging equipment. This approach is\nespecially beneficial when employing expensive and delicate instruments, such\nas hyperspectral cameras, for 3D plant phenotyping. Future work will focus on\noptimizing pose estimation techniques and further streamlining the methodology\nto facilitate seamless integration into automated, high-throughput 3D\nphenotyping pipelines.\n","authors":["Kibon Ku","Talukder Z Jubery","Elijah Rodriguez","Aditya Balu","Soumik Sarkar","Adarsh Krishnamurthy","Baskar Ganapathysubramanian"],"pdf_url":"https://arxiv.org/pdf/2503.21958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11379v1","updated":"2025-04-15T16:53:11Z","published":"2025-04-15T16:53:11Z","title":"Omni$^2$: Unifying Omnidirectional Image Generation and Editing in an\n  Omni Model","summary":"  $360^{\\circ}$ omnidirectional images (ODIs) have gained considerable\nattention recently, and are widely used in various virtual reality (VR) and\naugmented reality (AR) applications. However, capturing such images is\nexpensive and requires specialized equipment, making ODI synthesis increasingly\nimportant. While common 2D image generation and editing methods are rapidly\nadvancing, these models struggle to deliver satisfactory results when\ngenerating or editing ODIs due to the unique format and broad 360$^{\\circ}$\nField-of-View (FoV) of ODIs. To bridge this gap, we construct\n\\textbf{\\textit{Any2Omni}}, the first comprehensive ODI generation-editing\ndataset comprises 60,000+ training data covering diverse input conditions and\nup to 9 ODI generation and editing tasks. Built upon Any2Omni, we propose an\n\\textbf{\\underline{Omni}} model for \\textbf{\\underline{Omni}}-directional image\ngeneration and editing (\\textbf{\\textit{Omni$^2$}}), with the capability of\nhandling various ODI generation and editing tasks under diverse input\nconditions using one model. Extensive experiments demonstrate the superiority\nand effectiveness of the proposed Omni$^2$ model for both the ODI generation\nand editing tasks.\n","authors":["Liu Yang","Huiyu Duan","Yucheng Zhu","Xiaohong Liu","Lu Liu","Zitong Xu","Guangji Ma","Xiongkuo Min","Guangtao Zhai","Patrick Le Callet"],"pdf_url":"https://arxiv.org/pdf/2504.11379v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2407.21467v2","updated":"2025-04-15T16:41:09Z","published":"2024-07-31T09:26:20Z","title":"Deep Learning-Based Longitudinal Prediction of Childhood Myopia\n  Progression Using Fundus Image Sequences and Baseline Refraction Data","summary":"  Childhood myopia constitutes a significant global health concern. It exhibits\nan escalating prevalence and has the potential to evolve into severe,\nirreversible conditions that detrimentally impact familial well-being and\ncreate substantial economic costs. Contemporary research underscores the\nimportance of precisely predicting myopia progression to enable timely and\neffective interventions, thereby averting severe visual impairment in children.\nSuch predictions predominantly rely on subjective clinical assessments, which\nare inherently biased and resource-intensive, thus hindering their widespread\napplication. In this study, we introduce a novel, high-accuracy method for\nquantitatively predicting the myopic trajectory and myopia risk in children\nusing only fundus images and baseline refraction data. This approach was\nvalidated through a six-year longitudinal study of 3,408 children in Henan,\nutilizing 16,211 fundus images and corresponding refractive data. Our method\nbased on deep learning demonstrated predictive accuracy with an error margin of\n0.311D per year and AUC scores of 0.944 and 0.995 for forecasting the risks of\ndeveloping myopia and high myopia, respectively. These findings confirm the\nutility of our model in supporting early intervention strategies and in\nsignificantly reducing healthcare costs, particularly by obviating the need for\nadditional metadata and repeated consultations. Furthermore, our method was\ndesigned to rely only on fundus images and refractive error data, without the\nneed for meta data or multiple inquiries from doctors, strongly reducing the\nassociated medical costs and facilitating large-scale screening. Our model can\neven provide good predictions based on only a single time measurement.\nConsequently, the proposed method is an important means to reduce medical\ninequities caused by economic disparities.\n","authors":["Mengtian Kang","Yansong Hu","Shuo Gao","Yuanyuan Liu","Hongbei Meng","Xuemeng Li","Xuhang Chen","Hubin Zhao","Jing Fu","Guohua Hu","Wei Wang","Yanning Dai","Arokia Nathan","Peter Smielewski","Ningli Wang","Shiming Li"],"pdf_url":"https://arxiv.org/pdf/2407.21467v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11368v1","updated":"2025-04-15T16:32:15Z","published":"2025-04-15T16:32:15Z","title":"From Gaze to Insight: Bridging Human Visual Attention and Vision\n  Language Model Explanation for Weakly-Supervised Medical Image Segmentation","summary":"  Medical image segmentation remains challenging due to the high cost of\npixel-level annotations for training. In the context of weak supervision,\nclinician gaze data captures regions of diagnostic interest; however, its\nsparsity limits its use for segmentation. In contrast, vision-language models\n(VLMs) provide semantic context through textual descriptions but lack the\nexplanation precision required. Recognizing that neither source alone suffices,\nwe propose a teacher-student framework that integrates both gaze and language\nsupervision, leveraging their complementary strengths. Our key insight is that\ngaze data indicates where clinicians focus during diagnosis, while VLMs explain\nwhy those regions are significant. To implement this, the teacher model first\nlearns from gaze points enhanced by VLM-generated descriptions of lesion\nmorphology, establishing a foundation for guiding the student model. The\nteacher then directs the student through three strategies: (1) Multi-scale\nfeature alignment to fuse visual cues with textual semantics; (2)\nConfidence-weighted consistency constraints to focus on reliable predictions;\n(3) Adaptive masking to limit error propagation in uncertain areas. Experiments\non the Kvasir-SEG, NCI-ISBI, and ISIC datasets show that our method achieves\nDice scores of 80.78%, 80.53%, and 84.22%, respectively-improving 3-5% over\ngaze baselines without increasing the annotation burden. By preserving\ncorrelations among predictions, gaze data, and lesion descriptions, our\nframework also maintains clinical interpretability. This work illustrates how\nintegrating human visual attention with AI-generated semantic context can\neffectively overcome the limitations of individual weak supervision signals,\nthereby advancing the development of deployable, annotation-efficient medical\nAI systems. Code is available at: https://github.com/jingkunchen/FGI.git.\n","authors":["Jingkun Chen","Haoran Duan","Xiao Zhang","Boyan Gao","Tao Tan","Vicente Grau","Jungong Han"],"pdf_url":"https://arxiv.org/pdf/2504.11368v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.11366v1","updated":"2025-04-15T16:31:54Z","published":"2025-04-15T16:31:54Z","title":"A Decade of Wheat Mapping for Lebanon","summary":"  Wheat accounts for approximately 20% of the world's caloric intake, making it\na vital component of global food security. Given this importance, mapping wheat\nfields plays a crucial role in enabling various stakeholders, including policy\nmakers, researchers, and agricultural organizations, to make informed decisions\nregarding food security, supply chain management, and resource allocation. In\nthis paper, we tackle the problem of accurately mapping wheat fields out of\nsatellite images by introducing an improved pipeline for winter wheat\nsegmentation, as well as presenting a case study on a decade-long analysis of\nwheat mapping in Lebanon. We integrate a Temporal Spatial Vision Transformer\n(TSViT) with Parameter-Efficient Fine Tuning (PEFT) and a novel post-processing\npipeline based on the Fields of The World (FTW) framework. Our proposed\npipeline addresses key challenges encountered in existing approaches, such as\nthe clustering of small agricultural parcels in a single large field. By\nmerging wheat segmentation with precise field boundary extraction, our method\nproduces geometrically coherent and semantically rich maps that enable us to\nperform in-depth analysis such as tracking crop rotation pattern over years.\nExtensive evaluations demonstrate improved boundary delineation and field-level\nprecision, establishing the potential of the proposed framework in operational\nagricultural monitoring and historical trend analysis. By allowing for accurate\nmapping of wheat fields, this work lays the foundation for a range of critical\nstudies and future advances, including crop monitoring and yield estimation.\n","authors":["Hasan Wehbi","Hasan Nasrallah","Mohamad Hasan Zahweh","Zeinab Takach","Veera Ganesh Yalla","Ali J. Ghandour"],"pdf_url":"https://arxiv.org/pdf/2504.11366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11349v1","updated":"2025-04-15T16:21:47Z","published":"2025-04-15T16:21:47Z","title":"Explicit and Implicit Representations in AI-based 3D Reconstruction for\n  Radiology: A systematic literature review","summary":"  The demand for high-quality medical imaging in clinical practice and assisted\ndiagnosis has made 3D reconstruction in radiological imaging a key research\nfocus. Artificial intelligence (AI) has emerged as a promising approach to\nenhancing reconstruction accuracy while reducing acquisition and processing\ntime, thereby minimizing patient radiation exposure and discomfort and\nultimately benefiting clinical diagnosis. This review explores state-of-the-art\nAI-based 3D reconstruction algorithms in radiological imaging, categorizing\nthem into explicit and implicit approaches based on their underlying\nprinciples. Explicit methods include point-based, volume-based, and Gaussian\nrepresentations, while implicit methods encompass implicit prior embedding and\nneural radiance fields. Additionally, we examine commonly used evaluation\nmetrics and benchmark datasets. Finally, we discuss the current state of\ndevelopment, key challenges, and future research directions in this evolving\nfield. Our project available on: https://github.com/Bean-Young/AI4Med.\n","authors":["Yuezhe Yang","Boyu Yang","Yaqian Wang","Yang He","Xingbo Dong","Zhe Jin"],"pdf_url":"https://arxiv.org/pdf/2504.11349v1.pdf","comment":"43 pages, 5 figures, submit to Medical Image Analysis"},{"id":"http://arxiv.org/abs/2504.11347v1","updated":"2025-04-15T16:20:00Z","published":"2025-04-15T16:20:00Z","title":"DeepWheel: Generating a 3D Synthetic Wheel Dataset for Design and\n  Performance Evaluation","summary":"  Data-driven design is emerging as a powerful strategy to accelerate\nengineering innovation. However, its application to vehicle wheel design\nremains limited due to the lack of large-scale, high-quality datasets that\ninclude 3D geometry and physical performance metrics. To address this gap, this\nstudy proposes a synthetic design-performance dataset generation framework\nusing generative AI. The proposed framework first generates 2D rendered images\nusing Stable Diffusion, and then reconstructs the 3D geometry through 2.5D\ndepth estimation. Structural simulations are subsequently performed to extract\nengineering performance data. To further expand the design and performance\nspace, topology optimization is applied, enabling the generation of a more\ndiverse set of wheel designs. The final dataset, named DeepWheel, consists of\nover 6,000 photo-realistic images and 900 structurally analyzed 3D models. This\nmulti-modal dataset serves as a valuable resource for surrogate model training,\ndata-driven inverse design, and design space exploration. The proposed\nmethodology is also applicable to other complex design domains. The dataset is\nreleased under the Creative Commons Attribution-NonCommercial 4.0\nInternational(CC BY-NC 4.0) and is available on the\nhttps://www.smartdesignlab.org/datasets\n","authors":["Soyoung Yoo","Namwoo Kang"],"pdf_url":"https://arxiv.org/pdf/2504.11347v1.pdf","comment":"28 pages, 18 figures. Not yet submitted to a journal or conference"},{"id":"http://arxiv.org/abs/2504.11346v1","updated":"2025-04-15T16:19:07Z","published":"2025-04-15T16:19:07Z","title":"Seedream 3.0 Technical Report","summary":"  We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality.\n","authors":["Yu Gao","Lixue Gong","Qiushan Guo","Xiaoxia Hou","Zhichao Lai","Fanshi Li","Liang Li","Xiaochen Lian","Chao Liao","Liyang Liu","Wei Liu","Yichun Shi","Shiqi Sun","Yu Tian","Zhi Tian","Peng Wang","Rui Wang","Xuanda Wang","Xun Wang","Ye Wang","Guofeng Wu","Jie Wu","Xin Xia","Xuefeng Xiao","Zhonghua Zhai","Xinyu Zhang","Qi Zhang","Yuwei Zhang","Shijia Zhao","Jianchao Yang","Weilin Huang"],"pdf_url":"https://arxiv.org/pdf/2504.11346v1.pdf","comment":"Seedream 3.0 Technical Report"},{"id":"http://arxiv.org/abs/2504.11326v1","updated":"2025-04-15T16:02:47Z","published":"2025-04-15T16:02:47Z","title":"PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of\n  Complex Videos in the Wild","summary":"  This report provides a comprehensive overview of the 4th Pixel-level Video\nUnderstanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025.\nIt summarizes the challenge outcomes, participating methodologies, and future\nresearch directions. The challenge features two tracks: MOSE, which focuses on\ncomplex scene video object segmentation, and MeViS, which targets\nmotion-guided, language-based video segmentation. Both tracks introduce new,\nmore challenging datasets designed to better reflect real-world scenarios.\nThrough detailed evaluation and analysis, the challenge offers valuable\ninsights into the current state-of-the-art and emerging trends in complex video\nsegmentation. More information can be found on the workshop website:\nhttps://pvuw.github.io/.\n","authors":["Henghui Ding","Chang Liu","Nikhila Ravi","Shuting He","Yunchao Wei","Song Bai","Philip Torr","Kehuan Song","Xinglin Xie","Kexin Zhang","Licheng Jiao","Lingling Li","Shuyuan Yang","Xuqiang Cao","Linnan Zhao","Jiaxuan Zhao","Fang Liu","Mengjiao Wang","Junpei Zhang","Xu Liu","Yuting Yang","Mengru Ma","Hao Fang","Runmin Cong","Xiankai Lu","Zhiyang Che","Wei Zhan","Tianming Liang","Haichao Jiang","Wei-Shi Zheng","Jian-Fang Hu","Haobo Yuan","Xiangtai Li","Tao Zhang","Lu Qi","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2504.11326v1.pdf","comment":"Workshop Page: https://pvuw.github.io/. arXiv admin note: text\n  overlap with arXiv:2504.00476, arXiv:2504.05178"},{"id":"http://arxiv.org/abs/2504.11309v1","updated":"2025-04-15T15:50:54Z","published":"2025-04-15T15:50:54Z","title":"Big Brother is Watching: Proactive Deepfake Detection via Learnable\n  Hidden Face","summary":"  As deepfake technologies continue to advance, passive detection methods\nstruggle to generalize with various forgery manipulations and datasets.\nProactive defense techniques have been actively studied with the primary aim of\npreventing deepfake operation effectively working. In this paper, we aim to\nbridge the gap between passive detection and proactive defense, and seek to\nsolve the detection problem utilizing a proactive methodology. Inspired by\nseveral watermarking-based forensic methods, we explore a novel detection\nframework based on the concept of ``hiding a learnable face within a face''.\nSpecifically, relying on a semi-fragile invertible steganography network, a\nsecret template image is embedded into a host image imperceptibly, acting as an\nindicator monitoring for any malicious image forgery when being restored by the\ninverse steganography process. Instead of being manually specified, the secret\ntemplate is optimized during training to resemble a neutral facial appearance,\njust like a ``big brother'' hidden in the image to be protected. By\nincorporating a self-blending mechanism and robustness learning strategy with a\nsimulative transmission channel, a robust detector is built to accurately\ndistinguish if the steganographic image is maliciously tampered or benignly\nprocessed. Finally, extensive experiments conducted on multiple datasets\ndemonstrate the superiority of the proposed approach over competing passive and\nproactive detection methods.\n","authors":["Hongbo Li","Shangchao Yang","Ruiyang Xia","Lin Yuan","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2504.11309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11307v1","updated":"2025-04-15T15:48:51Z","published":"2025-04-15T15:48:51Z","title":"Uncertainty Estimation for Trust Attribution to Speed-of-Sound\n  Reconstruction with Variational Networks","summary":"  Speed-of-sound (SoS) is a biomechanical characteristic of tissue, and its\nimaging can provide a promising biomarker for diagnosis. Reconstructing SoS\nimages from ultrasound acquisitions can be cast as a limited-angle\ncomputed-tomography problem, with Variational Networks being a promising\nmodel-based deep learning solution. Some acquired data frames may, however, get\ncorrupted by noise due to, e.g., motion, lack of contact, and acoustic shadows,\nwhich in turn negatively affects the resulting SoS reconstructions. We propose\nto use the uncertainty in SoS reconstructions to attribute trust to each\nindividual acquired frame. Given multiple acquisitions, we then use an\nuncertainty based automatic selection among these retrospectively, to improve\ndiagnostic decisions. We investigate uncertainty estimation based on Monte\nCarlo Dropout and Bayesian Variational Inference. We assess our automatic frame\nselection method for differential diagnosis of breast cancer, distinguishing\nbetween benign fibroadenoma and malignant carcinoma. We evaluate 21 lesions\nclassified as BI-RADS~4, which represents suspicious cases for probable\nmalignancy. The most trustworthy frame among four acquisitions of each lesion\nwas identified using uncertainty based criteria. Selecting a frame informed by\nuncertainty achieved an area under curve of 76% and 80% for Monte Carlo Dropout\nand Bayesian Variational Inference, respectively, superior to any\nuncertainty-uninformed baselines with the best one achieving 64%. A novel use\nof uncertainty estimation is proposed for selecting one of multiple data\nacquisitions for further processing and decision making.\n","authors":["Sonia Laguna","Lin Zhang","Can Deniz Bezek","Monika Farkas","Dieter Schweizer","Rahel A. Kubik-Huch","Orcun Goksel"],"pdf_url":"https://arxiv.org/pdf/2504.11307v1.pdf","comment":"Published at the International Journal of Computer Assisted Radiology\n  and Surgery. Presented at the 16th International Conference on Information\n  Processing in Computer-Assisted Interventions 2025"},{"id":"http://arxiv.org/abs/2504.11306v1","updated":"2025-04-15T15:46:17Z","published":"2025-04-15T15:46:17Z","title":"Context-Aware Palmprint Recognition via a Relative Similarity Metric","summary":"  We propose a new approach to matching mechanism for palmprint recognition by\nintroducing a Relative Similarity Metric (RSM) that enhances the robustness and\ndiscriminability of existing matching frameworks. While conventional systems\nrely on direct pairwise similarity measures, such as cosine or Euclidean\ndistances, these metrics fail to capture how a pairwise similarity compares\nwithin the context of the entire dataset. Our method addresses this by\nevaluating the relative consistency of similarity scores across up to all\nidentities, allowing for better suppression of false positives and negatives.\nApplied atop the CCNet architecture, our method achieves a new state-of-the-art\n0.000036% Equal Error Rate (EER) on the Tongji dataset, outperforming previous\nmethods and demonstrating the efficacy of incorporating relational structure\ninto the palmprint matching process.\n","authors":["Trinnhallen Brisley","Aryan Gandhi","Joseph Magen"],"pdf_url":"https://arxiv.org/pdf/2504.11306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11305v1","updated":"2025-04-15T15:45:59Z","published":"2025-04-15T15:45:59Z","title":"CFIS-YOLO: A Lightweight Multi-Scale Fusion Network for Edge-Deployable\n  Wood Defect Detection","summary":"  Wood defect detection is critical for ensuring quality control in the wood\nprocessing industry. However, current industrial applications face two major\nchallenges: traditional methods are costly, subjective, and labor-intensive,\nwhile mainstream deep learning models often struggle to balance detection\naccuracy and computational efficiency for edge deployment. To address these\nissues, this study proposes CFIS-YOLO, a lightweight object detection model\noptimized for edge devices. The model introduces an enhanced C2f structure, a\ndynamic feature recombination module, and a novel loss function that\nincorporates auxiliary bounding boxes and angular constraints. These\ninnovations improve multi-scale feature fusion and small object localization\nwhile significantly reducing computational overhead. Evaluated on a public wood\ndefect dataset, CFIS-YOLO achieves a mean Average Precision (mAP@0.5) of\n77.5\\%, outperforming the baseline YOLOv10s by 4 percentage points. On SOPHON\nBM1684X edge devices, CFIS-YOLO delivers 135 FPS, reduces power consumption to\n17.3\\% of the original implementation, and incurs only a 0.5 percentage point\ndrop in mAP. These results demonstrate that CFIS-YOLO is a practical and\neffective solution for real-world wood defect detection in resource-constrained\nenvironments.\n","authors":["Jincheng Kang","Yi Cen","Yigang Cen","Ke Wang","Yuhan Liu"],"pdf_url":"https://arxiv.org/pdf/2504.11305v1.pdf","comment":"10 pages, 11 figures"},{"id":"http://arxiv.org/abs/2504.09209v2","updated":"2025-04-15T15:41:20Z","published":"2025-04-12T13:30:16Z","title":"EchoMask: Speech-Queried Attention-based Mask Modeling for Holistic\n  Co-Speech Motion Generation","summary":"  Masked modeling framework has shown promise in co-speech motion generation.\nHowever, it struggles to identify semantically significant frames for effective\nmotion masking. In this work, we propose a speech-queried attention-based mask\nmodeling framework for co-speech motion generation. Our key insight is to\nleverage motion-aligned speech features to guide the masked motion modeling\nprocess, selectively masking rhythm-related and semantically expressive motion\nframes. Specifically, we first propose a motion-audio alignment module (MAM) to\nconstruct a latent motion-audio joint space. In this space, both low-level and\nhigh-level speech features are projected, enabling motion-aligned speech\nrepresentation using learnable speech queries. Then, a speech-queried attention\nmechanism (SQA) is introduced to compute frame-level attention scores through\ninteractions between motion keys and speech queries, guiding selective masking\ntoward motion frames with high attention scores. Finally, the motion-aligned\nspeech features are also injected into the generation network to facilitate\nco-speech motion generation. Qualitative and quantitative evaluations confirm\nthat our method outperforms existing state-of-the-art approaches, successfully\nproducing high-quality co-speech motion.\n","authors":["Xiangyue Zhang","Jianfang Li","Jiaxu Zhang","Jianqiang Ren","Liefeng Bo","Zhigang Tu"],"pdf_url":"https://arxiv.org/pdf/2504.09209v2.pdf","comment":"12 pages, 12 figures"},{"id":"http://arxiv.org/abs/2503.11496v2","updated":"2025-04-15T15:35:24Z","published":"2025-03-14T15:21:54Z","title":"Cognitive Disentanglement for Referring Multi-Object Tracking","summary":"  As a significant application of multi-source information fusion in\nintelligent transportation perception systems, Referring Multi-Object Tracking\n(RMOT) involves localizing and tracking specific objects in video sequences\nbased on language references. However, existing RMOT approaches often treat\nlanguage descriptions as holistic embeddings and struggle to effectively\nintegrate the rich semantic information contained in language expressions with\nvisual features. This limitation is especially apparent in complex scenes\nrequiring comprehensive understanding of both static object attributes and\nspatial motion information. In this paper, we propose a Cognitive\nDisentanglement for Referring Multi-Object Tracking (CDRMT) framework that\naddresses these challenges. It adapts the \"what\" and \"where\" pathways from the\nhuman visual processing system to RMOT tasks. Specifically, our framework first\nestablishes cross-modal connections while preserving modality-specific\ncharacteristics. It then disentangles language descriptions and hierarchically\ninjects them into object queries, refining object understanding from coarse to\nfine-grained semantic levels. Finally, we reconstruct language representations\nbased on visual features, ensuring that tracked objects faithfully reflect the\nreferring expression. Extensive experiments on different benchmark datasets\ndemonstrate that CDRMT achieves substantial improvements over state-of-the-art\nmethods, with average gains of 6.0% in HOTA score on Refer-KITTI and 3.2% on\nRefer-KITTI-V2. Our approach advances the state-of-the-art in RMOT while\nsimultaneously providing new insights into multi-source information fusion.\n","authors":["Shaofeng Liang","Runwei Guan","Wangwang Lian","Daizong Liu","Xiaolou Sun","Dongming Wu","Yutao Yue","Weiping Ding","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.11496v2.pdf","comment":"26 pages, 11 figures"},{"id":"http://arxiv.org/abs/2504.11295v1","updated":"2025-04-15T15:33:49Z","published":"2025-04-15T15:33:49Z","title":"Autoregressive Distillation of Diffusion Transformers","summary":"  Diffusion models with transformer architectures have demonstrated promising\ncapabilities in generating high-fidelity images and scalability for high\nresolution. However, iterative sampling process required for synthesis is very\nresource-intensive. A line of work has focused on distilling solutions to\nprobability flow ODEs into few-step student models. Nevertheless, existing\nmethods have been limited by their reliance on the most recent denoised samples\nas input, rendering them susceptible to exposure bias. To address this\nlimitation, we propose AutoRegressive Distillation (ARD), a novel approach that\nleverages the historical trajectory of the ODE to predict future steps. ARD\noffers two key benefits: 1) it mitigates exposure bias by utilizing a predicted\nhistorical trajectory that is less susceptible to accumulated errors, and 2) it\nleverages the previous history of the ODE trajectory as a more effective source\nof coarse-grained information. ARD modifies the teacher transformer\narchitecture by adding token-wise time embedding to mark each input from the\ntrajectory history and employs a block-wise causal attention mask for training.\nFurthermore, incorporating historical inputs only in lower transformer layers\nenhances performance and efficiency. We validate the effectiveness of ARD in a\nclass-conditioned generation on ImageNet and T2I synthesis. Our model achieves\na $5\\times$ reduction in FID degradation compared to the baseline methods while\nrequiring only 1.1\\% extra FLOPs on ImageNet-256. Moreover, ARD reaches FID of\n1.84 on ImageNet-256 in merely 4 steps and outperforms the publicly available\n1024p text-to-image distilled models in prompt adherence score with a minimal\ndrop in FID compared to the teacher. Project page:\nhttps://github.com/alsdudrla10/ARD.\n","authors":["Yeongmin Kim","Sotiris Anagnostidis","Yuming Du","Edgar Schönfeld","Jonas Kohler","Markos Georgopoulos","Albert Pumarola","Ali Thabet","Artsiom Sanakoyeu"],"pdf_url":"https://arxiv.org/pdf/2504.11295v1.pdf","comment":"CVPR 2025 Oral"},{"id":"http://arxiv.org/abs/2503.16188v3","updated":"2025-04-15T15:31:31Z","published":"2025-03-20T14:37:45Z","title":"Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual\n  Reinforcement Fine-Tuning","summary":"  This paper investigates the thinking process in rule-based reinforcement\nlearning fine-tuning (RFT) for multi-modal large language models (MLLMs). We\nfirst propose CLS-RL for classification, using verifiable rewards to encourage\nMLLM thinking. Experiments show CLS-RL significantly outperforms SFT and yields\na 'free-lunch' generalization effect (improving performance on unseen datasets\nafter training on one dataset). We then question if this explicit thinking is\nalways necessary for RFT. Challenging convention that explicit thinking is\ncrucial for RFT, we introduce No-Thinking-RL, minimizing thinking via a simple\nequality accuracy reward. Experiments show No-Thinking-RL surpasses CLS-RL in\nin-domain and generalization abilities, with significantly less fine-tuning\ntime. This suggests reducing thinking can improve MLLM fine-tuning efficiency\nand effectiveness for certain visual tasks. We hypothesize explicit thinking\nnegatively impacts reward convergence during RFT. To test this, we propose the\nThink-After-Answerwer method to let models first output the answer and then\ngenerate thinking process to alliviate the negative impact of thinking. We\nfurther test No-Thinking-RL on diverse tasks (including math, spatial, puzzles)\nwith 2B and 7B models. For 2B models, No-Thinking-RL outperforms thinking-based\nRFT for all tasks, even on math, with Think-After-Answerwer performing\nintermediately. For 7B models, performance is comparable on simple visual\ntasks, but RFT with thinking excels on complex reasoning (math). This implies\nwhen dealing with complex math problems, smaller models struggle with\ngenerating effective reasoning, hurting performance on complex tasks.\nConversely, for simple visual tasks, thinking is not indispensable, and its\nremoval can boost performance and reduce training time. We hope our findings\noffer insights for better understanding the effect of the thinking process in\nRFT.\n","authors":["Ming Li","Jike Zhong","Shitian Zhao","Yuxiang Lai","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.16188v3.pdf","comment":"Preprint, work in progress. Add results on math, cvbench, and puzzle"},{"id":"http://arxiv.org/abs/2303.13913v2","updated":"2025-04-15T15:30:02Z","published":"2023-03-24T10:59:17Z","title":"GarmentTracking: Category-Level Garment Pose Tracking","summary":"  Garments are important to humans. A visual system that can estimate and track\nthe complete garment pose can be useful for many downstream tasks and\nreal-world applications. In this work, we present a complete package to address\nthe category-level garment pose tracking task: (1) A recording system\nVR-Garment, with which users can manipulate virtual garment models in\nsimulation through a VR interface. (2) A large-scale dataset VR-Folding, with\ncomplex garment pose configurations in manipulation like flattening and\nfolding. (3) An end-to-end online tracking framework GarmentTracking, which\npredicts complete garment pose both in canonical space and task space given a\npoint cloud sequence. Extensive experiments demonstrate that the proposed\nGarmentTracking achieves great performance even when the garment has large\nnon-rigid deformation. It outperforms the baseline approach on both speed and\naccuracy. We hope our proposed solution can serve as a platform for future\nresearch. Codes and datasets are available in\nhttps://garment-tracking.robotflow.ai.\n","authors":["Han Xue","Wenqiang Xu","Jieyi Zhang","Tutian Tang","Yutong Li","Wenxin Du","Ruolin Ye","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2303.13913v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2504.11289v1","updated":"2025-04-15T15:29:11Z","published":"2025-04-15T15:29:11Z","title":"UniAnimate-DiT: Human Image Animation with Large-Scale Video Diffusion\n  Transformer","summary":"  This report presents UniAnimate-DiT, an advanced project that leverages the\ncutting-edge and powerful capabilities of the open-source Wan2.1 model for\nconsistent human image animation. Specifically, to preserve the robust\ngenerative capabilities of the original Wan2.1 model, we implement Low-Rank\nAdaptation (LoRA) technique to fine-tune a minimal set of parameters,\nsignificantly reducing training memory overhead. A lightweight pose encoder\nconsisting of multiple stacked 3D convolutional layers is designed to encode\nmotion information of driving poses. Furthermore, we adopt a simple\nconcatenation operation to integrate the reference appearance into the model\nand incorporate the pose information of the reference image for enhanced pose\nalignment. Experimental results show that our approach achieves visually\nappearing and temporally consistent high-fidelity animations. Trained on 480p\n(832x480) videos, UniAnimate-DiT demonstrates strong generalization\ncapabilities to seamlessly upscale to 720P (1280x720) during inference. The\ntraining and inference code is publicly available at\nhttps://github.com/ali-vilab/UniAnimate-DiT.\n","authors":["Xiang Wang","Shiwei Zhang","Longxiang Tang","Yingya Zhang","Changxin Gao","Yuehuan Wang","Nong Sang"],"pdf_url":"https://arxiv.org/pdf/2504.11289v1.pdf","comment":"The training and inference code (based on Wan2.1) is available at\n  https://github.com/ali-vilab/UniAnimate-DiT"},{"id":"http://arxiv.org/abs/2410.02240v5","updated":"2025-04-15T15:28:00Z","published":"2024-10-03T06:25:53Z","title":"SCA: Highly Efficient Semantic-Consistent Unrestricted Adversarial\n  Attack","summary":"  Deep neural network based systems deployed in sensitive environments are\nvulnerable to adversarial attacks. Unrestricted adversarial attacks typically\nmanipulate the semantic content of an image (e.g., color or texture) to create\nadversarial examples that are both effective and photorealistic. Recent works\nhave utilized the diffusion inversion process to map images into a latent\nspace, where high-level semantics are manipulated by introducing perturbations.\nHowever, they often results in substantial semantic distortions in the denoised\noutput and suffers from low efficiency. In this study, we propose a novel\nframework called Semantic-Consistent Unrestricted Adversarial Attacks (SCA),\nwhich employs an inversion method to extract edit-friendly noise maps and\nutilizes Multimodal Large Language Model (MLLM) to provide semantic guidance\nthroughout the process. Under the condition of rich semantic information\nprovided by MLLM, we perform the DDPM denoising process of each step using a\nseries of edit-friendly noise maps, and leverage DPM Solver++ to accelerate\nthis process, enabling efficient sampling with semantic consistency. Compared\nto existing methods, our framework enables the efficient generation of\nadversarial examples that exhibit minimal discernible semantic changes.\nConsequently, we for the first time introduce Semantic-Consistent Adversarial\nExamples (SCAE). Extensive experiments and visualizations have demonstrated the\nhigh efficiency of SCA, particularly in being on average 12 times faster than\nthe state-of-the-art attacks. Our research can further draw attention to the\nsecurity of multimedia information.\n","authors":["Zihao Pan","Weibin Wu","Yuhang Cao","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.02240v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11286v1","updated":"2025-04-15T15:26:28Z","published":"2025-04-15T15:26:28Z","title":"Efficient Medical Image Restoration via Reliability Guided Learning in\n  Frequency Domain","summary":"  Medical image restoration tasks aim to recover high-quality images from\ndegraded observations, exhibiting emergent desires in many clinical scenarios,\nsuch as low-dose CT image denoising, MRI super-resolution, and MRI artifact\nremoval. Despite the success achieved by existing deep learning-based\nrestoration methods with sophisticated modules, they struggle with rendering\ncomputationally-efficient reconstruction results. Moreover, they usually ignore\nthe reliability of the restoration results, which is much more urgent in\nmedical systems. To alleviate these issues, we present LRformer, a Lightweight\nTransformer-based method via Reliability-guided learning in the frequency\ndomain. Specifically, inspired by the uncertainty quantification in Bayesian\nneural networks (BNNs), we develop a Reliable Lesion-Semantic Prior Producer\n(RLPP). RLPP leverages Monte Carlo (MC) estimators with stochastic sampling\noperations to generate sufficiently-reliable priors by performing multiple\ninferences on the foundational medical image segmentation model, MedSAM.\nAdditionally, instead of directly incorporating the priors in the spatial\ndomain, we decompose the cross-attention (CA) mechanism into real symmetric and\nimaginary anti-symmetric parts via fast Fourier transform (FFT), resulting in\nthe design of the Guided Frequency Cross-Attention (GFCA) solver. By leveraging\nthe conjugated symmetric property of FFT, GFCA reduces the computational\ncomplexity of naive CA by nearly half. Extensive experimental results in\nvarious tasks demonstrate the superiority of the proposed LRformer in both\neffectiveness and efficiency.\n","authors":["Pengcheng Zheng","Kecheng Chen","Jiaxin Huang","Bohao Chen","Ju Liu","Yazhou Ren","Xiaorong Pu"],"pdf_url":"https://arxiv.org/pdf/2504.11286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11271v1","updated":"2025-04-15T15:12:57Z","published":"2025-04-15T15:12:57Z","title":"Distillation-Supervised Convolutional Low-Rank Adaptation for Efficient\n  Image Super-Resolution","summary":"  Convolutional neural networks (CNNs) have been widely used in efficient image\nsuper-resolution. However, for CNN-based methods, performance gains often\nrequire deeper networks and larger feature maps, which increase complexity and\ninference costs. Inspired by LoRA's success in fine-tuning large language\nmodels, we explore its application to lightweight models and propose\nDistillation-Supervised Convolutional Low-Rank Adaptation (DSCLoRA), which\nimproves model performance without increasing architectural complexity or\ninference costs. Specifically, we integrate ConvLoRA into the efficient SR\nnetwork SPAN by replacing the SPAB module with the proposed SConvLB module and\nincorporating ConvLoRA layers into both the pixel shuffle block and its\npreceding convolutional layer. DSCLoRA leverages low-rank decomposition for\nparameter updates and employs a spatial feature affinity-based knowledge\ndistillation strategy to transfer second-order statistical information from\nteacher models (pre-trained SPAN) to student models (ours). This method\npreserves the core knowledge of lightweight models and facilitates optimal\nsolution discovery under certain conditions. Experiments on benchmark datasets\nshow that DSCLoRA improves PSNR and SSIM over SPAN while maintaining its\nefficiency and competitive image quality. Notably, DSCLoRA ranked first in the\nOverall Performance Track of the NTIRE 2025 Efficient Super-Resolution\nChallenge. Our code and models are made publicly available at\nhttps://github.com/Yaozzz666/DSCF-SR.\n","authors":["Xinning Chai","Yao Zhang","Yuxuan Zhang","Zhengxue Cheng","Yingsheng Qin","Yucai Yang","Li Song"],"pdf_url":"https://arxiv.org/pdf/2504.11271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11268v1","updated":"2025-04-15T15:10:46Z","published":"2025-04-15T15:10:46Z","title":"Single-Input Multi-Output Model Merging: Leveraging Foundation Models\n  for Dense Multi-Task Learning","summary":"  Model merging is a flexible and computationally tractable approach to merge\nsingle-task checkpoints into a multi-task model. Prior work has solely focused\non constrained multi-task settings where there is a one-to-one mapping between\na sample and a task, overlooking the paradigm where multiple tasks may operate\non the same sample, e.g., scene understanding. In this paper, we focus on the\nmulti-task setting with single-input-multiple-outputs (SIMO) and show that it\nqualitatively differs from the single-input-single-output model merging\nsettings studied in the literature due to the existence of task-specific\ndecoders and diverse loss objectives. We identify that existing model merging\nmethods lead to significant performance degradation, primarily due to\nrepresentation misalignment between the merged encoder and task-specific\ndecoders. We propose two simple and efficient fixes for the SIMO setting to\nre-align the feature representation after merging. Compared to joint\nfine-tuning, our approach is computationally effective and flexible, and sheds\nlight into identifying task relationships in an offline manner. Experiments on\nNYUv2, Cityscapes, and a subset of the Taskonomy dataset demonstrate: (1) task\narithmetic suffices to enable multi-task capabilities; however, the\nrepresentations generated by the merged encoder has to be re-aligned with the\ntask-specific heads; (2) the proposed architecture rivals traditional\nmulti-task learning in performance but requires fewer samples and training\nsteps by leveraging the existence of task-specific models.\n","authors":["Juan Garcia Giraldo","Nikolaos Dimitriadis","Ke Wang","Pascal Frossard"],"pdf_url":"https://arxiv.org/pdf/2504.11268v1.pdf","comment":"22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2504.10331v2","updated":"2025-04-15T15:06:33Z","published":"2025-04-14T15:39:31Z","title":"LL-Gaussian: Low-Light Scene Reconstruction and Enhancement via Gaussian\n  Splatting for Novel View Synthesis","summary":"  Novel view synthesis (NVS) in low-light scenes remains a significant\nchallenge due to degraded inputs characterized by severe noise, low dynamic\nrange (LDR) and unreliable initialization. While recent NeRF-based approaches\nhave shown promising results, most suffer from high computational costs, and\nsome rely on carefully captured or pre-processed data--such as RAW sensor\ninputs or multi-exposure sequences--which severely limits their practicality.\nIn contrast, 3D Gaussian Splatting (3DGS) enables real-time rendering with\ncompetitive visual fidelity; however, existing 3DGS-based methods struggle with\nlow-light sRGB inputs, resulting in unstable Gaussian initialization and\nineffective noise suppression. To address these challenges, we propose\nLL-Gaussian, a novel framework for 3D reconstruction and enhancement from\nlow-light sRGB images, enabling pseudo normal-light novel view synthesis. Our\nmethod introduces three key innovations: 1) an end-to-end Low-Light Gaussian\nInitialization Module (LLGIM) that leverages dense priors from learning-based\nMVS approach to generate high-quality initial point clouds; 2) a dual-branch\nGaussian decomposition model that disentangles intrinsic scene properties\n(reflectance and illumination) from transient interference, enabling stable and\ninterpretable optimization; 3) an unsupervised optimization strategy guided by\nboth physical constrains and diffusion prior to jointly steer decomposition and\nenhancement. Additionally, we contribute a challenging dataset collected in\nextreme low-light environments and demonstrate the effectiveness of\nLL-Gaussian. Compared to state-of-the-art NeRF-based methods, LL-Gaussian\nachieves up to 2,000 times faster inference and reduces training time to just\n2%, while delivering superior reconstruction and rendering quality.\n","authors":["Hao Sun","Fenggen Yu","Huiyao Xu","Tao Zhang","Changqing Zou"],"pdf_url":"https://arxiv.org/pdf/2504.10331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11262v1","updated":"2025-04-15T15:02:10Z","published":"2025-04-15T15:02:10Z","title":"Enhanced Small Target Detection via Multi-Modal Fusion and Attention\n  Mechanisms: A YOLOv5 Approach","summary":"  With the rapid development of information technology, modern warfare\nincreasingly relies on intelligence, making small target detection critical in\nmilitary applications. The growing demand for efficient, real-time detection\nhas created challenges in identifying small targets in complex environments due\nto interference. To address this, we propose a small target detection method\nbased on multi-modal image fusion and attention mechanisms. This method\nleverages YOLOv5, integrating infrared and visible light data along with a\nconvolutional attention module to enhance detection performance. The process\nbegins with multi-modal dataset registration using feature point matching,\nensuring accurate network training. By combining infrared and visible light\nfeatures with attention mechanisms, the model improves detection accuracy and\nrobustness. Experimental results on anti-UAV and Visdrone datasets demonstrate\nthe effectiveness and practicality of our approach, achieving superior\ndetection results for small and dim targets.\n","authors":["Xiaoxiao Ma","Junxiong Tong"],"pdf_url":"https://arxiv.org/pdf/2504.11262v1.pdf","comment":"Accepted by ATC 2024"},{"id":"http://arxiv.org/abs/2504.11257v1","updated":"2025-04-15T14:56:21Z","published":"2025-04-15T14:56:21Z","title":"UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction\n  Synthesis","summary":"  Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation.In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://colmon46.github.io/i2e-bench-leaderboard/\n","authors":["Xinyi Liu","Xiaoyi Zhang","Ziyun Zhang","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2504.11257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11249v1","updated":"2025-04-15T14:46:25Z","published":"2025-04-15T14:46:25Z","title":"Cryo-em images are intrinsically low dimensional","summary":"  Simulation-based inference provides a powerful framework for cryo-electron\nmicroscopy, employing neural networks in methods like CryoSBI to infer\nbiomolecular conformations via learned latent representations. This latent\nspace represents a rich opportunity, encoding valuable information about the\nphysical system and the inference process. Harnessing this potential hinges on\nunderstanding the underlying geometric structure of these representations. We\ninvestigate this structure by applying manifold learning techniques to CryoSBI\nrepresentations of hemagglutinin (simulated and experimental). We reveal that\nthese high-dimensional data inherently populate low-dimensional, smooth\nmanifolds, with simulated data effectively covering the experimental\ncounterpart. By characterizing the manifold's geometry using Diffusion Maps and\nidentifying its principal axes of variation via coordinate interpretation\nmethods, we establish a direct link between the latent structure and key\nphysical parameters. Discovering this intrinsic low-dimensionality and\ninterpretable geometric organization not only validates the CryoSBI approach\nbut enables us to learn more from the data structure and provides opportunities\nfor improving future inference strategies by exploiting this revealed manifold\ngeometry.\n","authors":["Luke Evans","Octavian-Vlad Murad","Lars Dingeldein","Pilar Cossio","Roberto Covino","Marina Meila"],"pdf_url":"https://arxiv.org/pdf/2504.11249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11247v1","updated":"2025-04-15T14:45:51Z","published":"2025-04-15T14:45:51Z","title":"Next-Future: Sample-Efficient Policy Learning for Robotic-Arm Tasks","summary":"  Hindsight Experience Replay (HER) is widely regarded as the state-of-the-art\nalgorithm for achieving sample-efficient multi-goal reinforcement learning (RL)\nin robotic manipulation tasks with binary rewards. HER facilitates learning\nfrom failed attempts by replaying trajectories with redefined goals. However,\nit relies on a heuristic-based replay method that lacks a principled framework.\nTo address this limitation, we introduce a novel replay strategy,\n\"Next-Future\", which focuses on rewarding single-step transitions. This\napproach significantly enhances sample efficiency and accuracy in learning\nmulti-goal Markov decision processes (MDPs), particularly under stringent\naccuracy requirements -- a critical aspect for performing complex and precise\nrobotic-arm tasks. We demonstrate the efficacy of our method by highlighting\nhow single-step learning enables improved value approximation within the\nmulti-goal RL framework. The performance of the proposed replay strategy is\nevaluated across eight challenging robotic manipulation tasks, using ten random\nseeds for training. Our results indicate substantial improvements in sample\nefficiency for seven out of eight tasks and higher success rates in six tasks.\nFurthermore, real-world experiments validate the practical feasibility of the\nlearned policies, demonstrating the potential of \"Next-Future\" in solving\ncomplex robotic-arm tasks.\n","authors":["Fikrican Özgür","René Zurbrügg","Suryansh Kumar"],"pdf_url":"https://arxiv.org/pdf/2504.11247v1.pdf","comment":"10 pages, 9 figures, 6 tables"},{"id":"http://arxiv.org/abs/2504.10458v2","updated":"2025-04-15T14:42:43Z","published":"2025-04-14T17:45:54Z","title":"GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI\n  Agents","summary":"  Existing efforts in building Graphical User Interface (GUI) agents largely\nrely on the training paradigm of supervised fine-tuning on Large\nVision-Language Models (LVLMs). However, this approach not only demands\nextensive amounts of training data but also struggles to effectively understand\nGUI screenshots and generalize to unseen interfaces. The issue significantly\nlimits its application in real-world scenarios, especially for high-level\ntasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models\n(e.g., DeepSeek-R1), which efficiently enhances the problem-solving\ncapabilities of large language models in real-world settings, we propose \\name,\nthe first reinforcement learning framework designed to enhance the GUI\ncapabilities of LVLMs in high-level real-world task scenarios, through unified\naction space rule modeling. By leveraging a small amount of carefully curated\nhigh-quality data across multiple platforms (including Windows, Linux, MacOS,\nAndroid, and Web) and employing policy optimization algorithms such as Group\nRelative Policy Optimization (GRPO) to update the model, \\name achieves\nsuperior performance using only 0.02\\% of the data (3K vs. 13M) compared to\nprevious state-of-the-art methods like OS-Atlas across eight benchmarks\nspanning three different platforms (mobile, desktop, and web). These results\ndemonstrate the immense potential of reinforcement learning based on unified\naction space rule modeling in improving the execution capabilities of LVLMs for\nreal-world GUI agent tasks.\n","authors":["Xiaobo Xia","Run Luo"],"pdf_url":"https://arxiv.org/pdf/2504.10458v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11232v1","updated":"2025-04-15T14:33:25Z","published":"2025-04-15T14:33:25Z","title":"Leveraging multimodal explanatory annotations for video interpretation\n  with Modality Specific Dataset","summary":"  We examine the impact of concept-informed supervision on multimodal video\ninterpretation models using MOByGaze, a dataset containing human-annotated\nexplanatory concepts. We introduce Concept Modality Specific Datasets (CMSDs),\nwhich consist of data subsets categorized by the modality (visual, textual, or\naudio) of annotated concepts. Models trained on CMSDs outperform those using\ntraditional legacy training in both early and late fusion approaches. Notably,\nthis approach enables late fusion models to achieve performance close to that\nof early fusion models. These findings underscore the importance of\nmodality-specific annotations in developing robust, self-explainable video\nmodels and contribute to advancing interpretable multimodal learning in complex\nvideo analysis.\n","authors":["Elisa Ancarani","Julie Tores","Lucile Sassatelli","Rémy Sun","Hui-Yin Wu","Frédéric Precioso"],"pdf_url":"https://arxiv.org/pdf/2504.11232v1.pdf","comment":"6 pages, 8 Figures"},{"id":"http://arxiv.org/abs/2504.11230v1","updated":"2025-04-15T14:30:26Z","published":"2025-04-15T14:30:26Z","title":"CAP-Net: A Unified Network for 6D Pose and Size Estimation of\n  Categorical Articulated Parts from a Single RGB-D Image","summary":"  This paper tackles category-level pose estimation of articulated objects in\nrobotic manipulation tasks and introduces a new benchmark dataset. While recent\nmethods estimate part poses and sizes at the category level, they often rely on\ngeometric cues and complex multi-stage pipelines that first segment parts from\nthe point cloud, followed by Normalized Part Coordinate Space (NPCS) estimation\nfor 6D poses. These approaches overlook dense semantic cues from RGB images,\nleading to suboptimal accuracy, particularly for objects with small parts. To\naddress these limitations, we propose a single-stage Network, CAP-Net, for\nestimating the 6D poses and sizes of Categorical Articulated Parts. This method\ncombines RGB-D features to generate instance segmentation and NPCS\nrepresentations for each part in an end-to-end manner. CAP-Net uses a unified\nnetwork to simultaneously predict point-wise class labels, centroid offsets,\nand NPCS maps. A clustering algorithm then groups points of the same predicted\nclass based on their estimated centroid distances to isolate each part.\nFinally, the NPCS region of each part is aligned with the point cloud to\nrecover its final pose and size. To bridge the sim-to-real domain gap, we\nintroduce the RGBD-Art dataset, the largest RGB-D articulated dataset to date,\nfeaturing photorealistic RGB images and depth noise simulated from real\nsensors. Experimental evaluations on the RGBD-Art dataset demonstrate that our\nmethod significantly outperforms the state-of-the-art approach. Real-world\ndeployments of our model in robotic tasks underscore its robustness and\nexceptional sim-to-real transfer capabilities, confirming its substantial\npractical utility. Our dataset, code and pre-trained models are available on\nthe project page.\n","authors":["Jingshun Huang","Haitao Lin","Tianyu Wang","Yanwei Fu","Xiangyang Xue","Yi Zhu"],"pdf_url":"https://arxiv.org/pdf/2504.11230v1.pdf","comment":"To appear in CVPR 2025 (Highlight)"},{"id":"http://arxiv.org/abs/2503.07365v2","updated":"2025-04-15T14:22:45Z","published":"2025-03-10T14:23:12Z","title":"MM-Eureka: Exploring the Frontiers of Multimodal Reasoning with\n  Rule-based Reinforcement Learning","summary":"  DeepSeek R1, and o1 have demonstrated powerful reasoning capabilities in the\ntext domain through stable large-scale reinforcement learning. To enable\nbroader applications, some works have attempted to transfer these capabilities\nto multimodal reasoning. However, these efforts have been limited by the\nlimited difficulty of selected tasks and relatively small training scales,\nmaking it challenging to demonstrate strong multimodal reasoning abilities. To\naddress this gap, we introduce the MMK12 dataset and MM-EUREKA with 7B and 32B\nparameters. The former is a high-quality multimodal mathematics reasoning\ndataset featuring diverse knowledge domains with human-verified answers and\nsolution processes. The latter is a multimodal model employing rule-based\nreinforcement learning on MMK12, utilizing online filtering and two-stage\ntraining strategy to enhance training stability. MM-EUREKA demonstrates\nremarkable performance gains in multimodal mathematical reasoning,\noutperforming previous powerful models like InternVL2.5-78B or\nInternVL2.5-38B-MPO. In particular, MM-EUREKA achieves competitive or superior\nperformance compared to both open-source and closed-source models, and trails\nslightly behind o1 in multidisciplinary reasoning tasks. We open-source our\ncomplete pipeline to foster further research in this area. We release all our\ncodes, models, data, etc. at https://github.com/ModalMinds/MM-EUREKA\n","authors":["Fanqing Meng","Lingxiao Du","Zongkai Liu","Zhixiang Zhou","Quanfeng Lu","Daocheng Fu","Tiancheng Han","Botian Shi","Wenhai Wang","Junjun He","Kaipeng Zhang","Ping Luo","Yu Qiao","Qiaosheng Zhang","Wenqi Shao"],"pdf_url":"https://arxiv.org/pdf/2503.07365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11218v1","updated":"2025-04-15T14:21:47Z","published":"2025-04-15T14:21:47Z","title":"3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians","summary":"  3D affordance reasoning is essential in associating human instructions with\nthe functional regions of 3D objects, facilitating precise, task-oriented\nmanipulations in embodied AI. However, current methods, which predominantly\ndepend on sparse 3D point clouds, exhibit limited generalizability and\nrobustness due to their sensitivity to coordinate variations and the inherent\nsparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers\nhigh-fidelity, real-time rendering with minimal computational overhead by\nrepresenting scenes as dense, continuous distributions. This positions 3DGS as\na highly effective approach for capturing fine-grained affordance details and\nimproving recognition accuracy. Nevertheless, its full potential remains\nlargely untapped due to the absence of large-scale, 3DGS-specific affordance\ndatasets. To overcome these limitations, we present 3DAffordSplat, the first\nlarge-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning.\nThis dataset includes 23,677 Gaussian instances, 8,354 point cloud instances,\nand 6,631 manually annotated affordance labels, encompassing 21 object\ncategories and 18 affordance types. Building upon this dataset, we introduce\nAffordSplatNet, a novel model specifically designed for affordance reasoning\nusing 3DGS representations. AffordSplatNet features an innovative cross-modal\nstructure alignment module that exploits structural consistency priors to align\n3D point cloud and 3DGS representations, resulting in enhanced affordance\nrecognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat\ndataset significantly advances affordance learning within the 3DGS domain,\nwhile AffordSplatNet consistently outperforms existing methods across both seen\nand unseen settings, highlighting its robust generalization capabilities.\n","authors":["Zeming wei","Junyi Lin","Yang Liu","Weixing Chen","Jingzhou Luo","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2504.11218v1.pdf","comment":"The first large-scale 3D Gaussians Affordance Reasoning Benchmark"},{"id":"http://arxiv.org/abs/2410.20502v3","updated":"2025-04-15T14:06:28Z","published":"2024-10-27T16:28:28Z","title":"ARLON: Boosting Diffusion Transformers with Autoregressive Models for\n  Long Video Generation","summary":"  Text-to-video models have recently undergone rapid and substantial\nadvancements. Nevertheless, due to limitations in data and computational\nresources, achieving efficient generation of long videos with rich motion\ndynamics remains a significant challenge. To generate high-quality, dynamic,\nand temporally consistent long videos, this paper presents ARLON, a novel\nframework that boosts diffusion Transformers with autoregressive models for\nlong video generation, by integrating the coarse spatial and long-range\ntemporal information provided by the AR model to guide the DiT model.\nSpecifically, ARLON incorporates several key innovations: 1) A latent Vector\nQuantized Variational Autoencoder (VQ-VAE) compresses the input latent space of\nthe DiT model into compact visual tokens, bridging the AR and DiT models and\nbalancing the learning complexity and information density; 2) An adaptive\nnorm-based semantic injection module integrates the coarse discrete visual\nunits from the AR model into the DiT model, ensuring effective guidance during\nvideo generation; 3) To enhance the tolerance capability of noise introduced\nfrom the AR inference, the DiT model is trained with coarser visual latent\ntokens incorporated with an uncertainty sampling module. Experimental results\ndemonstrate that ARLON significantly outperforms the baseline OpenSora-V1.2 on\neight out of eleven metrics selected from VBench, with notable improvements in\ndynamic degree and aesthetic quality, while delivering competitive results on\nthe remaining three and simultaneously accelerating the generation process. In\naddition, ARLON achieves state-of-the-art performance in long video generation.\nDetailed analyses of the improvements in inference efficiency are presented,\nalongside a practical application that demonstrates the generation of long\nvideos using progressive text prompts. See demos of ARLON at\nhttp://aka.ms/arlon.\n","authors":["Zongyi Li","Shujie Hu","Shujie Liu","Long Zhou","Jeongsoo Choi","Lingwei Meng","Xun Guo","Jinyu Li","Hefei Ling","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.20502v3.pdf","comment":"Accepted at ICLR2025"},{"id":"http://arxiv.org/abs/2504.11202v1","updated":"2025-04-15T14:01:36Z","published":"2025-04-15T14:01:36Z","title":"Focal Split: Untethered Snapshot Depth from Differential Defocus","summary":"  We introduce Focal Split, a handheld, snapshot depth camera with fully\nonboard power and computing based on depth-from-differential-defocus (DfDD).\nFocal Split is passive, avoiding power consumption of light sources. Its\nachromatic optical system simultaneously forms two differentially defocused\nimages of the scene, which can be independently captured using two photosensors\nin a snapshot. The data processing is based on the DfDD theory, which\nefficiently computes a depth and a confidence value for each pixel with only\n500 floating point operations (FLOPs) per pixel from the camera measurements.\nWe demonstrate a Focal Split prototype, which comprises a handheld custom\ncamera system connected to a Raspberry Pi 5 for real-time data processing. The\nsystem consumes 4.9 W and is powered on a 5 V, 10,000 mAh battery. The\nprototype can measure objects with distances from 0.4 m to 1.2 m, outputting\n480$\\times$360 sparse depth maps at 2.1 frames per second (FPS) using\nunoptimized Python scripts. Focal Split is DIY friendly. A comprehensive guide\nto building your own Focal Split depth camera, code, and additional data can be\nfound at https://focal-split.qiguo.org.\n","authors":["Junjie Luo","John Mamish","Alan Fu","Thomas Concannon","Josiah Hester","Emma Alexander","Qi Guo"],"pdf_url":"https://arxiv.org/pdf/2504.11202v1.pdf","comment":"CVPR 2025, 8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.11199v1","updated":"2025-04-15T13:56:14Z","published":"2025-04-15T13:56:14Z","title":"Video Summarization with Large Language Models","summary":"  The exponential increase in video content poses significant challenges in\nterms of efficient navigation, search, and retrieval, thus requiring advanced\nvideo summarization techniques. Existing video summarization methods, which\nheavily rely on visual features and temporal dynamics, often fail to capture\nthe semantics of video content, resulting in incomplete or incoherent\nsummaries. To tackle the challenge, we propose a new video summarization\nframework that leverages the capabilities of recent Large Language Models\n(LLMs), expecting that the knowledge learned from massive data enables LLMs to\nevaluate video frames in a manner that better aligns with diverse semantics and\nhuman judgments, effectively addressing the inherent subjectivity in defining\nkeyframes. Our method, dubbed LLM-based Video Summarization (LLMVS), translates\nvideo frames into a sequence of captions using a Muti-modal Large Language\nModel (M-LLM) and then assesses the importance of each frame using an LLM,\nbased on the captions in its local context. These local importance scores are\nrefined through a global attention mechanism in the entire context of video\ncaptions, ensuring that our summaries effectively reflect both the details and\nthe overarching narrative. Our experimental results demonstrate the superiority\nof the proposed method over existing ones in standard benchmarks, highlighting\nthe potential of LLMs in the processing of multimedia content.\n","authors":["Min Jung Lee","Dayoung Gong","Minsu Cho"],"pdf_url":"https://arxiv.org/pdf/2504.11199v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.11266v2","updated":"2025-04-15T13:54:10Z","published":"2025-03-14T10:22:26Z","title":"CyclePose -- Leveraging Cycle-Consistency for Annotation-Free Nuclei\n  Segmentation in Fluorescence Microscopy","summary":"  In recent years, numerous neural network architectures specifically designed\nfor the instance segmentation of nuclei in microscopic images have been\nreleased. These models embed nuclei-specific priors to outperform generic\narchitectures like U-Nets; however, they require large annotated datasets,\nwhich are often not available. Generative models (GANs, diffusion models) have\nbeen used to compensate for this by synthesizing training data. These two-stage\napproaches are computationally expensive, as first a generative model and then\na segmentation model has to be trained. We propose CyclePose, a hybrid\nframework integrating synthetic data generation and segmentation training.\nCyclePose builds on a CycleGAN architecture, which allows unpaired translation\nbetween microscopy images and segmentation masks. We embed a segmentation model\ninto CycleGAN and leverage a cycle consistency loss for self-supervision.\nWithout annotated data, CyclePose outperforms other weakly or unsupervised\nmethods on two public datasets. Code is available at\nhttps://github.com/jonasutz/CyclePose\n","authors":["Jonas Utz","Stefan Vocht","Anne Tjorven Buessen","Dennis Possart","Fabian Wagner","Mareike Thies","Mingxuan Gu","Stefan Uderhardt","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2503.11266v2.pdf","comment":"under review for MICCAI 2025"},{"id":"http://arxiv.org/abs/2411.19235v2","updated":"2025-04-15T13:50:52Z","published":"2024-11-28T16:08:36Z","title":"InstanceGaussian: Appearance-Semantic Joint Gaussian Representation for\n  3D Instance-Level Perception","summary":"  3D scene understanding has become an essential area of research with\napplications in autonomous driving, robotics, and augmented reality. Recently,\n3D Gaussian Splatting (3DGS) has emerged as a powerful approach, combining\nexplicit modeling with neural adaptability to provide efficient and detailed\nscene representations. However, three major challenges remain in leveraging\n3DGS for scene understanding: 1) an imbalance between appearance and semantics,\nwhere dense Gaussian usage for fine-grained texture modeling does not align\nwith the minimal requirements for semantic attributes; 2) inconsistencies\nbetween appearance and semantics, as purely appearance-based Gaussians often\nmisrepresent object boundaries; and 3) reliance on top-down instance\nsegmentation methods, which struggle with uneven category distributions,\nleading to over- or under-segmentation. In this work, we propose\nInstanceGaussian, a method that jointly learns appearance and semantic features\nwhile adaptively aggregating instances. Our contributions include: i) a novel\nSemantic-Scaffold-GS representation balancing appearance and semantics to\nimprove feature representations and boundary delineation; ii) a progressive\nappearance-semantic joint training strategy to enhance stability and\nsegmentation accuracy; and iii) a bottom-up, category-agnostic instance\naggregation approach that addresses segmentation challenges through farthest\npoint sampling and connected component analysis. Our approach achieves\nstate-of-the-art performance in category-agnostic, open-vocabulary 3D\npoint-level segmentation, highlighting the effectiveness of the proposed\nrepresentation and training strategies. Project page:\nhttps://lhj-git.github.io/InstanceGaussian/\n","authors":["Haijie Li","Yanmin Wu","Jiarui Meng","Qiankun Gao","Zhiyao Zhang","Ronggang Wang","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.19235v2.pdf","comment":"14 pages, accepted by CVPR 2025 as poster"},{"id":"http://arxiv.org/abs/2504.11195v1","updated":"2025-04-15T13:49:31Z","published":"2025-04-15T13:49:31Z","title":"R-TPT: Improving Adversarial Robustness of Vision-Language Models\n  through Test-Time Prompt Tuning","summary":"  Vision-language models (VLMs), such as CLIP, have gained significant\npopularity as foundation models, with numerous fine-tuning methods developed to\nenhance performance on downstream tasks. However, due to their inherent\nvulnerability and the common practice of selecting from a limited set of\nopen-source models, VLMs suffer from a higher risk of adversarial attacks than\ntraditional vision models. Existing defense techniques typically rely on\nadversarial fine-tuning during training, which requires labeled data and lacks\nof flexibility for downstream tasks. To address these limitations, we propose\nrobust test-time prompt tuning (R-TPT), which mitigates the impact of\nadversarial attacks during the inference stage. We first reformulate the\nclassic marginal entropy objective by eliminating the term that introduces\nconflicts under adversarial conditions, retaining only the pointwise entropy\nminimization. Furthermore, we introduce a plug-and-play reliability-based\nweighted ensembling strategy, which aggregates useful information from reliable\naugmented views to strengthen the defense. R-TPT enhances defense against\nadversarial attacks without requiring labeled training data while offering high\nflexibility for inference tasks. Extensive experiments on widely used\nbenchmarks with various attacks demonstrate the effectiveness of R-TPT. The\ncode is available in https://github.com/TomSheng21/R-TPT.\n","authors":["Lijun Sheng","Jian Liang","Zilei Wang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2504.11195v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2304.03807v5","updated":"2025-04-15T13:44:39Z","published":"2023-04-07T18:21:30Z","title":"Privacy-Preserving CNN Training with Transfer Learning: Multiclass\n  Logistic Regression","summary":"  In this paper, we present a practical solution to implement\nprivacy-preserving CNN training based on mere Homomorphic Encryption (HE)\ntechnique. To our best knowledge, this is the first attempt successfully to\ncrack this nut and no work ever before has achieved this goal. Several\ntechniques combine to accomplish the task:: (1) with transfer learning,\nprivacy-preserving CNN training can be reduced to homomorphic neural network\ntraining, or even multiclass logistic regression (MLR) training; (2) via a\nfaster gradient variant called $\\texttt{Quadratic Gradient}$, an enhanced\ngradient method for MLR with a state-of-the-art performance in convergence\nspeed is applied in this work to achieve high performance; (3) we employ the\nthought of transformation in mathematics to transform approximating Softmax\nfunction in the encryption domain to the approximation of the Sigmoid function.\nA new type of loss function termed $\\texttt{Squared Likelihood Error}$ has been\ndeveloped alongside to align with this change.; and (4) we use a simple but\nflexible matrix-encoding method named $\\texttt{Volley Revolver}$ to manage the\ndata flow in the ciphertexts, which is the key factor to complete the whole\nhomomorphic CNN training. The complete, runnable C++ code to implement our work\ncan be found at:\n\\href{https://github.com/petitioner/HE.CNNtraining}{$\\texttt{https://github.com/petitioner/HE.CNNtraining}$}.\nWe select $\\texttt{REGNET\\_X\\_400MF}$ as our pre-trained model for transfer\nlearning. We use the first 128 MNIST training images as training data and the\nwhole MNIST testing dataset as the testing data. The client only needs to\nupload 6 ciphertexts to the cloud and it takes $\\sim 21$ mins to perform 2\niterations on a cloud with 64 vCPUs, resulting in a precision of $21.49\\%$.\n","authors":["John Chiang"],"pdf_url":"https://arxiv.org/pdf/2304.03807v5.pdf","comment":"In this work, we initiated to implement privacy-persevering CNN\n  training based on mere HE techniques by presenting a faster HE-friendly\n  algorithm"},{"id":"http://arxiv.org/abs/2403.12922v3","updated":"2025-04-15T13:30:02Z","published":"2024-03-19T17:27:55Z","title":"Contextual AD Narration with Interleaved Multimodal Sequence","summary":"  The Audio Description (AD) task aims to generate descriptions of visual\nelements for visually impaired individuals to help them access long-form video\ncontent, like movies. With video feature, text, character bank and context\ninformation as inputs, the generated ADs are able to correspond to the\ncharacters by name and provide reasonable, contextual descriptions to help\naudience understand the storyline of movie. To achieve this goal, we propose to\nleverage pre-trained foundation models through a simple and unified framework\nto generate ADs with interleaved multimodal sequence as input, termed as\nUni-AD. To enhance the alignment of features across various modalities with\nfiner granularity, we introduce a simple and lightweight module that maps video\nfeatures into the textual feature space. Moreover, we also propose a\ncharacter-refinement module to provide more precise information by identifying\nthe main characters who play more significant roles in the video context. With\nthese unique designs, we further incorporate contextual information and a\ncontrastive loss into our architecture to generate smoother and more\ncontextually appropriate ADs. Experiments on multiple AD datasets show that\nUni-AD performs well on AD generation, which demonstrates the effectiveness of\nour approach. Our code is available at: https://github.com/ant-research/UniAD.\n","authors":["Hanlin Wang","Zhan Tong","Kecheng Zheng","Yujun Shen","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12922v3.pdf","comment":"Accepted by CVPR25"},{"id":"http://arxiv.org/abs/2504.11172v1","updated":"2025-04-15T13:20:35Z","published":"2025-04-15T13:20:35Z","title":"TerraMesh: A Planetary Mosaic of Multimodal Earth Observation Data","summary":"  Large-scale foundation models in Earth Observation can learn versatile,\nlabel-efficient representations by leveraging massive amounts of unlabeled\ndata. However, existing public datasets are often limited in scale, geographic\ncoverage, or sensor variety. We introduce TerraMesh, a new globally diverse,\nmultimodal dataset combining optical, synthetic aperture radar, elevation, and\nland-cover modalities in an Analysis-Ready Data format. TerraMesh includes over\n9 million samples with eight spatiotemporal aligned modalities, enabling\nlarge-scale pre-training and fostering robust cross-modal correlation learning.\nWe provide detailed data processing steps, comprehensive statistics, and\nempirical evidence demonstrating improved model performance when pre-trained on\nTerraMesh. The dataset will be made publicly available with a permissive\nlicense.\n","authors":["Benedikt Blumenstiel","Paolo Fraccaro","Valerio Marsocci","Johannes Jakubik","Stefano Maurogiovanni","Mikolaj Czerkawski","Rocco Sedona","Gabriele Cavallaro","Thomas Brunschwiler","Juan Bernabe-Moreno","Nicolas Longépé"],"pdf_url":"https://arxiv.org/pdf/2504.11172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11171v1","updated":"2025-04-15T13:17:39Z","published":"2025-04-15T13:17:39Z","title":"TerraMind: Large-Scale Generative Multimodality for Earth Observation","summary":"  We present TerraMind, the first any-to-any generative, multimodal foundation\nmodel for Earth observation (EO). Unlike other multimodal models, TerraMind is\npretrained on dual-scale representations combining both token-level and\npixel-level data across modalities. On a token level, TerraMind encodes\nhigh-level contextual information to learn cross-modal relationships, while on\na pixel level, TerraMind leverages fine-grained representations to capture\ncritical spatial nuances. We pretrained TerraMind on nine geospatial modalities\nof a global, large-scale dataset. In this paper, we demonstrate that (i)\nTerraMind's dual-scale early fusion approach unlocks a range of zero-shot and\nfew-shot applications for Earth observation, (ii) TerraMind introduces\n\"Thinking-in-Modalities\" (TiM) -- the capability of generating additional\nartificial data during finetuning and inference to improve the model output --\nand (iii) TerraMind achieves beyond state-of-the-art performance in\ncommunity-standard benchmarks for EO like PANGAEA. The pretraining dataset, the\nmodel weights, and our code is open-sourced under a permissive license.\n","authors":["Johannes Jakubik","Felix Yang","Benedikt Blumenstiel","Erik Scheurer","Rocco Sedona","Stefano Maurogiovanni","Jente Bosmans","Nikolaos Dionelis","Valerio Marsocci","Niklas Kopp","Rahul Ramachandran","Paolo Fraccaro","Thomas Brunschwiler","Gabriele Cavallaro","Juan Bernabe-Moreno","Nicolas Longépé"],"pdf_url":"https://arxiv.org/pdf/2504.11171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11165v1","updated":"2025-04-15T13:13:22Z","published":"2025-04-15T13:13:22Z","title":"YOLO-RS: Remote Sensing Enhanced Crop Detection Methods","summary":"  With the rapid development of remote sensing technology, crop classification\nand health detection based on deep learning have gradually become a research\nhotspot. However, the existing target detection methods show poor performance\nwhen dealing with small targets in remote sensing images, especially in the\ncase of complex background and image mixing, which is difficult to meet the\npractical application requirementsite. To address this problem, a novel target\ndetection model YOLO-RS is proposed in this paper. The model is based on the\nlatest Yolov11 which significantly enhances the detection of small targets by\nintroducing the Context Anchor Attention (CAA) mechanism and an efficient\nmulti-field multi-scale feature fusion network. YOLO-RS adopts a bidirectional\nfeature fusion strategy in the feature fusion process, which effectively\nenhances the model's performance in the detection of small targets. Small\ntarget detection. Meanwhile, the ACmix module at the end of the model backbone\nnetwork solves the category imbalance problem by adaptively adjusting the\ncontrast and sample mixing, thus enhancing the detection accuracy in complex\nscenes. In the experiments on the PDT remote sensing crop health detection\ndataset and the CWC crop classification dataset, YOLO-RS improves both the\nrecall and the mean average precision (mAP) by about 2-3\\% or so compared with\nthe existing state-of-the-art methods, while the F1-score is also significantly\nimproved. Moreover, the computational complexity of the model only increases by\nabout 5.2 GFLOPs, indicating its significant advantages in both performance and\nefficiency. The experimental results validate the effectiveness and application\npotential of YOLO-RS in the task of detecting small targets in remote sensing\nimages.\n","authors":["Linlin Xiao","Zhang Tiancong","Yutong Jia","Xinyu Nie","Mengyao Wang","Xiaohang Shao"],"pdf_url":"https://arxiv.org/pdf/2504.11165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11164v1","updated":"2025-04-15T13:12:42Z","published":"2025-04-15T13:12:42Z","title":"TSAL: Few-shot Text Segmentation Based on Attribute Learning","summary":"  Recently supervised learning rapidly develops in scene text segmentation.\nHowever, the lack of high-quality datasets and the high cost of pixel\nannotation greatly limit the development of them. Considering the\nwell-performed few-shot learning methods for downstream tasks, we investigate\nthe application of the few-shot learning method to scene text segmentation. We\npropose TSAL, which leverages CLIP's prior knowledge to learn text attributes\nfor segmentation. To fully utilize the semantic and texture information in the\nimage, a visual-guided branch is proposed to separately extract text and\nbackground features. To reduce data dependency and improve text detection\naccuracy, the adaptive prompt-guided branch employs effective adaptive prompt\ntemplates to capture various text attributes. To enable adaptive prompts\ncapture distinctive text features and complex background distribution, we\npropose Adaptive Feature Alignment module(AFA). By aligning learnable tokens of\ndifferent attributes with visual features and prompt prototypes, AFA enables\nadaptive prompts to capture both general and distinctive attribute information.\nTSAL can capture the unique attributes of text and achieve precise segmentation\nusing only few images. Experiments demonstrate that our method achieves SOTA\nperformance on multiple text segmentation datasets under few-shot settings and\nshow great potential in text-related domains.\n","authors":["Chenming Li","Chengxu Liu","Yuanting Fan","Xiao Jin","Xingsong Hou","Xueming Qian"],"pdf_url":"https://arxiv.org/pdf/2504.11164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18672v3","updated":"2025-04-15T13:10:16Z","published":"2025-03-24T13:44:12Z","title":"Feature Calibration enhanced Parameter Synthesis for CLIP-based\n  Class-incremental Learning","summary":"  Class-Incremental Learning (CIL) enables models to continuously learn new\nclass knowledge while retaining previous classes, facilitating adaptation and\nevolution in dynamic, real-world environments. Traditional CIL methods\nprimarily rely on visual features, which limits their effectiveness in complex,\nmultimodal scenarios. In contrast, VLMs show promising potential for enhancing\nCIL by leveraging pre-trained knowledge and integrating multi-modal semantic\ncues such as text and vision. However, existing approaches struggle to mitigate\ncatastrophic forgetting while preserving the generalization strengths of VLMs\nacross diverse modalities. To address these challenges, we propose a Feature\nCalibration Enhanced Parameter Synthesis (FCPS) framework. Specifically, FCPS\nintroduces a dynamic parameter adjustment mechanism that iteratively calibrates\nthe contribution of original visual features to the final class decision, thus\npreserving the model's intrinsic generalization capability across modalities.\nSimultaneously, parameter integration enables effective knowledge transfer,\nmaintaining a balance between acquiring new class representations and\npreserving old knowledge. Experimental results on popular benchmarks (e.g.,\nCIFAR100 and ImageNet100) validate the superiority of the proposed method.\n","authors":["Juncen Guo","Yang Liu","Xiaoguang Zhu","Lianlong Sun","Liangyu Teng","Jingyi Wu","Di Li","Wei Zhou","Liang Song"],"pdf_url":"https://arxiv.org/pdf/2503.18672v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11160v1","updated":"2025-04-15T13:08:43Z","published":"2025-04-15T13:08:43Z","title":"DMAGaze: Gaze Estimation Based on Feature Disentanglement and\n  Multi-Scale Attention","summary":"  Gaze estimation, which predicts gaze direction, commonly faces the challenge\nof interference from complex gaze-irrelevant information in face images. In\nthis work, we propose DMAGaze, a novel gaze estimation framework that exploits\ninformation from facial images in three aspects: gaze-relevant global features\n(disentangled from facial image), local eye features (extracted from cropped\neye patch), and head pose estimation features, to improve overall performance.\nFirstly, we design a new continuous mask-based Disentangler to accurately\ndisentangle gaze-relevant and gaze-irrelevant information in facial images by\nachieving the dual-branch disentanglement goal through separately\nreconstructing the eye and non-eye regions. Furthermore, we introduce a new\ncascaded attention module named Multi-Scale Global Local Attention Module\n(MS-GLAM). Through a customized cascaded attention structure, it effectively\nfocuses on global and local information at multiple scales, further enhancing\nthe information from the Disentangler. Finally, the global gaze-relevant\nfeatures disentangled by the upper face branch, combined with head pose and\nlocal eye features, are passed through the detection head for high-precision\ngaze estimation. Our proposed DMAGaze has been extensively validated on two\nmainstream public datasets, achieving state-of-the-art performance.\n","authors":["Haohan Chen","Hongjia Liu","Shiyong Lan","Wenwu Wang","Yixin Qiao","Yao Li","Guonan Deng"],"pdf_url":"https://arxiv.org/pdf/2504.11160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14914v2","updated":"2025-04-15T12:58:38Z","published":"2025-02-19T07:55:51Z","title":"What Is a Good Caption? A Comprehensive Visual Caption Benchmark for\n  Evaluating Both Correctness and Thoroughness","summary":"  Visual captioning benchmarks have become outdated with the emergence of\nmodern multimodal large language models (MLLMs), as the brief ground-truth\nsentences and traditional metrics fail to assess detailed captions effectively.\nWhile recent benchmarks attempt to address this by focusing on keyword\nextraction or object-centric evaluation, they remain limited to vague-view or\nobject-view analyses and incomplete visual element coverage. In this paper, we\nintroduce CAPability, a comprehensive multi-view benchmark for evaluating\nvisual captioning across 12 dimensions spanning six critical views. We curate\nnearly 11K human-annotated images and videos with visual element annotations to\nevaluate the generated captions. CAPability stably assesses both the\ncorrectness and thoroughness of captions using F1-score. By converting\nannotations to QA pairs, we further introduce a heuristic metric, \\textit{know\nbut cannot tell} ($K\\bar{T}$), indicating a significant performance gap between\nQA and caption capabilities. Our work provides the first holistic analysis of\nMLLMs' captioning abilities, as we identify their strengths and weaknesses\nacross various dimensions, guiding future research to enhance specific aspects\nof capabilities.\n","authors":["Zhihang Liu","Chen-Wei Xie","Bin Wen","Feiwu Yu","Jixuan Chen","Boqiang Zhang","Nianzu Yang","Pandeng Li","Yinglu Li","Zuan Gao","Yun Zheng","Hongtao Xie"],"pdf_url":"https://arxiv.org/pdf/2502.14914v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11154v1","updated":"2025-04-15T12:58:30Z","published":"2025-04-15T12:58:30Z","title":"SAR-to-RGB Translation with Latent Diffusion for Earth Observation","summary":"  Earth observation satellites like Sentinel-1 (S1) and Sentinel-2 (S2) provide\ncomplementary remote sensing (RS) data, but S2 images are often unavailable due\nto cloud cover or data gaps. To address this, we propose a diffusion model\n(DM)-based approach for SAR-to-RGB translation, generating synthetic optical\nimages from SAR inputs. We explore three different setups: two using Standard\nDiffusion, which reconstruct S2 images by adding and removing noise (one\nwithout and one with class conditioning), and one using Cold Diffusion, which\nblends S2 with S1 before removing the SAR signal. We evaluate the generated\nimages in downstream tasks, including land cover classification and cloud\nremoval. While generated images may not perfectly replicate real S2 data, they\nstill provide valuable information. Our results show that class conditioning\nimproves classification accuracy, while cloud removal performance remains\ncompetitive despite our approach not being optimized for it. Interestingly,\ndespite exhibiting lower perceptual quality, the Cold Diffusion setup performs\nwell in land cover classification, suggesting that traditional quantitative\nevaluation metrics may not fully reflect the practical utility of generated\nimages. Our findings highlight the potential of DMs for SAR-to-RGB translation\nin RS applications where RGB images are missing.\n","authors":["Kaan Aydin","Joelle Hanna","Damian Borth"],"pdf_url":"https://arxiv.org/pdf/2504.11154v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2504.11150v1","updated":"2025-04-15T12:53:07Z","published":"2025-04-15T12:53:07Z","title":"GC-GAT: Multimodal Vehicular Trajectory Prediction using Graph Goal\n  Conditioning and Cross-context Attention","summary":"  Predicting future trajectories of surrounding vehicles heavily relies on what\ncontextual information is given to a motion prediction model. The context\nitself can be static (lanes, regulatory elements, etc) or dynamic (traffic\nparticipants). This paper presents a lane graph-based motion prediction model\nthat first predicts graph-based goal proposals and later fuses them with cross\nattention over multiple contextual elements. We follow the famous\nencoder-interactor-decoder architecture where the encoder encodes scene context\nusing lightweight Gated Recurrent Units, the interactor applies cross-context\nattention over encoded scene features and graph goal proposals, and the decoder\nregresses multimodal trajectories via Laplacian Mixture Density Network from\nthe aggregated encodings. Using cross-attention over graph-based goal proposals\ngives robust trajectory estimates since the model learns to attend to future\ngoal-relevant scene elements for the intended agent. We evaluate our work on\nnuScenes motion prediction dataset, achieving state-of-the-art results.\n","authors":["Mahir Gulzar","Yar Muhammad","Naveed Muhammad"],"pdf_url":"https://arxiv.org/pdf/2504.11150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11143v1","updated":"2025-04-15T12:44:53Z","published":"2025-04-15T12:44:53Z","title":"Taming Consistency Distillation for Accelerated Human Image Animation","summary":"  Recent advancements in human image animation have been propelled by video\ndiffusion models, yet their reliance on numerous iterative denoising steps\nresults in high inference costs and slow speeds. An intuitive solution involves\nadopting consistency models, which serve as an effective acceleration paradigm\nthrough consistency distillation. However, simply employing this strategy in\nhuman image animation often leads to quality decline, including visual\nblurring, motion degradation, and facial distortion, particularly in dynamic\nregions. In this paper, we propose the DanceLCM approach complemented by\nseveral enhancements to improve visual quality and motion continuity at\nlow-step regime: (1) segmented consistency distillation with an auxiliary\nlight-weight head to incorporate supervision from real video latents,\nmitigating cumulative errors resulting from single full-trajectory generation;\n(2) a motion-focused loss to centre on motion regions, and explicit injection\nof facial fidelity features to improve face authenticity. Extensive qualitative\nand quantitative experiments demonstrate that DanceLCM achieves results\ncomparable to state-of-the-art video diffusion models with a mere 2-4 inference\nsteps, significantly reducing the inference burden without compromising video\nquality. The code and models will be made publicly available.\n","authors":["Xiang Wang","Shiwei Zhang","Hangjie Yuan","Yujie Wei","Yingya Zhang","Changxin Gao","Yuehuan Wang","Nong Sang"],"pdf_url":"https://arxiv.org/pdf/2504.11143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11134v1","updated":"2025-04-15T12:37:16Z","published":"2025-04-15T12:37:16Z","title":"Visual Re-Ranking with Non-Visual Side Information","summary":"  The standard approach for visual place recognition is to use global image\ndescriptors to retrieve the most similar database images for a given query\nimage. The results can then be further improved with re-ranking methods that\nre-order the top scoring images. However, existing methods focus on re-ranking\nbased on the same image descriptors that were used for the initial retrieval,\nwhich we argue provides limited additional signal.\n  In this work we propose Generalized Contextual Similarity Aggregation (GCSA),\nwhich is a graph neural network-based re-ranking method that, in addition to\nthe visual descriptors, can leverage other types of available side information.\nThis can for example be other sensor data (such as signal strength of nearby\nWiFi or BlueTooth endpoints) or geometric properties such as camera poses for\ndatabase images. In many applications this information is already present or\ncan be acquired with low effort. Our architecture leverages the concept of\naffinity vectors to allow for a shared encoding of the heterogeneous\nmulti-modal input. Two large-scale datasets, covering both outdoor and indoor\nlocalization scenarios, are utilized for training and evaluation. In\nexperiments we show significant improvement not only on image retrieval\nmetrics, but also for the downstream visual localization task.\n","authors":["Gustav Hanning","Gabrielle Flood","Viktor Larsson"],"pdf_url":"https://arxiv.org/pdf/2504.11134v1.pdf","comment":"Accepted at Scandinavian Conference on Image Analysis (SCIA) 2025"},{"id":"http://arxiv.org/abs/2408.09411v2","updated":"2025-04-15T12:26:51Z","published":"2024-08-18T08:54:53Z","title":"Weakly Supervised Lymph Nodes Segmentation Based on Partial Instance\n  Annotations with Pre-trained Dual-branch Network and Pseudo Label Learning","summary":"  Assessing the presence of potentially malignant lymph nodes aids in\nestimating cancer progression, and identifying surrounding benign lymph nodes\ncan assist in determining potential metastatic pathways for cancer. For\nquantitative analysis, automatic segmentation of lymph nodes is crucial.\nHowever, due to the labor-intensive and time-consuming manual annotation\nprocess required for a large number of lymph nodes, it is more practical to\nannotate only a subset of the lymph node instances to reduce annotation costs.\nIn this study, we propose a pre-trained Dual-Branch network with Dynamically\nMixed Pseudo label (DBDMP) to learn from partial instance annotations for lymph\nnodes segmentation. To obtain reliable pseudo labels for lymph nodes that are\nnot annotated, we employ a dual-decoder network to generate different outputs\nthat are then dynamically mixed. We integrate the original weak partial\nannotations with the mixed pseudo labels to supervise the network. To further\nleverage the extensive amount of unannotated voxels, we apply a self-supervised\npre-training strategy to enhance the model's feature extraction capability.\nExperiments on the mediastinal Lymph Node Quantification (LNQ) dataset\ndemonstrate that our method, compared to directly learning from partial\ninstance annotations, significantly improves the Dice Similarity Coefficient\n(DSC) from 11.04% to 54.10% and reduces the Average Symmetric Surface Distance\n(ASSD) from 20.83 $mm$ to 8.72 $mm$. The code is available at\nhttps://github.com/WltyBY/LNQ2023_training_code.git\n","authors":["Litingyu Wang","Yijie Qu","Xiangde Luo","Wenjun Liao","Shichuan Zhang","Guotai Wang"],"pdf_url":"https://arxiv.org/pdf/2408.09411v2.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:013"},{"id":"http://arxiv.org/abs/2504.11128v1","updated":"2025-04-15T12:25:42Z","published":"2025-04-15T12:25:42Z","title":"K-means Enhanced Density Gradient Analysis for Urban and Transport\n  Metrics Using Multi-Modal Satellite Imagery","summary":"  This paper presents a novel computational approach for evaluating urban\nmetrics through density gradient analysis using multi-modal satellite imagery,\nwith applications including public transport and other urban systems. By\ncombining optical and Synthetic Aperture Radar (SAR) data, we develop a method\nto segment urban areas, identify urban centers, and quantify density gradients.\nOur approach calculates two key metrics: the density gradient coefficient\n($\\alpha$) and the minimum effective distance (LD) at which density reaches a\ntarget threshold. We further employ machine learning techniques, specifically\nK-means clustering, to objectively identify uniform and high-variability\nregions within density gradient plots. We demonstrate that these metrics\nprovide an effective screening tool for public transport analyses by revealing\nthe underlying urban structure. Through comparative analysis of two\nrepresentative cities with contrasting urban morphologies (monocentric vs\npolycentric), we establish relationships between density gradient\ncharacteristics and public transport network topologies. Cities with clear\ndensity peaks in their gradient plots indicate distinct urban centers requiring\ndifferent transport strategies than those with more uniform density\ndistributions. This methodology offers urban planners a cost-effective,\nglobally applicable approach to preliminary public transport assessment using\nfreely available satellite data. The complete implementation, with additional\nexamples and documentation, is available in an open-source repository under the\nMIT license at https://github.com/nexri/Satellite-Imagery-Urban-Analysis.\n","authors":["P. Tomkiewicz","J. Jaworski","P. Zielonka","A. Wilinski"],"pdf_url":"https://arxiv.org/pdf/2504.11128v1.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.00740v3","updated":"2025-04-15T12:16:15Z","published":"2025-01-01T06:07:02Z","title":"RORem: Training a Robust Object Remover with Human-in-the-Loop","summary":"  Despite the significant advancements, existing object removal methods\nstruggle with incomplete removal, incorrect content synthesis and blurry\nsynthesized regions, resulting in low success rates. Such issues are mainly\ncaused by the lack of high-quality paired training data, as well as the\nself-supervised training paradigm adopted in these methods, which forces the\nmodel to in-paint the masked regions, leading to ambiguity between synthesizing\nthe masked objects and restoring the background. To address these issues, we\npropose a semi-supervised learning strategy with human-in-the-loop to create\nhigh-quality paired training data, aiming to train a Robust Object Remover\n(RORem). We first collect 60K training pairs from open-source datasets to train\nan initial object removal model for generating removal samples, and then\nutilize human feedback to select a set of high-quality object removal pairs,\nwith which we train a discriminator to automate the following training data\ngeneration process. By iterating this process for several rounds, we finally\nobtain a substantial object removal dataset with over 200K pairs. Fine-tuning\nthe pre-trained stable diffusion model with this dataset, we obtain our RORem,\nwhich demonstrates state-of-the-art object removal performance in terms of both\nreliability and image quality. Particularly, RORem improves the object removal\nsuccess rate over previous methods by more than 18\\%. The dataset, source code\nand trained model are available at https://github.com/leeruibin/RORem.\n","authors":["Ruibin Li","Tao Yang","Song Guo","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.00740v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14719v3","updated":"2025-04-15T12:11:11Z","published":"2024-12-19T10:41:24Z","title":"Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition","summary":"  Micro-Action Recognition (MAR) has gained increasing attention due to its\ncrucial role as a form of non-verbal communication in social interactions, with\npromising potential for applications in human communication and emotion\nanalysis. However, current approaches often overlook the inherent ambiguity in\nmicro-actions, which arises from the wide category range and subtle visual\ndifferences between categories. This oversight hampers the accuracy of\nmicro-action recognition. In this paper, we propose a novel Prototypical\nCalibrating Ambiguous Network (PCAN) to unleash and mitigate the ambiguity of\nMAR. Firstly, we employ a hierarchical action-tree to identify the ambiguous\nsample, categorizing them into distinct sets of ambiguous samples of false\nnegatives and false positives, considering both body- and action-level\ncategories. Secondly, we implement an ambiguous contrastive refinement module\nto calibrate these ambiguous samples by regulating the distance between\nambiguous samples and their corresponding prototypes. This calibration process\naims to pull false negative (FN) samples closer to their respective prototypes\nand push false positive (FP) samples apart from their affiliated prototypes. In\naddition, we propose a new prototypical diversity amplification loss to\nstrengthen the model's capacity by amplifying the differences between different\nprototypes. Finally, we propose a prototype-guided rectification to rectify\nprediction by incorporating the representability of prototypes. Extensive\nexperiments conducted on the benchmark dataset demonstrate the superior\nperformance of our method compared to existing approaches. The code is\navailable at https://github.com/kunli-cs/PCAN.\n","authors":["Kun Li","Dan Guo","Guoliang Chen","Chunxiao Fan","Jingyuan Xu","Zhiliang Wu","Hehe Fan","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14719v3.pdf","comment":"Fix typos; Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2504.11118v1","updated":"2025-04-15T12:07:14Z","published":"2025-04-15T12:07:14Z","title":"Revealing Covert Attention by Analyzing Human and Reinforcement Learning\n  Agent Gameplay","summary":"  This study introduces a novel method for revealing human covert attention\npatterns using gameplay data alone, utilizing offline attention techniques from\nreinforcement learning (RL). We propose the contextualized, task-relevant (CTR)\nattention network, which generates attention maps from both human and RL agent\ngameplay in Atari environments. These maps are sparse yet retain the necessary\ninformation for the current player's decision making. We compare the\nCTR-derived attention maps with a temporally integrated overt attention (TIOA)\nmodel based on eye-tracking data, serving as a point of comparison and\ndiscussion. Visual inspection reveals distinct attention patterns: human CTR\nmaps focus on the player and rather nearby opponents, occasionally shifting\nbetween stronger focus and broader views - sometimes even attending to empty\nspace ahead. In contrast, agent maps maintain a consistent broad focus on most\nobjects, including distant ones and the player. Quantitative analysis further\ndemonstrates that human CTR maps align more closely with TIOA than agent maps\ndo. Our findings indicate that the CTR attention network can effectively reveal\nhuman covert attention patterns from gameplay alone, without the need for\nadditional data like brain activity recordings. This work contributes to\nunderstanding human-agent attention differences and enables the development of\nRL agents augmented with human covert attention.\n","authors":["Henrik Krauss","Takehisa Yairi"],"pdf_url":"https://arxiv.org/pdf/2504.11118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10165v2","updated":"2025-04-15T12:06:09Z","published":"2025-04-14T12:21:16Z","title":"WildLive: Near Real-time Visual Wildlife Tracking onboard UAVs","summary":"  Live tracking of wildlife via high-resolution video processing directly\nonboard drones is widely unexplored and most existing solutions rely on\nstreaming video to ground stations to support navigation. Yet, both autonomous\nanimal-reactive flight control beyond visual line of sight and/or\nmission-specific individual and behaviour recognition tasks rely to some degree\non this capability. In response, we introduce WildLive -- a near real-time\nanimal detection and tracking framework for high-resolution imagery running\ndirectly onboard uncrewed aerial vehicles (UAVs). The system performs\nmulti-animal detection and tracking at 17fps+ for HD and 7fps+ on 4K video\nstreams suitable for operation during higher altitude flights to minimise\nanimal disturbance. Our system is optimised for Jetson Orin AGX onboard\nhardware. It integrates the efficiency of sparse optical flow tracking and\nmission-specific sampling with device-optimised and proven YOLO-driven object\ndetection and segmentation techniques. Essentially, computational resource is\nfocused onto spatio-temporal regions of high uncertainty to significantly\nimprove UAV processing speeds without domain-specific loss of accuracy.\nAlongside, we introduce our WildLive dataset, which comprises 200k+ annotated\nanimal instances across 19k+ frames from 4K UAV videos collected at the Ol\nPejeta Conservancy in Kenya. All frames contain ground truth bounding boxes,\nsegmentation masks, as well as individual tracklets and tracking point\ntrajectories. We compare our system against current object tracking approaches\nincluding OC-SORT, ByteTrack, and SORT. Our materials are available at:\nhttps://dat-nguyenvn.github.io/WildLive/\n","authors":["Nguyen Ngoc Dat","Tom Richardson","Matthew Watson","Kilian Meier","Jenna Kline","Sid Reid","Guy Maalouf","Duncan Hine","Majid Mirmehdi","Tilo Burghardt"],"pdf_url":"https://arxiv.org/pdf/2504.10165v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17932v2","updated":"2025-04-15T11:59:15Z","published":"2024-10-23T14:54:48Z","title":"VR-Splatting: Foveated Radiance Field Rendering via 3D Gaussian\n  Splatting and Neural Points","summary":"  Recent advances in novel view synthesis have demonstrated impressive results\nin fast photorealistic scene rendering through differentiable point rendering,\neither via Gaussian Splatting (3DGS) [Kerbl and Kopanas et al. 2023] or neural\npoint rendering [Aliev et al. 2020]. Unfortunately, these directions require\neither a large number of small Gaussians or expensive per-pixel post-processing\nfor reconstructing fine details, which negatively impacts rendering\nperformance. To meet the high performance demands of virtual reality (VR)\nsystems, primitive or pixel counts therefore must be kept low, affecting visual\nquality.\n  In this paper, we propose a novel hybrid approach based on foveated rendering\nas a promising solution that combines the strengths of both point rendering\ndirections regarding performance sweet spots. Analyzing the compatibility with\nthe human visual system, we find that using a low-detailed, few primitive\nsmooth Gaussian representation for the periphery is cheap to compute and meets\nthe perceptual demands of peripheral vision. For the fovea only, we use neural\npoints with a convolutional neural network for the small pixel footprint, which\nprovides sharp, detailed output within the rendering budget. This combination\nalso allows for synergistic method accelerations with point occlusion culling\nand reducing the demands on the neural network.\n  Our evaluation confirms that our approach increases sharpness and details\ncompared to a standard VR-ready 3DGS configuration, and participants of a user\nstudy overwhelmingly preferred our method. Our system meets the necessary\nperformance requirements for real-time VR interactions, ultimately enhancing\nthe user's immersive experience.\n  The project page can be found at: https://lfranke.github.io/vr_splatting\n","authors":["Linus Franke","Laura Fink","Marc Stamminger"],"pdf_url":"https://arxiv.org/pdf/2410.17932v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11112v1","updated":"2025-04-15T11:57:40Z","published":"2025-04-15T11:57:40Z","title":"Flyweight FLIM Networks for Salient Object Detection in Biomedical\n  Images","summary":"  Salient Object Detection (SOD) with deep learning often requires substantial\ncomputational resources and large annotated datasets, making it impractical for\nresource-constrained applications. Lightweight models address computational\ndemands but typically strive in complex and scarce labeled-data scenarios.\nFeature Learning from Image Markers (FLIM) learns an encoder's convolutional\nkernels among image patches extracted from discriminative regions marked on a\nfew representative images, dismissing large annotated datasets, pretraining,\nand backpropagation. Such a methodology exploits information redundancy\ncommonly found in biomedical image applications. This study presents methods to\nlearn dilated-separable convolutional kernels and multi-dilation layers without\nbackpropagation for FLIM networks. It also proposes a novel network\nsimplification method to reduce kernel redundancy and encoder size. By\ncombining a FLIM encoder with an adaptive decoder, a concept recently\nintroduced to estimate a pointwise convolution per image, this study presents\nvery efficient (named flyweight) SOD models for biomedical images. Experimental\nresults in challenging datasets demonstrate superior efficiency and\neffectiveness to lightweight models. By requiring significantly fewer\nparameters and floating-point operations, the results show competitive\neffectiveness to heavyweight models. These advances highlight the potential of\nFLIM networks for data-limited and resource-constrained applications with\ninformation redundancy.\n","authors":["Leonardo M. Joao","Jancarlo F. Gomes","Silvio J. F. Guimaraes","Ewa Kijak","Alexandre X. Falcao"],"pdf_url":"https://arxiv.org/pdf/2504.11112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11111v1","updated":"2025-04-15T11:57:00Z","published":"2025-04-15T11:57:00Z","title":"S$^2$Teacher: Step-by-step Teacher for Sparsely Annotated Oriented\n  Object Detection","summary":"  Although fully-supervised oriented object detection has made significant\nprogress in multimodal remote sensing image understanding, it comes at the cost\nof labor-intensive annotation. Recent studies have explored weakly and\nsemi-supervised learning to alleviate this burden. However, these methods\noverlook the difficulties posed by dense annotations in complex remote sensing\nscenes. In this paper, we introduce a novel setting called sparsely annotated\noriented object detection (SAOOD), which only labels partial instances, and\npropose a solution to address its challenges. Specifically, we focus on two key\nissues in the setting: (1) sparse labeling leading to overfitting on limited\nforeground representations, and (2) unlabeled objects (false negatives)\nconfusing feature learning. To this end, we propose the S$^2$Teacher, a novel\nmethod that progressively mines pseudo-labels for unlabeled objects, from easy\nto hard, to enhance foreground representations. Additionally, it reweights the\nloss of unlabeled objects to mitigate their impact during training. Extensive\nexperiments demonstrate that S$^2$Teacher not only significantly improves\ndetector performance across different sparse annotation levels but also\nachieves near-fully-supervised performance on the DOTA dataset with only 10%\nannotation instances, effectively balancing detection accuracy with annotation\nefficiency. The code will be public.\n","authors":["Yu Lin","Jianghang Lin","Kai Ye","You Shen","Yan Zhang","Shengchuan Zhang","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2504.11111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11106v1","updated":"2025-04-15T11:53:40Z","published":"2025-04-15T11:53:40Z","title":"Token-Level Constraint Boundary Search for Jailbreaking Text-to-Image\n  Models","summary":"  Recent advancements in Text-to-Image (T2I) generation have significantly\nenhanced the realism and creativity of generated images. However, such powerful\ngenerative capabilities pose risks related to the production of inappropriate\nor harmful content. Existing defense mechanisms, including prompt checkers and\npost-hoc image checkers, are vulnerable to sophisticated adversarial attacks.\nIn this work, we propose TCBS-Attack, a novel query-based black-box jailbreak\nattack that searches for tokens located near the decision boundaries defined by\ntext and image checkers. By iteratively optimizing tokens near these\nboundaries, TCBS-Attack generates semantically coherent adversarial prompts\ncapable of bypassing multiple defensive layers in T2I models. Extensive\nexperiments demonstrate that our method consistently outperforms\nstate-of-the-art jailbreak attacks across various T2I models, including\nsecurely trained open-source models and commercial online services like DALL-E\n3. TCBS-Attack achieves an ASR-4 of 45\\% and an ASR-1 of 21\\% on jailbreaking\nfull-chain T2I models, significantly surpassing baseline methods.\n","authors":["Jiangtao Liu","Zhaoxin Wang","Handing Wang","Cong Tian","Yaochu Jin"],"pdf_url":"https://arxiv.org/pdf/2504.11106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09897v2","updated":"2025-04-15T11:52:43Z","published":"2025-04-14T05:44:38Z","title":"TAMP: Token-Adaptive Layerwise Pruning in Multimodal Large Language\n  Models","summary":"  Multimodal Large Language Models (MLLMs) have shown remarkable versatility in\nunderstanding diverse multimodal data and tasks. However, these capabilities\ncome with an increased model scale. While post-training pruning reduces model\nsize in unimodal models, its application to MLLMs often yields limited success.\nOur analysis discovers that conventional methods fail to account for the unique\ntoken attributes across layers and modalities inherent to MLLMs. Inspired by\nthis observation, we propose TAMP, a simple yet effective pruning framework\ntailored for MLLMs, featuring two key components: (1) Diversity-Aware Sparsity,\nwhich adjusts sparsity ratio per layer based on diversities among multimodal\noutput tokens, preserving more parameters in high-diversity layers; and (2)\nAdaptive Multimodal Input Activation, which identifies representative\nmultimodal input tokens using attention scores to guide unstructured weight\npruning. We validate our method on two state-of-the-art MLLMs: LLaVA-NeXT,\ndesigned for vision-language tasks, and VideoLLaMA2, capable of processing\naudio, visual, and language modalities. Empirical experiments across various\nmultimodal evaluation benchmarks demonstrate that each component of our\napproach substantially outperforms existing pruning techniques.\n","authors":["Jaewoo Lee","Keyang Xuan","Chanakya Ekbote","Sandeep Polisetty","Yi R. Fung","Paul Pu Liang"],"pdf_url":"https://arxiv.org/pdf/2504.09897v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2504.11104v1","updated":"2025-04-15T11:52:20Z","published":"2025-04-15T11:52:20Z","title":"Using LLMs as prompt modifier to avoid biases in AI image generators","summary":"  This study examines how Large Language Models (LLMs) can reduce biases in\ntext-to-image generation systems by modifying user prompts. We define bias as a\nmodel's unfair deviation from population statistics given neutral prompts. Our\nexperiments with Stable Diffusion XL, 3.5 and Flux demonstrate that\nLLM-modified prompts significantly increase image diversity and reduce bias\nwithout the need to change the image generators themselves. While occasionally\nproducing results that diverge from original user intent for elaborate prompts,\nthis approach generally provides more varied interpretations of underspecified\nrequests rather than superficial variations. The method works particularly well\nfor less advanced image generators, though limitations persist for certain\ncontexts like disability representation. All prompts and generated images are\navailable at https://iisys-hof.github.io/llm-prompt-img-gen/\n","authors":["René Peinl"],"pdf_url":"https://arxiv.org/pdf/2504.11104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11101v1","updated":"2025-04-15T11:51:18Z","published":"2025-04-15T11:51:18Z","title":"Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and\n  Self-Improving OCR","summary":"  The Optical Character Recognition (OCR) task is important for evaluating\nVision-Language Models (VLMs) and providing high-quality data sources for LLM\ntraining data. While state-of-the-art VLMs show improved average OCR accuracy,\nthey still struggle with sample-level quality degradation and lack reliable\nautomatic detection of low-quality outputs. We introduce Consensus Entropy\n(CE), a training-free post-inference method that quantifies OCR uncertainty by\naggregating outputs from multiple VLMs. Our approach exploits a key insight:\ncorrect VLM OCR predictions converge in output space while errors diverge. We\ndevelop a lightweight multi-model framework that effectively identifies\nproblematic samples, selects the best outputs and combines model strengths.\nExperiments across multiple OCR benchmarks and VLMs demonstrate that CE\noutperforms VLM-as-judge approaches and single-model baselines at the same cost\nand achieves state-of-the-art results across multiple metrics. For instance,\nour solution demonstrates: achieving 15.2\\% higher F1 scores than VLM-as-judge\nmethods in quality verification, delivering 6.0\\% accuracy gains on\nmathematical calculation tasks, and requiring rephrasing only 7.3\\% of inputs\nwhile maintaining overall performance. Notably, the entire process requires\nneither training nor supervision while maintaining plug-and-play functionality\nthroughout.\n","authors":["Yulong Zhang","Tianyi Liang","Xinyue Huang","Erfei Cui","Xu Guo","Pei Chu","Chenhui Li","Ru Zhang","Wenhai Wang","Gongshen Liu"],"pdf_url":"https://arxiv.org/pdf/2504.11101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04072v2","updated":"2025-04-15T11:44:00Z","published":"2024-10-05T08:04:26Z","title":"MROSS: Multi-Round Region-based Optimization for Scene Sketching","summary":"  Scene sketching is to convert a scene into a simplified, abstract\nrepresentation that captures the essential elements and composition of the\noriginal scene. It requires a semantic understanding of the scene and\nconsideration of different regions within the scene. Since scenes often contain\ndiverse visual information across various regions, such as foreground objects,\nbackground elements, and spatial divisions, dealing with these different\nregions poses unique difficulties. In this paper, we define a sketch as some\nsets of B\\'ezier curves because of their smooth and versatile characteristics.\nWe optimize different regions of input scene in multiple rounds. In each\noptimization round, the strokes sampled from the next region can seamlessly be\nintegrated into the sketch generated in the previous optimization round. We\npropose an additional stroke initialization method to ensure the integrity of\nthe scene and the convergence of optimization. A novel CLIP-based Semantic Loss\nand a VGG-based Feature Loss are utilized to guide our multi-round\noptimization. Extensive experimental results on the quality and quantity of the\ngenerated sketches confirm the effectiveness of our method.\n","authors":["Yiqi Liang","Ying Liu","Dandan Long","Ruihui Li"],"pdf_url":"https://arxiv.org/pdf/2410.04072v2.pdf","comment":"6 pages, 8 figures"},{"id":"http://arxiv.org/abs/2504.11092v1","updated":"2025-04-15T11:38:14Z","published":"2025-04-15T11:38:14Z","title":"Vivid4D: Improving 4D Reconstruction from Monocular Video by Video\n  Inpainting","summary":"  Reconstructing 4D dynamic scenes from casually captured monocular videos is\nvaluable but highly challenging, as each timestamp is observed from a single\nviewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular\nvideo synthesis by augmenting observation views - synthesizing multi-view\nvideos from a monocular input. Unlike existing methods that either solely\nleverage geometric priors for supervision or use generative priors while\noverlooking geometry, we integrate both. This reformulates view augmentation as\na video inpainting task, where observed views are warped into new viewpoints\nbased on monocular depth priors. To achieve this, we train a video inpainting\nmodel on unposed web videos with synthetically generated masks that mimic\nwarping occlusions, ensuring spatially and temporally consistent completion of\nmissing regions. To further mitigate inaccuracies in monocular depth priors, we\nintroduce an iterative view augmentation strategy and a robust reconstruction\nloss. Experiments demonstrate that our method effectively improves monocular 4D\nscene reconstruction and completion.\n","authors":["Jiaxin Huang","Sheng Miao","BangBnag Yang","Yuewen Ma","Yiyi Liao"],"pdf_url":"https://arxiv.org/pdf/2504.11092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11089v1","updated":"2025-04-15T11:34:03Z","published":"2025-04-15T11:34:03Z","title":"InfoClus: Informative Clustering of High-dimensional Data Embeddings","summary":"  Developing an understanding of high-dimensional data can be facilitated by\nvisualizing that data using dimensionality reduction. However, the\nlow-dimensional embeddings are often difficult to interpret. To facilitate the\nexploration and interpretation of low-dimensional embeddings, we introduce a\nnew concept named partitioning with explanations. The idea is to partition the\ndata shown through the embedding into groups, each of which is given a sparse\nexplanation using the original high-dimensional attributes. We introduce an\nobjective function that quantifies how much we can learn through observing the\nexplanations of the data partitioning, using information theory, and also how\ncomplex the explanations are. Through parameterization of the complexity, we\ncan tune the solutions towards the desired granularity. We propose InfoClus,\nwhich optimizes the partitioning and explanations jointly, through greedy\nsearch constrained over a hierarchical clustering. We conduct a qualitative and\nquantitative analysis of InfoClus on three data sets. We contrast the results\non the Cytometry data with published manual analysis results, and compare with\ntwo other recent methods for explaining embeddings (RVX and VERA). These\ncomparisons highlight that InfoClus has distinct advantages over existing\nprocedures and methods. We find that InfoClus can automatically create good\nstarting points for the analysis of dimensionality-reduction-based scatter\nplots.\n","authors":["Fuyin Lai","Edith Heiter","Guillaume Bied","Jefrey Lijffijt"],"pdf_url":"https://arxiv.org/pdf/2504.11089v1.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2504.11080v1","updated":"2025-04-15T11:25:10Z","published":"2025-04-15T11:25:10Z","title":"Change State Space Models for Remote Sensing Change Detection","summary":"  Despite their frequent use for change detection, both ConvNets and Vision\ntransformers (ViT) exhibit well-known limitations, namely the former struggle\nto model long-range dependencies while the latter are computationally\ninefficient, rendering them challenging to train on large-scale datasets.\nVision Mamba, an architecture based on State Space Models has emerged as an\nalternative addressing the aforementioned deficiencies and has been already\napplied to remote sensing change detection, though mostly as a feature\nextracting backbone. In this article the Change State Space Model is\nintroduced, that has been specifically designed for change detection by\nfocusing on the relevant changes between bi-temporal images, effectively\nfiltering out irrelevant information. By concentrating solely on the changed\nfeatures, the number of network parameters is reduced, enhancing significantly\ncomputational efficiency while maintaining high detection performance and\nrobustness against input degradation. The proposed model has been evaluated via\nthree benchmark datasets, where it outperformed ConvNets, ViTs, and Mamba-based\ncounterparts at a fraction of their computational complexity. The\nimplementation will be made available at https://github.com/Elman295/CSSM upon\nacceptance.\n","authors":["Elman Ghazaei","Erchan Aptoula"],"pdf_url":"https://arxiv.org/pdf/2504.11080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11066v1","updated":"2025-04-15T11:01:06Z","published":"2025-04-15T11:01:06Z","title":"Improving fingerprint presentation attack detection by an approach\n  integrated into the personal verification stage","summary":"  Presentation Attack Detection (PAD) systems are usually designed\nindependently of the fingerprint verification system. While this can be\nacceptable for use cases where specific user templates are not predetermined,\nit represents a missed opportunity to enhance security in scenarios where\nintegrating PAD with the fingerprint verification system could significantly\nleverage users' templates, which are the real target of a potential\npresentation attack. This does not mean that a PAD should be specifically\ndesigned for such users; that would imply the availability of many enrolled\nusers' PAI and, consequently, complexity, time, and cost increase. On the\ncontrary, we propose to equip a basic PAD, designed according to the state of\nthe art, with an innovative add-on module called the Closeness Binary Code (CC)\nmodule. The term \"closeness\" refers to a peculiar property of the bona\nfide-related features: in an Euclidean feature space, genuine fingerprints tend\nto cluster in a specific pattern. First, samples from the same finger are close\nto each other, then samples from other fingers of the same user and finally,\nsamples from fingers of other users. This property is statistically verified in\nour previous publication, and further confirmed in this paper. It is\nindependent of the user population and the feature set class, which can be\nhandcrafted or deep network-based (embeddings). Therefore, the add-on can be\ndesigned without the need for the targeted user samples; moreover, it exploits\nher/his samples' \"closeness\" property during the verification stage. Extensive\nexperiments on benchmark datasets and state-of-the-art PAD methods confirm the\nbenefits of the proposed add-on, which can be easily coupled with the main PAD\nmodule integrated into the fingerprint verification system.\n","authors":["Marco Micheletto","Giulia Orrù","Luca Ghiani","Gian Luca Marcialis"],"pdf_url":"https://arxiv.org/pdf/2504.11066v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2504.10190v2","updated":"2025-04-15T10:59:35Z","published":"2025-04-14T12:50:37Z","title":"Differentially Private 2D Human Pose Estimation","summary":"  Human pose estimation (HPE) has become essential in numerous applications\nincluding healthcare, activity recognition, and human-computer interaction.\nHowever, the privacy implications of processing sensitive visual data present\nsignificant deployment barriers in critical domains. While traditional\nanonymization techniques offer limited protection and often compromise data\nutility for broader motion analysis, Differential Privacy (DP) provides formal\nprivacy guarantees but typically degrades model performance when applied\nnaively. In this work, we present the first differentially private 2D human\npose estimation (2D-HPE) by applying Differentially Private Stochastic Gradient\nDescent (DP-SGD) to this task. To effectively balance privacy with performance,\nwe adopt Projected DP-SGD (PDP-SGD), which projects the noisy gradients to a\nlow-dimensional subspace. Additionally, we adapt TinyViT, a compact and\nefficient vision transformer for coordinate classification in HPE, providing a\nlightweight yet powerful backbone that enhances privacy-preserving deployment\nfeasibility on resource-limited devices. Our approach is particularly valuable\nfor multimedia interpretation tasks, enabling privacy-safe analysis and\nunderstanding of human motion across diverse visual media while preserving the\nsemantic meaning required for downstream applications. Comprehensive\nexperiments on the MPII Human Pose Dataset demonstrate significant performance\nenhancement with PDP-SGD achieving 78.48% PCKh@0.5 at a strict privacy budget\n($\\epsilon=0.2$), compared to 63.85% for standard DP-SGD. This work lays\nfoundation for privacy-preserving human pose estimation in real-world,\nsensitive applications.\n","authors":["Kaushik Bhargav Sivangi","Idris Zakariyya","Paul Henderson","Fani Deligianni"],"pdf_url":"https://arxiv.org/pdf/2504.10190v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11063v1","updated":"2025-04-15T10:52:19Z","published":"2025-04-15T10:52:19Z","title":"UKDM: Underwater keypoint detection and matching using underwater image\n  enhancement techniques","summary":"  The purpose of this paper is to explore the use of underwater image\nenhancement techniques to improve keypoint detection and matching. By applying\nadvanced deep learning models, including generative adversarial networks and\nconvolutional neural networks, we aim to find the best method which improves\nthe accuracy of keypoint detection and the robustness of matching algorithms.\nWe evaluate the performance of these techniques on various underwater datasets,\ndemonstrating significant improvements over traditional methods.\n","authors":["Pedro Diaz-Garcia","Felix Escalona","Miguel Cazorla"],"pdf_url":"https://arxiv.org/pdf/2504.11063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04960v4","updated":"2025-04-15T10:50:18Z","published":"2023-12-08T10:50:02Z","title":"MIMIR: Masked Image Modeling for Mutual Information-based Adversarial\n  Robustness","summary":"  Vision Transformers (ViTs) have emerged as a fundamental architecture and\nserve as the backbone of modern vision-language models. Despite their\nimpressive performance, ViTs exhibit notable vulnerability to evasion attacks,\nnecessitating the development of specialized Adversarial Training (AT)\nstrategies tailored to their unique architecture. While a direct solution might\ninvolve applying existing AT methods to ViTs, our analysis reveals significant\nincompatibilities, particularly with state-of-the-art (SOTA) approaches such as\nGeneralist (CVPR 2023) and DBAT (USENIX Security 2024). This paper presents a\nsystematic investigation of adversarial robustness in ViTs and provides a novel\ntheoretical Mutual Information (MI) analysis in its autoencoder-based\nself-supervised pre-training. Specifically, we show that MI between the\nadversarial example and its latent representation in ViT-based autoencoders\nshould be constrained via derived MI bounds. Building on this insight, we\npropose a self-supervised AT method, MIMIR, that employs an MI penalty to\nfacilitate adversarial pre-training by masked image modeling with autoencoders.\nExtensive experiments on CIFAR-10, Tiny-ImageNet, and ImageNet-1K show that\nMIMIR can consistently provide improved natural and robust accuracy, where\nMIMIR outperforms SOTA AT results on ImageNet-1K. Notably, MIMIR demonstrates\nsuperior robustness against unforeseen attacks and common corruption data and\ncan also withstand adaptive attacks where the adversary possesses full\nknowledge of the defense mechanism.\n","authors":["Xiaoyun Xu","Shujian Yu","Zhuoran Liu","Stjepan Picek"],"pdf_url":"https://arxiv.org/pdf/2312.04960v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11055v1","updated":"2025-04-15T10:42:25Z","published":"2025-04-15T10:42:25Z","title":"Crane: Context-Guided Prompt Learning and Attention Refinement for\n  Zero-Shot Anomaly Detections","summary":"  Anomaly Detection (AD) involves identifying deviations from normal data\ndistributions and is critical in fields such as medical diagnostics and\nindustrial defect detection. Traditional AD methods typically require the\navailability of normal training samples; however, this assumption is not always\nfeasible, as collecting such data can be impractical. Additionally, these\nmethods often struggle to generalize across different domains. Recent\nadvancements, such as AnomalyCLIP and AdaCLIP, utilize the zero-shot\ngeneralization capabilities of CLIP but still face a performance gap between\nimage-level and pixel-level anomaly detection. To address this gap, we propose\na novel approach that conditions the prompts of the text encoder based on image\ncontext extracted from the vision encoder. Also, to capture fine-grained\nvariations more effectively, we have modified the CLIP vision encoder and\naltered the extraction of dense features. These changes ensure that the\nfeatures retain richer spatial and structural information for both normal and\nanomalous prompts. Our method achieves state-of-the-art performance, improving\nperformance by 2% to 29% across different metrics on 14 datasets. This\ndemonstrates its effectiveness in both image-level and pixel-level anomaly\ndetection.\n","authors":["Alireza Salehi","Mohammadreza Salehi","Reshad Hosseini","Cees G. M. Snoek","Makoto Yamada","Mohammad Sabokrou"],"pdf_url":"https://arxiv.org/pdf/2504.11055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11050v1","updated":"2025-04-15T10:34:23Z","published":"2025-04-15T10:34:23Z","title":"Leveraging LLMs and attention-mechanism for automatic annotation of\n  historical maps","summary":"  Historical maps are essential resources that provide insights into the\ngeographical landscapes of the past. They serve as valuable tools for\nresearchers across disciplines such as history, geography, and urban studies,\nfacilitating the reconstruction of historical environments and the analysis of\nspatial transformations over time. However, when constrained to analogue or\nscanned formats, their interpretation is limited to humans and therefore not\nscalable. Recent advancements in machine learning, particularly in computer\nvision and large language models (LLMs), have opened new avenues for automating\nthe recognition and classification of features and objects in historical maps.\nIn this paper, we propose a novel distillation method that leverages LLMs and\nattention mechanisms for the automatic annotation of historical maps. LLMs are\nemployed to generate coarse classification labels for low-resolution historical\nimage patches, while attention mechanisms are utilized to refine these labels\nto higher resolutions. Experimental results demonstrate that the refined labels\nachieve a high recall of more than 90%. Additionally, the intersection over\nunion (IoU) scores--84.2% for Wood and 72.0% for Settlement--along with\nprecision scores of 87.1% and 79.5%, respectively, indicate that most labels\nare well-aligned with ground-truth annotations. Notably, these results were\nachieved without the use of fine-grained manual labels during training,\nunderscoring the potential of our approach for efficient and scalable\nhistorical map analysis.\n","authors":["Yunshuang Yuan","Monika Sester"],"pdf_url":"https://arxiv.org/pdf/2504.11050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09353v2","updated":"2025-04-15T10:14:34Z","published":"2024-12-12T15:22:03Z","title":"Causal Graphical Models for Vision-Language Compositional Understanding","summary":"  Recent work has empirically shown that Vision-Language Models (VLMs) struggle\nto fully understand the compositional properties of the human language, usually\nmodeling an image caption as a \"bag of words\". As a result, they perform poorly\non compositional tasks, which require a deeper understanding of the different\nentities of a sentence (subject, verb, etc.) jointly with their mutual\nrelationships in order to be solved. In this paper, we model the dependency\nrelations among textual and visual tokens using a Causal Graphical Model (CGM),\nbuilt using a dependency parser, and we train a decoder conditioned by the VLM\nvisual encoder. Differently from standard autoregressive or parallel\npredictions, our decoder's generative process is partially-ordered following\nthe CGM structure. This structure encourages the decoder to learn only the main\ncausal dependencies in a sentence discarding spurious correlations. Using\nextensive experiments on five compositional benchmarks, we show that our method\nsignificantly outperforms all the state-of-the-art compositional approaches by\na large margin, and it also improves over methods trained using much larger\ndatasets.\n","authors":["Fiorenzo Parascandolo","Nicholas Moratelli","Enver Sangineto","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2412.09353v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2504.11038v1","updated":"2025-04-15T10:00:01Z","published":"2025-04-15T10:00:01Z","title":"QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models","summary":"  In typical multimodal tasks, such as Visual Question Answering (VQA),\nadversarial attacks targeting a specific image and question can lead large\nvision-language models (LVLMs) to provide incorrect answers. However, it is\ncommon for a single image to be associated with multiple questions, and LVLMs\nmay still answer other questions correctly even for an adversarial image\nattacked by a specific question. To address this, we introduce the\nquery-agnostic visual attack (QAVA), which aims to create robust adversarial\nexamples that generate incorrect responses to unspecified and unknown\nquestions. Compared to traditional adversarial attacks focused on specific\nimages and questions, QAVA significantly enhances the effectiveness and\nefficiency of attacks on images when the question is unknown, achieving\nperformance comparable to attacks on known target questions. Our research\nbroadens the scope of visual adversarial attacks on LVLMs in practical\nsettings, uncovering previously overlooked vulnerabilities, particularly in the\ncontext of visual adversarial threats. The code is available at\nhttps://github.com/btzyd/qava.\n","authors":["Yudong Zhang","Ruobing Xie","Jiansheng Chen","Xingwu Sun","Zhanhui Kang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2504.11038v1.pdf","comment":"Accepted by NAACL 2025 main"},{"id":"http://arxiv.org/abs/2504.11034v1","updated":"2025-04-15T09:57:17Z","published":"2025-04-15T09:57:17Z","title":"Defending Against Frequency-Based Attacks with Diffusion Models","summary":"  Adversarial training is a common strategy for enhancing model robustness\nagainst adversarial attacks. However, it is typically tailored to the specific\nattack types it is trained on, limiting its ability to generalize to unseen\nthreat models. Adversarial purification offers an alternative by leveraging a\ngenerative model to remove perturbations before classification. Since the\npurifier is trained independently of both the classifier and the threat models,\nit is better equipped to handle previously unseen attack scenarios. Diffusion\nmodels have proven highly effective for noise purification, not only in\ncountering pixel-wise adversarial perturbations but also in addressing\nnon-adversarial data shifts. In this study, we broaden the focus beyond\npixel-wise robustness to explore the extent to which purification can mitigate\nboth spectral and spatial adversarial attacks. Our findings highlight its\neffectiveness in handling diverse distortion patterns across low- to\nhigh-frequency regions.\n","authors":["Fatemeh Amerehi","Patrick Healy"],"pdf_url":"https://arxiv.org/pdf/2504.11034v1.pdf","comment":"Conference on Computer Vision and Pattern Recognition Workshops\n  (CVPRW), 5th Workshop on Adversarial Machine Learning in Computer Vision:\n  Foundation Models + X"},{"id":"http://arxiv.org/abs/2403.11646v2","updated":"2025-04-15T09:56:49Z","published":"2024-03-18T10:42:24Z","title":"MedMerge: Merging Models for Effective Transfer Learning to Medical\n  Imaging Tasks","summary":"  Transfer learning has become a powerful tool to initialize deep learning\nmodels to achieve faster convergence and higher performance. This is especially\nuseful in the medical imaging analysis domain, where data scarcity limits\npossible performance gains for deep learning models. Some advancements have\nbeen made in boosting the transfer learning performance gain by merging models\nstarting from the same initialization. However, in the medical imaging analysis\ndomain, there is an opportunity to merge models starting from different\ninitializations, thus combining the features learned from different tasks. In\nthis work, we propose MedMerge, a method whereby the weights of different\nmodels can be merged, and their features can be effectively utilized to boost\nperformance on a new task. With MedMerge, we learn kernel-level weights that\ncan later be used to merge the models into a single model, even when starting\nfrom different initializations. Testing on various medical imaging analysis\ntasks, we show that our merged model can achieve significant performance gains,\nwith up to 7% improvement on the F1 score. The code implementation of this work\nis available at github.com/BioMedIA-MBZUAI/MedMerge.\n","authors":["Ibrahim Almakky","Santosh Sanjeev","Anees Ur Rehman Hashmi","Mohammad Areeb Qazi","Hu Wang","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.11646v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11031v1","updated":"2025-04-15T09:54:43Z","published":"2025-04-15T09:54:43Z","title":"Acquisition of high-quality images for camera calibration in robotics\n  applications via speech prompts","summary":"  Accurate intrinsic and extrinsic camera calibration can be an important\nprerequisite for robotic applications that rely on vision as input. While there\nis ongoing research on enabling camera calibration using natural images, many\nsystems in practice still rely on using designated calibration targets with\ne.g. checkerboard patterns or April tag grids. Once calibration images from\ndifferent perspectives have been acquired and feature descriptors detected,\nthose are typically used in an optimization process to minimize the geometric\nreprojection error. For this optimization to converge, input images need to be\nof sufficient quality and particularly sharpness; they should neither contain\nmotion blur nor rolling-shutter artifacts that can arise when the calibration\nboard was not static during image capture. In this work, we present a novel\ncalibration image acquisition technique controlled via voice commands recorded\nwith a clip-on microphone, that can be more robust and user-friendly than e.g.\ntriggering capture with a remote control, or filtering out blurry frames from a\nvideo sequence in postprocessing. To achieve this, we use a state-of-the-art\nspeech-to-text transcription model with accurate per-word timestamping to\ncapture trigger words with precise temporal alignment. Our experiments show\nthat the proposed method improves user experience by being fast and efficient,\nallowing us to successfully calibrate complex multi-camera setups.\n","authors":["Timm Linder","Kadir Yilmaz","David B. Adrian","Bastian Leibe"],"pdf_url":"https://arxiv.org/pdf/2504.11031v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2504.11024v1","updated":"2025-04-15T09:49:51Z","published":"2025-04-15T09:49:51Z","title":"Easy3D: A Simple Yet Effective Method for 3D Interactive Segmentation","summary":"  The increasing availability of digital 3D environments, whether through\nimage-based 3D reconstruction, generation, or scans obtained by robots, is\ndriving innovation across various applications. These come with a significant\ndemand for 3D interaction, such as 3D Interactive Segmentation, which is useful\nfor tasks like object selection and manipulation. Additionally, there is a\npersistent need for solutions that are efficient, precise, and performing well\nacross diverse settings, particularly in unseen environments and with\nunfamiliar objects. In this work, we introduce a 3D interactive segmentation\nmethod that consistently surpasses previous state-of-the-art techniques on both\nin-domain and out-of-domain datasets. Our simple approach integrates a\nvoxel-based sparse encoder with a lightweight transformer-based decoder that\nimplements implicit click fusion, achieving superior performance and maximizing\nefficiency. Our method demonstrates substantial improvements on benchmark\ndatasets, including ScanNet, ScanNet++, S3DIS, and KITTI-360, and also on\nunseen geometric distributions such as the ones obtained by Gaussian Splatting.\nThe project web-page is available at https://simonelli-andrea.github.io/easy3d.\n","authors":["Andrea Simonelli","Norman Müller","Peter Kontschieder"],"pdf_url":"https://arxiv.org/pdf/2504.11024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11022v1","updated":"2025-04-15T09:47:57Z","published":"2025-04-15T09:47:57Z","title":"Meta-learning For Few-Shot Time Series Crop Type Classification: A\n  Benchmark On The EuroCropsML Dataset","summary":"  Spatial imbalances in crop type data pose significant challenges for accurate\nclassification in remote sensing applications. Algorithms aiming at\ntransferring knowledge from data-rich to data-scarce tasks have thus surged in\npopularity. However, despite their effectiveness in previous evaluations, their\nperformance in challenging real-world applications is unclear and needs to be\nevaluated. This study benchmarks transfer learning and several meta-learning\nalgorithms, including (First-Order) Model-Agnostic Meta-Learning ((FO)-MAML),\nAlmost No Inner Loop (ANIL), and Task-Informed Meta-Learning (TIML), on the\nreal-world EuroCropsML time series dataset, which combines farmer-reported crop\ndata with Sentinel-2 satellite observations from Estonia, Latvia, and Portugal.\nOur findings indicate that MAML-based meta-learning algorithms achieve slightly\nhigher accuracy compared to simpler transfer learning methods when applied to\ncrop type classification tasks in Estonia after pre-training on data from\nLatvia. However, this improvement comes at the cost of increased computational\ndemands and training time. Moreover, we find that the transfer of knowledge\nbetween geographically disparate regions, such as Estonia and Portugal, poses\nsignificant challenges to all investigated algorithms. These insights\nunderscore the trade-offs between accuracy and computational resource\nrequirements in selecting machine learning methods for real-world crop type\nclassification tasks and highlight the difficulties of transferring knowledge\nbetween different regions of the Earth. To facilitate future research in this\ndomain, we present the first comprehensive benchmark for evaluating transfer\nand meta-learning methods for crop type classification under real-world\nconditions. The corresponding code is publicly available at\nhttps://github.com/dida-do/eurocrops-meta-learning.\n","authors":["Joana Reuss","Jan Macdonald","Simon Becker","Konrad Schultka","Lorenz Richter","Marco Körner"],"pdf_url":"https://arxiv.org/pdf/2504.11022v1.pdf","comment":"19 pages, 7 figures, 12 tables"},{"id":"http://arxiv.org/abs/2503.08580v3","updated":"2025-04-15T09:45:59Z","published":"2025-03-11T16:15:54Z","title":"Comparing Next-Day Wildfire Predictability of MODIS and VIIRS Satellite\n  Data","summary":"  Multiple studies have performed next-day fire prediction using satellite\nimagery. Two main satellites are used to detect wildfires: MODIS and VIIRS.\nBoth satellites provide fire mask products, called MOD14 and VNP14,\nrespectively. Studies have used one or the other, but there has been no\ncomparison between them to determine which might be more suitable for next-day\nfire prediction. In this paper, we first evaluate how well VIIRS and MODIS data\ncan be used to forecast wildfire spread one day ahead. We find that the model\nusing VIIRS as input and VNP14 as target achieves the best results.\nInterestingly, the model using MODIS as input and VNP14 as target performs\nsignificantly better than using VNP14 as input and MOD14 as target. Next, we\ndiscuss why MOD14 might be harder to use for predicting next-day fires. We find\nthat the MOD14 fire mask is highly stochastic and does not correlate with\nreasonable fire spread patterns. This is detrimental for machine learning\ntasks, as the model learns irrational patterns. Therefore, we conclude that\nMOD14 is unsuitable for next-day fire prediction and that VNP14 is a much\nbetter option. However, using MODIS input and VNP14 as target, we achieve a\nsignificant improvement in predictability. This indicates that an improved fire\ndetection model is possible for MODIS. The full code and dataset is available\nonline: https://github.com/justuskarlsson/wildfire-mod14-vnp14\n","authors":["Justus Karlsson","Yonghao Xu","Amanda Berg","Leif Haglund"],"pdf_url":"https://arxiv.org/pdf/2503.08580v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10267v2","updated":"2025-04-15T09:43:41Z","published":"2025-04-14T14:29:38Z","title":"Trade-offs in Privacy-Preserving Eye Tracking through Iris Obfuscation:\n  A Benchmarking Study","summary":"  Recent developments in hardware, computer graphics, and AI may soon enable\nAR/VR head-mounted displays (HMDs) to become everyday devices like smartphones\nand tablets. Eye trackers within HMDs provide a special opportunity for such\nsetups as it is possible to facilitate gaze-based research and interaction.\nHowever, estimating users' gaze information often requires raw eye images and\nvideos that contain iris textures, which are considered a gold standard\nbiometric for user authentication, and this raises privacy concerns. Previous\nresearch in the eye-tracking community focused on obfuscating iris textures\nwhile keeping utility tasks such as gaze estimation accurate. Despite these\nattempts, there is no comprehensive benchmark that evaluates state-of-the-art\napproaches. Considering all, in this paper, we benchmark blurring, noising,\ndownsampling, rubber sheet model, and iris style transfer to obfuscate user\nidentity, and compare their impact on image quality, privacy, utility, and risk\nof imposter attack on two datasets. We use eye segmentation and gaze estimation\nas utility tasks, and reduction in iris recognition accuracy as a measure of\nprivacy protection, and false acceptance rate to estimate risk of attack. Our\nexperiments show that canonical image processing methods like blurring and\nnoising cause a marginal impact on deep learning-based tasks. While\ndownsampling, rubber sheet model, and iris style transfer are effective in\nhiding user identifiers, iris style transfer, with higher computation cost,\noutperforms others in both utility tasks, and is more resilient against spoof\nattacks. Our analyses indicate that there is no universal optimal approach to\nbalance privacy, utility, and computation burden. Therefore, we recommend\npractitioners consider the strengths and weaknesses of each approach, and\npossible combinations of those to reach an optimal privacy-utility trade-off.\n","authors":["Mengdi Wang","Efe Bozkir","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2504.10267v2.pdf","comment":"The 25th International Conference on Digital Signal Processing (DSP\n  2025)"},{"id":"http://arxiv.org/abs/2504.11019v1","updated":"2025-04-15T09:43:13Z","published":"2025-04-15T09:43:13Z","title":"DRIFT open dataset: A drone-derived intelligence for traffic analysis in\n  urban environmen","summary":"  Reliable traffic data are essential for understanding urban mobility and\ndeveloping effective traffic management strategies. This study introduces the\nDRone-derived Intelligence For Traffic analysis (DRIFT) dataset, a large-scale\nurban traffic dataset collected systematically from synchronized drone videos\nat approximately 250 meters altitude, covering nine interconnected\nintersections in Daejeon, South Korea. DRIFT provides high-resolution vehicle\ntrajectories that include directional information, processed through video\nsynchronization and orthomap alignment, resulting in a comprehensive dataset of\n81,699 vehicle trajectories. Through our DRIFT dataset, researchers can\nsimultaneously analyze traffic at multiple scales - from individual vehicle\nmaneuvers like lane-changes and safety metrics such as time-to-collision to\naggregate network flow dynamics across interconnected urban intersections. The\nDRIFT dataset is structured to enable immediate use without additional\npreprocessing, complemented by open-source models for object detection and\ntrajectory extraction, as well as associated analytical tools. DRIFT is\nexpected to significantly contribute to academic research and practical\napplications, such as traffic flow analysis and simulation studies. The dataset\nand related resources are publicly accessible at\nhttps://github.com/AIxMobility/The-DRIFT.\n","authors":["Hyejin Lee","Seokjun Hong","Jeonghoon Song","Haechan Cho","Zhixiong Jin","Byeonghun Kim","Joobin Jin","Jaegyun Im","Byeongjoon Noh","Hwasoo Yeo"],"pdf_url":"https://arxiv.org/pdf/2504.11019v1.pdf","comment":"30 pages, 15 figures"},{"id":"http://arxiv.org/abs/2504.11015v1","updated":"2025-04-15T09:41:08Z","published":"2025-04-15T09:41:08Z","title":"AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and\n  Localization in Diffusion Era","summary":"  Recent advances in image generation, particularly diffusion models, have\nsignificantly lowered the barrier for creating sophisticated forgeries, making\nimage manipulation detection and localization (IMDL) increasingly challenging.\nWhile prior work in IMDL has focused largely on natural images, the anime\ndomain remains underexplored-despite its growing vulnerability to AI-generated\nforgeries. Misrepresentations of AI-generated images as hand-drawn artwork,\ncopyright violations, and inappropriate content modifications pose serious\nthreats to the anime community and industry. To address this gap, we propose\nAnimeDL-2M, the first large-scale benchmark for anime IMDL with comprehensive\nannotations. It comprises over two million images including real, partially\nmanipulated, and fully AI-generated samples. Experiments indicate that models\ntrained on existing IMDL datasets of natural images perform poorly when applied\nto anime images, highlighting a clear domain gap between anime and natural\nimages. To better handle IMDL tasks in anime domain, we further propose\nAniXplore, a novel model tailored to the visual characteristics of anime\nimagery. Extensive evaluations demonstrate that AniXplore achieves superior\nperformance compared to existing methods. Dataset and code can be found in\nhttps://flytweety.github.io/AnimeDL2M/.\n","authors":["Chenyang Zhu","Xing Zhang","Yuyang Sun","Ching-Chun Chang","Isao Echizen"],"pdf_url":"https://arxiv.org/pdf/2504.11015v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11014v1","updated":"2025-04-15T09:37:54Z","published":"2025-04-15T09:37:54Z","title":"GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*","summary":"  The emerging trend in computer vision emphasizes developing universal models\ncapable of simultaneously addressing multiple diverse tasks. Such universality\ntypically requires joint training across multi-domain datasets to ensure\neffective generalization. However, monocular 3D object detection presents\nunique challenges in multi-domain training due to the scarcity of datasets\nannotated with accurate 3D ground-truth labels, especially beyond typical\nroad-based autonomous driving contexts. To address this challenge, we introduce\na novel weakly supervised framework leveraging pseudo-labels. Current\npretrained models often struggle to accurately detect pedestrians in non-road\nenvironments due to inherent dataset biases. Unlike generalized image-based 2D\nobject detection models, achieving similar generalization in monocular 3D\ndetection remains largely unexplored. In this paper, we propose GATE3D, a novel\nframework designed specifically for generalized monocular 3D object detection\nvia weak supervision. GATE3D effectively bridges domain gaps by employing\nconsistency losses between 2D and 3D predictions. Remarkably, our model\nachieves competitive performance on the KITTI benchmark as well as on an\nindoor-office dataset collected by us to evaluate the generalization\ncapabilities of our framework. Our results demonstrate that GATE3D\nsignificantly accelerates learning from limited annotated data through\neffective pre-training strategies, highlighting substantial potential for\nbroader impacts in robotics, augmented reality, and virtual reality\napplications. Project page: https://ies0411.github.io/GATE3D/\n","authors":["Eunsoo Im","Jung Kwon Lee","Changhyun Jee"],"pdf_url":"https://arxiv.org/pdf/2504.11014v1.pdf","comment":"9pages, 1 supple"},{"id":"http://arxiv.org/abs/2504.11008v1","updated":"2025-04-15T09:28:53Z","published":"2025-04-15T09:28:53Z","title":"MediSee: Reasoning-based Pixel-level Perception in Medical Images","summary":"  Despite remarkable advancements in pixel-level medical image perception,\nexisting methods are either limited to specific tasks or heavily rely on\naccurate bounding boxes or text labels as input prompts. However, the medical\nknowledge required for input is a huge obstacle for general public, which\ngreatly reduces the universality of these methods. Compared with these\ndomain-specialized auxiliary information, general users tend to rely on oral\nqueries that require logical reasoning. In this paper, we introduce a novel\nmedical vision task: Medical Reasoning Segmentation and Detection (MedSD),\nwhich aims to comprehend implicit queries about medical images and generate the\ncorresponding segmentation mask and bounding box for the target object. To\naccomplish this task, we first introduce a Multi-perspective, Logic-driven\nMedical Reasoning Segmentation and Detection (MLMR-SD) dataset, which\nencompasses a substantial collection of medical entity targets along with their\ncorresponding reasoning. Furthermore, we propose MediSee, an effective baseline\nmodel designed for medical reasoning segmentation and detection. The\nexperimental results indicate that the proposed method can effectively address\nMedSD with implicit colloquial queries and outperform traditional medical\nreferring segmentation methods.\n","authors":["Qinyue Tong","Ziqian Lu","Jun Liu","Yangming Zheng","Zheming Lu"],"pdf_url":"https://arxiv.org/pdf/2504.11008v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.04918v4","updated":"2025-04-15T09:19:42Z","published":"2025-03-06T19:31:51Z","title":"Fine-Tuning Florence2 for Enhanced Object Detection in Un-constructed\n  Environments: Vision-Language Model Approach","summary":"  Vision-Language Models (VLMs) have emerged as powerful tools in artificial\nintelli-gence, capable of integrating textual and visual data for a unified\nunderstanding of complex scenes. While models such as Florence2, built on\ntransformer architectures, have shown promise across general tasks, their\nperformance in object detection within unstructured or cluttered environments\nremains underexplored. In this study, we fi-ne-tuned the Florence2 model for\nobject detection tasks in non-constructed, complex environments. A\ncomprehensive experimental framework was established involving multiple\nhardware configurations (NVIDIA T4, L4, and A100 GPUs), optimizers (AdamW,\nSGD), and varied hyperparameters including learning rates and LoRA (Low-Rank\nAdaptation) setups. Model training and evaluation were conducted on challenging\ndatasets representative of real-world, disordered settings. The optimized\nFlorence2 models exhibited significant improvements in object detection\naccuracy, with Mean Average Precision (mAP) metrics approaching or matching\nthose of estab-lished models such as YOLOv8, YOLOv9, and YOLOv10. The\nintegration of LoRA and careful fine-tuning of transformer layers contributed\nnotably to these gains. Our find-ings highlight the adaptability of\ntransformer-based VLMs like Florence2 for do-main-specific tasks, particularly\nin visually complex environments. The study under-scores the potential of\nfine-tuned VLMs to rival traditional convolution-based detec-tors, offering a\nflexible and scalable approach for advanced vision applications in re-al-world,\nunstructured settings.\n","authors":["Aysegul Ucar","Soumyadeep Ro","Sanapala Satwika","Pamarthi Yasoda Gayathri","Mohmmad Ghaith Balsha"],"pdf_url":"https://arxiv.org/pdf/2503.04918v4.pdf","comment":"22 pages, 13 Figures, 6 Tables"},{"id":"http://arxiv.org/abs/2504.10995v1","updated":"2025-04-15T09:14:04Z","published":"2025-04-15T09:14:04Z","title":"TMCIR: Token Merge Benefits Composed Image Retrieval","summary":"  Composed Image Retrieval (CIR) retrieves target images using a multi-modal\nquery that combines a reference image with text describing desired\nmodifications. The primary challenge is effectively fusing this visual and\ntextual information. Current cross-modal feature fusion approaches for CIR\nexhibit an inherent bias in intention interpretation. These methods tend to\ndisproportionately emphasize either the reference image features\n(visual-dominant fusion) or the textual modification intent (text-dominant\nfusion through image-to-text conversion). Such an imbalanced representation\noften fails to accurately capture and reflect the actual search intent of the\nuser in the retrieval results. To address this challenge, we propose TMCIR, a\nnovel framework that advances composed image retrieval through two key\ninnovations: 1) Intent-Aware Cross-Modal Alignment. We first fine-tune CLIP\nencoders contrastively using intent-reflecting pseudo-target images,\nsynthesized from reference images and textual descriptions via a diffusion\nmodel. This step enhances the encoder ability of text to capture nuanced\nintents in textual descriptions. 2) Adaptive Token Fusion. We further fine-tune\nall encoders contrastively by comparing adaptive token-fusion features with the\ntarget image. This mechanism dynamically balances visual and textual\nrepresentations within the contrastive learning pipeline, optimizing the\ncomposed feature for retrieval. Extensive experiments on Fashion-IQ and CIRR\ndatasets demonstrate that TMCIR significantly outperforms state-of-the-art\nmethods, particularly in capturing nuanced user intent.\n","authors":["Chaoyang Wang","Zeyu Zhang","Long Teng","Zijun Li","Shichao Kan"],"pdf_url":"https://arxiv.org/pdf/2504.10995v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2310.05473 by other authors"},{"id":"http://arxiv.org/abs/2504.10986v1","updated":"2025-04-15T08:49:29Z","published":"2025-04-15T08:49:29Z","title":"PraNet-V2: Dual-Supervised Reverse Attention for Medical Image\n  Segmentation","summary":"  Accurate medical image segmentation is essential for effective diagnosis and\ntreatment. Previously, PraNet-V1 was proposed to enhance polyp segmentation by\nintroducing a reverse attention (RA) module that utilizes background\ninformation. However, PraNet-V1 struggles with multi-class segmentation tasks.\nTo address this limitation, we propose PraNet-V2, which, compared to PraNet-V1,\neffectively performs a broader range of tasks including multi-class\nsegmentation. At the core of PraNet-V2 is the Dual-Supervised Reverse Attention\n(DSRA) module, which incorporates explicit background supervision, independent\nbackground modeling, and semantically enriched attention fusion. Our PraNet-V2\nframework demonstrates strong performance on four polyp segmentation datasets.\nAdditionally, by integrating DSRA to iteratively enhance foreground\nsegmentation results in three state-of-the-art semantic segmentation models, we\nachieve up to a 1.36% improvement in mean Dice score. Code is available at:\nhttps://github.com/ai4colonoscopy/PraNet-V2/tree/main/binary_seg/jittor.\n","authors":["Bo-Cheng Hu","Ge-Peng Ji","Dian Shao","Deng-Ping Fan"],"pdf_url":"https://arxiv.org/pdf/2504.10986v1.pdf","comment":"Technical report (4 tables 3 figures 8 pages)"},{"id":"http://arxiv.org/abs/2504.10985v1","updated":"2025-04-15T08:48:41Z","published":"2025-04-15T08:48:41Z","title":"DMPT: Decoupled Modality-aware Prompt Tuning for Multi-modal Object\n  Re-identification","summary":"  Current multi-modal object re-identification approaches based on large-scale\npre-trained backbones (i.e., ViT) have displayed remarkable progress and\nachieved excellent performance. However, these methods usually adopt the\nstandard full fine-tuning paradigm, which requires the optimization of\nconsiderable backbone parameters, causing extensive computational and storage\nrequirements. In this work, we propose an efficient prompt-tuning framework\ntailored for multi-modal object re-identification, dubbed DMPT, which freezes\nthe main backbone and only optimizes several newly added decoupled\nmodality-aware parameters. Specifically, we explicitly decouple the visual\nprompts into modality-specific prompts which leverage prior modality knowledge\nfrom a powerful text encoder and modality-independent semantic prompts which\nextract semantic information from multi-modal inputs, such as visible,\nnear-infrared, and thermal-infrared. Built upon the extracted features, we\nfurther design a Prompt Inverse Bind (PromptIBind) strategy that employs bind\nprompts as a medium to connect the semantic prompt tokens of different\nmodalities and facilitates the exchange of complementary multi-modal\ninformation, boosting final re-identification results. Experimental results on\nmultiple common benchmarks demonstrate that our DMPT can achieve competitive\nresults to existing state-of-the-art methods while requiring only 6.5%\nfine-tuning of the backbone parameters.\n","authors":["Minghui Lin","Shu Wang","Xiang Wang","Jianhua Tang","Longbin Fu","Zhengrong Zuo","Nong Sang"],"pdf_url":"https://arxiv.org/pdf/2504.10985v1.pdf","comment":"2025 IEEE/CVF Winter Conference on Applications of Computer Vision\n  (WACV)"},{"id":"http://arxiv.org/abs/2504.10984v1","updated":"2025-04-15T08:47:11Z","published":"2025-04-15T08:47:11Z","title":"Seeing like a Cephalopod: Colour Vision with a Monochrome Event Camera","summary":"  Cephalopods exhibit unique colour discrimination capabilities despite having\none type of photoreceptor, relying instead on chromatic aberration induced by\ntheir ocular optics and pupil shapes to perceive spectral information. We took\ninspiration from this biological mechanism to design a spectral imaging system\nthat combines a ball lens with an event-based camera. Our approach relies on a\nmotorised system that shifts the focal position, mirroring the adaptive lens\nmotion in cephalopods. This approach has enabled us to achieve\nwavelength-dependent focusing across the visible light and near-infrared\nspectrum, making the event a spectral sensor. We characterise chromatic\naberration effects, using both event-based and conventional frame-based\nsensors, validating the effectiveness of bio-inspired spectral discrimination\nboth in simulation and in a real setup as well as assessing the spectral\ndiscrimination performance. Our proposed approach provides a robust spectral\nsensing capability without conventional colour filters or computational\ndemosaicing. This approach opens new pathways toward new spectral sensing\nsystems inspired by nature's evolutionary solutions. Code and analysis are\navailable at: https://samiarja.github.io/neuromorphic_octopus_eye/\n","authors":["Sami Arja","Nimrod Kruger","Alexandre Marcireau","Nicholas Owen Ralph","Saeed Afshar","Gregory Cohen"],"pdf_url":"https://arxiv.org/pdf/2504.10984v1.pdf","comment":"15 pages, 14 figures, 1 table. Accepted at CVPR 2025 (Workshop on\n  Event-based Vision)"},{"id":"http://arxiv.org/abs/2504.10979v1","updated":"2025-04-15T08:44:42Z","published":"2025-04-15T08:44:42Z","title":"Deep Learning in Concealed Dense Prediction","summary":"  Deep learning is developing rapidly and handling common computer vision tasks\nwell. It is time to pay attention to more complex vision tasks, as model size,\nknowledge, and reasoning capabilities continue to improve. In this paper, we\nintroduce and review a family of complex tasks, termed Concealed Dense\nPrediction (CDP), which has great value in agriculture, industry, etc. CDP's\nintrinsic trait is that the targets are concealed in their surroundings, thus\nfully perceiving them requires fine-grained representations, prior knowledge,\nauxiliary reasoning, etc. The contributions of this review are three-fold: (i)\nWe introduce the scope, characteristics, and challenges specific to CDP tasks\nand emphasize their essential differences from generic vision tasks. (ii) We\ndevelop a taxonomy based on concealment counteracting to summarize deep\nlearning efforts in CDP through experiments on three tasks. We compare 25\nstate-of-the-art methods across 12 widely used concealed datasets. (iii) We\ndiscuss the potential applications of CDP in the large model era and summarize\n6 potential research directions. We offer perspectives for the future\ndevelopment of CDP by constructing a large-scale multimodal instruction\nfine-tuning dataset, CvpINST, and a concealed visual perception agent,\nCvpAgent.\n","authors":["Pancheng Zhao","Deng-Ping Fan","Shupeng Cheng","Salman Khan","Fahad Shahbaz Khan","David Clifton","Peng Xu","Jufeng Yang"],"pdf_url":"https://arxiv.org/pdf/2504.10979v1.pdf","comment":"Technique Report"},{"id":"http://arxiv.org/abs/2504.09975v2","updated":"2025-04-15T08:40:13Z","published":"2025-04-14T08:31:26Z","title":"OctGPT: Octree-based Multiscale Autoregressive Models for 3D Shape\n  Generation","summary":"  Autoregressive models have achieved remarkable success across various\ndomains, yet their performance in 3D shape generation lags significantly behind\nthat of diffusion models. In this paper, we introduce OctGPT, a novel\nmultiscale autoregressive model for 3D shape generation that dramatically\nimproves the efficiency and performance of prior 3D autoregressive approaches,\nwhile rivaling or surpassing state-of-the-art diffusion models. Our method\nemploys a serialized octree representation to efficiently capture the\nhierarchical and spatial structures of 3D shapes. Coarse geometry is encoded\nvia octree structures, while fine-grained details are represented by binary\ntokens generated using a vector quantized variational autoencoder (VQVAE),\ntransforming 3D shapes into compact multiscale binary sequences suitable for\nautoregressive prediction. To address the computational challenges of handling\nlong sequences, we incorporate octree-based transformers enhanced with 3D\nrotary positional encodings, scale-specific embeddings, and token-parallel\ngeneration schemes. These innovations reduce training time by 13 folds and\ngeneration time by 69 folds, enabling the efficient training of high-resolution\n3D shapes, e.g.,$1024^3$, on just four NVIDIA 4090 GPUs only within days.\nOctGPT showcases exceptional versatility across various tasks, including text-,\nsketch-, and image-conditioned generation, as well as scene-level synthesis\ninvolving multiple objects. Extensive experiments demonstrate that OctGPT\naccelerates convergence and improves generation quality over prior\nautoregressive methods, offering a new paradigm for high-quality, scalable 3D\ncontent creation. Our code and trained models are available at\nhttps://github.com/octree-nn/octgpt.\n","authors":["Si-Tong Wei","Rui-Huan Wang","Chuan-Zhi Zhou","Baoquan Chen","Peng-Shuai Wang"],"pdf_url":"https://arxiv.org/pdf/2504.09975v2.pdf","comment":"SIGGRAPH 2025"},{"id":"http://arxiv.org/abs/2504.10978v1","updated":"2025-04-15T08:39:35Z","published":"2025-04-15T08:39:35Z","title":"AgentPolyp: Accurate Polyp Segmentation via Image Enhancement Agent","summary":"  Since human and environmental factors interfere, captured polyp images\nusually suffer from issues such as dim lighting, blur, and overexposure, which\npose challenges for downstream polyp segmentation tasks. To address the\nchallenges of noise-induced degradation in polyp images, we present AgentPolyp,\na novel framework integrating CLIP-based semantic guidance and dynamic image\nenhancement with a lightweight neural network for segmentation. The agent first\nevaluates image quality using CLIP-driven semantic analysis (e.g., identifying\n``low-contrast polyps with vascular textures\") and adapts reinforcement\nlearning strategies to dynamically apply multi-modal enhancement operations\n(e.g., denoising, contrast adjustment). A quality assessment feedback loop\noptimizes pixel-level enhancement and segmentation focus in a collaborative\nmanner, ensuring robust preprocessing before neural network segmentation. This\nmodular architecture supports plug-and-play extensions for various enhancement\nalgorithms and segmentation networks, meeting deployment requirements for\nendoscopic devices.\n","authors":["Pu Wang","Zhihua Zhang","Dianjie Lu","Guijuan Zhang","Youshan Zhang","Zhuoran Zheng"],"pdf_url":"https://arxiv.org/pdf/2504.10978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10976v1","updated":"2025-04-15T08:37:24Z","published":"2025-04-15T08:37:24Z","title":"Adaptive Decision Boundary for Few-Shot Class-Incremental Learning","summary":"  Few-Shot Class-Incremental Learning (FSCIL) aims to continuously learn new\nclasses from a limited set of training samples without forgetting knowledge of\npreviously learned classes. Conventional FSCIL methods typically build a robust\nfeature extractor during the base training session with abundant training\nsamples and subsequently freeze this extractor, only fine-tuning the classifier\nin subsequent incremental phases. However, current strategies primarily focus\non preventing catastrophic forgetting, considering only the relationship\nbetween novel and base classes, without paying attention to the specific\ndecision spaces of each class. To address this challenge, we propose a\nplug-and-play Adaptive Decision Boundary Strategy (ADBS), which is compatible\nwith most FSCIL methods. Specifically, we assign a specific decision boundary\nto each class and adaptively adjust these boundaries during training to\noptimally refine the decision spaces for the classes in each session.\nFurthermore, to amplify the distinctiveness between classes, we employ a novel\ninter-class constraint loss that optimizes the decision boundaries and\nprototypes for each class. Extensive experiments on three benchmarks, namely\nCIFAR100, miniImageNet, and CUB200, demonstrate that incorporating our ADBS\nmethod with existing FSCIL techniques significantly improves performance,\nachieving overall state-of-the-art results.\n","authors":["Linhao Li","Yongzhang Tan","Siyuan Yang","Hao Cheng","Yongfeng Dong","Liang Yang"],"pdf_url":"https://arxiv.org/pdf/2504.10976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10974v1","updated":"2025-04-15T08:34:56Z","published":"2025-04-15T08:34:56Z","title":"Self-Supervised Enhancement of Forward-Looking Sonar Images: Bridging\n  Cross-Modal Degradation Gaps through Feature Space Transformation and\n  Multi-Frame Fusion","summary":"  Enhancing forward-looking sonar images is critical for accurate underwater\ntarget detection. Current deep learning methods mainly rely on supervised\ntraining with simulated data, but the difficulty in obtaining high-quality\nreal-world paired data limits their practical use and generalization. Although\nself-supervised approaches from remote sensing partially alleviate data\nshortages, they neglect the cross-modal degradation gap between sonar and\nremote sensing images. Directly transferring pretrained weights often leads to\noverly smooth sonar images, detail loss, and insufficient brightness. To\naddress this, we propose a feature-space transformation that maps sonar images\nfrom the pixel domain to a robust feature domain, effectively bridging the\ndegradation gap. Additionally, our self-supervised multi-frame fusion strategy\nleverages complementary inter-frame information to naturally remove speckle\nnoise and enhance target-region brightness. Experiments on three self-collected\nreal-world forward-looking sonar datasets show that our method significantly\noutperforms existing approaches, effectively suppressing noise, preserving\ndetailed edges, and substantially improving brightness, demonstrating strong\npotential for underwater target detection applications.\n","authors":["Zhisheng Zhang","Peng Zhang","Fengxiang Wang","Liangli Ma","Fuchun Sun"],"pdf_url":"https://arxiv.org/pdf/2504.10974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10972v1","updated":"2025-04-15T08:29:54Z","published":"2025-04-15T08:29:54Z","title":"AFiRe: Anatomy-Driven Self-Supervised Learning for Fine-Grained\n  Representation in Radiographic Images","summary":"  Current self-supervised methods, such as contrastive learning, predominantly\nfocus on global discrimination, neglecting the critical fine-grained anatomical\ndetails required for accurate radiographic analysis. To address this challenge,\nwe propose an Anatomy-driven self-supervised framework for enhancing\nFine-grained Representation in radiographic image analysis (AFiRe). The core\nidea of AFiRe is to align the anatomical consistency with the unique\ntoken-processing characteristics of Vision Transformer. Specifically, AFiRe\nsynergistically performs two self-supervised schemes: (i) Token-wise\nanatomy-guided contrastive learning, which aligns image tokens based on\nstructural and categorical consistency, thereby enhancing fine-grained\nspatial-anatomical discrimination; (ii) Pixel-level anomaly-removal\nrestoration, which particularly focuses on local anomalies, thereby refining\nthe learned discrimination with detailed geometrical information. Additionally,\nwe propose Synthetic Lesion Mask to enhance anatomical diversity while\npreserving intra-consistency, which is typically corrupted by traditional data\naugmentations, such as Cropping and Affine transformations. Experimental\nresults show that AFiRe: (i) provides robust anatomical discrimination,\nachieving more cohesive feature clusters compared to state-of-the-art\ncontrastive learning methods; (ii) demonstrates superior generalization,\nsurpassing 7 radiography-specific self-supervised methods in multi-label\nclassification tasks with limited labeling; and (iii) integrates fine-grained\ninformation, enabling precise anomaly detection using only image-level\nannotations.\n","authors":["Yihang Liu","Lianghua He","Ying Wen","Longzhen Yang","Hongzhou Chen"],"pdf_url":"https://arxiv.org/pdf/2504.10972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09048v2","updated":"2025-04-15T08:25:21Z","published":"2025-04-12T02:05:55Z","title":"BlockGaussian: Efficient Large-Scale Scene Novel View Synthesis via\n  Adaptive Block-Based Gaussian Splatting","summary":"  The recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated\nremarkable potential in novel view synthesis tasks. The divide-and-conquer\nparadigm has enabled large-scale scene reconstruction, but significant\nchallenges remain in scene partitioning, optimization, and merging processes.\nThis paper introduces BlockGaussian, a novel framework incorporating a\ncontent-aware scene partition strategy and visibility-aware block optimization\nto achieve efficient and high-quality large-scale scene reconstruction.\nSpecifically, our approach considers the content-complexity variation across\ndifferent regions and balances computational load during scene partitioning,\nenabling efficient scene reconstruction. To tackle the supervision mismatch\nissue during independent block optimization, we introduce auxiliary points\nduring individual block optimization to align the ground-truth supervision,\nwhich enhances the reconstruction quality. Furthermore, we propose a\npseudo-view geometry constraint that effectively mitigates rendering\ndegradation caused by airspace floaters during block merging. Extensive\nexperiments on large-scale scenes demonstrate that our approach achieves\nstate-of-the-art performance in both reconstruction efficiency and rendering\nquality, with a 5x speedup in optimization and an average PSNR improvement of\n1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces\ncomputational requirements, enabling large-scale scene reconstruction on a\nsingle 24GB VRAM device. The project page is available at\nhttps://github.com/SunshineWYC/BlockGaussian\n","authors":["Yongchang Wu","Zipeng Qi","Zhenwei Shi","Zhengxia Zou"],"pdf_url":"https://arxiv.org/pdf/2504.09048v2.pdf","comment":"https://github.com/SunshineWYC/BlockGaussian"},{"id":"http://arxiv.org/abs/2503.08306v4","updated":"2025-04-15T08:24:40Z","published":"2025-03-11T11:16:47Z","title":"Reasoning in visual navigation of end-to-end trained agents: a dynamical\n  systems approach","summary":"  Progress in Embodied AI has made it possible for end-to-end-trained agents to\nnavigate in photo-realistic environments with high-level reasoning and\nzero-shot or language-conditioned behavior, but benchmarks are still dominated\nby simulation. In this work, we focus on the fine-grained behavior of\nfast-moving real robots and present a large-scale experimental study involving\n\\numepisodes{} navigation episodes in a real environment with a physical robot,\nwhere we analyze the type of reasoning emerging from end-to-end training. In\nparticular, we study the presence of realistic dynamics which the agent learned\nfor open-loop forecasting, and their interplay with sensing. We analyze the way\nthe agent uses latent memory to hold elements of the scene structure and\ninformation gathered during exploration. We probe the planning capabilities of\nthe agent, and find in its memory evidence for somewhat precise plans over a\nlimited horizon. Furthermore, we show in a post-hoc analysis that the value\nfunction learned by the agent relates to long-term planning. Put together, our\nexperiments paint a new picture on how using tools from computer vision and\nsequential decision making have led to new capabilities in robotics and\ncontrol. An interactive tool is available at\neurope.naverlabs.com/research/publications/reasoning-in-visual-navigation-of-end-to-end-trained-agents.\n","authors":["Steeven Janny","Hervé Poirier","Leonid Antsfeld","Guillaume Bono","Gianluca Monaci","Boris Chidlovskii","Francesco Giuliari","Alessio Del Bue","Christian Wolf"],"pdf_url":"https://arxiv.org/pdf/2503.08306v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18035v2","updated":"2025-04-15T08:22:14Z","published":"2025-03-23T11:36:19Z","title":"Text-Driven 3D Lidar Place Recognition for Autonomous Driving","summary":"  Environment description-based localization in large-scale point cloud maps\nconstructed through remote sensing is critically significant for the\nadvancement of large-scale autonomous systems, such as delivery robots\noperating in the last mile. However, current approaches encounter challenges\ndue to the inability of point cloud encoders to effectively capture local\ndetails and long-range spatial relationships, as well as a significant modality\ngap between text and point cloud representations. To address these challenges,\nwe present Des4Pos, a novel two-stage text-driven remote sensing localization\nframework. In the coarse stage, the point-cloud encoder utilizes the\nMulti-scale Fusion Attention Mechanism (MFAM) to enhance local geometric\nfeatures, followed by a bidirectional Long Short-Term Memory (LSTM) module to\nstrengthen global spatial relationships. Concurrently, the Stepped Text Encoder\n(STE) integrates cross-modal prior knowledge from CLIP [1] and aligns text and\npoint-cloud features using this prior knowledge, effectively bridging modality\ndiscrepancies. In the fine stage, we introduce a Cascaded Residual Attention\n(CRA) module to fuse cross-modal features and predict relative localization\noffsets, thereby achieving greater localization precision. Experiments on the\nKITTI360Pose test set demonstrate that Des4Pos achieves state-of-the-art\nperformance in text-to-point-cloud place recognition. Specifically, it attains\na top-1 accuracy of 40% and a top-10 accuracy of 77% under a 5-meter radius\nthreshold, surpassing the best existing methods by 7% and 7%, respectively.\n","authors":["Tianyi Shang","Zhenyu Li","Pengjie Xu","Zhaojun Deng","Ruirui Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.18035v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2504.10967v1","updated":"2025-04-15T08:19:12Z","published":"2025-04-15T08:19:12Z","title":"An Efficient and Mixed Heterogeneous Model for Image Restoration","summary":"  Image restoration~(IR), as a fundamental multimedia data processing task, has\na significant impact on downstream visual applications. In recent years,\nresearchers have focused on developing general-purpose IR models capable of\nhandling diverse degradation types, thereby reducing the cost and complexity of\nmodel development. Current mainstream approaches are based on three\narchitectural paradigms: CNNs, Transformers, and Mambas. CNNs excel in\nefficient inference, whereas Transformers and Mamba excel at capturing\nlong-range dependencies and modeling global contexts. While each architecture\nhas demonstrated success in specialized, single-task settings, limited efforts\nhave been made to effectively integrate heterogeneous architectures to jointly\naddress diverse IR challenges. To bridge this gap, we propose RestorMixer, an\nefficient and general-purpose IR model based on mixed-architecture fusion.\nRestorMixer adopts a three-stage encoder-decoder structure, where each stage is\ntailored to the resolution and feature characteristics of the input. In the\ninitial high-resolution stage, CNN-based blocks are employed to rapidly extract\nshallow local features. In the subsequent stages, we integrate a refined\nmulti-directional scanning Mamba module with a multi-scale window-based\nself-attention mechanism. This hierarchical and adaptive design enables the\nmodel to leverage the strengths of CNNs in local feature extraction, Mamba in\nglobal context modeling, and attention mechanisms in dynamic feature\nrefinement. Extensive experimental results demonstrate that RestorMixer\nachieves leading performance across multiple IR tasks while maintaining high\ninference efficiency. The official code can be accessed at\nhttps://github.com/ClimBin/RestorMixer.\n","authors":["Yubin Gu","Yuan Meng","Kaihang Zheng","Xiaoshuai Sun","Jiayi Ji","Weijian Ruan","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2504.10967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07966v6","updated":"2025-04-15T08:18:18Z","published":"2024-06-12T07:44:22Z","title":"Real-world Image Dehazing with Coherence-based Pseudo Labeling and\n  Cooperative Unfolding Network","summary":"  Real-world Image Dehazing (RID) aims to alleviate haze-induced degradation in\nreal-world settings. This task remains challenging due to the complexities in\naccurately modeling real haze distributions and the scarcity of paired\nreal-world data. To address these challenges, we first introduce a cooperative\nunfolding network that jointly models atmospheric scattering and image scenes,\neffectively integrating physical knowledge into deep networks to restore\nhaze-contaminated details. Additionally, we propose the first RID-oriented\niterative mean-teacher framework, termed the Coherence-based Label Generator,\nto generate high-quality pseudo labels for network training. Specifically, we\nprovide an optimal label pool to store the best pseudo-labels during network\ntraining, leveraging both global and local coherence to select high-quality\ncandidates and assign weights to prioritize haze-free regions. We verify the\neffectiveness of our method, with experiments demonstrating that it achieves\nstate-of-the-art performance on RID tasks. Code will be available at\nhttps://github.com/cnyvfang/CORUN-Colabator.\n","authors":["Chengyu Fang","Chunming He","Fengyang Xiao","Yulun Zhang","Longxiang Tang","Yuelin Zhang","Kai Li","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2406.07966v6.pdf","comment":"Accepted at NeurIPS 2024 as a Spotlight Paper"},{"id":"http://arxiv.org/abs/2504.05062v2","updated":"2025-04-15T08:15:27Z","published":"2025-04-07T13:33:54Z","title":"LDGNet: A Lightweight Difference Guiding Network for Remote Sensing\n  Change Detection","summary":"  With the rapid advancement of deep learning, the field of change detection\n(CD) in remote sensing imagery has achieved remarkable progress. Existing\nchange detection methods primarily focus on achieving higher accuracy with\nincreased computational costs and parameter sizes, leaving development of\nlightweight methods for rapid real-world processing an underexplored challenge.\nTo address this challenge, we propose a Lightweight Difference Guiding Network\n(LDGNet), leveraging absolute difference image to guide optical remote sensing\nchange detection. First, to enhance the feature representation capability of\nthe lightweight backbone network, we propose the Difference Guiding Module\n(DGM), which leverages multi-scale features extracted from the absolute\ndifference image to progressively influence the original image encoder at each\nlayer, thereby reinforcing feature extraction. Second, we propose the\nDifference-Aware Dynamic Fusion (DADF) module with Visual State Space Model\n(VSSM) for lightweight long-range dependency modeling. The module first uses\nfeature absolute differences to guide VSSM's global contextual modeling of\nchange regions, then employs difference attention to dynamically fuse these\nlong-range features with feature differences, enhancing change semantics while\nsuppressing noise and background. Extensive experiments on multiple datasets\ndemonstrate that our method achieves comparable or superior performance to\ncurrent state-of-the-art (SOTA) methods requiring several times more\ncomputation, while maintaining only 3.43M parameters and 1.12G FLOPs.\n","authors":["Chenfeng Xu"],"pdf_url":"https://arxiv.org/pdf/2504.05062v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14122v2","updated":"2025-04-15T08:15:08Z","published":"2025-01-23T22:36:06Z","title":"Reinforcement Learning Platform for Adversarial Black-box Attacks with\n  Custom Distortion Filters","summary":"  We present a Reinforcement Learning Platform for Adversarial Black-box\nuntargeted and targeted attacks, RLAB, that allows users to select from various\ndistortion filters to create adversarial examples. The platform uses a\nReinforcement Learning agent to add minimum distortion to input images while\nstill causing misclassification by the target model. The agent uses a novel\ndual-action method to explore the input image at each step to identify\nsensitive regions for adding distortions while removing noises that have less\nimpact on the target model. This dual action leads to faster and more efficient\nconvergence of the attack. The platform can also be used to measure the\nrobustness of image classification models against specific distortion types.\nAlso, retraining the model with adversarial samples significantly improved\nrobustness when evaluated on benchmark datasets. The proposed platform\noutperforms state-of-the-art methods in terms of the average number of queries\nrequired to cause misclassification. This advances trustworthiness with a\npositive social impact.\n","authors":["Soumyendu Sarkar","Ashwin Ramesh Babu","Sajad Mousavi","Vineet Gundecha","Sahand Ghorbanpour","Avisek Naug","Ricardo Luna Gutierrez","Antonio Guillen"],"pdf_url":"https://arxiv.org/pdf/2501.14122v2.pdf","comment":"Accepted at the 2025 AAAI Conference on Artificial Intelligence\n  Proceedings"},{"id":"http://arxiv.org/abs/2504.10958v1","updated":"2025-04-15T08:05:16Z","published":"2025-04-15T08:05:16Z","title":"Recognition of Geometrical Shapes by Dictionary Learning","summary":"  Dictionary learning is a versatile method to produce an overcomplete set of\nvectors, called atoms, to represent a given input with only a few atoms. In the\nliterature, it has been used primarily for tasks that explore its powerful\nrepresentation capabilities, such as for image reconstruction. In this work, we\npresent a first approach to make dictionary learning work for shape\nrecognition, considering specifically geometrical shapes. As we demonstrate,\nthe choice of the underlying optimization method has a significant impact on\nrecognition quality. Experimental results confirm that dictionary learning may\nbe an interesting method for shape recognition tasks.\n","authors":["Alexander Köhler","Michael Breuß"],"pdf_url":"https://arxiv.org/pdf/2504.10958v1.pdf","comment":"6 pages, 4 figures, ACDSA 2025 conference"},{"id":"http://arxiv.org/abs/2407.02068v5","updated":"2025-04-15T07:54:24Z","published":"2024-07-02T08:58:19Z","title":"LPViT: Low-Power Semi-structured Pruning for Vision Transformers","summary":"  Vision transformers have emerged as a promising alternative to convolutional\nneural networks for various image analysis tasks, offering comparable or\nsuperior performance. However, one significant drawback of ViTs is their\nresource-intensive nature, leading to increased memory footprint, computation\ncomplexity, and power consumption. To democratize this high-performance\ntechnology and make it more environmentally friendly, it is essential to\ncompress ViT models, reducing their resource requirements while maintaining\nhigh performance. In this paper, we introduce a new block-structured pruning to\naddress the resource-intensive issue for ViTs, offering a balanced trade-off\nbetween accuracy and hardware acceleration. Unlike unstructured pruning or\nchannel-wise structured pruning, block pruning leverages the block-wise\nstructure of linear layers, resulting in more efficient matrix multiplications.\nTo optimize this pruning scheme, our paper proposes a novel hardware-aware\nlearning objective that simultaneously maximizes speedup and minimizes power\nconsumption during inference, tailored to the block sparsity structure. This\nobjective eliminates the need for empirical look-up tables and focuses solely\non reducing parametrized layer connections. Moreover, our paper provides a\nlightweight algorithm to achieve post-training pruning for ViTs, utilizing\nsecond-order Taylor approximation and empirical optimization to solve the\nproposed hardware-aware objective. Extensive experiments on ImageNet are\nconducted across various ViT architectures, including DeiT-B and DeiT-S,\ndemonstrating competitive performance with other pruning methods and achieving\na remarkable balance between accuracy preservation and power savings.\nEspecially, we achieve 3.93x speedup on dedicated hardware and GPUs\nrespectively for DeiT-B, and a power reduction by 1.4x on GPUs. Code released\nto https://github.com/Akimoto-Cris/LPViT.\n","authors":["Kaixin Xu","Zhe Wang","Chunyun Chen","Xue Geng","Jie Lin","Mohamed M. Sabry Aly","Xulei Yang","Min Wu","Xiaoli Li","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2407.02068v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08389v2","updated":"2025-04-15T07:44:57Z","published":"2025-04-11T09:42:46Z","title":"Light-YOLOv8-Flame: A Lightweight High-Performance Flame Detection\n  Algorithm","summary":"  Fire detection algorithms, particularly those based on computer vision,\nencounter significant challenges such as high computational costs and delayed\nresponse times, which hinder their application in real-time systems. To address\nthese limitations, this paper introduces Light-YOLOv8-Flame, a lightweight\nflame detection algorithm specifically designed for fast and efficient\nreal-time deployment. The proposed model enhances the YOLOv8 architecture\nthrough the substitution of the original C2f module with the FasterNet Block\nmodule. This new block combines Partial Convolution (PConv) and Convolution\n(Conv) layers, reducing both computational complexity and model size. A dataset\ncomprising 7,431 images, representing both flame and non-flame scenarios, was\ncollected and augmented for training purposes. Experimental findings indicate\nthat the modified YOLOv8 model achieves a 0.78% gain in mean average precision\n(mAP) and a 2.05% boost in recall, while reducing the parameter count by\n25.34%, with only a marginal decrease in precision by 0.82%. These findings\nhighlight that Light-YOLOv8-Flame offers enhanced detection performance and\nspeed, making it well-suited for real-time fire detection on\nresource-constrained devices.\n","authors":["Jiawei Lan","Ye Tao","Zhibiao Wang","Haoyang Yu","Wenhua Cui"],"pdf_url":"https://arxiv.org/pdf/2504.08389v2.pdf","comment":"12 pages, 19 figures, 6 tables. Submitted to Engineering Letters"},{"id":"http://arxiv.org/abs/2504.10174v2","updated":"2025-04-15T07:41:21Z","published":"2025-04-14T12:26:31Z","title":"LLaVA-ReID: Selective Multi-image Questioner for Interactive Person\n  Re-Identification","summary":"  Traditional text-based person ReID assumes that person descriptions from\nwitnesses are complete and provided at once. However, in real-world scenarios,\nsuch descriptions are often partial or vague. To address this limitation, we\nintroduce a new task called interactive person re-identification (Inter-ReID).\nInter-ReID is a dialogue-based retrieval task that iteratively refines initial\ndescriptions through ongoing interactions with the witnesses. To facilitate the\nstudy of this new task, we construct a dialogue dataset that incorporates\nmultiple types of questions by decomposing fine-grained attributes of\nindividuals. We further propose LLaVA-ReID, a question model that generates\ntargeted questions based on visual and textual contexts to elicit additional\ndetails about the target person. Leveraging a looking-forward strategy, we\nprioritize the most informative questions as supervision during training.\nExperimental results on both Inter-ReID and text-based ReID benchmarks\ndemonstrate that LLaVA-ReID significantly outperforms baselines.\n","authors":["Yiding Lu","Mouxing Yang","Dezhong Peng","Peng Hu","Yijie Lin","Xi Peng"],"pdf_url":"https://arxiv.org/pdf/2504.10174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05810v2","updated":"2025-04-15T07:20:46Z","published":"2025-04-08T08:41:41Z","title":"PaMi-VDPO: Mitigating Video Hallucinations by Prompt-Aware\n  Multi-Instance Video Preference Learning","summary":"  Direct Preference Optimization (DPO) helps reduce hallucinations in Video\nMultimodal Large Language Models (VLLMs), but its reliance on offline\npreference data limits adaptability and fails to capture true video-response\nmisalignment. We propose Video Direct Preference Optimization (VDPO), an online\npreference learning framework that eliminates the need for preference\nannotation by leveraging video augmentations to generate rejected samples while\nkeeping responses fixed. However, selecting effective augmentations is\nnon-trivial, as some clips may be semantically identical to the original under\nspecific prompts, leading to false rejections and disrupting alignment. To\naddress this, we introduce Prompt-aware Multi-instance Learning VDPO\n(PaMi-VDPO), which selects augmentations based on prompt context. Instead of a\nsingle rejection, we construct a candidate set of augmented clips and apply a\nclose-to-far selection strategy, initially ensuring all clips are semantically\nrelevant while then prioritizing the most prompt-aware distinct clip. This\nallows the model to better capture meaningful visual differences, mitigating\nhallucinations, while avoiding false rejections, and improving alignment.\nPaMi-VDPOseamlessly integrates into existing VLLMs without additional\nparameters, GPT-4/human supervision. With only 10k SFT data, it improves the\nbase model by 5.3% on VideoHallucer, surpassing GPT-4o, while maintaining\nstable performance on general video benchmarks.\n","authors":["Xinpeng Ding","Kui Zhang","Jianhua Han","Lanqing Hong","Hang Xu","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2504.05810v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10929v1","updated":"2025-04-15T07:14:35Z","published":"2025-04-15T07:14:35Z","title":"Cross-Frequency Implicit Neural Representation with Self-Evolving\n  Parameters","summary":"  Implicit neural representation (INR) has emerged as a powerful paradigm for\nvisual data representation. However, classical INR methods represent data in\nthe original space mixed with different frequency components, and several\nfeature encoding parameters (e.g., the frequency parameter $\\omega$ or the rank\n$R$) need manual configurations. In this work, we propose a self-evolving\ncross-frequency INR using the Haar wavelet transform (termed CF-INR), which\ndecouples data into four frequency components and employs INRs in the wavelet\nspace. CF-INR allows the characterization of different frequency components\nseparately, thus enabling higher accuracy for data representation. To more\nprecisely characterize cross-frequency components, we propose a cross-frequency\ntensor decomposition paradigm for CF-INR with self-evolving parameters, which\nautomatically updates the rank parameter $R$ and the frequency parameter\n$\\omega$ for each frequency component through self-evolving optimization. This\nself-evolution paradigm eliminates the laborious manual tuning of these\nparameters, and learns a customized cross-frequency feature encoding\nconfiguration for each dataset. We evaluate CF-INR on a variety of visual data\nrepresentation and recovery tasks, including image regression, inpainting,\ndenoising, and cloud removal. Extensive experiments demonstrate that CF-INR\noutperforms state-of-the-art methods in each case.\n","authors":["Chang Yu","Yisi Luo","Kai Ye","Xile Zhao","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2504.10929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19824v4","updated":"2025-04-15T07:11:00Z","published":"2024-11-29T16:34:46Z","title":"SAT-HMR: Real-Time Multi-Person 3D Mesh Estimation via Scale-Adaptive\n  Tokens","summary":"  We propose a one-stage framework for real-time multi-person 3D human mesh\nestimation from a single RGB image. While current one-stage methods, which\nfollow a DETR-style pipeline, achieve state-of-the-art (SOTA) performance with\nhigh-resolution inputs, we observe that this particularly benefits the\nestimation of individuals in smaller scales of the image (e.g., those far from\nthe camera), but at the cost of significantly increased computation overhead.\nTo address this, we introduce scale-adaptive tokens that are dynamically\nadjusted based on the relative scale of each individual in the image within the\nDETR framework. Specifically, individuals in smaller scales are processed at\nhigher resolutions, larger ones at lower resolutions, and background regions\nare further distilled. These scale-adaptive tokens more efficiently encode the\nimage features, facilitating subsequent decoding to regress the human mesh,\nwhile allowing the model to allocate computational resources more effectively\nand focus on more challenging cases. Experiments show that our method preserves\nthe accuracy benefits of high-resolution processing while substantially\nreducing computational cost, achieving real-time inference with performance\ncomparable to SOTA methods.\n","authors":["Chi Su","Xiaoxuan Ma","Jiajun Su","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2411.19824v4.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2504.10920v1","updated":"2025-04-15T07:00:18Z","published":"2025-04-15T07:00:18Z","title":"Towards Efficient Partially Relevant Video Retrieval with Active Moment\n  Discovering","summary":"  Partially relevant video retrieval (PRVR) is a practical yet challenging task\nin text-to-video retrieval, where videos are untrimmed and contain much\nbackground content. The pursuit here is of both effective and efficient\nsolutions to capture the partial correspondence between text queries and\nuntrimmed videos. Existing PRVR methods, which typically focus on modeling\nmulti-scale clip representations, however, suffer from content independence and\ninformation redundancy, impairing retrieval performance. To overcome these\nlimitations, we propose a simple yet effective approach with active moment\ndiscovering (AMDNet). We are committed to discovering video moments that are\nsemantically consistent with their queries. By using learnable span anchors to\ncapture distinct moments and applying masked multi-moment attention to\nemphasize salient moments while suppressing redundant backgrounds, we achieve\nmore compact and informative video representations. To further enhance moment\nmodeling, we introduce a moment diversity loss to encourage different moments\nof distinct regions and a moment relevance loss to promote semantically\nquery-relevant moments, which cooperate with a partially relevant retrieval\nloss for end-to-end optimization. Extensive experiments on two large-scale\nvideo datasets (\\ie, TVR and ActivityNet Captions) demonstrate the superiority\nand efficiency of our AMDNet. In particular, AMDNet is about 15.5 times smaller\n(\\#parameters) while 6.0 points higher (SumR) than the up-to-date method\nGMMFormer on TVR.\n","authors":["Peipei Song","Long Zhang","Long Lan","Weidong Chen","Dan Guo","Xun Yang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2504.10920v1.pdf","comment":"Accepted by IEEE Transactions on Multimedia (TMM) on January 19,\n  2025. The code is available at https://github.com/songpipi/AMDNet"},{"id":"http://arxiv.org/abs/2504.10916v1","updated":"2025-04-15T06:55:58Z","published":"2025-04-15T06:55:58Z","title":"Embedding Radiomics into Vision Transformers for Multimodal Medical\n  Image Classification","summary":"  Background: Deep learning has significantly advanced medical image analysis,\nwith Vision Transformers (ViTs) offering a powerful alternative to\nconvolutional models by modeling long-range dependencies through\nself-attention. However, ViTs are inherently data-intensive and lack\ndomain-specific inductive biases, limiting their applicability in medical\nimaging. In contrast, radiomics provides interpretable, handcrafted descriptors\nof tissue heterogeneity but suffers from limited scalability and integration\ninto end-to-end learning frameworks. In this work, we propose the\nRadiomics-Embedded Vision Transformer (RE-ViT) that combines radiomic features\nwith data-driven visual embeddings within a ViT backbone.\n  Purpose: To develop a hybrid RE-ViT framework that integrates radiomics and\npatch-wise ViT embeddings through early fusion, enhancing robustness and\nperformance in medical image classification.\n  Methods: Following the standard ViT pipeline, images were divided into\npatches. For each patch, handcrafted radiomic features were extracted and fused\nwith linearly projected pixel embeddings. The fused representations were\nnormalized, positionally encoded, and passed to the ViT encoder. A learnable\n[CLS] token aggregated patch-level information for classification. We evaluated\nRE-ViT on three public datasets (including BUSI, ChestXray2017, and Retinal\nOCT) using accuracy, macro AUC, sensitivity, and specificity. RE-ViT was\nbenchmarked against CNN-based (VGG-16, ResNet) and hybrid (TransMed) models.\n  Results: RE-ViT achieved state-of-the-art results: on BUSI,\nAUC=0.950+/-0.011; on ChestXray2017, AUC=0.989+/-0.004; on Retinal OCT,\nAUC=0.986+/-0.001, which outperforms other comparison models.\n  Conclusions: The RE-ViT framework effectively integrates radiomics with ViT\narchitectures, demonstrating improved performance and generalizability across\nmultimodal medical image classification tasks.\n","authors":["Zhenyu Yang","Haiming Zhu","Rihui Zhang","Haipeng Zhang","Jianliang Wang","Chunhao Wang","Minbin Chen","Fang-Fang Yin"],"pdf_url":"https://arxiv.org/pdf/2504.10916v1.pdf","comment":"27 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.03897v4","updated":"2025-04-15T06:53:12Z","published":"2025-02-06T09:18:30Z","title":"UniForm: A Unified Multi-Task Diffusion Transformer for Audio-Video\n  Generation","summary":"  With the rise of diffusion models, audio-video generation has been\nrevolutionized. However, most existing methods rely on separate modules for\neach modality, with limited exploration of unified generative architectures. In\naddition, many are confined to a single task and small-scale datasets. To\naddress these limitations, we first propose UniForm, a unified multi-task\ndiffusion transformer that jointly generates audio and visual modalities in a\nshared latent space. A single diffusion process models both audio and video,\ncapturing the inherent correlations between sound and vision. Second, we\nintroduce task-specific noise schemes and task tokens, enabling a single model\nto support multiple tasks, including text-to-audio-video, audio-to-video, and\nvideo-to-audio generation. Furthermore, by leveraging large language models and\na large-scale text-audio-video combined dataset, UniForm achieves greater\ngenerative diversity than prior approaches. Extensive experiments show that\nUniForm achieves the state-of-the-art performance across audio-video generation\ntasks, producing content that is both well-aligned and close to real-world data\ndistributions. Our demos are available at https://uniform-t2av.github.io/.\n","authors":["Lei Zhao","Linfeng Feng","Dongxu Ge","Rujin Chen","Fangqiu Yi","Chi Zhang","Xiao-Lei Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2502.03897v4.pdf","comment":"Our demos are available at https://uniform-t2av.github.io/"},{"id":"http://arxiv.org/abs/2410.19115v3","updated":"2025-04-15T06:33:45Z","published":"2024-10-24T19:29:02Z","title":"MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain\n  Images with Optimal Training Supervision","summary":"  We present MoGe, a powerful model for recovering 3D geometry from monocular\nopen-domain images. Given a single image, our model directly predicts a 3D\npoint map of the captured scene with an affine-invariant representation, which\nis agnostic to true global scale and shift. This new representation precludes\nambiguous supervision in training and facilitate effective geometry learning.\nFurthermore, we propose a set of novel global and local geometry supervisions\nthat empower the model to learn high-quality geometry. These include a robust,\noptimal, and efficient point cloud alignment solver for accurate global shape\nlearning, and a multi-scale local geometry loss promoting precise local\ngeometry supervision. We train our model on a large, mixed dataset and\ndemonstrate its strong generalizability and high accuracy. In our comprehensive\nevaluation on diverse unseen datasets, our model significantly outperforms\nstate-of-the-art methods across all tasks, including monocular estimation of 3D\npoint map, depth map, and camera field of view. Code and models can be found on\nour project page.\n","authors":["Ruicheng Wang","Sicheng Xu","Cassie Dai","Jianfeng Xiang","Yu Deng","Xin Tong","Jiaolong Yang"],"pdf_url":"https://arxiv.org/pdf/2410.19115v3.pdf","comment":"Project page: https://wangrc.site/MoGePage/"},{"id":"http://arxiv.org/abs/2504.10905v1","updated":"2025-04-15T06:32:45Z","published":"2025-04-15T06:32:45Z","title":"InterAnimate: Taming Region-aware Diffusion Model for Realistic Human\n  Interaction Animation","summary":"  Recent video generation research has focused heavily on isolated actions,\nleaving interactive motions-such as hand-face interactions-largely unexamined.\nThese interactions are essential for emerging biometric authentication systems,\nwhich rely on interactive motion-based anti-spoofing approaches. From a\nsecurity perspective, there is a growing need for large-scale, high-quality\ninteractive videos to train and strengthen authentication models. In this work,\nwe introduce a novel paradigm for animating realistic hand-face interactions.\nOur approach simultaneously learns spatio-temporal contact dynamics and\nbiomechanically plausible deformation effects, enabling natural interactions\nwhere hand movements induce anatomically accurate facial deformations while\nmaintaining collision-free contact. To facilitate this research, we present\nInterHF, a large-scale hand-face interaction dataset featuring 18 interaction\npatterns and 90,000 annotated videos. Additionally, we propose InterAnimate, a\nregion-aware diffusion model designed specifically for interaction animation.\nInterAnimate leverages learnable spatial and temporal latents to effectively\ncapture dynamic interaction priors and integrates a region-aware interaction\nmechanism that injects these priors into the denoising process. To the best of\nour knowledge, this work represents the first large-scale effort to\nsystematically study human hand-face interactions. Qualitative and quantitative\nresults show InterAnimate produces highly realistic animations, setting a new\nbenchmark. Code and data will be made public to advance research.\n","authors":["Yukang Lin","Yan Hong","Zunnan Xu","Xindi Li","Chao Xu","Chuanbiao Song","Ronghui Li","Haoxing Chen","Jun Lan","Huijia Zhu","Weiqiang Wang","Jianfu Zhang","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2504.10905v1.pdf","comment":"under preview"},{"id":"http://arxiv.org/abs/2406.09211v3","updated":"2025-04-15T06:21:22Z","published":"2024-06-13T15:15:07Z","title":"WildlifeReID-10k: Wildlife re-identification dataset with 10k individual\n  animals","summary":"  This paper introduces WildlifeReID-10k, a new large-scale re-identification\nbenchmark with more than 10k animal identities of around 33 species across more\nthan 140k images, re-sampled from 37 existing datasets. WildlifeReID-10k covers\ndiverse animal species and poses significant challenges for SoTA methods,\nensuring fair and robust evaluation through its time-aware and similarity-aware\nsplit protocol. The latter is designed to address the common issue of\ntraining-to-test data leakage caused by visually similar images appearing in\nboth training and test sets. The WildlifeReID-10k dataset and benchmark are\npublicly available on Kaggle, along with strong baselines for both closed-set\nand open-set evaluation, enabling fair, transparent, and standardized\nevaluation of not just multi-species animal re-identification models.\n","authors":["Lukáš Adam","Vojtěch Čermák","Kostas Papafitsoros","Lukas Picek"],"pdf_url":"https://arxiv.org/pdf/2406.09211v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10889v1","updated":"2025-04-15T05:47:09Z","published":"2025-04-15T05:47:09Z","title":"Fine-Grained Rib Fracture Diagnosis with Hyperbolic Embeddings: A\n  Detailed Annotation Framework and Multi-Label Classification Model","summary":"  Accurate rib fracture identification and classification are essential for\ntreatment planning. However, existing datasets often lack fine-grained\nannotations, particularly regarding rib fracture characterization, type, and\nprecise anatomical location on individual ribs. To address this, we introduce a\nnovel rib fracture annotation protocol tailored for fracture classification.\nFurther, we enhance fracture classification by leveraging cross-modal\nembeddings that bridge radiological images and clinical descriptions. Our\napproach employs hyperbolic embeddings to capture the hierarchical nature of\nfracture, mapping visual features and textual descriptions into a shared\nnon-Euclidean manifold. This framework enables more nuanced similarity\ncomputations between imaging characteristics and clinical descriptions,\naccounting for the inherent hierarchical relationships in fracture taxonomy.\nExperimental results demonstrate that our approach outperforms existing methods\nacross multiple classification tasks, with average recall improvements of 6% on\nthe AirRib dataset and 17.5% on the public RibFrac dataset.\n","authors":["Shripad Pate","Aiman Farooq","Suvrankar Dutta","Musadiq Aadil Sheikh","Atin Kumar","Deepak Mishra"],"pdf_url":"https://arxiv.org/pdf/2504.10889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10888v1","updated":"2025-04-15T05:46:00Z","published":"2025-04-15T05:46:00Z","title":"CDUPatch: Color-Driven Universal Adversarial Patch Attack for Dual-Modal\n  Visible-Infrared Detectors","summary":"  Adversarial patches are widely used to evaluate the robustness of object\ndetection systems in real-world scenarios. These patches were initially\ndesigned to deceive single-modal detectors (e.g., visible or infrared) and have\nrecently been extended to target visible-infrared dual-modal detectors.\nHowever, existing dual-modal adversarial patch attacks have limited attack\neffectiveness across diverse physical scenarios. To address this, we propose\nCDUPatch, a universal cross-modal patch attack against visible-infrared object\ndetectors across scales, views, and scenarios. Specifically, we observe that\ncolor variations lead to different levels of thermal absorption, resulting in\ntemperature differences in infrared imaging. Leveraging this property, we\npropose an RGB-to-infrared adapter that maps RGB patches to infrared patches,\nenabling unified optimization of cross-modal patches. By learning an optimal\ncolor distribution on the adversarial patch, we can manipulate its thermal\nresponse and generate an adversarial infrared texture. Additionally, we\nintroduce a multi-scale clipping strategy and construct a new visible-infrared\ndataset, MSDrone, which contains aerial vehicle images in varying scales and\nperspectives. These data augmentation strategies enhance the robustness of our\npatch in real-world conditions. Experiments on four benchmark datasets (e.g.,\nDroneVehicle, LLVIP, VisDrone, MSDrone) show that our method outperforms\nexisting patch attacks in the digital domain. Extensive physical tests further\nconfirm strong transferability across scales, views, and scenarios.\n","authors":["Jiahuan Long","Wen Yao","Tingsong Jiang","Chao Ma"],"pdf_url":"https://arxiv.org/pdf/2504.10888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10885v1","updated":"2025-04-15T05:29:31Z","published":"2025-04-15T05:29:31Z","title":"PuzzleBench: A Fully Dynamic Evaluation Framework for Large Multimodal\n  Models on Puzzle Solving","summary":"  Large Multimodal Models (LMMs) have demonstrated impressive capabilities\nacross a wide range of multimodal tasks, achieving ever-increasing performance\non various evaluation benchmarks. However, existing benchmarks are typically\nstatic and often overlap with pre-training datasets, leading to fixed\ncomplexity constraints and substantial data contamination issues. Meanwhile,\nmanually annotated datasets are labor-intensive, time-consuming, and subject to\nhuman bias and inconsistency, leading to reliability and reproducibility\nissues. To address these problems, we propose a fully dynamic multimodal\nevaluation framework, named Open-ended Visual Puzzle Generation (OVPG), which\naims to generate fresh, diverse, and verifiable evaluation data automatically\nin puzzle-solving tasks. Specifically, the OVPG pipeline consists of a raw\nmaterial sampling module, a visual content generation module, and a puzzle rule\ndesign module, which ensures that each evaluation instance is primitive, highly\nrandomized, and uniquely solvable, enabling continual adaptation to the\nevolving capabilities of LMMs. Built upon OVPG, we construct PuzzleBench, a\ndynamic and scalable benchmark comprising 11,840 VQA samples. It features six\ncarefully designed puzzle tasks targeting three core LMM competencies, visual\nrecognition, logical reasoning, and context understanding. PuzzleBench differs\nfrom static benchmarks that quickly become outdated. It enables ongoing dataset\nrefreshing through OVPG and a rich set of open-ended puzzle designs, allowing\nseamless adaptation to the evolving capabilities of LMMs.\n","authors":["Zeyu Zhang","Zijian Chen","Zicheng Zhang","Yuze Sun","Yuan Tian","Ziheng Jia","Chunyi Li","Xiaohong Liu","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2504.10885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10883v1","updated":"2025-04-15T05:26:42Z","published":"2025-04-15T05:26:42Z","title":"Bringing together invertible UNets with invertible attention modules for\n  memory-efficient diffusion models","summary":"  Diffusion models have recently gained state of the art performance on many\nimage generation tasks. However, most models require significant computational\nresources to achieve this. This becomes apparent in the application of medical\nimage synthesis due to the 3D nature of medical datasets like CT-scans, MRIs,\nelectron microscope, etc. In this paper we propose a novel architecture for a\nsingle GPU memory-efficient training for diffusion models for high dimensional\nmedical datasets. The proposed model is built by using an invertible UNet\narchitecture with invertible attention modules. This leads to the following two\ncontributions: 1. denoising diffusion models and thus enabling memory usage to\nbe independent of the dimensionality of the dataset, and 2. reducing the energy\nusage during training. While this new model can be applied to a multitude of\nimage generation tasks, we showcase its memory-efficiency on the 3D BraTS2020\ndataset leading to up to 15\\% decrease in peak memory consumption during\ntraining with comparable results to SOTA while maintaining the image quality.\n","authors":["Karan Jain","Mohammad Nayeem Teli"],"pdf_url":"https://arxiv.org/pdf/2504.10883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10880v1","updated":"2025-04-15T05:21:09Z","published":"2025-04-15T05:21:09Z","title":"Safe-Construct: Redefining Construction Safety Violation Recognition as\n  3D Multi-View Engagement Task","summary":"  Recognizing safety violations in construction environments is critical yet\nremains underexplored in computer vision. Existing models predominantly rely on\n2D object detection, which fails to capture the complexities of real-world\nviolations due to: (i) an oversimplified task formulation treating violation\nrecognition merely as object detection, (ii) inadequate validation under\nrealistic conditions, (iii) absence of standardized baselines, and (iv) limited\nscalability from the unavailability of synthetic dataset generators for diverse\nconstruction scenarios. To address these challenges, we introduce\nSafe-Construct, the first framework that reformulates violation recognition as\na 3D multi-view engagement task, leveraging scene-level worker-object context\nand 3D spatial understanding. We also propose the Synthetic Indoor Construction\nSite Generator (SICSG) to create diverse, scalable training data, overcoming\ndata limitations. Safe-Construct achieves a 7.6% improvement over\nstate-of-the-art methods across four violation types. We rigorously evaluate\nour approach in near-realistic settings, incorporating four violations, four\nworkers, 14 objects, and challenging conditions like occlusions (worker-object,\nworker-worker) and variable illumination (back-lighting, overexposure,\nsunlight). By integrating 3D multi-view spatial understanding and synthetic\ndata generation, Safe-Construct sets a new benchmark for scalable and robust\nsafety monitoring in high-risk industries. Project Website:\nhttps://Safe-Construct.github.io/Safe-Construct\n","authors":["Aviral Chharia","Tianyu Ren","Tomotake Furuhata","Kenji Shimada"],"pdf_url":"https://arxiv.org/pdf/2504.10880v1.pdf","comment":"CVPR Workshop 2025; Project Website:\n  https://Safe-Construct.github.io/Safe-Construct"},{"id":"http://arxiv.org/abs/2312.01632v5","updated":"2025-04-15T05:15:34Z","published":"2023-12-04T05:24:45Z","title":"GaussianHead: High-fidelity Head Avatars with Learnable Gaussian\n  Derivation","summary":"  Constructing vivid 3D head avatars for given subjects and realizing a series\nof animations on them is valuable yet challenging. This paper presents\nGaussianHead, which models the actional human head with anisotropic 3D\nGaussians. In our framework, a motion deformation field and multi-resolution\ntri-plane are constructed respectively to deal with the head's dynamic geometry\nand complex texture. Notably, we impose an exclusive derivation scheme on each\nGaussian, which generates its multiple doppelgangers through a set of learnable\nparameters for position transformation. With this design, we can compactly and\naccurately encode the appearance information of Gaussians, even those fitting\nthe head's particular components with sophisticated structures. In addition, an\ninherited derivation strategy for newly added Gaussians is adopted to\nfacilitate training acceleration. Extensive experiments show that our method\ncan produce high-fidelity renderings, outperforming state-of-the-art approaches\nin reconstruction, cross-identity reenactment, and novel view synthesis tasks.\nOur code is available at: https://github.com/chiehwangs/gaussian-head.\n","authors":["Jie Wang","Jiu-Cheng Xie","Xianyan Li","Feng Xu","Chi-Man Pun","Hao Gao"],"pdf_url":"https://arxiv.org/pdf/2312.01632v5.pdf","comment":"15 pages, 14 figures, published to TVCG"},{"id":"http://arxiv.org/abs/2312.08558v2","updated":"2025-04-15T05:15:08Z","published":"2023-12-13T23:06:30Z","title":"Leveraging Driver Field-of-View for Multimodal Ego-Trajectory Prediction","summary":"  Understanding drivers' decision-making is crucial for road safety. Although\npredicting the ego-vehicle's path is valuable for driver-assistance systems,\nexisting methods mainly focus on external factors like other vehicles' motions,\noften neglecting the driver's attention and intent. To address this gap, we\ninfer the ego-trajectory by integrating the driver's gaze and the surrounding\nscene. We introduce RouteFormer, a novel multimodal ego-trajectory prediction\nnetwork combining GPS data, environmental context, and the driver's\nfield-of-view, comprising first-person video and gaze fixations. We also\npresent the Path Complexity Index (PCI), a new metric for trajectory complexity\nthat enables a more nuanced evaluation of challenging scenarios. To tackle data\nscarcity and enhance diversity, we introduce GEM, a comprehensive dataset of\nurban driving scenarios enriched with synchronized driver field-of-view and\ngaze data. Extensive evaluations on GEM and DR(eye)VE demonstrate that\nRouteFormer significantly outperforms state-of-the-art methods, achieving\nnotable improvements in prediction accuracy across diverse conditions. Ablation\nstudies reveal that incorporating driver field-of-view data yields\nsignificantly better average displacement error, especially in challenging\nscenarios with high PCI scores, underscoring the importance of modeling driver\nattention. All data and code are available at\nhttps://meakbiyik.github.io/routeformer.\n","authors":["M. Eren Akbiyik","Nedko Savov","Danda Pani Paudel","Nikola Popovic","Christian Vater","Otmar Hilliges","Luc Van Gool","Xi Wang"],"pdf_url":"https://arxiv.org/pdf/2312.08558v2.pdf","comment":"Accepted to 13th International Conference on Learning Representations\n  (ICLR 2025), 29 pages"},{"id":"http://arxiv.org/abs/2504.03193v2","updated":"2025-04-15T05:14:58Z","published":"2025-04-04T05:44:45Z","title":"Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language\n  Models for Domain-Generalized Semantic Segmentation","summary":"  Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained\ntraction in Domain Generalized Semantic Segmentation (DGSS) due to their strong\ngeneralization capabilities. However, existing DGSS methods often rely\nexclusively on either VFMs or VLMs, overlooking their complementary strengths.\nVFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g.,\nCLIP) provide robust text alignment but struggle with coarse granularity.\nDespite their complementary strengths, effectively integrating VFMs and VLMs\nwith attention mechanisms is challenging, as the increased patch tokens\ncomplicate long-sequence modeling. To address this, we propose MFuser, a novel\nMamba-based fusion framework that efficiently combines the strengths of VFMs\nand VLMs while maintaining linear scalability in sequence length. MFuser\nconsists of two key components: MVFuser, which acts as a co-adapter to jointly\nfine-tune the two models by capturing both sequential and spatial dynamics; and\nMTEnhancer, a hybrid attention-Mamba module that refines text embeddings by\nincorporating image priors. Our approach achieves precise feature locality and\nstrong text alignment without incurring significant computational overhead.\nExtensive experiments demonstrate that MFuser significantly outperforms\nstate-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and\n71.87 mIoU on real-to-real benchmarks. The code is available at\nhttps://github.com/devinxzhang/MFuser.\n","authors":["Xin Zhang","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2504.03193v2.pdf","comment":"Accepted to CVPR 2025 (Highlight)"},{"id":"http://arxiv.org/abs/2504.10878v1","updated":"2025-04-15T05:11:40Z","published":"2025-04-15T05:11:40Z","title":"Large Language Model-Informed Feature Discovery Improves Prediction and\n  Interpretation of Credibility Perceptions of Visual Content","summary":"  In today's visually dominated social media landscape, predicting the\nperceived credibility of visual content and understanding what drives human\njudgment are crucial for countering misinformation. However, these tasks are\nchallenging due to the diversity and richness of visual features. We introduce\na Large Language Model (LLM)-informed feature discovery framework that\nleverages multimodal LLMs, such as GPT-4o, to evaluate content credibility and\nexplain its reasoning. We extract and quantify interpretable features using\ntargeted prompts and integrate them into machine learning models to improve\ncredibility predictions. We tested this approach on 4,191 visual social media\nposts across eight topics in science, health, and politics, using credibility\nratings from 5,355 crowdsourced workers. Our method outperformed zero-shot\nGPT-based predictions by 13 percent in R2, and revealed key features like\ninformation concreteness and image format. We discuss the implications for\nmisinformation mitigation, visual credibility, and the role of LLMs in social\nscience.\n","authors":["Yilang Peng","Sijia Qian","Yingdan Lu","Cuihua Shen"],"pdf_url":"https://arxiv.org/pdf/2504.10878v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2504.10877v1","updated":"2025-04-15T05:11:18Z","published":"2025-04-15T05:11:18Z","title":"Weather-Aware Object Detection Transformer for Domain Adaptation","summary":"  RT-DETRs have shown strong performance across various computer vision tasks\nbut are known to degrade under challenging weather conditions such as fog. In\nthis work, we investigate three novel approaches to enhance RT-DETR robustness\nin foggy environments: (1) Domain Adaptation via Perceptual Loss, which\ndistills domain-invariant features from a teacher network to a student using\nperceptual supervision; (2) Weather Adaptive Attention, which augments the\nattention mechanism with fog-sensitive scaling by introducing an auxiliary\nfoggy image stream; and (3) Weather Fusion Encoder, which integrates a\ndual-stream encoder architecture that fuses clear and foggy image features via\nmulti-head self and cross-attention. Despite the architectural innovations,\nnone of the proposed methods consistently outperform the baseline RT-DETR. We\nanalyze the limitations and potential causes, offering insights for future\nresearch in weather-aware object detection.\n","authors":["Soheil Gharatappeh","Salimeh Sekeh","Vikas Dhiman"],"pdf_url":"https://arxiv.org/pdf/2504.10877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10873v1","updated":"2025-04-15T05:04:25Z","published":"2025-04-15T05:04:25Z","title":"Can Vision-Language Models Understand and Interpret Dynamic Gestures\n  from Pedestrians? Pilot Datasets and Exploration Towards Instructive\n  Nonverbal Commands for Cooperative Autonomous Vehicles","summary":"  In autonomous driving, it is crucial to correctly interpret traffic gestures\n(TGs), such as those of an authority figure providing orders or instructions,\nor a pedestrian signaling the driver, to ensure a safe and pleasant traffic\nenvironment for all road users. This study investigates the capabilities of\nstate-of-the-art vision-language models (VLMs) in zero-shot interpretation,\nfocusing on their ability to caption and classify human gestures in traffic\ncontexts. We create and publicly share two custom datasets with varying formal\nand informal TGs, such as 'Stop', 'Reverse', 'Hail', etc. The datasets are\n\"Acted TG (ATG)\" and \"Instructive TG In-The-Wild (ITGI)\". They are annotated\nwith natural language, describing the pedestrian's body position and gesture.\nWe evaluate models using three methods utilizing expert-generated captions as\nbaseline and control: (1) caption similarity, (2) gesture classification, and\n(3) pose sequence reconstruction similarity. Results show that current VLMs\nstruggle with gesture understanding: sentence similarity averages below 0.59,\nand classification F1 scores reach only 0.14-0.39, well below the expert\nbaseline of 0.70. While pose reconstruction shows potential, it requires more\ndata and refined metrics to be reliable. Our findings reveal that although some\nSOTA VLMs can interpret zero-shot human traffic gestures, none are accurate and\nrobust enough to be trustworthy, emphasizing the need for further research in\nthis domain.\n","authors":["Tonko E. W. Bossen","Andreas Møgelmose","Ross Greer"],"pdf_url":"https://arxiv.org/pdf/2504.10873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10871v1","updated":"2025-04-15T05:02:49Z","published":"2025-04-15T05:02:49Z","title":"DAAF:Degradation-Aware Adaptive Fusion Framework for Robust Infrared and\n  Visible Images Fusion","summary":"  Existing infrared and visible image fusion(IVIF) algorithms often prioritize\nhigh-quality images, neglecting image degradation such as low light and noise,\nwhich limits the practical potential. This paper propose Degradation-Aware\nAdaptive image Fusion (DAAF), which achieves unified modeling of adaptive\ndegradation optimization and image fusion. Specifically, DAAF comprises an\nauxiliary Adaptive Degradation Optimization Network (ADON) and a Feature\nInteractive Local-Global Fusion (FILGF) Network. Firstly, ADON includes\ninfrared and visible-light branches. Within the infrared branch,\nfrequency-domain feature decomposition and extraction are employed to isolate\nGaussian and stripe noise. In the visible-light branch, Retinex decomposition\nis applied to extract illumination and reflectance components, enabling\ncomplementary enhancement of detail and illumination distribution.\nSubsequently, FILGF performs interactive multi-scale local-global feature\nfusion. Local feature fusion consists of intra-inter model feature complement,\nwhile global feature fusion is achieved through a interactive cross-model\nattention. Extensive experiments have shown that DAAF outperforms current IVIF\nalgorithms in normal and complex degradation scenarios.\n","authors":["Tianpei Zhang","Jufeng Zhao","Yiming Zhu","Guangmang Cui","Yuxin Jing","Yuhan Lyu"],"pdf_url":"https://arxiv.org/pdf/2504.10871v1.pdf","comment":null}]},"2025-04-16T00:00:00Z":{"Statistical Machine Learning (Theory)":[{"id":"http://arxiv.org/abs/2504.12287v1","updated":"2025-04-16T17:51:59Z","published":"2025-04-16T17:51:59Z","title":"Trend Filtered Mixture of Experts for Automated Gating of High-Frequency\n  Flow Cytometry Data","summary":"  Ocean microbes are critical to both ocean ecosystems and the global climate.\nFlow cytometry, which measures cell optical properties in fluid samples, is\nroutinely used in oceanographic research. Despite decades of accumulated data,\nidentifying key microbial populations (a process known as ``gating'') remains a\nsignificant analytical challenge. To address this, we focus on gating\nmultidimensional, high-frequency flow cytometry data collected {\\it\ncontinuously} on board oceanographic research vessels, capturing time- and\nspace-wise variations in the dynamic ocean. Our paper proposes a novel\nmixture-of-experts model in which both the gating function and the experts are\ngiven by trend filtering. The model leverages two key assumptions: (1) Each\nsnapshot of flow cytometry data is a mixture of multivariate Gaussians and (2)\nthe parameters of these Gaussians vary smoothly over time. Our method uses\nregularization and a constraint to ensure smoothness and that cluster means\nmatch biologically distinct microbe types. We demonstrate, using flow cytometry\ndata from the North Pacific Ocean, that our proposed model accurately matches\nhuman-annotated gating and corrects significant errors.\n","authors":["Sangwon Hyun","Tim Coleman","Francois Ribalet","Jacob Bien"],"pdf_url":"https://arxiv.org/pdf/2504.12287v1.pdf","comment":"23 page (including supplement), 9 figures (including supplement)"},{"id":"http://arxiv.org/abs/2502.12999v2","updated":"2025-04-16T17:14:07Z","published":"2025-02-18T16:19:28Z","title":"Asymptotic Optimism of Random-Design Linear and Kernel Regression Models","summary":"  We derived the closed-form asymptotic optimism of linear regression models\nunder random designs, and generalizes it to kernel ridge regression. Using\nscaled asymptotic optimism as a generic predictive model complexity measure, we\nstudied the fundamental different behaviors of linear regression model, tangent\nkernel (NTK) regression model and three-layer fully connected neural networks\n(NN). Our contribution is two-fold: we provided theoretical ground for using\nscaled optimism as a model predictive complexity measure; and we show\nempirically that NN with ReLUs behaves differently from kernel models under\nthis measure. With resampling techniques, we can also compute the optimism for\nregression models with real data.\n","authors":["Hengrui Luo","Yunzhang Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.12999v2.pdf","comment":"56 pages;"},{"id":"http://arxiv.org/abs/2504.12189v1","updated":"2025-04-16T15:44:24Z","published":"2025-04-16T15:44:24Z","title":"Leave-One-Out Stable Conformal Prediction","summary":"  Conformal prediction (CP) is an important tool for distribution-free\npredictive uncertainty quantification. Yet, a major challenge is to balance\ncomputational efficiency and prediction accuracy, particularly for multiple\npredictions. We propose Leave-One-Out Stable Conformal Prediction (LOO-StabCP),\na novel method to speed up full conformal using algorithmic stability without\nsample splitting. By leveraging leave-one-out stability, our method is much\nfaster in handling a large number of prediction requests compared to existing\nmethod RO-StabCP based on replace-one stability. We derived stability bounds\nfor several popular machine learning tools: regularized loss minimization (RLM)\nand stochastic gradient descent (SGD), as well as kernel method, neural\nnetworks and bagging. Our method is theoretically justified and demonstrates\nsuperior numerical performance on synthetic and real-world data. We applied our\nmethod to a screening problem, where its effective exploitation of training\ndata led to improved test power compared to state-of-the-art method based on\nsplit conformal.\n","authors":["Kiljae Lee","Yuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.12189v1.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2407.00143v2","updated":"2025-04-16T15:26:14Z","published":"2024-06-28T16:08:26Z","title":"InfoNCE: Identifying the Gap Between Theory and Practice","summary":"  Prior theory work on Contrastive Learning via the InfoNCE loss showed that,\nunder certain assumptions, the learned representations recover the ground-truth\nlatent factors. We argue that these theories overlook crucial aspects of how CL\nis deployed in practice. Specifically, they either assume equal variance across\nall latents or that certain latents are kept invariant. However, in practice,\npositive pairs are often generated using augmentations such as strong cropping\nto just a few pixels. Hence, a more realistic assumption is that all latent\nfactors change with a continuum of variability across all factors. We introduce\nAnInfoNCE, a generalization of InfoNCE that can provably uncover the latent\nfactors in this anisotropic setting, broadly generalizing previous\nidentifiability results in CL. We validate our identifiability results in\ncontrolled experiments and show that AnInfoNCE increases the recovery of\npreviously collapsed information in CIFAR10 and ImageNet, albeit at the cost of\ndownstream accuracy. Finally, we discuss the remaining mismatches between\ntheoretical assumptions and practical implementations.\n","authors":["Evgenia Rusak","Patrik Reizinger","Attila Juhos","Oliver Bringmann","Roland S. Zimmermann","Wieland Brendel"],"pdf_url":"https://arxiv.org/pdf/2407.00143v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12175v1","updated":"2025-04-16T15:25:58Z","published":"2025-04-16T15:25:58Z","title":"Approximation Bounds for Transformer Networks with Application to\n  Regression","summary":"  We explore the approximation capabilities of Transformer networks for\nH\\\"older and Sobolev functions, and apply these results to address\nnonparametric regression estimation with dependent observations. First, we\nestablish novel upper bounds for standard Transformer networks approximating\nsequence-to-sequence mappings whose component functions are H\\\"older continuous\nwith smoothness index $\\gamma \\in (0,1]$. To achieve an approximation error\n$\\varepsilon$ under the $L^p$-norm for $p \\in [1, \\infty]$, it suffices to use\na fixed-depth Transformer network whose total number of parameters scales as\n$\\varepsilon^{-d_x n / \\gamma}$. This result not only extends existing findings\nto include the case $p = \\infty$, but also matches the best known upper bounds\non number of parameters previously obtained for fixed-depth FNNs and RNNs.\nSimilar bounds are also derived for Sobolev functions. Second, we derive\nexplicit convergence rates for the nonparametric regression problem under\nvarious $\\beta$-mixing data assumptions, which allow the dependence between\nobservations to weaken over time. Our bounds on the sample complexity impose no\nconstraints on weight magnitudes. Lastly, we propose a novel proof strategy to\nestablish approximation bounds, inspired by the Kolmogorov-Arnold\nrepresentation theorem. We show that if the self-attention layer in a\nTransformer can perform column averaging, the network can approximate\nsequence-to-sequence H\\\"older functions, offering new insights into the\ninterpretability of self-attention mechanisms.\n","authors":["Yuling Jiao","Yanming Lai","Defeng Sun","Yang Wang","Bokai Yan"],"pdf_url":"https://arxiv.org/pdf/2504.12175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12156v1","updated":"2025-04-16T15:04:00Z","published":"2025-04-16T15:04:00Z","title":"Predictive Multiplicity in Survival Models: A Method for Quantifying\n  Model Uncertainty in Predictive Maintenance Applications","summary":"  In many applications, especially those involving prediction, models may yield\nnear-optimal performance yet significantly disagree on individual-level\noutcomes. This phenomenon, known as predictive multiplicity, has been formally\ndefined in binary, probabilistic, and multi-target classification, and\nundermines the reliability of predictive systems. However, its implications\nremain unexplored in the context of survival analysis, which involves\nestimating the time until a failure or similar event while properly handling\ncensored data. We frame predictive multiplicity as a critical concern in\nsurvival-based models and introduce formal measures -- ambiguity, discrepancy,\nand obscurity -- to quantify it. This is particularly relevant for downstream\ntasks such as maintenance scheduling, where precise individual risk estimates\nare essential. Understanding and reporting predictive multiplicity helps build\ntrust in models deployed in high-stakes environments. We apply our methodology\nto benchmark datasets from predictive maintenance, extending the notion of\nmultiplicity to survival models. Our findings show that ambiguity steadily\nincreases, reaching up to 40-45% of observations; discrepancy is lower but\nexhibits a similar trend; and obscurity remains mild and concentrated in a few\nmodels. These results demonstrate that multiple accurate survival models may\nyield conflicting estimations of failure risk and degradation progression for\nthe same equipment. This highlights the need to explicitly measure and\ncommunicate predictive multiplicity to ensure reliable decision-making in\nprocess health management.\n","authors":["Mustafa Cavus"],"pdf_url":"https://arxiv.org/pdf/2504.12156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17112v2","updated":"2025-04-16T11:44:53Z","published":"2024-07-24T09:23:22Z","title":"Neural Dueling Bandits: Preference-Based Optimization with Human\n  Feedback","summary":"  Contextual dueling bandit is used to model the bandit problems, where a\nlearner's goal is to find the best arm for a given context using observed noisy\nhuman preference feedback over the selected arms for the past contexts.\nHowever, existing algorithms assume the reward function is linear, which can be\ncomplex and non-linear in many real-life applications like online\nrecommendations or ranking web search results. To overcome this challenge, we\nuse a neural network to estimate the reward function using preference feedback\nfor the previously selected arms. We propose upper confidence bound- and\nThompson sampling-based algorithms with sub-linear regret guarantees that\nefficiently select arms in each round. We also extend our theoretical results\nto contextual bandit problems with binary feedback, which is in itself a\nnon-trivial contribution. Experimental results on the problem instances derived\nfrom synthetic datasets corroborate our theoretical results.\n","authors":["Arun Verma","Zhongxiang Dai","Xiaoqiang Lin","Patrick Jaillet","Bryan Kian Hsiang Low"],"pdf_url":"https://arxiv.org/pdf/2407.17112v2.pdf","comment":"Accepted at ICLR 2025. Also, accepted at ICML 2024 Workshop on\n  Foundations of Reinforcement Learning and Control"},{"id":"http://arxiv.org/abs/2410.00078v2","updated":"2025-04-16T10:42:51Z","published":"2024-09-30T16:26:40Z","title":"Shuffled Linear Regression via Spectral Matching","summary":"  Shuffled linear regression (SLR) seeks to estimate latent features through a\nlinear transformation, complicated by unknown permutations in the measurement\ndimensions. This problem extends traditional least-squares (LS) and Least\nAbsolute Shrinkage and Selection Operator (LASSO) approaches by jointly\nestimating the permutation, resulting in shuffled LS and shuffled LASSO\nformulations. Existing methods, constrained by the combinatorial complexity of\npermutation recovery, often address small-scale cases with limited\nmeasurements. In contrast, we focus on large-scale SLR, particularly suited for\nenvironments with abundant measurement samples. We propose a spectral matching\nmethod that efficiently resolves permutations by aligning spectral components\nof the measurement and feature covariances. Rigorous theoretical analyses\ndemonstrate that our method achieves accurate estimates in both shuffled LS and\nshuffled LASSO settings, given a sufficient number of samples. Furthermore, we\nextend our approach to address simultaneous pose and correspondence estimation\nin image registration tasks. Experiments on synthetic datasets and real-world\nimage registration scenarios show that our method outperforms existing\nalgorithms in both estimation accuracy and registration performance.\n","authors":["Hang Liu","Anna Scaglione"],"pdf_url":"https://arxiv.org/pdf/2410.00078v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2309.14073v3","updated":"2025-04-16T10:42:17Z","published":"2023-09-25T12:07:00Z","title":"Neural Network Parameter-optimization of Gaussian pmDAGs","summary":"  Finding the parameters of a latent variable causal model is central to causal\ninference and causal identification. In this article, we show that existing\ngraphical structures that are used in causal inference are not stable under\nmarginalization of Gaussian Bayesian networks, and present a graphical\nstructure that faithfully represent margins of Gaussian Bayesian networks. We\npresent the first duality between parameter optimization of a latent variable\nmodel and training a feed-forward neural network in the parameter space of the\nassumed family of distributions. Based on this observation, we develop an\nalgorithm for parameter optimization of these graphical structures based on a\ngiven observational distribution. Then, we provide conditions for causal effect\nidentifiability in the Gaussian setting. We propose an meta-algorithm that\nchecks whether a causal effect is identifiable or not. Moreover, we lay a\ngrounding for generalizing the duality between a neural network and a causal\nmodel from the Gaussian to other distributions.\n","authors":["Mehrzad Saremi"],"pdf_url":"https://arxiv.org/pdf/2309.14073v3.pdf","comment":"48 pages"},{"id":"http://arxiv.org/abs/2402.10504v4","updated":"2025-04-16T09:25:06Z","published":"2024-02-16T08:27:55Z","title":"Resilience of Rademacher chaos of low degree","summary":"  The resilience of a Rademacher chaos is the maximum number of adversarial\nsign-flips that the chaos can sustain without having its largest atom\nprobability significantly altered. Inspired by probabilistic lower-bound\nguarantees for the resilience of linear Rademacher chaos, obtained by Bandeira,\nFerber, and Kwan (Advances in Mathematics, Vol. $319$, $2017$), we provide\nprobabilistic lower-bound guarantees for the resilience of Rademacher chaos of\narbitrary yet sufficiently low degree.\n  Our main results distinguish between Rademacher chaos of order two and those\nof higher order. In that, our first main result pertains to the resilience of\ndecoupled bilinear Rademacher forms where different asymptotic behaviour is\nobserved for sparse and dense matrices. For our second main result, we\nbootstrap our first result in order to provide resilience guarantees for\nquadratic Rademacher chaos. Our third main result, generalises the first and\nhandles the resilience of decoupled Rademacher chaos of arbitrary yet\nsufficiently low order.\n  Our results for decoupled Rademacher chaos of order two and that of higher\norder whilst are established through the same conceptual framework, differ\nsubstantially. A difference incurred due to the implementation of the same\nconceptual argument. The order two result is established using Dudley's maximal\ninequality for sub-Gaussian processes, the Hanson-Wright inequality, as well as\nthe Kolmogorov-Rogozin inequality. To handle higher order chaos, appeals to\nDudley's inequality as well as the Hanson-Wright inequality are replaced with\ntools suited for random tensors. Appeals to the Hanson-Wright inequality are\nreplaced with appeals to a concentration result for random tensors put forth by\nAdamczak and Wolff.\n  Our results are instance-dependent and thus allow for the efficient\ncomputation of resilience guarantees provided the order of the chaos is\nconstant.\n","authors":["Elad Aigner-Horev","Daniel Rosenberg","Roi Weiss"],"pdf_url":"https://arxiv.org/pdf/2402.10504v4.pdf","comment":"Small corrections from previous version"},{"id":"http://arxiv.org/abs/2311.13017v5","updated":"2025-04-16T08:30:31Z","published":"2023-11-21T21:46:04Z","title":"W-Kernel and Its Principal Space for Frequentist Evaluation of Bayesian\n  Estimators","summary":"  Evaluating the variability of posterior estimates is a key aspect of Bayesian\nmodel assessment. In this study, we focus on the posterior covariance matrix W,\ndefined using the log-likelihood of each observation. Previous studies have\nexamined the role of the principal space of W in Bayesian sensitivity analysis,\nnotably MacEachern and Peruggia (2002) and Thomas et al. (2018). In this work,\nwe show that the principal space of W is also relevant for frequentist\nevaluation, using the recently proposed Bayesian infinitesimal jackknife (IJ)\napproximation Giordano and Broderick (2023) as a key tool. We next consider the\nrelationship between the matrix W and the Fisher kernel. We show that the\nFisher kernel can be regarded as an approximation to W; the matrix W, in\nitself, can be interpreted as a reproducing kernel, which we refer to as the\nW-kernel. Based on this connection, we examine the dual relationship between\nthe W-kernel formulation in the data space and the classical asymptotic\nformulation in the parameter space. These ideas suggest a form of\nBayesian-frequentist duality that emerges through the dual structure of kernel\nPCA, where posterior and frequentist covariances serve as inner products in\ntheir respective spaces. As an application, we consider an approximate\nbootstrap of posterior means based on posterior samples generated by MCMC. We\nshow that the projection onto the principal space of W facilitates frequentist\nevaluation, particularly of the higher-order term in this procedure. In one of\nthe appendices, we introduce incomplete Cholesky decomposition as an efficient\nmethod for computing the principal space of W and discuss the related concept\nof representative subsets of the observations.\n","authors":["Yukito Iba"],"pdf_url":"https://arxiv.org/pdf/2311.13017v5.pdf","comment":"The introductory sections have been revised to clarify the\n  relationship with previous work. The discussion of Bayesian-frequentist\n  duality and the Z matrix has also been revised. The analysis of numerical\n  experiments is substantially extended. The title has been updated"},{"id":"http://arxiv.org/abs/2504.11848v1","updated":"2025-04-16T08:14:55Z","published":"2025-04-16T08:14:55Z","title":"Proximal Inference on Population Intervention Indirect Effect","summary":"  The population intervention indirect effect (PIIE) is a novel mediation\neffect representing the indirect component of the population intervention\neffect. Unlike traditional mediation measures, such as the natural indirect\neffect, the PIIE holds particular relevance in observational studies involving\nunethical exposures, when hypothetical interventions that impose harmful\nexposures are inappropriate. Although prior research has identified PIIE under\nunmeasured confounders between exposure and outcome, it has not fully addressed\nthe confounding that affects the mediator. This study extends the PIIE\nidentification to settings where unmeasured confounders influence\nexposure-outcome, exposure-mediator, and mediator-outcome relationships.\nSpecifically, we leverage observed covariates as proxy variables for unmeasured\nconfounders, constructing three proximal identification frameworks.\nAdditionally, we characterize the semiparametric efficiency bound and develop\nmultiply robust and locally efficient estimators. To handle high-dimensional\nnuisance parameters, we propose a debiased machine learning approach that\nachieves $\\sqrt{n}$-consistency and asymptotic normality to estimate the true\nPIIE values, even when the machine learning estimators for the nuisance\nfunctions do not converge at $\\sqrt{n}$-rate. In simulations, our estimators\ndemonstrate higher confidence interval coverage rates than conventional methods\nacross various model misspecifications. In a real data application, our\napproaches reveal an indirect effect of alcohol consumption on depression risk\nmediated by depersonalization symptoms.\n","authors":["Yang Bai","Yifan Cui","Baoluo Sun"],"pdf_url":"https://arxiv.org/pdf/2504.11848v1.pdf","comment":"60 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.20068v2","updated":"2025-04-16T07:55:46Z","published":"2024-10-26T04:19:52Z","title":"Understanding the Effect of GCN Convolutions in Regression Tasks","summary":"  Graph Convolutional Networks (GCNs) have become a pivotal method in machine\nlearning for modeling functions over graphs. Despite their widespread success\nacross various applications, their statistical properties (e.g., consistency,\nconvergence rates) remain ill-characterized. To begin addressing this knowledge\ngap, we consider networks for which the graph structure implies that\nneighboring nodes exhibit similar signals and provide statistical theory for\nthe impact of convolution operators. Focusing on estimators based solely on\nneighborhood aggregation, we examine how two common convolutions - the original\nGCN and GraphSAGE convolutions - affect the learning error as a function of the\nneighborhood topology and the number of convolutional layers. We explicitly\ncharacterize the bias-variance type trade-off incurred by GCNs as a function of\nthe neighborhood size and identify specific graph topologies where convolution\noperators are less effective. Our theoretical findings are corroborated by\nsynthetic experiments, and provide a start to a deeper quantitative\nunderstanding of convolutional effects in GCNs for offering rigorous guidelines\nfor practitioners.\n","authors":["Juntong Chen","Johannes Schmidt-Hieber","Claire Donnat","Olga Klopp"],"pdf_url":"https://arxiv.org/pdf/2410.20068v2.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2504.11831v1","updated":"2025-04-16T07:41:40Z","published":"2025-04-16T07:41:40Z","title":"Support is All You Need for Certified VAE Training","summary":"  Variational Autoencoders (VAEs) have become increasingly popular and deployed\nin safety-critical applications. In such applications, we want to give\ncertified probabilistic guarantees on performance under adversarial attacks. We\npropose a novel method, CIVET, for certified training of VAEs. CIVET depends on\nthe key insight that we can bound worst-case VAE error by bounding the error on\ncarefully chosen support sets at the latent layer. We show this point\nmathematically and present a novel training algorithm utilizing this insight.\nWe show in an extensive evaluation across different datasets (in both the\nwireless and vision application areas), architectures, and perturbation\nmagnitudes that our method outperforms SOTA methods achieving good standard\nperformance with strong robustness guarantees.\n","authors":["Changming Xu","Debangshu Banerjee","Deepak Vasisht","Gagandeep Singh"],"pdf_url":"https://arxiv.org/pdf/2504.11831v1.pdf","comment":"21 pages, 3 figures, ICLR '25"},{"id":"http://arxiv.org/abs/2503.21138v2","updated":"2025-04-16T07:26:19Z","published":"2025-03-27T04:00:49Z","title":"A Computational Framework for Efficient Model Evaluation with Causal\n  Guarantees","summary":"  In order to reduce the cost of experimental evaluation for models, we\nintroduce a computational theory of evaluation for prediction and decision\nmodels: build evaluation model to accelerate the evaluation procedures. We\nprove upper bounds of generalized error and generalized causal effect error of\ngiven evaluation models. We also prove efficiency, and consistency to estimated\ncausal effect from deployed subject to evaluation metric by prediction. To\nlearn evaluation models, we propose a meta-learner to handle heterogeneous\nevaluation subjects space problem. Comparing with existed evaluation\napproaches, our (conditional) evaluation model reduced 24.1\\%-99.0\\% evaluation\nerrors across 12 scenes, including individual medicine, scientific simulation,\nsocial experiment, business activity, and quantum trade. The evaluation time is\nreduced 3-7 order of magnitude comparing with experiments or simulations.\n","authors":["Hedong Yan"],"pdf_url":"https://arxiv.org/pdf/2503.21138v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18309v2","updated":"2025-04-16T07:24:54Z","published":"2025-03-24T03:19:45Z","title":"Efficient Transformed Gaussian Process State-Space Models for\n  Non-Stationary High-Dimensional Dynamical Systems","summary":"  Gaussian process state-space models (GPSSMs) offer a principled framework for\nlearning and inference in nonlinear dynamical systems with uncertainty\nquantification. However, existing GPSSMs are limited by the use of multiple\nindependent stationary Gaussian processes (GPs), leading to prohibitive\ncomputational and parametric complexity in high-dimensional settings and\nrestricted modeling capacity for non-stationary dynamics. To address these\nchallenges, we propose an efficient transformed Gaussian process state-space\nmodel (ETGPSSM) for scalable and flexible modeling of high-dimensional,\nnon-stationary dynamical systems. Specifically, our ETGPSSM integrates a single\nshared GP with input-dependent normalizing flows, yielding an expressive\nimplicit process prior that captures complex, non-stationary transition\ndynamics while significantly reducing model complexity. For the inference of\nthe implicit process, we develop a variational inference algorithm that jointly\napproximates the posterior over the underlying GP and the neural network\nparameters defining the normalizing flows. To avoid explicit variational\nparameterization of the latent states, we further incorporate the ensemble\nKalman filter (EnKF) into the variational framework, enabling accurate and\nefficient state estimation. Extensive empirical evaluations on synthetic and\nreal-world datasets demonstrate the superior performance of our ETGPSSM in\nsystem dynamics learning, high-dimensional state estimation, and time-series\nforecasting, outperforming existing GPSSMs and neural network-based SSMs in\nterms of computational efficiency and accuracy.\n","authors":["Zhidi Lin","Ying Li","Feng Yin","Juan Maroñas","Alexandre H. Thiéry"],"pdf_url":"https://arxiv.org/pdf/2503.18309v2.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2210.06672v3","updated":"2025-04-16T06:23:46Z","published":"2022-10-13T01:58:06Z","title":"Variance-Aware Estimation of Kernel Mean Embedding","summary":"  An important feature of kernel mean embeddings (KME) is that the rate of\nconvergence of the empirical KME to the true distribution KME can be bounded\nindependently of the dimension of the space, properties of the distribution and\nsmoothness features of the kernel. We show how to speed-up convergence by\nleveraging variance information in the reproducing kernel Hilbert space.\nFurthermore, we show that even when such information is a priori unknown, we\ncan efficiently estimate it from the data, recovering the desiderata of a\ndistribution agnostic bound that enjoys acceleration in fortuitous settings. We\nfurther extend our results from independent data to stationary mixing sequences\nand illustrate our methods in the context of hypothesis testing and robust\nparametric estimation.\n","authors":["Geoffrey Wolfer","Pierre Alquier"],"pdf_url":"https://arxiv.org/pdf/2210.06672v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11775v1","updated":"2025-04-16T05:29:11Z","published":"2025-04-16T05:29:11Z","title":"Discrimination-free Insurance Pricing with Privatized Sensitive\n  Attributes","summary":"  Fairness has emerged as a critical consideration in the landscape of machine\nlearning algorithms, particularly as AI continues to transform decision-making\nacross societal domains. To ensure that these algorithms are free from bias and\ndo not discriminate against individuals based on sensitive attributes such as\ngender and race, the field of algorithmic bias has introduced various fairness\nconcepts, along with methodologies to achieve these notions in different\ncontexts. Despite the rapid advancement, not all sectors have embraced these\nfairness principles to the same extent. One specific sector that merits\nattention in this regard is insurance. Within the realm of insurance pricing,\nfairness is defined through a distinct and specialized framework. Consequently,\nachieving fairness according to established notions does not automatically\nensure fair pricing in insurance. In particular, regulators are increasingly\nemphasizing transparency in pricing algorithms and imposing constraints on\ninsurance companies on the collection and utilization of sensitive consumer\nattributes. These factors present additional challenges in the implementation\nof fairness in pricing algorithms. To address these complexities and comply\nwith regulatory demands, we propose an efficient method for constructing fair\nmodels that are tailored to the insurance domain, using only privatized\nsensitive attributes. Notably, our approach ensures statistical guarantees,\ndoes not require direct access to sensitive attributes, and adapts to varying\ntransparency requirements, addressing regulatory demands while ensuring\nfairness in insurance pricing.\n","authors":["Tianhe Zhang","Suhan Liu","Peng Shi"],"pdf_url":"https://arxiv.org/pdf/2504.11775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03098v3","updated":"2025-04-16T03:28:59Z","published":"2024-10-04T02:47:49Z","title":"Forest Proximities for Time Series","summary":"  RF-GAP has recently been introduced as an improved random forest proximity\nmeasure. In this paper, we present PF-GAP, an extension of RF-GAP proximities\nto proximity forests, an accurate and efficient time series classification\nmodel. We use the forest proximities in connection with Multi-Dimensional\nScaling to obtain vector embeddings of univariate time series, comparing the\nembeddings to those obtained using various time series distance measures. We\nalso use the forest proximities alongside Local Outlier Factors to investigate\nthe connection between misclassified points and outliers, comparing with\nnearest neighbor classifiers which use time series distance measures. We show\nthat the forest proximities seem to exhibit a stronger connection between\nmisclassified points and outliers than nearest neighbor classifiers.\n","authors":["Ben Shaw","Jake Rhodes","Soukaina Filali Boubrahimi","Kevin R. Moon"],"pdf_url":"https://arxiv.org/pdf/2410.03098v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12528v1","updated":"2025-04-16T23:20:43Z","published":"2025-04-16T23:20:43Z","title":"Robust and Scalable Variational Bayes","summary":"  We propose a robust and scalable framework for variational Bayes (VB) that\neffectively handles outliers and contamination of arbitrary nature in large\ndatasets. Our approach divides the dataset into disjoint subsets, computes the\nposterior for each subset, and applies VB approximation independently to these\nposteriors. The resulting variational posteriors with respect to the subsets\nare then aggregated using the geometric median of probability measures,\ncomputed with respect to the Wasserstein distance. This novel aggregation\nmethod yields the Variational Median Posterior (VM-Posterior) distribution. We\nrigorously demonstrate that the VM-Posterior preserves contraction properties\nakin to those of the true posterior, while accounting for approximation errors\nor the variational gap inherent in VB methods. We also provide provable\nrobustness guarantee of the VM-Posterior. Furthermore, we establish a\nvariational Bernstein-von Mises theorem for both multivariate Gaussian\ndistributions with general covariance structures and the mean-field variational\nfamily. To facilitate practical implementation, we adapt existing algorithms\nfor computing the VM-Posterior and evaluate its performance through extensive\nnumerical experiments. The results highlight its robustness and scalability,\nmaking it a reliable tool for Bayesian inference in the presence of complex,\ncontaminated datasets.\n","authors":["Carlos Misael Madrid Padilla","Shitao Fan","Lizhen Lin"],"pdf_url":"https://arxiv.org/pdf/2504.12528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12465v1","updated":"2025-04-16T20:01:00Z","published":"2025-04-16T20:01:00Z","title":"Geometric Generality of Transformer-Based Gröbner Basis Computation","summary":"  The intersection of deep learning and symbolic mathematics has seen rapid\nprogress in recent years, exemplified by the work of Lample and Charton. They\ndemonstrated that effective training of machine learning models for solving\nmathematical problems critically depends on high-quality, domain-specific\ndatasets. In this paper, we address the computation of Gr\\\"obner basis using\nTransformers. While a dataset generation method tailored to Transformer-based\nGr\\\"obner basis computation has previously been proposed, it lacked theoretical\nguarantees regarding the generality or quality of the generated datasets. In\nthis work, we prove that datasets generated by the previously proposed\nalgorithm are sufficiently general, enabling one to ensure that Transformers\ncan learn a sufficiently diverse range of Gr\\\"obner bases. Moreover, we propose\nan extended and generalized algorithm to systematically construct datasets of\nideal generators, further enhancing the training effectiveness of Transformer.\nOur results provide a rigorous geometric foundation for Transformers to address\na mathematical problem, which is an answer to Lample and Charton's idea of\ntraining on diverse or representative inputs.\n","authors":["Yuta Kambe","Yota Maeda","Tristan Vaccon"],"pdf_url":"https://arxiv.org/pdf/2504.12465v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2002.08907v4","updated":"2025-04-16T19:41:25Z","published":"2020-02-20T17:52:18Z","title":"Second-order Conditional Gradient Sliding","summary":"  Constrained second-order convex optimization algorithms are the method of\nchoice when a high accuracy solution to a problem is needed, due to their local\nquadratic convergence. These algorithms require the solution of a constrained\nquadratic subproblem at every iteration. We present the \\emph{Second-Order\nConditional Gradient Sliding} (SOCGS) algorithm, which uses a projection-free\nalgorithm to solve the constrained quadratic subproblems inexactly. When the\nfeasible region is a polytope the algorithm converges quadratically in primal\ngap after a finite number of linearly convergent iterations. Once in the\nquadratic regime the SOCGS algorithm requires $\\mathcal{O}(\\log(\\log\n1/\\varepsilon))$ first-order and Hessian oracle calls and $\\mathcal{O}(\\log\n(1/\\varepsilon) \\log(\\log1/\\varepsilon))$ linear minimization oracle calls to\nachieve an $\\varepsilon$-optimal solution. This algorithm is useful when the\nfeasible region can only be accessed efficiently through a linear optimization\noracle, and computing first-order information of the function, although\npossible, is costly.\n","authors":["Alejandro Carderera","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2002.08907v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12450v1","updated":"2025-04-16T19:31:42Z","published":"2025-04-16T19:31:42Z","title":"Can Moran Eigenvectors Improve Machine Learning of Spatial Data?\n  Insights from Synthetic Data Validation","summary":"  Moran Eigenvector Spatial Filtering (ESF) approaches have shown promise in\naccounting for spatial effects in statistical models. Can this extend to\nmachine learning? This paper examines the effectiveness of using Moran\nEigenvectors as additional spatial features in machine learning models. We\ngenerate synthetic datasets with known processes involving spatially varying\nand nonlinear effects across two different geometries. Moran Eigenvectors\ncalculated from different spatial weights matrices, with and without a priori\neigenvector selection, are tested. We assess the performance of popular machine\nlearning models, including Random Forests, LightGBM, XGBoost, and TabNet, and\nbenchmark their accuracies in terms of cross-validated R2 values against models\nthat use only coordinates as features. We also extract coefficients and\nfunctions from the models using GeoShapley and compare them with the true\nprocesses. Results show that machine learning models using only location\ncoordinates achieve better accuracies than eigenvector-based approaches across\nvarious experiments and datasets. Furthermore, we discuss that while these\nfindings are relevant for spatial processes that exhibit positive spatial\nautocorrelation, they do not necessarily apply when modeling network\nautocorrelation and cases with negative spatial autocorrelation, where Moran\nEigenvectors would still be useful.\n","authors":["Ziqi Li","Zhan Peng"],"pdf_url":"https://arxiv.org/pdf/2504.12450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12392v1","updated":"2025-04-16T18:01:05Z","published":"2025-04-16T18:01:05Z","title":"A Survey on Archetypal Analysis","summary":"  Archetypal analysis (AA) was originally proposed in 1994 by Adele Cutler and\nLeo Breiman as a computational procedure to extract the distinct aspects called\narchetypes in observations with each observational record approximated as a\nmixture (i.e., convex combination) of these archetypes. AA thereby provides\nstraightforward, interpretable, and explainable representations for feature\nextraction and dimensionality reduction, facilitating the understanding of the\nstructure of high-dimensional data with wide applications throughout the\nsciences. However, AA also faces challenges, particularly as the associated\noptimization problem is non-convex. This survey provides researchers and data\nmining practitioners an overview of methodologies and opportunities that AA has\nto offer surveying the many applications of AA across disparate fields of\nscience, as well as best practices for modeling data using AA and limitations.\nThe survey concludes by explaining important future research directions\nconcerning AA.\n","authors":["Aleix Alcacer","Irene Epifanio","Sebastian Mair","Morten Mørup"],"pdf_url":"https://arxiv.org/pdf/2504.12392v1.pdf","comment":"20 pages, 13 figures, under review"},{"id":"http://arxiv.org/abs/2504.12374v1","updated":"2025-04-16T18:00:00Z","published":"2025-04-16T18:00:00Z","title":"Resonances in reflective Hamiltonian Monte Carlo","summary":"  In high dimensions, reflective Hamiltonian Monte Carlo with inexact\nreflections exhibits slow mixing when the particle ensemble is initialised from\na Dirac delta distribution and the uniform distribution is targeted. By\nquantifying the instantaneous non-uniformity of the distribution with the\nSinkhorn divergence, we elucidate the principal mechanisms underlying the\nmixing problems. In spheres and cubes, we show that the collective motion\ntransitions between fluid-like and discretisation-dominated behaviour, with the\ncritical step size scaling as a power law in the dimension. In both regimes,\nthe particles can spontaneously unmix, leading to resonances in the particle\ndensity and the aforementioned problems. Additionally, low-dimensional toy\nmodels of the dynamics are constructed which reproduce the dominant features of\nthe high-dimensional problem. Finally, the dynamics is contrasted with the\nexact Hamiltonian particle flow and tuning practices are discussed.\n","authors":["Namu Kroupa","Gábor Csányi","Will Handley"],"pdf_url":"https://arxiv.org/pdf/2504.12374v1.pdf","comment":null}],"Optimization":[{"id":"http://arxiv.org/abs/2504.12297v1","updated":"2025-04-16T17:58:07Z","published":"2025-04-16T17:58:07Z","title":"Optimal flock formation induced by agent heterogeneity","summary":"  The study of flocking in biological systems has identified conditions for\nself-organized collective behavior, inspiring the development of decentralized\nstrategies to coordinate the dynamics of swarms of drones and other autonomous\nvehicles. Previous research has focused primarily on the role of the\ntime-varying interaction network among agents while assuming that the agents\nthemselves are identical or nearly identical. Here, we depart from this\nconventional assumption to investigate how inter-individual differences between\nagents affect the stability and convergence in flocking dynamics. We show that\nflocks of agents with optimally assigned heterogeneous parameters significantly\noutperform their homogeneous counterparts, achieving 20-40% faster convergence\nto desired formations across various control tasks. These tasks include target\ntracking, flock formation, and obstacle maneuvering. In systems with\ncommunication delays, heterogeneity can enable convergence even when flocking\nis unstable for identical agents. Our results challenge existing paradigms in\nmulti-agent control and establish system disorder as an adaptive, distributed\nmechanism to promote collective behavior in flocking dynamics.\n","authors":["Arthur N. Montanari","Ana Elisa D. Barioni","Chao Duan","Adilson E. Motter"],"pdf_url":"https://arxiv.org/pdf/2504.12297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12260v1","updated":"2025-04-16T17:11:39Z","published":"2025-04-16T17:11:39Z","title":"On resolution of L1-norm minimization via a two-metric adaptive\n  projection method","summary":"  The two-metric projection method is a simple yet elegant algorithm proposed\nby Bertsekas\n  to address bound/box-constrained optimization problems. The algorithm's low\nper-iteration\n  cost and potential for using Hessian information make it a favorable\ncomputation method\n  for this problem class. Inspired by this algorithm, we propose a two-metric\nadaptive projection\n  method for solving the $\\ell_1$-norm regularization problem that inherits\nthese advantages. We demonstrate that the method is theoretically sound -\n  it has global convergence. Furthermore, it is capable of manifold\nidentification and has\n  superlinear convergence rate under the error bound condition and strict\ncomplementarity.\n  Therefore, given sparsity in the solution, the method enjoys superfast\nconvergence in iteration\n  while maintaining scalability, making it desirable for large-scale problems.\nWe also equip\n  the algorithm with competitive complexity to solve nonconvex problems.\nNumerical experiments\n  are conducted to illustrate the advantages of this algorithm implied by the\ntheory compared\n  to other competitive methods, especially in large-scale scenarios. In\ncontrast to the original two-metric projection method, our algorithm directly\nsolves the $\\ell_1$-norm minimization problem without resorting to the\nintermediate reformulation as a bound-constrained problem, so it circumvents\nthe issue of numerical instability.\n","authors":["Hanju Wu","Yue Xie"],"pdf_url":"https://arxiv.org/pdf/2504.12260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00598v2","updated":"2025-04-16T16:35:08Z","published":"2024-08-01T14:28:34Z","title":"HOT: An Efficient Halpern Accelerating Algorithm for Optimal Transport\n  Problems","summary":"  This paper proposes an efficient HOT algorithm for solving the optimal\ntransport (OT) problems with finite supports. We particularly focus on an\nefficient implementation of the HOT algorithm for the case where the supports\nare in $\\mathbb{R}^2$ with ground distances calculated by $L_2^2$-norm.\nSpecifically, we design a Halpern accelerating algorithm to solve the\nequivalent reduced model of the discrete OT problem. Moreover, we derive a\nnovel procedure to solve the involved linear systems in the HOT algorithm in\nlinear time complexity. Consequently, we can obtain an\n$\\varepsilon$-approximate solution to the optimal transport problem with $M$\nsupports in $O(M^{1.5}/\\varepsilon)$ flops, which significantly improves the\nbest-known computational complexity. We further propose an efficient procedure\nto recover an optimal transport plan for the original OT problem based on a\nsolution to the reduced model, thereby overcoming the limitations of the\nreduced OT model in applications that require the transport plan. We implement\nthe HOT algorithm in PyTorch and extensive numerical results show the superior\nperformance of the HOT algorithm compared to existing state-of-the-art\nalgorithms for solving the OT problems.\n","authors":["Guojun Zhang","Zhexuan Gu","Yancheng Yuan","Defeng Sun"],"pdf_url":"https://arxiv.org/pdf/2408.00598v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09475v3","updated":"2025-04-16T16:28:06Z","published":"2024-09-14T16:20:26Z","title":"MALADY: Multiclass Active Learning with Auction Dynamics on Graphs","summary":"  Active learning enhances the performance of machine learning methods,\nparticularly in semi-supervised cases, by judiciously selecting a limited\nnumber of unlabeled data points for labeling, with the goal of improving the\nperformance of an underlying classifier. In this work, we introduce the\nMulticlass Active Learning with Auction Dynamics on Graphs (MALADY) framework\nwhich leverages the auction dynamics algorithm on similarity graphs for\nefficient active learning. In particular, we generalize the auction dynamics\nalgorithm on similarity graphs for semi-supervised learning in [24] to\nincorporate a more general optimization functional. Moreover, we introduce a\nnovel active learning acquisition function that uses the dual variable of the\nauction algorithm to measure the uncertainty in the classifier to prioritize\nqueries near the decision boundaries between different classes. Lastly, using\nexperiments on classification tasks, we evaluate the performance of our\nproposed method and show that it exceeds that of comparison algorithms.\n","authors":["Gokul Bhusal","Kevin Miller","Ekaterina Merkurjev"],"pdf_url":"https://arxiv.org/pdf/2409.09475v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07624v2","updated":"2025-04-16T16:13:21Z","published":"2025-02-28T09:46:52Z","title":"An Improved Adaptive Orthogonal Basis Deflation Method for Multiple\n  Solutions with Applications to Nonlinear Elliptic Equations in Varying\n  Domains","summary":"  Multiple solutions are common in various non-convex problems arising from\nindustrial and scientific computing. Nonetheless, understanding the nontrivial\nsolutions' qualitative properties seems limited, partially due to the lack of\nefficient and reliable numerical methods. In this paper, we design a dedicated\nnumerical method to explore these nontrivial solutions further. We first design\nan improved adaptive orthogonal basis deflation method by combining the\nadaptive orthogonal basis method with a bisection-deflation algorithm. We then\napply the proposed new method to study the impact of domain changes on multiple\nsolutions of certain nonlinear elliptic equations. When the domain varies from\na circular disk to an elliptical disk, the corresponding functional value\nchanges dramatically for some particular solutions, which indicates that these\nnontrivial solutions in the circular domain may become unstable in the\nelliptical domain. Moreover, several theoretical results on multiple solutions\nin existing literature are verified. For the nonlinear Sine-Gordon equation\nwith parameter $\\lambda$, nontrivial solutions are found for $\\lambda >\n\\lambda_2$, here $\\lambda_2$ is the second eigenvalue of the corresponding\nlinear eigenvalue problem. For the singularly perturbed Ginzburg-Landau\nequation, highly concentrated solutions are numerically found which suggests\nthat their convergent limit is a delta function when the perturbation parameter\ngoes to zero\n","authors":["Yangyi Ye","Lin Li","Pengcheng Xie","Haijun Yu"],"pdf_url":"https://arxiv.org/pdf/2503.07624v2.pdf","comment":"24pages"},{"id":"http://arxiv.org/abs/2503.01912v2","updated":"2025-04-16T16:00:11Z","published":"2025-03-01T06:39:00Z","title":"A spectral Levenberg-Marquardt-Deflation method for multiple solutions\n  of semilinear elliptic systems","summary":"  Many nonlinear differential equations arising from practical problems may\npermit nontrivial multiple solutions relevant to applications, and these\nmultiple solutions are helpful to deeply understand these practical problems\nand to improve some applications. Developing an efficient numerical method for\nfinding multiple solutions is very necessary due to the nonlinearity and\nmultiple solutions of these equations. Moreover, providing an efficient\niteration plays an important role in successfully obtaining multiple solutions\nwith fast and stable convergence. In the current paper, an efficient algorithm\nfor finding multiple solutions of semilinear elliptic systems is proposed,\nwhere the trust region Levenberg-Marquardt method is firstly used to iterate\nthe resulted nonlinear algebraic system. When the nonlinear term in these\nequations has only the first derivative, our algorithm can efficiently find\nmultiple solutions as well. Several numerical experiments are tested to show\nthe efficiency of our algorithm, and some solutions which have not been shown\nin the literature are also found and shown.\n","authors":["Lin Li","Yuheng Zhou","Pengcheng Xie","Huiyuan Li"],"pdf_url":"https://arxiv.org/pdf/2503.01912v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2504.02832v2","updated":"2025-04-16T15:41:23Z","published":"2025-03-06T22:29:51Z","title":"A novel numerical method tailored for unconstrained optimization\n  problems","summary":"  Unconstrained optimization problems become more common in scientific\ncomputing and engineering applications with the rapid development of artificial\nintelligence, and numerical methods for solving them more quickly and\nefficiently have been getting more attention and research. Moreover, an\nefficient method to minimize all kinds of objective functions is urgently\nneeded, especially the nonsmooth objective function. Therefore, in the current\npaper, we focus on proposing a novel numerical method tailored for\nunconstrained optimization problems whether the objective function is smooth or\nnot. To be specific, based on the variational procedure to refine the gradient\nand Hessian matrix approximations, an efficient quadratic model with $2n$\nconstrained conditions is established. Moreover, to improve the computational\nefficiency, a simplified model with 2 constrained conditions is also proposed,\nwhere the gradient and Hessian matrix can be explicitly updated, and the\ncorresponding boundedness of the remaining $2n-2$ constrained conditions is\nderived. On the other hand, the novel numerical method is summarized, and\napproximation results on derivative information are also analyzed and shown.\nNumerical experiments involving smooth, derivative blasting, and non-smooth\nproblems are tested, demonstrating its feasibility and efficiency. Compared\nwith existing methods, our proposed method can efficiently solve smooth and\nnon-smooth unconstrained optimization problems for the first time, and it is\nvery easy to program the code, indicating that our proposed method not also has\ngreat application prospects, but is also very meaningful to explore practical\ncomplex engineering and scientific problems.\n","authors":["Lin Li","Pengcheng Xie","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.02832v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2504.12184v1","updated":"2025-04-16T15:40:03Z","published":"2025-04-16T15:40:03Z","title":"Feature Selection for Data-driven Explainable Optimization","summary":"  Mathematical optimization, although often leading to NP-hard models, is now\ncapable of solving even large-scale instances within reasonable time. However,\nthe primary focus is often placed solely on optimality. This implies that while\nobtained solutions are globally optimal, they are frequently not comprehensible\nto humans, in particular when obtained by black-box routines. In contrast,\nexplainability is a standard requirement for results in Artificial\nIntelligence, but it is rarely considered in optimization yet. There are only a\nfew studies that aim to find solutions that are both of high quality and\nexplainable. In recent work, explainability for optimization was defined in a\ndata-driven manner: a solution is considered explainable if it closely\nresembles solutions that have been used in the past under similar\ncircumstances. To this end, it is crucial to identify a preferably small subset\nof features from a presumably large set that can be used to explain a solution.\nIn mathematical optimization, feature selection has received little attention\nyet. In this work, we formally define the feature selection problem for\nexplainable optimization and prove that its decision version is NP-complete. We\nintroduce mathematical models for optimized feature selection. As their global\nsolution requires significant computation time with modern mixed-integer linear\nsolvers, we employ local heuristics. Our computational study using data that\nreflect real-world scenarios demonstrates that the problem can be solved\npractically efficiently for instances of reasonable size.\n","authors":["Kevin-Martin Aigner","Marc Goerigk","Michael Hartisch","Frauke Liers","Arthur Miehlich","Florian Rösel"],"pdf_url":"https://arxiv.org/pdf/2504.12184v1.pdf","comment":"21 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.01819v2","updated":"2025-04-16T15:36:36Z","published":"2025-02-03T20:50:05Z","title":"Score as Action: Fine-Tuning Diffusion Generative Models by\n  Continuous-time Reinforcement Learning","summary":"  Reinforcement learning from human feedback (RLHF), which aligns a diffusion\nmodel with input prompt, has become a crucial step in building reliable\ngenerative AI models. Most works in this area use a discrete-time formulation,\nwhich is prone to induced errors, and often not applicable to models with\nhigher-order/black-box solvers. The objective of this study is to develop a\ndisciplined approach to fine-tune diffusion models using continuous-time RL,\nformulated as a stochastic control problem with a reward function that aligns\nthe end result (terminal state) with input prompt. The key idea is to treat\nscore matching as controls or actions, and thereby making connections to policy\noptimization and regularization in continuous-time RL. To carry out this idea,\nwe lay out a new policy optimization framework for continuous-time RL, and\nillustrate its potential in enhancing the value networks design space via\nleveraging the structural property of diffusion models. We validate the\nadvantages of our method by experiments in downstream tasks of fine-tuning\nlarge-scale Text2Image models of Stable Diffusion v1.5.\n","authors":["Hanyang Zhao","Haoxian Chen","Ji Zhang","David D. Yao","Wenpin Tang"],"pdf_url":"https://arxiv.org/pdf/2502.01819v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2409.08400"},{"id":"http://arxiv.org/abs/2405.10581v2","updated":"2025-04-16T15:24:39Z","published":"2024-05-17T07:09:52Z","title":"Future Aware Safe Active Learning of Time Varying Systems using Gaussian\n  Processes","summary":"  Experimental exploration of high-cost systems with safety constraints, common\nin engineering applications, is a challenging endeavor. Data-driven models\noffer a promising solution, but acquiring the requisite data remains expensive\nand is potentially unsafe. Safe active learning techniques prove essential,\nenabling the learning of high-quality models with minimal expensive data points\nand high safety. This paper introduces a safe active learning framework\ntailored for time-varying systems, addressing drift, seasonal changes, and\ncomplexities due to dynamic behavior. The proposed Time-aware Integrated Mean\nSquared Prediction Error (T-IMSPE) method minimizes posterior variance over\ncurrent and future states, optimizing information gathering also in the time\ndomain. Empirical results highlight T-IMSPE's advantages in model quality\nthrough toy and real-world examples. State of the art Gaussian processes are\ncompatible with T-IMSPE. Our theoretical contributions include a clear\ndelineation which Gaussian process kernels, domains, and weighting measures are\nsuitable for T-IMSPE and even beyond for its non-time aware predecessor IMSPE.\n","authors":["Markus Lange-Hegermann","Christoph Zimmer"],"pdf_url":"https://arxiv.org/pdf/2405.10581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12168v1","updated":"2025-04-16T15:19:08Z","published":"2025-04-16T15:19:08Z","title":"A simple algorithm for the simple bilevel programming (SBP) problem","summary":"  In this article we intend to develop a simple and implementable algorithm for\nminimizing a convex function over the solution set of another convex\noptimization problem. Such a problem is often referred to as a simple bilevel\nprogramming (SBP) problem. One of the key features of our algorithm is that we\nmake no assumption on the diferentiability of the upper level objective, though\nwe will assume that the lower level objective is smooth. Another key feature of\nthe algorithm is that it does not assume that the lower level objective has a\nLipschitz gradient, which is a standard assumption in most of the well-known\nalgorithms for this class of problems. We present the convergence analysis and\nalso some numerical experiments demonstrating the efectiveness of the\nalgorithm.\n","authors":["Stephan Dempe","Joydeep Duta","Tanushree Pandit","K. S. Mallikarjuna Rao"],"pdf_url":"https://arxiv.org/pdf/2504.12168v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2504.12152v1","updated":"2025-04-16T15:01:41Z","published":"2025-04-16T15:01:41Z","title":"Evaluating the Effects of Organic vs. Conventional Farming on Aquifer\n  Water Quality","summary":"  This study analyzes water quality dynamics and aquifer recharge through\nirrigated agriculture, contributing to the literature on Managed Aquifer\nRecharge (MAR) amidst growing water scarcity. We develop two optimal control\nmodels-- a linear and a non-linear extension of (Martin and Stahn, 2013) --that\nincorporate the impact of fertilizers on aquifer water quality, distinguishing\nbetween organic and conventional farming practices. The linear model applies a\nconstant rebate mechanism, whereas the non-linear model employs a concave\nrebate scheme. Our results show that, depending on climate change scenarios,\nfertilizer-induced food price discounts, and pollution levels, a socially\noptimal equilibrium in fertilizer use can be attained. Policy implications are\ndiscussed, emphasizing the trade-off between environmental sustainability and\nsocial welfare.\n","authors":["Marta Llorente","Marta Suárez"],"pdf_url":"https://arxiv.org/pdf/2504.12152v1.pdf","comment":"40 pages, 9 figures, 8 tables"},{"id":"http://arxiv.org/abs/2302.08779v2","updated":"2025-04-16T14:41:01Z","published":"2023-02-17T09:41:56Z","title":"On the convergence result of the gradient-push algorithm on directed\n  graphs with constant stepsize","summary":"  Distributed optimization has recieved a lot of interest due to its wide\napplications in various fields. It consists of multiple agents that connected\nby a graph and optimize a total cost in a collaborative way. Often in the\napplications, the graph of the agents is given by a directed graph. The\ngradient-push algorithm is a fundamental method for distributed optimization\nfor which the agents are connected by a directed graph. Despite of its wide\nusage in the literatures, its convergence property has not been established\nwell for the important case that the stepsize is constant and the domain is the\nentire space. This work proves that the gradient-push algorithm with stepsize\n$\\alpha>0$ converges exponentially fast to an $O(\\alpha)$-neighborhood of the\noptimizer if the stepsize $\\alpha$ is less than a specific value. For the\nresult, we assume that each cost is smooth and the total cost is strongly\nconvex. Numerical experiments are provided to support the theoretical\nconvergence result. \\textcolor{black}{We also present a numerical test showing\nthat the gradient-push algorithm may approach a small neighborhood of the\nminimizer faster than the Push-DIGing algorithm which is a variant of the\ngradient-push algorithm involving the communication of the gradient\ninformations of the agents.\n","authors":["Woocheol Choi","Doheon Kim","Seok-Bae Yun"],"pdf_url":"https://arxiv.org/pdf/2302.08779v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12124v1","updated":"2025-04-16T14:36:51Z","published":"2025-04-16T14:36:51Z","title":"Adaptive Controller For Simultaneous Spacecraft Attitude Tracking And\n  Reaction Wheel Fault Detection","summary":"  The attitude control of a spacecraft is integral to achieving mission\nsuccess. However, failures in actuators such as reaction wheels are detrimental\nand can often lead to an early end of mission. We propose a Lyapunov-based\nadaptive controller that can estimate and compensate for reaction wheels\ndegradation simultaneously. The controller incorporates an adaptive update\ncontrol law with a gradient-based term and an integral concurrent learning term\nthat collects input-output data for online estimation of uncertain parameters.\nThe proposed controller guarantees attitude tracking and its performance is\ntested through numerical simulations.\n","authors":["Camilo Riano-Rios","George Nehma","Madhur Tiwari"],"pdf_url":"https://arxiv.org/pdf/2504.12124v1.pdf","comment":"17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2504.12059v1","updated":"2025-04-16T13:11:21Z","published":"2025-04-16T13:11:21Z","title":"Sustainable cooperation on the hybrid pollution-control game with\n  heterogeneous players","summary":"  This paper considers a hybrid pollution-control differential game with two\nfarsighted players and one myopic player. Both the seasonal regime shifts in\nthe state dynamics and the players' heterogeneous preferences are introduced\ninto the model. The strategies under cooperative, noncooperative and partially\ncooperative scenarios are obtained by utilizing the Pontryagin's Maximum\nPrinciple. Under all feasible coalition structures, the convergence of the\nstate variable is proved. A new sustainably--cooperative optimality principle\nis proposed according to the coalition structures, which belongs to the\nimputation set. The prerequisite for the existence of time-consistency in the\nsustainably-cooperative optimality principle is explicitly obtained. The\nseasonal imputation distribution procedure (IDP) is designed to maintain the\ntime-consistentcy (dynamic stability) of cooperation over time.\n","authors":["Yilun Wu","Anna Tur","Peichen Ye"],"pdf_url":"https://arxiv.org/pdf/2504.12059v1.pdf","comment":"25 pages, 2 figures"},{"id":"http://arxiv.org/abs/2405.19770v5","updated":"2025-04-16T13:06:19Z","published":"2024-05-30T07:34:05Z","title":"MIP-DD: A Delta Debugger for Mixed Integer Programming Solvers","summary":"  The recent performance improvements in mixed-integer programming (MIP) have\nbeen accompanied by a significantly increased complexity of the codes of MIP\nsolvers, which poses challenges in fixing implementation errors. In this paper,\nwe introduce MIP-DD, a solver-independent tool, which to the best of our\nknowledge is the first open-source delta debugger for MIP. Delta debugging is a\nhypothesis-trial-result approach to isolate the cause of a solver failure.\nMIP-DD simplifies MIP instances while maintaining the undesired behavior.\nPreliminary versions already supported and motivated fixes for many bugs in the\nSCIP releases 8.0.1 to 8.1.1. In these versions, MIP-DD successfully\ncontributed to 24 out of all 51 documented MIP-related bugfixes even for some\nlong-known issues. In selected case studies we highlight that instances\ntriggering fundamental bugs in SCIP can typically be reduced to a few variables\nand constraints in less than an hour. This makes it significantly easier to\nmanually trace and check the solution process on the resulting simplified\ninstances. A promising future application of MIP-DD is the analysis of\nperformance bottlenecks, which could very well benefit from simple adversarial\ninstances.\n","authors":["Alexander Hoen","Dominik Kamp","Ambros Gleixner"],"pdf_url":"https://arxiv.org/pdf/2405.19770v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15300v2","updated":"2025-04-16T11:41:44Z","published":"2025-01-25T18:50:26Z","title":"Modified Dai-Liao Spectral Conjugate Gradient Method with Application to\n  Signal Processing","summary":"  In this article, we present a modified variant of the Dai-Liao spectral\nconjugate gradient method, developed through an analysis of eigenvalues and\ninspired by a modified secant condition. We show that the proposed method is\nglobally convergent for general nonlinear functions under standard assumptions.\nBy incorporating the new secant condition and a quasi-Newton direction, we\nintroduce updated spectral parameters. These changes ensure that the resulting\nsearch direction satisfies the sufficient descent property without relying on\nany line search. Numerical experiments show that the proposed algorithm\nperforms better than several existing methods in terms of convergence speed and\ncomputational efficiency. Its effectiveness is further demonstrated through an\napplication in signal processing.\n","authors":["D. R. Sahu","Shikher Sharma","Pankaj Gautam"],"pdf_url":"https://arxiv.org/pdf/2501.15300v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11982v1","updated":"2025-04-16T11:23:30Z","published":"2025-04-16T11:23:30Z","title":"Efficient identification of linear, parameter-varying, and nonlinear\n  systems with noise models","summary":"  We present a general system identification procedure capable of estimating of\na broad spectrum of state-space dynamical models, including linear\ntime-invariant (LTI), linear parameter-varying} (LPV), and nonlinear (NL)\ndynamics, along with rather general classes of noise models. Similar to the LTI\ncase, we show that for this general class of model structures, including the NL\ncase, the model dynamics can be separated into a deterministic process and a\nstochastic noise part, allowing to seamlessly tune the complexity of the\ncombined model both in terms of nonlinearity and noise modeling. We\nparameterize the involved nonlinear functional relations by means of artificial\nneural-networks (ANNs), although alternative parametric nonlinear mappings can\nalso be used. To estimate the resulting model structures, we optimize a\nprediction-error-based criterion using an efficient combination of a\nconstrained quasi-Newton approach and automatic differentiation, achieving\ntraining times in the order of seconds compared to existing state-of-the-art\nANN methods which may require hours for models of similar complexity. We\nformally establish the consistency guarantees for the proposed approach and\ndemonstrate its superior estimation accuracy and computational efficiency on\nseveral benchmark LTI, LPV, and NL system identification problems.\n","authors":["Alberto Bemporad","Roland Tóth"],"pdf_url":"https://arxiv.org/pdf/2504.11982v1.pdf","comment":"28 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.16385v2","updated":"2025-04-16T11:07:33Z","published":"2024-08-29T09:50:00Z","title":"Stochastic optimal control of Lévy tax processes with bailouts","summary":"  We consider controlling the paths of a spectrally negative L\\'evy process by\ntwo means: the subtraction of `taxes' when the process is at an all-time\nmaximum, and the addition of `bailouts' which keep the value of the process\nabove zero. We solve the corresponding stochastic optimal control problem of\nmaximising the expected present value of the difference between taxes received\nand cost of bailouts given. Our class of taxation controls is larger than has\nbeen considered up till now in the literature and makes the problem truly\ntwo-dimensional rather than one-dimensional. Along the way, we define and\ncharacterise a large class of controlled L\\'evy processes to which the optimal\nsolution belongs, which extends a known result for perturbed Brownian motions\nto the case of a general L\\'evy process with no positive jumps.\n","authors":["Dalal Al Ghanim","Ronnie Loeffen","Alexander R. Watson"],"pdf_url":"https://arxiv.org/pdf/2408.16385v2.pdf","comment":"29 pages, 3 figures"},{"id":"http://arxiv.org/abs/2504.11962v1","updated":"2025-04-16T10:44:02Z","published":"2025-04-16T10:44:02Z","title":"Curvilinear Mask Optimization for Inverse Lithography Based on B-splines\n  and Delaunay Triangulation","summary":"  In this paper, we propose a novel gradient-based method to optimize\ncurvilinear masks in optical lithography. The mask pattern is represented by\nperiodic B-spline curves. We apply Delaunay triangulation to discretize the\ndomains circled by the spline curves. Subsequently, we establish an explicit\nrelationship between the integral points and the control points of the boundary\nspline curve. Based on the relationship, we derive explicit formulas of the\ngradient of the optimization objective function with respect to the coordinates\nof the control points. Then we propose an inverse lithography algorithm to\noptimize the curvilinear mask pattern. Finally, the results of the numerical\nexperiments demonstrate the feasibility and extensive adaptability of our\nmethod.\n","authors":["Xiaoru Yi","Junqing Chen"],"pdf_url":"https://arxiv.org/pdf/2504.11962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11958v1","updated":"2025-04-16T10:41:06Z","published":"2025-04-16T10:41:06Z","title":"Initial Condition Independent Stabilisability of Switched Affine Systems","summary":"  We have previously demonstrated that a switched affine system is stabilisable\nindependently of the initial condition, i.e. there exists an asymptotically\nstabilising switching function which is the same for all initial conditions, if\nand only if there exists a stable convex combination of the sub-system\nmatrices. This result was proven by constructing a stabilising switching\nfunction of unbounded switching frequency. The current paper proves that there\nexists a switching function with bounded switching frequency which stabilises a\nswitched affine system independent of its initial condition.\n","authors":["Christopher Townsend","Maria M. Seron"],"pdf_url":"https://arxiv.org/pdf/2504.11958v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2504.11903v1","updated":"2025-04-16T09:28:26Z","published":"2025-04-16T09:28:26Z","title":"FedCanon: Non-Convex Composite Federated Learning with Efficient\n  Proximal Operation on Heterogeneous Data","summary":"  Composite federated learning offers a general framework for solving machine\nlearning problems with additional regularization terms. However, many existing\nmethods require clients to perform multiple proximal operations to handle\nnon-smooth terms and their performance are often susceptible to data\nheterogeneity. To overcome these limitations, we propose a novel composite\nfederated learning algorithm called \\textbf{FedCanon}, designed to solve the\noptimization problems comprising a possibly non-convex loss function and a\nweakly convex, potentially non-smooth regularization term. By decoupling\nproximal mappings from local updates, FedCanon requires only a single proximal\nevaluation on the server per iteration, thereby reducing the overall proximal\ncomputation cost. It also introduces control variables that incorporate global\ngradient information into client updates, which helps mitigate the effects of\ndata heterogeneity. Theoretical analysis demonstrates that FedCanon achieves\nsublinear convergence rates under general non-convex settings and linear\nconvergence under the Polyak-{\\L}ojasiewicz condition, without relying on\nbounded heterogeneity assumptions. Experiments demonstrate that FedCanon\noutperforms the state-of-the-art methods in terms of both accuracy and\ncomputational efficiency, particularly under heterogeneous data distributions.\n","authors":["Yuan Zhou","Jiachen Zhong","Xinli Shi","Guanghui Wen","Xinghuo Yu"],"pdf_url":"https://arxiv.org/pdf/2504.11903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11832v1","updated":"2025-04-16T07:44:02Z","published":"2025-04-16T07:44:02Z","title":"Generation of Paths for Motion Planning for a Dubins Vehicle on Sphere","summary":"  In this article, the candidate optimal paths for a Dubins vehicle on a sphere\nare analytically derived. In particular, the arc angles for segments in $CGC$,\n$CCC$, $CCCC$, and $CCCCC$ paths, which have previously been shown to be\noptimal depending on the turning radius $r$ of the vehicle by Kumar \\textit{et\nal.}, are analytically derived. The derived expressions are used for the\nimplementation provided in\nhttps://github.com/DeepakPrakashKumar/Motion-planning-on-sphere.\n","authors":["Deepak Prakash Kumar","Swaroop Darbha","Satyanarayana Gupta Manyam","David Casbeer"],"pdf_url":"https://arxiv.org/pdf/2504.11832v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16689v2","updated":"2025-04-16T07:38:53Z","published":"2024-01-30T02:26:23Z","title":"Expansions of the Characteristic Polynomial of a Perturbed Positive\n  Semidefinite Matrix and Convergence Analysis of Alternating Projections","summary":"  We observe that the characteristic polynomial of a linearly perturbed\nsemidefinite matrix can be used to give the convergence rate of alternating\nprojections for the positive semidefinite cone and a line. As a consequence, we\nshow that such alternating projections converge at $O(k^{-1/2})$, independently\nof the singularity degree. A sufficient condition for the linear convergence is\nalso obtained. Our method directly analyzes the defining equation for an\nalternating projection sequence and does not use error bounds.\n","authors":["Hiroyuki Ochiai","Yoshiyuki Sekiguchi","Hayato Waki"],"pdf_url":"https://arxiv.org/pdf/2401.16689v2.pdf","comment":"22 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.22124v3","updated":"2025-04-16T07:28:35Z","published":"2025-03-28T03:56:12Z","title":"Scheduling problem of aircrafts on a same runway and dual runways","summary":"  In this paper, the scheduling problems of landing and takeoff aircraft on a\nsame runway and on dual runways are addressed. In contrast to the approaches\nbased on mixed-integer optimization models in existing works, our approach\nfocuses on the minimum separation times between aircraft by introducing some\nnecessary assumptions and new concepts including relevance, breakpoint\naircraft, path and class-monotonically-decreasing sequence. Four scheduling\nproblems are discussed including landing scheduling problem, takeoff scheduling\nproblem, and mixed landing and takeoff scheduling problems on a same runway and\non dual runways with the consideration of conversions between different\naircraft sequences in typical scenarios. Two real-time optimal algorithms are\nproposed for the four scheduling problems by fully exploiting the combinations\nof different classes of aircraft, and necessary definitions, lemmas and\ntheorems are presented for the optimal convergence of the algorithms. Numerical\nexamples are presented to show the effectiveness of the proposed algorithms. In\nparticular, when $100$ aircraft are considered, by using the algorithm in this\npaper, the optimal solution can be obtained in less than $5$ seconds, while by\nusing the CPLEX software to solve the mix-integer optimization model, the\noptimal solution cannot be obtained within $1$ hour.\n","authors":["Peng Lin","Haopeng Yang","Weihua Gui"],"pdf_url":"https://arxiv.org/pdf/2503.22124v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11815v1","updated":"2025-04-16T07:01:00Z","published":"2025-04-16T07:01:00Z","title":"On projection mappings and the gradient projection method on hyperbolic\n  space forms","summary":"  This paper presents several new properties of the intrinsic\n$\\kappa$-projection into $\\kappa$-hyperbolically convex sets of\n$\\kappa$-hyperbolic space forms, along with closed-form formulas for the\nintrinsic $\\kappa$-projection into specific $\\kappa$-hyperbolically convex\nsets. It also discusses the relationship between the intrinsic\n$\\kappa$-projection, the Euclidean orthogonal projection, and the Lorentz\nprojection. These properties lay the groundwork for analyzing the gradient\nprojection method and hold importance in their own right. Additionally, new\nproperties of the gradient projection method to solve constrained optimization\nproblems in $\\kappa$-hyperbolic space forms are established, considering both\nconstant and backtracking step sizes in the analysis. It is shown that every\naccumulation point of the sequence generated by the method for both step sizes\nis a stationary point for the given problem. Additionally, an iteration\ncomplexity bound is provided that upper bounds the number of iterations needed\nto achieve a suitable measure of stationarity for both step sizes. Finally, the\nproperties of the constrained Fermat-Weber problem are explored, demonstrating\nthat the sequence generated by the gradient projection method converges to its\nunique solution. Numerical experiments on solving the Fermat-Weber problem are\npresented, illustrating the theoretical findings and demonstrating the\neffectiveness of the proposed methods.\n","authors":["Ronny Bergmann","Orizon P. Ferreira","Sandor Németh","Jinzhen Zhu"],"pdf_url":"https://arxiv.org/pdf/2504.11815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05451v3","updated":"2025-04-16T06:17:36Z","published":"2024-07-07T17:25:33Z","title":"Debunking the Speed-Fidelity Trade-Off: Speeding-up Large-Scale Energy\n  Models while Keeping Fidelity","summary":"  Energy system models are essential for planning and supporting the energy\ntransition. However, increasing temporal, spatial, and sectoral resolutions\nhave led to large-scale linear programming (LP) models that are often\n(over)simplified to remain computationally tractable-frequently at the expense\nof model fidelity. This paper challenges the common belief that LP formulations\ncannot be improved without sacrificing their accuracy. Inspired by graph\ntheory, we propose to model energy systems using energy assets (vertices), as a\nsingle building-block, and flows to connect between them. This reduces the need\nfor additional components such as nodes and connections. The resulting\nformulation is more compact, without sacrificing accuracy, and leverages the\ninherent graph structure of energy systems. To evaluate performance, we\nimplemented and compared four common modelling approaches varying in their use\nof building blocks and flow representations. We conducted experiments using\nTulipaEnergyModel.jl and applied them to a multi-sector case study with varying\nproblem sizes. Results show that our single-building-block (1BB-1F) approach\nreduces variables and constraints by 26 and 35 percentage, respectively, and\nachieves a 1.27 times average speedup in solving time without any loss in model\nfidelity. The speedup increases with problem size, making this approach\nparticularly advantageous for large-scale models. Our findings demonstrate that\nnot all LPs are equal in quality and that better reformulations can lead to\nsubstantial computational benefits. This paper also aims to raise awareness of\nmodel quality considerations in energy system optimisation and promote more\nefficient formulations without compromising fidelity.\n","authors":["Diego A. Tejada-Arango","German Morales-Espana","Juha Kiviluoma"],"pdf_url":"https://arxiv.org/pdf/2407.05451v3.pdf","comment":"Preprint submitted to Energy (IJEPES)"},{"id":"http://arxiv.org/abs/2504.11261v2","updated":"2025-04-16T06:16:36Z","published":"2025-04-15T15:00:34Z","title":"Robust MPC for Uncertain Linear Systems -- Combining Model Adaptation\n  and Iterative Learning","summary":"  This paper presents a robust adaptive learning Model Predictive Control (MPC)\nframework for linear systems with parametric uncertainties and additive\ndisturbances performing iterative tasks. The approach iteratively refines the\nparameter estimates using set membership estimation. Performance enhancement\nover iterations is achieved by learning the terminal cost from data. Safety is\nenforced using a terminal set, which is also learned iteratively. The proposed\nmethod guarantees recursive feasibility, constraint satisfaction, and a robust\nbound on the closed-loop cost. Numerical simulations on a mass-spring-damper\nsystem demonstrate improved computational efficiency and control performance\ncompared to an existing robust adaptive MPC approach.\n","authors":["Hannes Petrenz","Johannes Köhler","Francesco Borrelli"],"pdf_url":"https://arxiv.org/pdf/2504.11261v2.pdf","comment":"Github link to the example:\n  https://github.com/HannesPetrenz/RALMPC_Linear_Uncertain_Systems"},{"id":"http://arxiv.org/abs/2412.02382v2","updated":"2025-04-16T05:24:55Z","published":"2024-12-03T11:07:58Z","title":"Decentralized projected Riemannian stochastic recursive momentum method\n  for nonconvex optimization","summary":"  This paper studies decentralized optimization over a compact submanifold\nwithin a communication network of $n$ nodes, where each node possesses a smooth\nnon-convex local cost function, and the goal is to jointly minimize the sum of\nthese local costs. We focus particularly on the online setting, where local\ndata is processed in real-time as it streams in, without the need for full data\nstorage. We propose a decentralized projected Riemannian stochastic recursive\nmomentum (DPRSRM) method that employs local hybrid stochastic gradient\nestimators and uses the network to track the global gradient. DPRSRM achieves\nan oracle complexity of \\(\\mathcal{O}(\\epsilon^{-\\frac{3}{2}})\\), outperforming\nexisting methods that have at most \\(\\mathcal{O}(\\epsilon^{-2})\\) complexity.\nOur method requires only $\\mathcal{O}(1)$ gradient evaluations per iteration\nfor each local node and does not require restarting with a large batch\ngradient. Furthermore, we demonstrate the effectiveness of our proposed methods\ncompared to state-of-the-art ones through numerical experiments on principal\ncomponent analysis problems and low-rank matrix completion.\n","authors":["Kangkang Deng","Jiang Hu"],"pdf_url":"https://arxiv.org/pdf/2412.02382v2.pdf","comment":"25 Pages"},{"id":"http://arxiv.org/abs/2504.11771v1","updated":"2025-04-16T05:20:48Z","published":"2025-04-16T05:20:48Z","title":"Design and Continuation of Nonlinear Teardrop Hovering Formation along\n  the Near Rectilinear Halo Orbit","summary":"  This short communication is devoted to the design and continuation of a\nteardrop hovering formation along the Near Rectilinear Halo orbit and provides\nfurther insights into future on-orbit services in the cislunar space. First, we\nextend the concept of the teardrop hovering formation to scenarios along the\nNear Rectilinear Halo orbit in the Earth-Moon circular restricted three-body\nproblem. Then, we develop two methods for designing these formations based on\nthe nonlinear model for relative motion. The first method addresses the design\nof the teardrop hovering formations with relatively short revisit distances,\nwhile the second method continues hovering trajectories from short to longer\nrevisit distances. In particular, new continuation method is developed to meet\nthe design requirements of this new scenario. Simulation results verify the\neffectiveness of the proposed methods, and a near-natural teardrop hovering\nformation is achieved by considering the dynamical properties near the NRHO.\nComparisons between design results obtained using linear and nonlinear models\nfurther strengthen the necessity of using the nonlinear model.\n","authors":["Shuyue Fu","Yihan Peng","Shengping Gong","Peng Shi"],"pdf_url":"https://arxiv.org/pdf/2504.11771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14522v3","updated":"2025-04-16T03:22:12Z","published":"2023-10-23T03:09:53Z","title":"A kernel-based method for Schrödinger bridges","summary":"  We characterize the Schr\\\"odinger bridge problems by a family of\nMckean-Vlasov stochastic control problems with no terminal time distribution\nconstraint. In doing so, we use the theory of Hilbert space embeddings of\nprobability measures and then describe the constraint as penalty terms defined\nby the maximum mean discrepancy in the control problems. A sequence of the\nprobability laws of the state processes resulting from $\\epsilon$-optimal\ncontrols converges to a unique solution of the Schr\\\"odinger's problem under\nmild conditions on given initial and terminal time distributions and an\nunderlying diffusion process. We propose a neural SDE based deep learning\nalgorithm for the Mckean-Vlasov stochastic control problems. Several numerical\nexperiments validate our methods.\n","authors":["Yumiharu Nakano"],"pdf_url":"https://arxiv.org/pdf/2310.14522v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10955v2","updated":"2025-04-16T03:01:16Z","published":"2025-04-15T08:00:46Z","title":"Designing optimal subsidy schemes and recycling plans for sustainable\n  treatment of construction and demolition waste","summary":"  More than 10 billion tons of construction and demolition waste (CW) are\ngenerated globally each year, exerting a significant impact on the environment.\nIn the CW recycling process, the government and the carrier are the two primary\nstakeholders. The carrier is responsible for transporting CW from production\nsites to backfill sites or processing facilities, with a primary focus on\ntransport efficiency and revenue. Meanwhile, the government aims to minimize\npollution from the recycling system, which is influenced by transport modes,\nshipment distances, and the processing methods used for CW. This paper develops\na bi-objective, bi-level optimization model to address these challenges. The\nupper-level model is a linear programming model that optimizes the government's\nsubsidy scheme, while the lower-level model is a minimum-cost flow model that\noptimizes the carrier's recycling plan. A hybrid heuristic solution method is\nproposed to tackle the problem's complexity. A case study in Chengdu, China,\ndemonstrates the computational efficiency of the model and its small solution\ngap. With an optimized subsidy scheme and recycling plan, pollution can be\nreduced by over 29.29% through a relatively small investment in subsidies.\n","authors":["Lei Yu","Qian Ge","Ke Han","Wen Ji","Yueqi Liu"],"pdf_url":"https://arxiv.org/pdf/2504.10955v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10796v2","updated":"2025-04-16T01:02:18Z","published":"2025-04-15T01:47:11Z","title":"Wasserstein Distributionally Robust Regret Optimization","summary":"  Distributionally Robust Optimization (DRO) is a popular framework for\ndecision-making under uncertainty, but its adversarial nature can lead to\noverly conservative solutions. To address this, we study ex-ante\nDistributionally Robust Regret Optimization (DRRO), focusing on\nWasserstein-based ambiguity sets which are popular due to their links to\nregularization and machine learning. We provide a systematic analysis of\nWasserstein DRRO, paralleling known results for Wasserstein DRO. Under\nsmoothness and regularity conditions, we show that Wasserstein DRRO coincides\nwith Empirical Risk Minimization (ERM) up to first-order terms, and exactly so\nin convex quadratic settings. We revisit the Wasserstein DRRO newsvendor\nproblem, where the loss is the maximum of two linear functions of demand and\ndecision. Extending [25], we show that the regret can be computed by maximizing\ntwo one-dimensional concave functions. For more general loss functions\ninvolving the maximum of multiple linear terms in multivariate random variables\nand decision vectors, we prove that computing the regret and thus also the DRRO\npolicy is NP-hard. We then propose a convex relaxation for these more general\nWasserstein DRRO problems and demonstrate its strong empirical performance.\nFinally, we provide an upper bound on the optimality gap of our relaxation and\nshow it improves over recent alternatives.\n","authors":["Lukas-Benedikt Fiechtner","Jose Blanchet"],"pdf_url":"https://arxiv.org/pdf/2504.10796v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03331v2","updated":"2025-04-16T23:01:28Z","published":"2024-03-05T21:28:03Z","title":"Verification of First-Order Methods for Parametric Quadratic\n  Optimization","summary":"  We introduce a numerical framework to verify the finite step convergence of\nfirst-order methods for parametric convex quadratic optimization. We formulate\nthe verification problem as a mathematical optimization problem where we\nmaximize a performance metric (e.g., fixed-point residual at the last\niteration) subject to constraints representing proximal algorithm steps (e.g.,\nlinear system solutions, projections, or gradient steps). Our framework is\nhighly modular because we encode a wide range of proximal algorithms as\nvariations of two primitive steps: affine steps and element-wise maximum steps.\nCompared to standard convergence analysis and performance estimation\ntechniques, we can explicitly quantify the effects of warm-starting by directly\nrepresenting the sets where the initial iterates and parameters live. We show\nthat the verification problem is NP-hard, and we construct strong semidefinite\nprogramming relaxations using various constraint tightening techniques.\nNumerical examples in nonnegative least squares, network utility maximization,\nLasso, and optimal control show a significant reduction in pessimism of our\nframework compared to standard worst-case convergence analysis techniques.\n","authors":["Vinit Ranjan","Bartolomeo Stellato"],"pdf_url":"https://arxiv.org/pdf/2403.03331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12519v1","updated":"2025-04-16T22:39:41Z","published":"2025-04-16T22:39:41Z","title":"Corner Gradient Descent","summary":"  We consider SGD-type optimization on infinite-dimensional quadratic problems\nwith power law spectral conditions. It is well-known that on such problems\ndeterministic GD has loss convergence rates $L_t=O(t^{-\\zeta})$, which can be\nimproved to $L_t=O(t^{-2\\zeta})$ by using Heavy Ball with a non-stationary\nJacobi-based schedule (and the latter rate is optimal among fixed schedules).\nHowever, in the mini-batch Stochastic GD setting, the sampling noise causes the\nJacobi HB to diverge; accordingly no $O(t^{-2\\zeta})$ algorithm is known. In\nthis paper we show that rates up to $O(t^{-2\\zeta})$ can be achieved by a\ngeneralized stationary SGD with infinite memory. We start by identifying\ngeneralized (S)GD algorithms with contours in the complex plane. We then show\nthat contours that have a corner with external angle $\\theta\\pi$ accelerate the\nplain GD rate $O(t^{-\\zeta})$ to $O(t^{-\\theta\\zeta})$. For deterministic GD,\nincreasing $\\theta$ allows to achieve rates arbitrarily close to\n$O(t^{-2\\zeta})$. However, in Stochastic GD, increasing $\\theta$ also amplifies\nthe sampling noise, so in general $\\theta$ needs to be optimized by balancing\nthe acceleration and noise effects. We prove that the optimal rate is given by\n$\\theta_{\\max}=\\min(2,\\nu,\\tfrac{2}{\\zeta+1/\\nu})$, where $\\nu,\\zeta$ are the\nexponents appearing in the capacity and source spectral conditions.\nFurthermore, using fast rational approximations of the power functions, we show\nthat ideal corner algorithms can be efficiently approximated by finite-memory\nalgorithms, and demonstrate their practical efficiency on a synthetic problem\nand MNIST.\n","authors":["Dmitry Yarotsky"],"pdf_url":"https://arxiv.org/pdf/2504.12519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01756v2","updated":"2025-04-16T20:57:09Z","published":"2024-06-03T19:49:33Z","title":"On the completeness of several fortification-interdiction games in the\n  Polynomial Hierarchy","summary":"  Fortification-interdiction games are tri-level adversarial games where two\nopponents act in succession to protect, disrupt and simply use an\ninfrastructure for a specific purpose. Many such games have been formulated and\ntackled in the literature through specific algorithmic methods, however very\nfew investigations exist on the completeness of such fortification problems in\norder to locate them rigorously in the polynomial hierarchy. We clarify the\ncompleteness status of several well-known fortification problems, such as the\nTri-level Interdiction Knapsack Problem with unit fortification and attack\nweights, the Max-flow Interdiction Problem and Shortest Path Interdiction\nProblem with Fortification, the Multi-level Critical Node Problem with unit\nweights, as well as a well-studied electric grid defence planning problem. For\nall of these problems, we prove their completeness either for the $\\Sigma^p_2$\nor the $\\Sigma^p_3$ class of the polynomial hierarchy. We also prove that the\nMulti-level Fortification-Interdiction Knapsack Problem with an arbitrary\nnumber of protection and interdiction rounds and unit fortification and attack\nweights is complete for any level of the polynomial hierarchy, therefore\nproviding a useful basis for further attempts at proving the completeness of\nprotection-interdiction games at any level of said hierarchy.\n","authors":["Alberto Boggio Tomasaz","Margarida Carvalho","Roberto Cordone","Pierre Hosteins"],"pdf_url":"https://arxiv.org/pdf/2406.01756v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2002.08907v4","updated":"2025-04-16T19:41:25Z","published":"2020-02-20T17:52:18Z","title":"Second-order Conditional Gradient Sliding","summary":"  Constrained second-order convex optimization algorithms are the method of\nchoice when a high accuracy solution to a problem is needed, due to their local\nquadratic convergence. These algorithms require the solution of a constrained\nquadratic subproblem at every iteration. We present the \\emph{Second-Order\nConditional Gradient Sliding} (SOCGS) algorithm, which uses a projection-free\nalgorithm to solve the constrained quadratic subproblems inexactly. When the\nfeasible region is a polytope the algorithm converges quadratically in primal\ngap after a finite number of linearly convergent iterations. Once in the\nquadratic regime the SOCGS algorithm requires $\\mathcal{O}(\\log(\\log\n1/\\varepsilon))$ first-order and Hessian oracle calls and $\\mathcal{O}(\\log\n(1/\\varepsilon) \\log(\\log1/\\varepsilon))$ linear minimization oracle calls to\nachieve an $\\varepsilon$-optimal solution. This algorithm is useful when the\nfeasible region can only be accessed efficiently through a linear optimization\noracle, and computing first-order information of the function, although\npossible, is costly.\n","authors":["Alejandro Carderera","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2002.08907v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12426v1","updated":"2025-04-16T18:54:13Z","published":"2025-04-16T18:54:13Z","title":"Multi-material topology optimization of electric machines under maximum\n  temperature and stress constraints","summary":"  The use of topology optimization methods for the design of electric machines\nhas become increasingly popular over the past years. Due to a desired increase\nin power density and a recent trend to high speed machines, thermal aspects\nplay a more and more important role. In this work, we perform multi-material\ntopology optimization of an electric machine, where the cost function depends\non both electromagnetic fields and the temperature distribution generated by\nelectromagnetic losses. We provide the topological derivative for this coupled\nmulti-physics problem consisting of the magnetoquasistatic approximation to\nMaxwell's equations and the stationary heat equation. We use it within a\nmulti-material level set algorithm in order to maximize the machine's average\ntorque for a fixed volume of permanent-magnet material, while keeping the\ntemperature below a prescribed value. Finally, in order to ensure mechanical\nstability, we additionally enforce a bound on mechanical stresses. Numerical\nresults for the optimization of a permanent magnet synchronous machine are\npresented, showing a significantly improved performance compared to the\nreference design while meeting temperature and stress constraints.\n","authors":["Peter Gangl","Nepomuk Krenn","Herbert De Gersem"],"pdf_url":"https://arxiv.org/pdf/2504.12426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12419v1","updated":"2025-04-16T18:35:07Z","published":"2025-04-16T18:35:07Z","title":"Standardization of Multi-Objective QUBOs","summary":"  Multi-objective optimization involving Quadratic Unconstrained Binary\nOptimization (QUBO) problems arises in various domains. A fundamental challenge\nin this context is the effective balancing of multiple objectives, each\npotentially operating on very different scales. This imbalance introduces\ncomplications such as the selection of appropriate weights when scalarizing\nmultiple objectives into a single objective function. In this paper, we propose\na novel technique for scaling QUBO objectives that uses an exact computation of\nthe variance of each individual QUBO objective. By scaling each objective to\nhave unit variance, we align all objectives onto a common scale, thereby\nallowing for more balanced solutions to be found when scalarizing the\nobjectives with equal weights, as well as potentially assisting in the search\nor choice of weights during scalarization. Finally, we demonstrate its\nadvantages through empirical evaluations on various multi-objective\noptimization problems. Our results are noteworthy since manually selecting\nscalarization weights is cumbersome, and reliable, efficient solutions are\nscarce.\n","authors":["Loong Kuan Lee","Thore Thassilo Gerlach","Nico Piatkowski"],"pdf_url":"https://arxiv.org/pdf/2504.12419v1.pdf","comment":"7 pages, 3 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2504.12299v1","updated":"2025-04-16T17:59:54Z","published":"2025-04-16T17:59:54Z","title":"Adapting a World Model for Trajectory Following in a 3D Game","summary":"  Imitation learning is a powerful tool for training agents by leveraging\nexpert knowledge, and being able to replicate a given trajectory is an integral\npart of it. In complex environments, like modern 3D video games, distribution\nshift and stochasticity necessitate robust approaches beyond simple action\nreplay. In this study, we apply Inverse Dynamics Models (IDM) with different\nencoders and policy heads to trajectory following in a modern 3D video game --\nBleeding Edge. Additionally, we investigate several future alignment strategies\nthat address the distribution shift caused by the aleatoric uncertainty and\nimperfections of the agent. We measure both the trajectory deviation distance\nand the first significant deviation point between the reference and the agent's\ntrajectory and show that the optimal configuration depends on the chosen\nsetting. Our results show that in a diverse data setting, a GPT-style policy\nhead with an encoder trained from scratch performs the best, DINOv2 encoder\nwith the GPT-style policy head gives the best results in the low data regime,\nand both GPT-style and MLP-style policy heads had comparable results when\npre-trained on a diverse setting and fine-tuned for a specific behaviour\nsetting.\n","authors":["Marko Tot","Shu Ishida","Abdelhak Lemkhenter","David Bignell","Pallavi Choudhury","Chris Lovett","Luis França","Matheus Ribeiro Furtado de Mendonça","Tarun Gupta","Darren Gehring","Sam Devlin","Sergio Valcarcel Macua","Raluca Georgescu"],"pdf_url":"https://arxiv.org/pdf/2504.12299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12292v1","updated":"2025-04-16T17:55:02Z","published":"2025-04-16T17:55:02Z","title":"SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians","summary":"  Accurate, real-time 3D reconstruction of human heads from monocular images\nand videos underlies numerous visual applications. As 3D ground truth data is\nhard to come by at scale, previous methods have sought to learn from abundant\n2D videos in a self-supervised manner. Typically, this involves the use of\ndifferentiable mesh rendering, which is effective but faces limitations. To\nimprove on this, we propose SHeaP (Self-supervised Head Geometry Predictor\nLearned via 2D Gaussians). Given a source image, we predict a 3DMM mesh and a\nset of Gaussians that are rigged to this mesh. We then reanimate this rigged\nhead avatar to match a target frame, and backpropagate photometric losses to\nboth the 3DMM and Gaussian prediction networks. We find that using Gaussians\nfor rendering substantially improves the effectiveness of this self-supervised\napproach. Training solely on 2D data, our method surpasses existing\nself-supervised approaches in geometric evaluations on the NoW benchmark for\nneutral faces and a new benchmark for non-neutral expressions. Our method also\nproduces highly expressive meshes, outperforming state-of-the-art in emotion\nclassification.\n","authors":["Liam Schoneveld","Zhe Chen","Davide Davoli","Jiapeng Tang","Saimon Terazawa","Ko Nishino","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2504.12292v1.pdf","comment":"For video demonstrations and additional materials please see\n  https://nlml.github.io/sheap/"},{"id":"http://arxiv.org/abs/2504.12285v1","updated":"2025-04-16T17:51:43Z","published":"2025-04-16T17:51:43Z","title":"BitNet b1.58 2B4T Technical Report","summary":"  We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.\n","authors":["Shuming Ma","Hongyu Wang","Shaohan Huang","Xingxing Zhang","Ying Hu","Ting Song","Yan Xia","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2504.12285v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2504.12284v1","updated":"2025-04-16T17:48:12Z","published":"2025-04-16T17:48:12Z","title":"How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday\n  Interactions","summary":"  We tackle the novel problem of predicting 3D hand motion and contact maps (or\nInteraction Trajectories) given a single RGB view, action text, and a 3D\ncontact point on the object as input. Our approach consists of (1) Interaction\nCodebook: a VQVAE model to learn a latent codebook of hand poses and contact\npoints, effectively tokenizing interaction trajectories, (2) Interaction\nPredictor: a transformer-decoder module to predict the interaction trajectory\nfrom test time inputs by using an indexer module to retrieve a latent\naffordance from the learned codebook. To train our model, we develop a data\nengine that extracts 3D hand poses and contact trajectories from the diverse\nHoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger\nthan existing works, in terms of diversity of objects and interactions\nobserved, and test for generalization of the model across object categories,\naction categories, tasks, and scenes. Experimental results show the\neffectiveness of our approach over transformer & diffusion baselines across all\nsettings.\n","authors":["Aditya Prakash","Benjamin Lundell","Dmitry Andreychuk","David Forsyth","Saurabh Gupta","Harpreet Sawhney"],"pdf_url":"https://arxiv.org/pdf/2504.12284v1.pdf","comment":"CVPR 2025, Project page:\n  https://ap229997.github.io/projects/latentact"},{"id":"http://arxiv.org/abs/2504.12279v1","updated":"2025-04-16T17:41:19Z","published":"2025-04-16T17:41:19Z","title":"Dysarthria Normalization via Local Lie Group Transformations for Robust\n  ASR","summary":"  We present a geometry-driven method for normalizing dysarthric speech using\nlocal Lie group transformations of spectrograms. Time, frequency, and amplitude\ndistortions are modeled as smooth, invertible deformations, parameterized by\nscalar fields and applied via exponential maps. A neural network is trained to\ninfer these fields from synthetic distortions of typical speech-without using\nany pathological data. At test time, the model applies an approximate inverse\nto real dysarthric inputs. Despite zero-shot generalization, we observe\nsubstantial ASR gains, including up to 16 percentage points WER reduction on\nchallenging TORGO samples, with no degradation on clean speech. This work\nintroduces a principled, interpretable approach for robust speech recognition\nunder motor speech disorders\n","authors":["Mikhail Osipov"],"pdf_url":"https://arxiv.org/pdf/2504.12279v1.pdf","comment":"Preprint. 11 pages, 3 figures, 2 tables, 8 appendices. Code and data\n  available upon request"},{"id":"http://arxiv.org/abs/2504.09775v2","updated":"2025-04-16T17:34:04Z","published":"2025-04-14T00:29:49Z","title":"Understanding and Optimizing Multi-Stage AI Inference Pipelines","summary":"  The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.\n","authors":["Abhimanyu Rajeshkumar Bambhaniya","Hanjiang Wu","Suvinay Subramanian","Sudarshan Srinivasan","Souvik Kundu","Amir Yazdanbakhsh","Midhilesh Elavazhagan","Madhu Kumar","Tushar Krishna"],"pdf_url":"https://arxiv.org/pdf/2504.09775v2.pdf","comment":"Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables"},{"id":"http://arxiv.org/abs/2504.12262v1","updated":"2025-04-16T17:17:31Z","published":"2025-04-16T17:17:31Z","title":"SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via\n  Scalable Conditioned Neural Fields","summary":"  Spatiotemporal learning is challenging due to the intricate interplay between\nspatial and temporal dependencies, the high dimensionality of the data, and\nscalability constraints. These challenges are further amplified in scientific\ndomains, where data is often irregularly distributed (e.g., missing values from\nsensor failures) and high-volume (e.g., high-fidelity simulations), posing\nadditional computational and modeling difficulties. In this paper, we present\nSCENT, a novel framework for scalable and continuity-informed spatiotemporal\nrepresentation learning. SCENT unifies interpolation, reconstruction, and\nforecasting within a single architecture. Built on a transformer-based\nencoder-processor-decoder backbone, SCENT introduces learnable queries to\nenhance generalization and a query-wise cross-attention mechanism to\neffectively capture multi-scale dependencies. To ensure scalability in both\ndata size and model complexity, we incorporate a sparse attention mechanism,\nenabling flexible output representations and efficient evaluation at arbitrary\nresolutions. We validate SCENT through extensive simulations and real-world\nexperiments, demonstrating state-of-the-art performance across multiple\nchallenging tasks while achieving superior scalability.\n","authors":["David Keetae Park","Xihaier Luo","Guang Zhao","Seungjun Lee","Miruna Oprescu","Shinjae Yoo"],"pdf_url":"https://arxiv.org/pdf/2504.12262v1.pdf","comment":"25 pages, 5 main figures, 3 tables, under review"},{"id":"http://arxiv.org/abs/2502.12999v2","updated":"2025-04-16T17:14:07Z","published":"2025-02-18T16:19:28Z","title":"Asymptotic Optimism of Random-Design Linear and Kernel Regression Models","summary":"  We derived the closed-form asymptotic optimism of linear regression models\nunder random designs, and generalizes it to kernel ridge regression. Using\nscaled asymptotic optimism as a generic predictive model complexity measure, we\nstudied the fundamental different behaviors of linear regression model, tangent\nkernel (NTK) regression model and three-layer fully connected neural networks\n(NN). Our contribution is two-fold: we provided theoretical ground for using\nscaled optimism as a model predictive complexity measure; and we show\nempirically that NN with ReLUs behaves differently from kernel models under\nthis measure. With resampling techniques, we can also compute the optimism for\nregression models with real data.\n","authors":["Hengrui Luo","Yunzhang Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.12999v2.pdf","comment":"56 pages;"},{"id":"http://arxiv.org/abs/2504.12256v1","updated":"2025-04-16T17:07:16Z","published":"2025-04-16T17:07:16Z","title":"FLIP Reasoning Challenge","summary":"  Over the past years, advances in artificial intelligence (AI) have\ndemonstrated how AI can solve many perception and generation tasks, such as\nimage classification and text writing, yet reasoning remains a challenge. This\npaper introduces the FLIP dataset, a benchmark for evaluating AI reasoning\ncapabilities based on human verification tasks on the Idena blockchain. FLIP\nchallenges present users with two orderings of 4 images, requiring them to\nidentify the logically coherent one. By emphasizing sequential reasoning,\nvisual storytelling, and common sense, FLIP provides a unique testbed for\nmultimodal AI systems. Our experiments evaluate state-of-the-art models,\nleveraging both vision-language models (VLMs) and large language models (LLMs).\nResults reveal that even the best open-sourced and closed-sourced models\nachieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot\nsettings, compared to human performance of 95.3%. Captioning models aid\nreasoning models by providing text descriptions of images, yielding better\nresults than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5\nPro. Combining the predictions from 15 models in an ensemble increases the\naccuracy to 85.2%. These findings highlight the limitations of existing\nreasoning models and the need for robust multimodal benchmarks like FLIP. The\nfull codebase and dataset will be available at\nhttps://github.com/aplesner/FLIP-Reasoning-Challenge.\n","authors":["Andreas Plesner","Turlan Kuzhagaliyev","Roger Wattenhofer"],"pdf_url":"https://arxiv.org/pdf/2504.12256v1.pdf","comment":"Published at First Workshop on Open Science for Foundation Models at\n  ICLR 2025"},{"id":"http://arxiv.org/abs/2408.16118v3","updated":"2025-04-16T16:56:58Z","published":"2024-08-28T20:10:46Z","title":"RAIN: Reinforcement Algorithms for Improving Numerical Weather and\n  Climate Models","summary":"  This study explores integrating reinforcement learning (RL) with idealised\nclimate models to address key parameterisation challenges in climate science.\nCurrent climate models rely on complex mathematical parameterisations to\nrepresent sub-grid scale processes, which can introduce substantial\nuncertainties. RL offers capabilities to enhance these parameterisation\nschemes, including direct interaction, handling sparse or delayed feedback,\ncontinuous online learning, and long-term optimisation. We evaluate the\nperformance of eight RL algorithms on two idealised environments: one for\ntemperature bias correction, another for radiative-convective equilibrium (RCE)\nimitating real-world computational constraints. Results show different RL\napproaches excel in different climate scenarios with exploration algorithms\nperforming better in bias correction, while exploitation algorithms proving\nmore effective for RCE. These findings support the potential of RL-based\nparameterisation schemes to be integrated into global climate models, improving\naccuracy and efficiency in capturing complex climate dynamics. Overall, this\nwork represents an important first step towards leveraging RL to enhance\nclimate model accuracy, critical for improving climate understanding and\npredictions. Code accessible at https://github.com/p3jitnath/climate-rl.\n","authors":["Pritthijit Nath","Henry Moss","Emily Shuckburgh","Mark Webb"],"pdf_url":"https://arxiv.org/pdf/2408.16118v3.pdf","comment":"Accepted for poster presentation at the NeurIPS 2024 workshop on\n  Tackling Climate Change with Machine Learning. 24 pages, 9 figures"},{"id":"http://arxiv.org/abs/2504.12249v1","updated":"2025-04-16T16:54:37Z","published":"2025-04-16T16:54:37Z","title":"Comparative Evaluation of Radiomics and Deep Learning Models for Disease\n  Detection in Chest Radiography","summary":"  The application of artificial intelligence (AI) in medical imaging has\nrevolutionized diagnostic practices, enabling advanced analysis and\ninterpretation of radiological data. This study presents a comprehensive\nevaluation of radiomics-based and deep learning-based approaches for disease\ndetection in chest radiography, focusing on COVID-19, lung opacity, and viral\npneumonia. While deep learning models, particularly convolutional neural\nnetworks (CNNs) and vision transformers (ViTs), learn directly from image data,\nradiomics-based models extract and analyze quantitative features, potentially\nproviding advantages in data-limited scenarios. This study systematically\ncompares the diagnostic accuracy and robustness of various AI models, including\nDecision Trees, Gradient Boosting, Random Forests, Support Vector Machines\n(SVM), and Multi-Layer Perceptrons (MLP) for radiomics, against\nstate-of-the-art computer vision deep learning architectures. Performance\nmetrics across varying sample sizes reveal insights into each model's efficacy,\nhighlighting the contexts in which specific AI approaches may offer enhanced\ndiagnostic capabilities. The results aim to inform the integration of AI-driven\ndiagnostic tools in clinical practice, particularly in automated and\nhigh-throughput environments where timely, reliable diagnosis is critical. This\ncomparative study addresses an essential gap, establishing guidance for the\nselection of AI models based on clinical and operational needs.\n","authors":["Zhijin He","Alan B. McMillan"],"pdf_url":"https://arxiv.org/pdf/2504.12249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07085v2","updated":"2025-04-16T16:51:21Z","published":"2025-04-09T17:57:54Z","title":"Identifying Unknown Stochastic Dynamics via Finite expression methods","summary":"  Modeling stochastic differential equations (SDEs) is crucial for\nunderstanding complex dynamical systems in various scientific fields. Recent\nmethods often employ neural network-based models, which typically represent\nSDEs through a combination of deterministic and stochastic terms. However,\nthese models usually lack interpretability and have difficulty generalizing\nbeyond their training domain. This paper introduces the Finite Expression\nMethod (FEX), a symbolic learning approach designed to derive interpretable\nmathematical representations of the deterministic component of SDEs. For the\nstochastic component, we integrate FEX with advanced generative modeling\ntechniques to provide a comprehensive representation of SDEs. The numerical\nexperiments on linear, nonlinear, and multidimensional SDEs demonstrate that\nFEX generalizes well beyond the training domain and delivers more accurate\nlong-term predictions compared to neural network-based methods. The symbolic\nexpressions identified by FEX not only improve prediction accuracy but also\noffer valuable scientific insights into the underlying dynamics of the systems,\npaving the way for new scientific discoveries.\n","authors":["Senwei Liang","Chunmei Wang","Xingjian Xu"],"pdf_url":"https://arxiv.org/pdf/2504.07085v2.pdf","comment":"27 pages, 20 figures"},{"id":"http://arxiv.org/abs/2502.16660v4","updated":"2025-04-16T16:49:34Z","published":"2025-02-23T17:38:10Z","title":"BioMaze: Benchmarking and Enhancing Large Language Models for Biological\n  Pathway Reasoning","summary":"  The applications of large language models (LLMs) in various biological\ndomains have been explored recently, but their reasoning ability in complex\nbiological systems, such as pathways, remains underexplored, which is crucial\nfor predicting biological phenomena, formulating hypotheses, and designing\nexperiments. This work explores the potential of LLMs in pathway reasoning. We\nintroduce BioMaze, a dataset with 5.1K complex pathway problems derived from\nreal research, covering various biological contexts including natural dynamic\nchanges, disturbances, additional intervention conditions, and multi-scale\nresearch targets. Our evaluation of methods such as CoT and graph-augmented\nreasoning, shows that LLMs struggle with pathway reasoning, especially in\nperturbed systems. To address this, we propose PathSeeker, an LLM agent that\nenhances reasoning through interactive subgraph-based navigation, enabling a\nmore effective approach to handling the complexities of biological systems in a\nscientifically aligned manner. The dataset and code are available at\nhttps://github.com/zhao-ht/BioMaze.\n","authors":["Haiteng Zhao","Chang Ma","Fangzhi Xu","Lingpeng Kong","Zhi-Hong Deng"],"pdf_url":"https://arxiv.org/pdf/2502.16660v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19950v2","updated":"2025-04-16T16:43:35Z","published":"2024-05-30T11:14:01Z","title":"Multimodal Lego: Model Merging and Fine-Tuning Across Topologies and\n  Modalities in Biomedicine","summary":"  Learning holistic computational representations in physical, chemical or\nbiological systems requires the ability to process information from different\ndistributions and modalities within the same model. Thus, the demand for\nmultimodal machine learning models has sharply risen for modalities that go\nbeyond vision and language, such as sequences, graphs, time series, or tabular\ndata. While there are many available multimodal fusion and alignment\napproaches, most of them require end-to-end training, scale quadratically with\nthe number of modalities, cannot handle cases of high modality imbalance in the\ntraining set, or are highly topology-specific, making them too restrictive for\nmany biomedical learning tasks. This paper presents Multimodal Lego (MM-Lego),\na general-purpose fusion framework to turn any set of encoders into a\ncompetitive multimodal model with no or minimal fine-tuning. We achieve this by\nintroducing a wrapper for any unimodal encoder that enforces shape consistency\nbetween modality representations. It harmonises these representations by\nlearning features in the frequency domain to enable model merging with little\nsignal interference. We show that MM-Lego 1) can be used as a model merging\nmethod which achieves competitive performance with end-to-end fusion models\nwithout any fine-tuning, 2) can operate on any unimodal encoder, and 3) is a\nmodel fusion method that, with minimal fine-tuning, surpasses all benchmarks in\nfive out of seven datasets.\n","authors":["Konstantin Hemker","Nikola Simidjievski","Mateja Jamnik"],"pdf_url":"https://arxiv.org/pdf/2405.19950v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09475v3","updated":"2025-04-16T16:28:06Z","published":"2024-09-14T16:20:26Z","title":"MALADY: Multiclass Active Learning with Auction Dynamics on Graphs","summary":"  Active learning enhances the performance of machine learning methods,\nparticularly in semi-supervised cases, by judiciously selecting a limited\nnumber of unlabeled data points for labeling, with the goal of improving the\nperformance of an underlying classifier. In this work, we introduce the\nMulticlass Active Learning with Auction Dynamics on Graphs (MALADY) framework\nwhich leverages the auction dynamics algorithm on similarity graphs for\nefficient active learning. In particular, we generalize the auction dynamics\nalgorithm on similarity graphs for semi-supervised learning in [24] to\nincorporate a more general optimization functional. Moreover, we introduce a\nnovel active learning acquisition function that uses the dual variable of the\nauction algorithm to measure the uncertainty in the classifier to prioritize\nqueries near the decision boundaries between different classes. Lastly, using\nexperiments on classification tasks, we evaluate the performance of our\nproposed method and show that it exceeds that of comparison algorithms.\n","authors":["Gokul Bhusal","Kevin Miller","Ekaterina Merkurjev"],"pdf_url":"https://arxiv.org/pdf/2409.09475v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12229v1","updated":"2025-04-16T16:25:26Z","published":"2025-04-16T16:25:26Z","title":"Watermarking Needs Input Repetition Masking","summary":"  Recent advancements in Large Language Models (LLMs) raised concerns over\npotential misuse, such as for spreading misinformation. In response two counter\nmeasures emerged: machine learning-based detectors that predict if text is\nsynthetic, and LLM watermarking, which subtly marks generated text for\nidentification and attribution. Meanwhile, humans are known to adjust language\nto their conversational partners both syntactically and lexically. By\nimplication, it is possible that humans or unwatermarked LLMs could\nunintentionally mimic properties of LLM generated text, making counter measures\nunreliable. In this work we investigate the extent to which such conversational\nadaptation happens. We call the concept $\\textit{mimicry}$ and demonstrate that\nboth humans and LLMs end up mimicking, including the watermarking signal even\nin seemingly improbable settings. This challenges current academic assumptions\nand suggests that for long-term watermarking to be reliable, the likelihood of\nfalse positives needs to be significantly lower, while longer word sequences\nshould be used for seeding watermarking mechanisms.\n","authors":["David Khachaturov","Robert Mullins","Ilia Shumailov","Sumanth Dathathri"],"pdf_url":"https://arxiv.org/pdf/2504.12229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05309v2","updated":"2025-04-16T16:14:58Z","published":"2024-02-07T23:02:53Z","title":"Investigating Generalization Behaviours of Generative Flow Networks","summary":"  Generative Flow Networks (GFlowNets, GFNs) are a generative framework for\nlearning unnormalized probability mass functions over discrete spaces. Since\ntheir inception, GFlowNets have proven to be useful for learning generative\nmodels in applications where the majority of the discrete space is unvisited\nduring training. This has inspired some to hypothesize that GFlowNets, when\npaired with deep neural networks (DNNs), have favorable generalization\nproperties. In this work, we empirically verify some of the hypothesized\nmechanisms of generalization of GFlowNets. We accomplish this by introducing a\nnovel graph-based benchmark environment where reward difficulty can be easily\nvaried, $p(x)$ can be computed exactly, and an unseen test set can be\nconstructed to quantify generalization performance. Using this graph-based\nenvironment, we are able to systematically test the hypothesized mechanisms of\ngeneralization of GFlowNets and put forth a set of empirical observations that\nsummarize our findings. In particular, we find (and confirm) that the functions\nthat GFlowNets learn to approximate have an implicit underlying structure which\nfacilitate generalization. Surprisingly -- and somewhat contradictory to\nexisting knowledge -- we also find that GFlowNets are sensitive to being\ntrained offline and off-policy. However, the reward implicitly learned by\nGFlowNets is robust to changes in the training distribution.\n","authors":["Lazar Atanackovic","Emmanuel Bengio"],"pdf_url":"https://arxiv.org/pdf/2402.05309v2.pdf","comment":"Accepted to TMLR"},{"id":"http://arxiv.org/abs/2504.12216v1","updated":"2025-04-16T16:08:45Z","published":"2025-04-16T16:08:45Z","title":"d1: Scaling Reasoning in Diffusion Large Language Models via\n  Reinforcement Learning","summary":"  Recent large language models (LLMs) have demonstrated strong reasoning\ncapabilities that benefits from online reinforcement learning (RL). These\ncapabilities have primarily been demonstrated within the left-to-right\nautoregressive (AR) generation paradigm. In contrast, non-autoregressive\nparadigms based on diffusion generate text in a coarse-to-fine manner. Although\nrecent diffusion-based large language models (dLLMs) have achieved competitive\nlanguage modeling performance compared to their AR counterparts, it remains\nunclear if dLLMs can also leverage recent advances in LLM reasoning. To this\nend, we propose d1, a framework to adapt pre-trained masked dLLMs into\nreasoning models via a combination of supervised finetuning (SFT) and RL.\nSpecifically, we develop and extend techniques to improve reasoning in\npretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge\nand instill self-improvement behavior directly from existing datasets, and (b)\nwe introduce a novel critic-free, policy-gradient based RL algorithm called\ndiffu-GRPO. Through empirical studies, we investigate the performance of\ndifferent post-training recipes on multiple mathematical and logical reasoning\nbenchmarks. We find that d1 yields the best performance and significantly\nimproves performance of a state-of-the-art dLLM.\n","authors":["Siyan Zhao","Devaansh Gupta","Qinqing Zheng","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2504.12216v1.pdf","comment":"25 pages, project page at https://dllm-reasoning.github.io/"},{"id":"http://arxiv.org/abs/2410.23228v2","updated":"2025-04-16T16:07:50Z","published":"2024-10-30T17:16:38Z","title":"Emergence of meta-stable clustering in mean-field transformer models","summary":"  We model the evolution of tokens within a deep stack of Transformer layers as\na continuous-time flow on the unit sphere, governed by a mean-field interacting\nparticle system, building on the framework introduced in (Geshkovski et al.,\n2023). Studying the corresponding mean-field Partial Differential Equation\n(PDE), which can be interpreted as a Wasserstein gradient flow, in this paper\nwe provide a mathematical investigation of the long-term behavior of this\nsystem, with a particular focus on the emergence and persistence of meta-stable\nphases and clustering phenomena, key elements in applications like next-token\nprediction. More specifically, we perform a perturbative analysis of the\nmean-field PDE around the iid uniform initialization and prove that, in the\nlimit of large number of tokens, the model remains close to a meta-stable\nmanifold of solutions with a given structure (e.g., periodicity). Further, the\nstructure characterizing the meta-stable manifold is explicitly identified, as\na function of the inverse temperature parameter of the model, by the index\nmaximizing a certain rescaling of Gegenbauer polynomials.\n","authors":["Giuseppe Bruno","Federico Pasqualotto","Andrea Agazzi"],"pdf_url":"https://arxiv.org/pdf/2410.23228v2.pdf","comment":"37 Pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.03311v3","updated":"2025-04-16T15:58:58Z","published":"2023-10-05T04:59:58Z","title":"Deep Variational Multivariate Information Bottleneck -- A Framework for\n  Variational Losses","summary":"  Variational dimensionality reduction methods are widely used for their\naccuracy, generative capabilities, and robustness. We introduce a unifying\nframework that generalizes both such as traditional and state-of-the-art\nmethods. The framework is based on an interpretation of the multivariate\ninformation bottleneck, trading off the information preserved in an encoder\ngraph (defining what to compress) against that in a decoder graph (defining a\ngenerative model for data). Using this approach, we rederive existing methods,\nincluding the deep variational information bottleneck, variational\nautoencoders, and deep multiview information bottleneck. We naturally extend\nthe deep variational CCA (DVCCA) family to beta-DVCCA and introduce a new\nmethod, the deep variational symmetric information bottleneck (DVSIB). DSIB,\nthe deterministic limit of DVSIB, connects to modern contrastive learning\napproaches such as Barlow Twins, among others. We evaluate these methods on\nNoisy MNIST and Noisy CIFAR-100, showing that algorithms better matched to the\nstructure of the problem like DVSIB and beta-DVCCA produce better latent spaces\nas measured by classification accuracy, dimensionality of the latent variables,\nsample efficiency, and consistently outperform other approaches under\ncomparable conditions. Additionally, we benchmark against state-of-the-art\nmodels, achieving superior or competitive accuracy. Our results demonstrate\nthat this framework can seamlessly incorporate diverse multi-view\nrepresentation learning algorithms, providing a foundation for designing novel,\nproblem-specific loss functions.\n","authors":["Eslam Abdelaleem","Ilya Nemenman","K. Michael Martini"],"pdf_url":"https://arxiv.org/pdf/2310.03311v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12210v1","updated":"2025-04-16T15:56:57Z","published":"2025-04-16T15:56:57Z","title":"Communication Optimization for Decentralized Learning atop\n  Bandwidth-limited Edge Networks","summary":"  Decentralized federated learning (DFL) is a promising machine learning\nparadigm for bringing artificial intelligence (AI) capabilities to the network\nedge. Running DFL on top of edge networks, however, faces severe performance\nchallenges due to the extensive parameter exchanges between agents. Most\nexisting solutions for these challenges were based on simplistic communication\nmodels, which cannot capture the case of learning over a multi-hop\nbandwidth-limited network. In this work, we address this problem by jointly\ndesigning the communication scheme for the overlay network formed by the agents\nand the mixing matrix that controls the communication demands between the\nagents. By carefully analyzing the properties of our problem, we cast each\ndesign problem into a tractable optimization and develop an efficient algorithm\nwith guaranteed performance. Our evaluations based on real topology and data\nshow that the proposed algorithm can reduce the total training time by over\n$80\\%$ compared to the baseline without sacrificing accuracy, while\nsignificantly improving the computational efficiency over the state of the art.\n","authors":["Tingyang Sun","Tuan Nguyen","Ting He"],"pdf_url":"https://arxiv.org/pdf/2504.12210v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2408.04705"},{"id":"http://arxiv.org/abs/2211.04509v2","updated":"2025-04-16T15:54:53Z","published":"2022-11-08T19:09:39Z","title":"Care for the Mind Amid Chronic Diseases: An Interpretable AI Approach\n  Using IoT","summary":"  Health sensing for chronic disease management creates immense benefits for\nsocial welfare. Existing health sensing studies primarily focus on the\nprediction of physical chronic diseases. Depression, a widespread complication\nof chronic diseases, is however understudied. We draw on the medical literature\nto support depression detection using motion sensor data. To connect humans in\nthis decision-making, safeguard trust, and ensure algorithm transparency, we\ndevelop an interpretable deep learning model: Temporal Prototype Network\n(TempPNet). TempPNet is built upon the emergent prototype learning models. To\naccommodate the temporal characteristic of sensor data and the progressive\nproperty of depression, TempPNet differs from existing prototype learning\nmodels in its capability of capturing temporal progressions of prototypes.\nExtensive empirical analyses using real-world motion sensor data show that\nTempPNet outperforms state-of-the-art benchmarks in depression detection.\nMoreover, TempPNet interprets its decision by visualizing the temporal\nprogression of depression and its corresponding symptoms detected from sensor\ndata. We further employ a user study and a medical expert panel to demonstrate\nits superiority over the benchmarks in interpretability. This study offers an\nalgorithmic solution for impactful social good -- collaborative care of chronic\ndiseases and depression in health sensing. Methodologically, it contributes to\nextant literature with a novel interpretable deep learning model for depression\ndetection from sensor data. Patients, doctors, and caregivers can deploy our\nmodel on mobile devices to monitor patients' depression risks in real-time. Our\nmodel's interpretability also allows human experts to participate in the\ndecision-making by reviewing the interpretation and making informed\ninterventions.\n","authors":["Jiaheng Xie","Xiaohang Zhao","Xiang Liu","Xiao Fang"],"pdf_url":"https://arxiv.org/pdf/2211.04509v2.pdf","comment":"Accepted by Management Science"},{"id":"http://arxiv.org/abs/2504.02735v2","updated":"2025-04-16T15:51:41Z","published":"2025-04-03T16:22:15Z","title":"Reliable Physiological Monitoring on the Wrist Using Generative Deep\n  Learning to Address Poor Skin-Sensor Contact","summary":"  Photoplethysmography (PPG) is a widely adopted, non-invasive technique for\nmonitoring cardiovascular health and physiological parameters in both consumer\nand clinical settings. While motion artifacts in dynamic environments have been\nextensively studied, suboptimal skin-sensor contact in sedentary conditions - a\ncritical yet underexplored issue - can distort PPG waveform morphology, leading\nto the loss or misalignment of key features and compromising sensing accuracy.\nIn this work, we propose CP-PPG, a novel framework that transforms Contact\nPressure-distorted PPG signals into high-fidelity waveforms with ideal\nmorphology. CP-PPG integrates a custom data collection protocol, a carefully\ndesigned signal processing pipeline, and a novel deep adversarial model trained\nwith a custom PPG-aware loss function. We validated CP-PPG through\ncomprehensive evaluations, including 1) morphology transformation performance\non our self-collected dataset, 2) downstream physiological monitoring\nperformance on public datasets, and 3) in-the-wild study. Extensive experiments\ndemonstrate substantial and consistent improvements in signal fidelity (Mean\nAbsolute Error: 0.09, 40% improvement over the original signal) as well as\ndownstream performance across all evaluations in Heart Rate (HR), Heart Rate\nVariability (HRV), Respiration Rate (RR), and Blood Pressure (BP) estimation\n(on average, 21% improvement in HR; 41-46% in HRV; 6% in RR; and 4-5% in BP).\nThese findings highlight the critical importance of addressing skin-sensor\ncontact issues to enhance the reliability and effectiveness of PPG-based\nphysiological monitoring. CP-PPG thus holds significant potential to improve\nthe accuracy of wearable health technologies in clinical and consumer\napplications.\n","authors":["Manh Pham Hung","Matthew Yiwen Ho","Yiming Zhang","Dimitris Spathis","Aaqib Saeed","Dong Ma"],"pdf_url":"https://arxiv.org/pdf/2504.02735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12189v1","updated":"2025-04-16T15:44:24Z","published":"2025-04-16T15:44:24Z","title":"Leave-One-Out Stable Conformal Prediction","summary":"  Conformal prediction (CP) is an important tool for distribution-free\npredictive uncertainty quantification. Yet, a major challenge is to balance\ncomputational efficiency and prediction accuracy, particularly for multiple\npredictions. We propose Leave-One-Out Stable Conformal Prediction (LOO-StabCP),\na novel method to speed up full conformal using algorithmic stability without\nsample splitting. By leveraging leave-one-out stability, our method is much\nfaster in handling a large number of prediction requests compared to existing\nmethod RO-StabCP based on replace-one stability. We derived stability bounds\nfor several popular machine learning tools: regularized loss minimization (RLM)\nand stochastic gradient descent (SGD), as well as kernel method, neural\nnetworks and bagging. Our method is theoretically justified and demonstrates\nsuperior numerical performance on synthetic and real-world data. We applied our\nmethod to a screening problem, where its effective exploitation of training\ndata led to improved test power compared to state-of-the-art method based on\nsplit conformal.\n","authors":["Kiljae Lee","Yuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.12189v1.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2504.12187v1","updated":"2025-04-16T15:42:33Z","published":"2025-04-16T15:42:33Z","title":"What Do Large Language Models Know? Tacit Knowledge as a Potential\n  Causal-Explanatory Structure","summary":"  It is sometimes assumed that Large Language Models (LLMs) know language, or\nfor example that they know that Paris is the capital of France. But what -- if\nanything -- do LLMs actually know? In this paper, I argue that LLMs can acquire\ntacit knowledge as defined by Martin Davies (1990). Whereas Davies himself\ndenies that neural networks can acquire tacit knowledge, I demonstrate that\ncertain architectural features of LLMs satisfy the constraints of semantic\ndescription, syntactic structure, and causal systematicity. Thus, tacit\nknowledge may serve as a conceptual framework for describing, explaining, and\nintervening on LLMs and their behavior.\n","authors":["Céline Budding"],"pdf_url":"https://arxiv.org/pdf/2504.12187v1.pdf","comment":"Accepted for publication in Philosophy of Science"},{"id":"http://arxiv.org/abs/2404.19165v3","updated":"2025-04-16T15:42:10Z","published":"2024-04-30T00:02:34Z","title":"DelGrad: Exact event-based gradients for training delays and weights on\n  spiking neuromorphic hardware","summary":"  Spiking neural networks (SNNs) inherently rely on the timing of signals for\nrepresenting and processing information. Incorporating trainable transmission\ndelays, alongside synaptic weights, is crucial for shaping these temporal\ndynamics. While recent methods have shown the benefits of training delays and\nweights in terms of accuracy and memory efficiency, they rely on discrete time,\napproximate gradients, and full access to internal variables like membrane\npotentials. This limits their precision, efficiency, and suitability for\nneuromorphic hardware due to increased memory requirements and I/O bandwidth\ndemands. To address these challenges, we propose DelGrad, an analytical,\nevent-based method to compute exact loss gradients for both synaptic weights\nand delays. The inclusion of delays in the training process emerges naturally\nwithin our proposed formalism, enriching the model's search space with a\ntemporal dimension. Moreover, DelGrad, grounded purely in spike timing,\neliminates the need to track additional variables such as membrane potentials.\nTo showcase this key advantage, we demonstrate the functionality and benefits\nof DelGrad on the BrainScaleS-2 neuromorphic platform, by training SNNs in a\nchip-in-the-loop fashion. For the first time, we experimentally demonstrate the\nmemory efficiency and accuracy benefits of adding delays to SNNs on noisy\nmixed-signal hardware. Additionally, these experiments also reveal the\npotential of delays for stabilizing networks against noise. DelGrad opens a new\nway for training SNNs with delays on neuromorphic hardware, which results in\nfewer required parameters, higher accuracy and ease of hardware training.\n","authors":["Julian Göltz","Jimmy Weber","Laura Kriener","Sebastian Billaudelle","Peter Lake","Johannes Schemmel","Melika Payvand","Mihai A. Petrovici"],"pdf_url":"https://arxiv.org/pdf/2404.19165v3.pdf","comment":"30 pages, 14 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.07425v2","updated":"2025-04-16T15:40:37Z","published":"2025-02-11T10:12:28Z","title":"Towards a Foundation Model for Physics-Informed Neural Networks:\n  Multi-PDE Learning with Active Sampling","summary":"  Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework\nfor solving partial differential equations (PDEs) by embedding physical laws\ninto neural network training. However, traditional PINN models are typically\ndesigned for single PDEs, limiting their generalizability across different\nphysical systems. In this work, we explore the potential of a foundation PINN\nmodel capable of solving multiple PDEs within a unified architecture. We\ninvestigate the efficacy of a single PINN framework trained on four distinct\nPDEs-the Simple Harmonic Oscillator (SHO), the 1D Heat Equation, the 1D Wave\nEquation, and the 2D Laplace Equation, demonstrating its ability to learn\ndiverse physical dynamics.\n  To enhance sample efficiency, we incorporate Active Learning (AL) using Monte\nCarlo (MC) Dropout-based uncertainty estimation, selecting the most informative\ntraining samples iteratively. We evaluate different active learning strategies,\ncomparing models trained on 10%, 20%, 30%, 40%, and 50% of the full dataset,\nand analyze their impact on solution accuracy. Our results indicate that\ntargeted uncertainty sampling significantly improves performance with fewer\ntraining samples, leading to efficient learning across multiple PDEs.\n  This work highlights the feasibility of a generalizable PINN-based foundation\nmodel, capable of adapting to different physics-based problems without\nredesigning network architectures. Our findings suggest that multi-PDE PINNs\nwith active learning can serve as an effective approach for reducing\ncomputational costs while maintaining high accuracy in physics-based deep\nlearning applications.\n","authors":["Keon Vin Park"],"pdf_url":"https://arxiv.org/pdf/2502.07425v2.pdf","comment":"This paper should be rewritten"},{"id":"http://arxiv.org/abs/2504.12186v1","updated":"2025-04-16T15:40:15Z","published":"2025-04-16T15:40:15Z","title":"CoMotion: Concurrent Multi-person 3D Motion","summary":"  We introduce an approach for detecting and tracking detailed 3D poses of\nmultiple people from a single monocular camera stream. Our system maintains\ntemporally coherent predictions in crowded scenes filled with difficult poses\nand occlusions. Our model performs both strong per-frame detection and a\nlearned pose update to track people from frame to frame. Rather than match\ndetections across time, poses are updated directly from a new input image,\nwhich enables online tracking through occlusion. We train on numerous image and\nvideo datasets leveraging pseudo-labeled annotations to produce a model that\nmatches state-of-the-art systems in 3D pose estimation accuracy while being\nfaster and more accurate in tracking multiple people through time. Code and\nweights are provided at https://github.com/apple/ml-comotion\n","authors":["Alejandro Newell","Peiyun Hu","Lahav Lipson","Stephan R. Richter","Vladlen Koltun"],"pdf_url":"https://arxiv.org/pdf/2504.12186v1.pdf","comment":"Accepted at ICLR 2025, for code and weights go to\n  https://github.com/apple/ml-comotion"},{"id":"http://arxiv.org/abs/2502.03963v2","updated":"2025-04-16T15:40:13Z","published":"2025-02-06T10:54:28Z","title":"AL-PINN: Active Learning-Driven Physics-Informed Neural Networks for\n  Efficient Sample Selection in Solving Partial Differential Equations","summary":"  Physics-Informed Neural Networks (PINNs) have emerged as a promising approach\nfor solving Partial Differential Equations (PDEs) by incorporating physical\nconstraints into deep learning models. However, standard PINNs often require a\nlarge number of training samples to achieve high accuracy, leading to increased\ncomputational costs. To address this issue, we propose Active Learning-Driven\nPINNs (AL-PINN), which integrates Uncertainty Quantification (UQ) and Active\nLearning (AL) strategies to optimize sample selection dynamically.\n  AL-PINN utilizes Monte Carlo Dropout to estimate epistemic uncertainty in the\nmodel predictions, enabling the adaptive selection of high-uncertainty regions\nfor additional training. This approach significantly enhances learning\nefficiency by focusing computational resources on the most informative data\npoints. We evaluate AL-PINN on benchmark PDE problems with known analytical\nsolutions and real-world WeatherBench climate data. Our results demonstrate\nthat AL-PINN achieves comparable or superior accuracy compared to traditional\nPINNs while reducing the number of required training samples.\n  The proposed framework is particularly beneficial for scientific and\nengineering applications where data collection is expensive or limited, such as\nclimate modeling, medical simulations, and material science. Our findings\nhighlight the potential of active learning in accelerating PINN-based PDE\nsolvers while maintaining high accuracy and computational efficiency.\n","authors":["Keon Vin Park"],"pdf_url":"https://arxiv.org/pdf/2502.03963v2.pdf","comment":"This paper should be rewritten"},{"id":"http://arxiv.org/abs/2504.12181v1","updated":"2025-04-16T15:38:38Z","published":"2025-04-16T15:38:38Z","title":"Battery-aware Cyclic Scheduling in Energy-harvesting Federated Learning","summary":"  Federated Learning (FL) has emerged as a promising framework for distributed\nlearning, but its growing complexity has led to significant energy consumption,\nparticularly from computations on the client side. This challenge is especially\ncritical in energy-harvesting FL (EHFL) systems, where device availability\nfluctuates due to limited and time-varying energy resources. We propose\nFedBacys, a battery-aware FL framework that introduces cyclic client\nparticipation based on users' battery levels to cope with these issues.\nFedBacys enables clients to save energy and strategically perform local\ntraining just before their designated transmission time by clustering clients\nand scheduling their involvement sequentially. This design minimizes redundant\ncomputation, reduces system-wide energy usage, and improves learning stability.\nOur experiments demonstrate that FedBacys outperforms existing approaches in\nterms of energy efficiency and performance consistency, exhibiting robustness\neven under non-i.i.d. training data distributions and with very infrequent\nbattery charging. This work presents the first comprehensive evaluation of\ncyclic client participation in EHFL, incorporating both communication and\ncomputation costs into a unified, resource-aware scheduling strategy.\n","authors":["Eunjeong Jeong","Nikolaos Pappas"],"pdf_url":"https://arxiv.org/pdf/2504.12181v1.pdf","comment":"This paper is currently under review for presentation at a\n  peer-reviewed conference"},{"id":"http://arxiv.org/abs/2502.01819v2","updated":"2025-04-16T15:36:36Z","published":"2025-02-03T20:50:05Z","title":"Score as Action: Fine-Tuning Diffusion Generative Models by\n  Continuous-time Reinforcement Learning","summary":"  Reinforcement learning from human feedback (RLHF), which aligns a diffusion\nmodel with input prompt, has become a crucial step in building reliable\ngenerative AI models. Most works in this area use a discrete-time formulation,\nwhich is prone to induced errors, and often not applicable to models with\nhigher-order/black-box solvers. The objective of this study is to develop a\ndisciplined approach to fine-tune diffusion models using continuous-time RL,\nformulated as a stochastic control problem with a reward function that aligns\nthe end result (terminal state) with input prompt. The key idea is to treat\nscore matching as controls or actions, and thereby making connections to policy\noptimization and regularization in continuous-time RL. To carry out this idea,\nwe lay out a new policy optimization framework for continuous-time RL, and\nillustrate its potential in enhancing the value networks design space via\nleveraging the structural property of diffusion models. We validate the\nadvantages of our method by experiments in downstream tasks of fine-tuning\nlarge-scale Text2Image models of Stable Diffusion v1.5.\n","authors":["Hanyang Zhao","Haoxian Chen","Ji Zhang","David D. Yao","Wenpin Tang"],"pdf_url":"https://arxiv.org/pdf/2502.01819v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2409.08400"},{"id":"http://arxiv.org/abs/2504.11168v2","updated":"2025-04-16T15:33:06Z","published":"2025-04-15T13:16:02Z","title":"Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails","summary":"  Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems.\n","authors":["William Hackett","Lewis Birch","Stefan Trawicki","Neeraj Suri","Peter Garraghan"],"pdf_url":"https://arxiv.org/pdf/2504.11168v2.pdf","comment":"12 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2503.11720v3","updated":"2025-04-16T15:28:55Z","published":"2025-03-13T21:10:29Z","title":"Fine-Tuning Diffusion Generative Models via Rich Preference Optimization","summary":"  We introduce Rich Preference Optimization (RPO), a novel pipeline that\nleverages rich feedback signals to improve the curation of preference pairs for\nfine-tuning text-to-image diffusion models. Traditional methods, like\nDiffusion-DPO, often rely solely on reward model labeling, which can be opaque,\noffer limited insights into the rationale behind preferences, and are prone to\nissues such as reward hacking or overfitting. In contrast, our approach begins\nwith generating detailed critiques of synthesized images to extract reliable\nand actionable image editing instructions. By implementing these instructions,\nwe create refined images, resulting in synthetic, informative preference pairs\nthat serve as enhanced tuning datasets. We demonstrate the effectiveness of our\npipeline and the resulting datasets in fine-tuning state-of-the-art diffusion\nmodels.\n","authors":["Hanyang Zhao","Haoxian Chen","Yucheng Guo","Genta Indra Winata","Tingting Ou","Ziyu Huang","David D. Yao","Wenpin Tang"],"pdf_url":"https://arxiv.org/pdf/2503.11720v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08965v3","updated":"2025-04-16T15:28:12Z","published":"2024-03-13T21:11:58Z","title":"Deep Learning Based Dynamics Identification and Linearization of Orbital\n  Problems using Koopman Theory","summary":"  The study of the Two-Body and Circular Restricted Three-Body Problems in the\nfield of aerospace engineering and sciences is deeply important because they\nhelp describe the motion of both celestial and artificial satellites. With the\ngrowing demand for satellites and satellite formation flying, fast and\nefficient control of these systems is becoming ever more important. Global\nlinearization of these systems allows engineers to employ methods of control in\norder to achieve these desired results. We propose a data-driven framework for\nsimultaneous system identification and global linearization of the Circular,\nElliptical and Perturbed Two-Body Problem as well as the Circular Restricted\nThree-Body Problem around the L1 Lagrange point via deep learning-based Koopman\nTheory, i.e., a framework that can identify the underlying dynamics and\nglobally linearize it into a linear time-invariant (LTI) system. The linear\nKoopman operator is discovered through purely data-driven training of a Deep\nNeural Network with a custom architecture. This paper displays the ability of\nthe Koopman operator to generalize to various other Two-Body systems without\nthe need for retraining. We also demonstrate the capability of the same\narchitecture to be utilized to accurately learn a Koopman operator that\napproximates the Circular Restricted Three-Body Problem.\n","authors":["George Nehma","Madhur Tiwari","Manasvi Lingam"],"pdf_url":"https://arxiv.org/pdf/2403.08965v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00143v2","updated":"2025-04-16T15:26:14Z","published":"2024-06-28T16:08:26Z","title":"InfoNCE: Identifying the Gap Between Theory and Practice","summary":"  Prior theory work on Contrastive Learning via the InfoNCE loss showed that,\nunder certain assumptions, the learned representations recover the ground-truth\nlatent factors. We argue that these theories overlook crucial aspects of how CL\nis deployed in practice. Specifically, they either assume equal variance across\nall latents or that certain latents are kept invariant. However, in practice,\npositive pairs are often generated using augmentations such as strong cropping\nto just a few pixels. Hence, a more realistic assumption is that all latent\nfactors change with a continuum of variability across all factors. We introduce\nAnInfoNCE, a generalization of InfoNCE that can provably uncover the latent\nfactors in this anisotropic setting, broadly generalizing previous\nidentifiability results in CL. We validate our identifiability results in\ncontrolled experiments and show that AnInfoNCE increases the recovery of\npreviously collapsed information in CIFAR10 and ImageNet, albeit at the cost of\ndownstream accuracy. Finally, we discuss the remaining mismatches between\ntheoretical assumptions and practical implementations.\n","authors":["Evgenia Rusak","Patrik Reizinger","Attila Juhos","Oliver Bringmann","Roland S. Zimmermann","Wieland Brendel"],"pdf_url":"https://arxiv.org/pdf/2407.00143v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12175v1","updated":"2025-04-16T15:25:58Z","published":"2025-04-16T15:25:58Z","title":"Approximation Bounds for Transformer Networks with Application to\n  Regression","summary":"  We explore the approximation capabilities of Transformer networks for\nH\\\"older and Sobolev functions, and apply these results to address\nnonparametric regression estimation with dependent observations. First, we\nestablish novel upper bounds for standard Transformer networks approximating\nsequence-to-sequence mappings whose component functions are H\\\"older continuous\nwith smoothness index $\\gamma \\in (0,1]$. To achieve an approximation error\n$\\varepsilon$ under the $L^p$-norm for $p \\in [1, \\infty]$, it suffices to use\na fixed-depth Transformer network whose total number of parameters scales as\n$\\varepsilon^{-d_x n / \\gamma}$. This result not only extends existing findings\nto include the case $p = \\infty$, but also matches the best known upper bounds\non number of parameters previously obtained for fixed-depth FNNs and RNNs.\nSimilar bounds are also derived for Sobolev functions. Second, we derive\nexplicit convergence rates for the nonparametric regression problem under\nvarious $\\beta$-mixing data assumptions, which allow the dependence between\nobservations to weaken over time. Our bounds on the sample complexity impose no\nconstraints on weight magnitudes. Lastly, we propose a novel proof strategy to\nestablish approximation bounds, inspired by the Kolmogorov-Arnold\nrepresentation theorem. We show that if the self-attention layer in a\nTransformer can perform column averaging, the network can approximate\nsequence-to-sequence H\\\"older functions, offering new insights into the\ninterpretability of self-attention mechanisms.\n","authors":["Yuling Jiao","Yanming Lai","Defeng Sun","Yang Wang","Bokai Yan"],"pdf_url":"https://arxiv.org/pdf/2504.12175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10581v2","updated":"2025-04-16T15:24:39Z","published":"2024-05-17T07:09:52Z","title":"Future Aware Safe Active Learning of Time Varying Systems using Gaussian\n  Processes","summary":"  Experimental exploration of high-cost systems with safety constraints, common\nin engineering applications, is a challenging endeavor. Data-driven models\noffer a promising solution, but acquiring the requisite data remains expensive\nand is potentially unsafe. Safe active learning techniques prove essential,\nenabling the learning of high-quality models with minimal expensive data points\nand high safety. This paper introduces a safe active learning framework\ntailored for time-varying systems, addressing drift, seasonal changes, and\ncomplexities due to dynamic behavior. The proposed Time-aware Integrated Mean\nSquared Prediction Error (T-IMSPE) method minimizes posterior variance over\ncurrent and future states, optimizing information gathering also in the time\ndomain. Empirical results highlight T-IMSPE's advantages in model quality\nthrough toy and real-world examples. State of the art Gaussian processes are\ncompatible with T-IMSPE. Our theoretical contributions include a clear\ndelineation which Gaussian process kernels, domains, and weighting measures are\nsuitable for T-IMSPE and even beyond for its non-time aware predecessor IMSPE.\n","authors":["Markus Lange-Hegermann","Christoph Zimmer"],"pdf_url":"https://arxiv.org/pdf/2405.10581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12167v1","updated":"2025-04-16T15:18:56Z","published":"2025-04-16T15:18:56Z","title":"RADLER: Radar Object Detection Leveraging Semantic 3D City Models and\n  Self-Supervised Radar-Image Learning","summary":"  Semantic 3D city models are worldwide easy-accessible, providing accurate,\nobject-oriented, and semantic-rich 3D priors. To date, their potential to\nmitigate the noise impact on radar object detection remains under-explored. In\nthis paper, we first introduce a unique dataset, RadarCity, comprising 54K\nsynchronized radar-image pairs and semantic 3D city models. Moreover, we\npropose a novel neural network, RADLER, leveraging the effectiveness of\ncontrastive self-supervised learning (SSL) and semantic 3D city models to\nenhance radar object detection of pedestrians, cyclists, and cars.\nSpecifically, we first obtain the robust radar features via a SSL network in\nthe radar-image pretext task. We then use a simple yet effective feature fusion\nstrategy to incorporate semantic-depth features from semantic 3D city models.\nHaving prior 3D information as guidance, RADLER obtains more fine-grained\ndetails to enhance radar object detection. We extensively evaluate RADLER on\nthe collected RadarCity dataset and demonstrate average improvements of 5.46%\nin mean avarage precision (mAP) and 3.51% in mean avarage recall (mAR) over\nprevious radar object detection methods. We believe this work will foster\nfurther research on semantic-guided and map-supported radar object detection.\nOur project page is publicly available\nathttps://gpp-communication.github.io/RADLER .\n","authors":["Yuan Luo","Rudolf Hoffmann","Yan Xia","Olaf Wysocki","Benedikt Schwab","Thomas H. Kolbe","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2504.12167v1.pdf","comment":"The paper accepted for CVPRW '25 (PBVS 2025 - the Perception Beyond\n  the Visible Spectrum)"},{"id":"http://arxiv.org/abs/2302.06352v3","updated":"2025-04-16T15:17:51Z","published":"2023-02-13T13:35:09Z","title":"Deep Anatomical Federated Network (Dafne): An open client-server\n  framework for the continuous, collaborative improvement of deep\n  learning-based medical image segmentation","summary":"  Purpose: To present and evaluate Dafne (deep anatomical federated network), a\nfreely available decentralized, collaborative deep learning system for the\nsemantic segmentation of radiological images through federated incremental\nlearning. Materials and Methods: Dafne is free software with a client-server\narchitecture. The client side is an advanced user interface that applies the\ndeep learning models stored on the server to the user's data and allows the\nuser to check and refine the prediction. Incremental learning is then performed\nat the client's side and sent back to the server, where it is integrated into\nthe root model. Dafne was evaluated locally, by assessing the performance gain\nacross model generations on 38 MRI datasets of the lower legs, and through the\nanalysis of real-world usage statistics (n = 639 use-cases). Results: Dafne\ndemonstrated a statistically improvement in the accuracy of semantic\nsegmentation over time (average increase of the Dice Similarity Coefficient by\n0.007 points/generation on the local validation set, p < 0.001). Qualitatively,\nthe models showed enhanced performance on various radiologic image types,\nincluding those not present in the initial training sets, indicating good model\ngeneralizability. Conclusion: Dafne showed improvement in segmentation quality\nover time, demonstrating potential for learning and generalization.\n","authors":["Francesco Santini","Jakob Wasserthal","Abramo Agosti","Xeni Deligianni","Kevin R. Keene","Hermien E. Kan","Stefan Sommer","Fengdan Wang","Claudia Weidensteiner","Giulia Manco","Matteo Paoletti","Valentina Mazzoli","Arjun Desai","Anna Pichiecchio"],"pdf_url":"https://arxiv.org/pdf/2302.06352v3.pdf","comment":"In this new version: accepted version in Radiology: Artificial\n  Intelligence. Note regarding the license/copyright: This submission is\n  conforming with the RSNA Preprint policy available here:\n  https://pubs.rsna.org/page/ai/author-instructions, which REQUIRES authors to\n  update the version on preprint servers with the accepted version and the\n  copyright notice as indicated in the PDF"},{"id":"http://arxiv.org/abs/2402.09063v2","updated":"2025-04-16T15:15:56Z","published":"2024-02-14T10:20:03Z","title":"Soft Prompt Threats: Attacking Safety Alignment and Unlearning in\n  Open-Source LLMs through the Embedding Space","summary":"  Current research in adversarial robustness of LLMs focuses on discrete input\nmanipulations in the natural language space, which can be directly transferred\nto closed-source models. However, this approach neglects the steady progression\nof open-source models. As open-source models advance in capability, ensuring\ntheir safety also becomes increasingly imperative. Yet, attacks tailored to\nopen-source LLMs that exploit full model access remain largely unexplored. We\naddress this research gap and propose the embedding space attack, which\ndirectly attacks the continuous embedding representation of input tokens. We\nfind that embedding space attacks circumvent model alignments and trigger\nharmful behaviors more efficiently than discrete attacks or model fine-tuning.\nFurthermore, we present a novel threat model in the context of unlearning and\nshow that embedding space attacks can extract supposedly deleted information\nfrom unlearned LLMs across multiple datasets and models. Our findings highlight\nembedding space attacks as an important threat model in open-source LLMs.\nTrigger Warning: the appendix contains LLM-generated text with violence and\nharassment.\n","authors":["Leo Schwinn","David Dobre","Sophie Xhonneux","Gauthier Gidel","Stephan Gunnemann"],"pdf_url":"https://arxiv.org/pdf/2402.09063v2.pdf","comment":"Trigger Warning: the appendix contains LLM-generated text with\n  violence and harassment"},{"id":"http://arxiv.org/abs/2401.05308v2","updated":"2025-04-16T15:14:34Z","published":"2024-01-10T18:22:00Z","title":"Strategic Client Selection to Address Non-IIDness in HAPS-enabled FL\n  Networks","summary":"  The deployment of federated learning (FL) in non-terrestrial networks (NTN)\nthat are supported by high-altitude platform stations (HAPS) offers numerous\nadvantages. Due to its large footprint, it facilitates interaction with a large\nnumber of line-of-sight (LoS) ground clients, each possessing diverse datasets\nalong with distinct communication and computational capabilities. The presence\nof many clients enhances the accuracy of the FL model and speeds up\nconvergence. However, the variety of datasets among these clients poses a\nsignificant challenge, as it leads to pervasive non-independent and identically\ndistributed (non-IID) data. The data non-IIDness results in markedly reduced\ntraining accuracy and slower convergence rates. To address this issue, we\npropose a novel weighted attribute-based client selection strategy that\nleverages multiple user-specific attributes, including historical traffic\npatterns, instantaneous channel conditions, computational capabilities, and\nprevious-round learning performance. By combining these attributes into a\ncomposite score for each user at every FL round and selecting users with higher\nscores as FL clients, the framework ensures more uniform and representative\ndata distributions, effectively mitigating the adverse effects of non-IID data.\nSimulation results corroborate the effectiveness of the proposed client\nselection strategy in enhancing FL model accuracy and convergence rate, as well\nas reducing training loss, by effectively addressing the critical challenge of\ndata non-IIDness in large-scale FL system implementations.\n","authors":["Amin Farajzadeh","Animesh Yadav","Halim Yanikomeroglu"],"pdf_url":"https://arxiv.org/pdf/2401.05308v2.pdf","comment":"Submitted to IEEE for possible publication"},{"id":"http://arxiv.org/abs/2504.12156v1","updated":"2025-04-16T15:04:00Z","published":"2025-04-16T15:04:00Z","title":"Predictive Multiplicity in Survival Models: A Method for Quantifying\n  Model Uncertainty in Predictive Maintenance Applications","summary":"  In many applications, especially those involving prediction, models may yield\nnear-optimal performance yet significantly disagree on individual-level\noutcomes. This phenomenon, known as predictive multiplicity, has been formally\ndefined in binary, probabilistic, and multi-target classification, and\nundermines the reliability of predictive systems. However, its implications\nremain unexplored in the context of survival analysis, which involves\nestimating the time until a failure or similar event while properly handling\ncensored data. We frame predictive multiplicity as a critical concern in\nsurvival-based models and introduce formal measures -- ambiguity, discrepancy,\nand obscurity -- to quantify it. This is particularly relevant for downstream\ntasks such as maintenance scheduling, where precise individual risk estimates\nare essential. Understanding and reporting predictive multiplicity helps build\ntrust in models deployed in high-stakes environments. We apply our methodology\nto benchmark datasets from predictive maintenance, extending the notion of\nmultiplicity to survival models. Our findings show that ambiguity steadily\nincreases, reaching up to 40-45% of observations; discrepancy is lower but\nexhibits a similar trend; and obscurity remains mild and concentrated in a few\nmodels. These results demonstrate that multiple accurate survival models may\nyield conflicting estimations of failure risk and degradation progression for\nthe same equipment. This highlights the need to explicitly measure and\ncommunicate predictive multiplicity to ensure reliable decision-making in\nprocess health management.\n","authors":["Mustafa Cavus"],"pdf_url":"https://arxiv.org/pdf/2504.12156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12151v1","updated":"2025-04-16T15:00:06Z","published":"2025-04-16T15:00:06Z","title":"Towards Explainable Fusion and Balanced Learning in Multimodal Sentiment\n  Analysis","summary":"  Multimodal Sentiment Analysis (MSA) faces two critical challenges: the lack\nof interpretability in the decision logic of multimodal fusion and modality\nimbalance caused by disparities in inter-modal information density. To address\nthese issues, we propose KAN-MCP, a novel framework that integrates the\ninterpretability of Kolmogorov-Arnold Networks (KAN) with the robustness of the\nMultimodal Clean Pareto (MCPareto) framework. First, KAN leverages its\nunivariate function decomposition to achieve transparent analysis of\ncross-modal interactions. This structural design allows direct inspection of\nfeature transformations without relying on external interpretation tools,\nthereby ensuring both high expressiveness and interpretability. Second, the\nproposed MCPareto enhances robustness by addressing modality imbalance and\nnoise interference. Specifically, we introduce the Dimensionality Reduction and\nDenoising Modal Information Bottleneck (DRD-MIB) method, which jointly denoises\nand reduces feature dimensionality. This approach provides KAN with\ndiscriminative low-dimensional inputs to reduce the modeling complexity of KAN\nwhile preserving critical sentiment-related information. Furthermore, MCPareto\ndynamically balances gradient contributions across modalities using the\npurified features output by DRD-MIB, ensuring lossless transmission of\nauxiliary signals and effectively alleviating modality imbalance. This synergy\nof interpretability and robustness not only achieves superior performance on\nbenchmark datasets such as CMU-MOSI, CMU-MOSEI, and CH-SIMS v2 but also offers\nan intuitive visualization interface through KAN's interpretable architecture.\n","authors":["Miaosen Luo","Yuncheng Jiang","Sijie Mai"],"pdf_url":"https://arxiv.org/pdf/2504.12151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16514v3","updated":"2025-04-16T14:58:48Z","published":"2025-03-15T23:43:06Z","title":"VeriMind: Agentic LLM for Automated Verilog Generation with a Novel\n  Evaluation Metric","summary":"  Designing Verilog modules requires meticulous attention to correctness,\nefficiency, and adherence to design specifications. However, manually writing\nVerilog code remains a complex and time-consuming task that demands both expert\nknowledge and iterative refinement. Leveraging recent advancements in large\nlanguage models (LLMs) and their structured text generation capabilities, we\npropose VeriMind, an agentic LLM framework for Verilog code generation that\nsignificantly automates and optimizes the synthesis process. Unlike traditional\nLLM-based code generators, VeriMind employs a structured reasoning approach:\ngiven a user-provided prompt describing design requirements, the system first\nformulates a detailed train of thought before the final Verilog code is\ngenerated. This multi-step methodology enhances interpretability, accuracy, and\nadaptability in hardware design. In addition, we introduce a novel evaluation\nmetric-pass@ARC-which combines the conventional pass@k measure with Average\nRefinement Cycles (ARC) to capture both success rate and the efficiency of\niterative refinement. Experimental results on diverse hardware design tasks\ndemonstrated that our approach achieved up to $8.3\\%$ improvement on pass@k\nmetric and $8.1\\%$ on pass@ARC metric. These findings underscore the\ntransformative potential of agentic LLMs in automated hardware design, RTL\ndevelopment, and digital system synthesis.\n","authors":["Bardia Nadimi","Ghali Omar Boutaib","Hao Zheng"],"pdf_url":"https://arxiv.org/pdf/2503.16514v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12137v1","updated":"2025-04-16T14:50:25Z","published":"2025-04-16T14:50:25Z","title":"Efficient Contrastive Decoding with Probabilistic Hallucination\n  Detection - Mitigating Hallucinations in Large Vision Language Models -","summary":"  Despite recent advances in Large Vision Language Models (LVLMs), these models\nstill suffer from generating hallucinatory responses that do not align with the\nvisual input provided. To mitigate such hallucinations, we introduce Efficient\nContrastive Decoding (ECD), a simple method that leverages probabilistic\nhallucination detection to shift the output distribution towards contextually\naccurate answers at inference time. By contrasting token probabilities and\nhallucination scores, ECD subtracts hallucinated concepts from the original\ndistribution, effectively suppressing hallucinations. Notably, our proposed\nmethod can be applied to any open-source LVLM and does not require additional\nLVLM training. We evaluate our method on several benchmark datasets and across\ndifferent LVLMs. Our experiments show that ECD effectively mitigates\nhallucinations, outperforming state-of-the-art methods with respect to\nperformance on LVLM benchmarks and computation time.\n","authors":["Laura Fieback","Nishilkumar Balar","Jakob Spiegelberg","Hanno Gottschalk"],"pdf_url":"https://arxiv.org/pdf/2504.12137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10185v2","updated":"2025-04-16T14:45:55Z","published":"2025-04-14T12:38:37Z","title":"LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in\n  Current Benchmarks","summary":"  Large language model unlearning has become a critical challenge in ensuring\nsafety and controlled model behavior by removing undesired data-model\ninfluences from the pretrained model while preserving general utility.\nSignificant recent efforts have been dedicated to developing LLM unlearning\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\nUnlearning Six-way Evaluation), facilitating standardized unlearning\nperformance assessment and method comparison. Despite their usefulness, we\nuncover for the first time a novel coreset effect within these benchmarks.\nSpecifically, we find that LLM unlearning achieved with the original (full)\nforget set can be effectively maintained using a significantly smaller subset\n(functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\ncan be performed surprisingly easily, even in an extremely low-data regime. We\ndemonstrate that this coreset effect remains strong, regardless of the LLM\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\nThe surprisingly strong coreset effect is also robust across various data\nselection methods, ranging from random selection to more sophisticated\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\nkeyword-based perspective, showing that keywords extracted from the forget set\nalone contribute significantly to unlearning effectiveness and indicating that\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\nmodels along additional dimensions, such as mode connectivity and robustness to\njailbreaking attacks. Codes are available at\nhttps://github.com/OPTML-Group/MU-Coreset.\n","authors":["Soumyadeep Pal","Changsheng Wang","James Diffenderfer","Bhavya Kailkhura","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2504.10185v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12088v1","updated":"2025-04-16T13:51:16Z","published":"2025-04-16T13:51:16Z","title":"AttentionDrop: A Novel Regularization Method for Transformer Models","summary":"  Transformer-based architectures achieve state-of-the-art performance across a\nwide range of tasks in natural language processing, computer vision, and\nspeech. However, their immense capacity often leads to overfitting, especially\nwhen training data is limited or noisy. We propose AttentionDrop, a unified\nfamily of stochastic regularization techniques that operate directly on the\nself-attention distributions. We introduces three variants: 1. Hard Attention\nMasking: randomly zeroes out top-k attention logits per query to encourage\ndiverse context utilization. 2. Blurred Attention Smoothing: applies a dynamic\nGaussian convolution over attention logits to diffuse overly peaked\ndistributions. 3. Consistency-Regularized AttentionDrop: enforces output\nstability under multiple independent AttentionDrop perturbations via a KL-based\nconsistency loss.\n","authors":["Mirza Samad Ahmed Baig","Syeda Anshrah Gillani","Abdul Akbar Khan","Shahid Munir Shah"],"pdf_url":"https://arxiv.org/pdf/2504.12088v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2504.12086v1","updated":"2025-04-16T13:47:25Z","published":"2025-04-16T13:47:25Z","title":"Neural Contextual Bandits Under Delayed Feedback Constraints","summary":"  This paper presents a new algorithm for neural contextual bandits (CBs) that\naddresses the challenge of delayed reward feedback, where the reward for a\nchosen action is revealed after a random, unknown delay. This scenario is\ncommon in applications such as online recommendation systems and clinical\ntrials, where reward feedback is delayed because the outcomes or results of a\nuser's actions (such as recommendations or treatment responses) take time to\nmanifest and be measured. The proposed algorithm, called Delayed NeuralUCB,\nuses an upper confidence bound (UCB)-based exploration strategy. Under the\nassumption of independent and identically distributed sub-exponential reward\ndelays, we derive an upper bound on the cumulative regret over a T-length\nhorizon. We further consider a variant of the algorithm, called Delayed\nNeuralTS, that uses Thompson Sampling-based exploration. Numerical experiments\non real-world datasets, such as MNIST and Mushroom, along with comparisons to\nbenchmark approaches, demonstrate that the proposed algorithms effectively\nmanage varying delays and are well-suited for complex real-world scenarios.\n","authors":["Mohammadali Moghimi","Sharu Theresa Jose","Shana Moothedath"],"pdf_url":"https://arxiv.org/pdf/2504.12086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14773v2","updated":"2025-04-16T13:38:58Z","published":"2024-03-21T18:27:29Z","title":"StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation\n  from Text","summary":"  Text-to-video diffusion models enable the generation of high-quality videos\nthat follow text instructions, making it easy to create diverse and individual\ncontent. However, existing approaches mostly focus on high-quality short video\ngeneration (typically 16 or 24 frames), ending up with hard-cuts when naively\nextended to the case of long video synthesis. To overcome these limitations, we\nintroduce StreamingT2V, an autoregressive approach for long video generation of\n80, 240, 600, 1200 or more frames with smooth transitions. The key components\nare:(i) a short-term memory block called conditional attention module (CAM),\nwhich conditions the current generation on the features extracted from the\nprevious chunk via an attentional mechanism, leading to consistent chunk\ntransitions, (ii) a long-term memory block called appearance preservation\nmodule, which extracts high-level scene and object features from the first\nvideo chunk to prevent the model from forgetting the initial scene, and (iii) a\nrandomized blending approach that enables to apply a video enhancer\nautoregressively for infinitely long videos without inconsistencies between\nchunks. Experiments show that StreamingT2V generates high motion amount. In\ncontrast, all competing image-to-video methods are prone to video stagnation\nwhen applied naively in an autoregressive manner. Thus, we propose with\nStreamingT2V a high-quality seamless text-to-long video generator that\noutperforms competitors with consistency and motion. Our code will be available\nat: https://github.com/Picsart-AI-Research/StreamingT2V\n","authors":["Roberto Henschel","Levon Khachatryan","Hayk Poghosyan","Daniil Hayrapetyan","Vahram Tadevosyan","Zhangyang Wang","Shant Navasardyan","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.14773v2.pdf","comment":"https://github.com/Picsart-AI-Research/StreamingT2V"},{"id":"http://arxiv.org/abs/2504.12075v1","updated":"2025-04-16T13:32:25Z","published":"2025-04-16T13:32:25Z","title":"Generative Deep Learning Framework for Inverse Design of Fuels","summary":"  In the present work, a generative deep learning framework combining a\nCo-optimized Variational Autoencoder (Co-VAE) architecture with quantitative\nstructure-property relationship (QSPR) techniques is developed to enable\naccelerated inverse design of fuels. The Co-VAE integrates a property\nprediction component coupled with the VAE latent space, enhancing molecular\nreconstruction and accurate estimation of Research Octane Number (RON) (chosen\nas the fuel property of interest). A subset of the GDB-13 database, enriched\nwith a curated RON database, is used for model training. Hyperparameter tuning\nis further utilized to optimize the balance among reconstruction fidelity,\nchemical validity, and RON prediction. An independent regression model is then\nused to refine RON prediction, while a differential evolution algorithm is\nemployed to efficiently navigate the VAE latent space and identify promising\nfuel molecule candidates with high RON. This methodology addresses the\nlimitations of traditional fuel screening approaches by capturing complex\nstructure-property relationships within a comprehensive latent representation.\nThe generative model provides a flexible tool for systematically exploring vast\nchemical spaces, paving the way for discovering fuels with superior anti-knock\nproperties. The demonstrated approach can be readily extended to incorporate\nadditional fuel properties and synthesizability criteria to enhance\napplicability and reliability for de novo design of new fuels.\n","authors":["Kiran K. Yalamanchi","Pinaki Pal","Balaji Mohan","Abdullah S. AlRamadan","Jihad A. Badra","Yuanjiang Pei"],"pdf_url":"https://arxiv.org/pdf/2504.12075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15065v2","updated":"2025-04-16T13:23:25Z","published":"2024-04-23T14:12:48Z","title":"Formal Verification of Graph Convolutional Networks with Uncertain Node\n  Features and Uncertain Graph Structure","summary":"  Graph neural networks are becoming increasingly popular in the field of\nmachine learning due to their unique ability to process data structured in\ngraphs. They have also been applied in safety-critical environments where\nperturbations inherently occur. However, these perturbations require us to\nformally verify neural networks before their deployment in safety-critical\nenvironments as neural networks are prone to adversarial attacks. While there\nexists research on the formal verification of neural networks, there is no work\nverifying the robustness of generic graph convolutional network architectures\nwith uncertainty in the node features and in the graph structure over multiple\nmessage-passing steps. This work addresses this research gap by explicitly\npreserving the non-convex dependencies of all elements in the underlying\ncomputations through reachability analysis with (matrix) polynomial zonotopes.\nWe demonstrate our approach on three popular benchmark datasets.\n","authors":["Tobias Ladner","Michael Eichelbeck","Matthias Althoff"],"pdf_url":"https://arxiv.org/pdf/2404.15065v2.pdf","comment":"published at Transactions on Machine Learning Research (TMLR) 2025"},{"id":"http://arxiv.org/abs/2504.12063v1","updated":"2025-04-16T13:18:16Z","published":"2025-04-16T13:18:16Z","title":"Optimizing Compound Retrieval Systems","summary":"  Modern retrieval systems do not rely on a single ranking model to construct\ntheir rankings. Instead, they generally take a cascading approach where a\nsequence of ranking models are applied in multiple re-ranking stages. Thereby,\nthey balance the quality of the top-K ranking with computational costs by\nlimiting the number of documents each model re-ranks. However, the cascading\napproach is not the only way models can interact to form a retrieval system.\n  We propose the concept of compound retrieval systems as a broader class of\nretrieval systems that apply multiple prediction models. This encapsulates\ncascading models but also allows other types of interactions than top-K\nre-ranking. In particular, we enable interactions with large language models\n(LLMs) which can provide relative relevance comparisons. We focus on the\noptimization of compound retrieval system design which uniquely involves\nlearning where to apply the component models and how to aggregate their\npredictions into a final ranking. This work shows how our compound approach can\ncombine the classic BM25 retrieval model with state-of-the-art (pairwise) LLM\nrelevance predictions, while optimizing a given ranking metric and efficiency\ntarget. Our experimental results show optimized compound retrieval systems\nprovide better trade-offs between effectiveness and efficiency than cascading\napproaches, even when applied in a self-supervised manner.\n  With the introduction of compound retrieval systems, we hope to inspire the\ninformation retrieval field to more out-of-the-box thinking on how prediction\nmodels can interact to form rankings.\n","authors":["Harrie Oosterhuis","Rolf Jagerman","Zhen Qin","Xuanhui Wang"],"pdf_url":"https://arxiv.org/pdf/2504.12063v1.pdf","comment":"SIGIR 2025"},{"id":"http://arxiv.org/abs/2504.10149v2","updated":"2025-04-16T13:16:19Z","published":"2025-04-14T12:00:00Z","title":"BoTTA: Benchmarking on-device Test Time Adaptation","summary":"  The performance of deep learning models depends heavily on test samples at\nruntime, and shifts from the training data distribution can significantly\nreduce accuracy. Test-time adaptation (TTA) addresses this by adapting models\nduring inference without requiring labeled test data or access to the original\ntraining set. While research has explored TTA from various perspectives like\nalgorithmic complexity, data and class distribution shifts, model\narchitectures, and offline versus continuous learning, constraints specific to\nmobile and edge devices remain underexplored. We propose BoTTA, a benchmark\ndesigned to evaluate TTA methods under practical constraints on mobile and edge\ndevices. Our evaluation targets four key challenges caused by limited resources\nand usage conditions: (i) limited test samples, (ii) limited exposure to\ncategories, (iii) diverse distribution shifts, and (iv) overlapping shifts\nwithin a sample. We assess state-of-the-art TTA methods under these scenarios\nusing benchmark datasets and report system-level metrics on a real testbed.\nFurthermore, unlike prior work, we align with on-device requirements by\nadvocating periodic adaptation instead of continuous inference-time adaptation.\nExperiments reveal key insights: many recent TTA algorithms struggle with small\ndatasets, fail to generalize to unseen categories, and depend on the diversity\nand complexity of distribution shifts. BoTTA also reports device-specific\nresource use. For example, while SHOT improves accuracy by $2.25\\times$ with\n$512$ adaptation samples, it uses $1.08\\times$ peak memory on Raspberry Pi\nversus the base model. BoTTA offers actionable guidance for TTA in real-world,\nresource-constrained deployments.\n","authors":["Michal Danilowski","Soumyajit Chatterjee","Abhirup Ghosh"],"pdf_url":"https://arxiv.org/pdf/2504.10149v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12051v1","updated":"2025-04-16T13:06:20Z","published":"2025-04-16T13:06:20Z","title":"On the calibration of Just-in-time Defect Prediction","summary":"  Just in time defect prediction (JIT DP) leverages ML to identify defect-prone\ncode commits, enabling quality assurance (QA) teams to allocate resources more\nefficiently by focusing on commits that are most likely to contain defects.\nAlthough JIT DP techniques have introduced improvements in terms of predictive\naccuracy, they are still susceptible to misclassification errors such as false\npositives and negatives. This can lead to wasted resources or undetected\ndefects, a particularly critical concern when QA resources are limited. To\nmitigate these challenges and preserve the practical utility of JIT DP tools,\nit becomes essential to estimate the reliability of the predictions, i.e.,\ncomputing confidence scores. Such scores can help practitioners determine the\ntrustworthiness of predictions and thus prioritize them efficiently. A simple\napproach to computing confidence scores is to extract, alongside each\nprediction, the corresponding prediction probabilities and use them as\nindicators of confidence. However, for these probabilities to reliably serve as\nconfidence scores, the predictive model must be well-calibrated. This means\nthat the prediction probabilities must accurately represent the true likelihood\nof each prediction being correct. Miscalibration, common in modern ML models,\ndistorts probability scores such that they do not align with the actual\ncorrectness probability. In this study, we evaluate the calibration of three\nJIT DP techniques to determine whether and to what extent they exhibit poor\ncalibration. Furthermore, we assess whether post-calibration methods can\nimprove the calibration of existing JIT defect prediction models. Our results\nreveal that all evaluated JIT DP models exhibit some level of miscalibration,\nwith ECE ranging from 2-35%. Furthermore, post-calibration methods do not\nconsistently improve the calibration.\n","authors":["Xhulja Shahini","Jone Bartel","Klaus Pohl"],"pdf_url":"https://arxiv.org/pdf/2504.12051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12045v1","updated":"2025-04-16T13:01:44Z","published":"2025-04-16T13:01:44Z","title":"pix2pockets: Shot Suggestions in 8-Ball Pool from a Single Image in the\n  Wild","summary":"  Computer vision models have seen increased usage in sports, and reinforcement\nlearning (RL) is famous for beating humans in strategic games such as Chess and\nGo. In this paper, we are interested in building upon these advances and\nexamining the game of classic 8-ball pool. We introduce pix2pockets, a\nfoundation for an RL-assisted pool coach. Given a single image of a pool table,\nwe first aim to detect the table and the balls and then propose the optimal\nshot suggestion. For the first task, we build a dataset with 195 diverse images\nwhere we manually annotate all balls and table dots, leading to 5748 object\nsegmentation masks. For the second task, we build a standardized RL environment\nthat allows easy development and benchmarking of any RL algorithm. Our object\ndetection model yields an AP50 of 91.2 while our ball location pipeline obtains\nan error of only 0.4 cm. Furthermore, we compare standard RL algorithms to set\na baseline for the shot suggestion task and we show that all of them fail to\npocket all balls without making a foul move. We also present a simple baseline\nthat achieves a per-shot success rate of 94.7% and clears a full game in a\nsingle turn 30% of the time.\n","authors":["Jonas Myhre Schiøtt","Viktor Sebastian Petersen","Dimitrios P. Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2504.12045v1.pdf","comment":"15 pages, 7 figures, to be published in SCIA 2025"},{"id":"http://arxiv.org/abs/2504.07722v2","updated":"2025-04-16T12:57:23Z","published":"2025-04-10T13:15:52Z","title":"Relaxing the Markov Requirements on Reinforcement Learning Under Weak\n  Partial Ignorability","summary":"  Incomplete data, confounding effects, and violations of the Markov property\nare interrelated problems which are ubiquitous in Reinforcement Learning\napplications. We introduce the concept of ``partial ignorabilty\" and leverage\nit to establish a novel convergence theorem for adaptive Reinforcement\nLearning. This theoretical result relaxes the Markov assumption on the\nstochastic process underlying conventional $Q$-learning, deploying a\ngeneralized form of the Robbins-Monro stochastic approximation theorem to\nestablish optimality. This result has clear downstream implications for most\nactive subfields of Reinforcement Learning, with clear paths for extension to\nthe field of Causal Inference.\n","authors":["MaryLena Bleile"],"pdf_url":"https://arxiv.org/pdf/2504.07722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12039v1","updated":"2025-04-16T12:54:11Z","published":"2025-04-16T12:54:11Z","title":"RadMamba: Efficient Human Activity Recognition through Radar-based\n  Micro-Doppler-Oriented Mamba State-Space Model","summary":"  Radar-based HAR has emerged as a promising alternative to conventional\nmonitoring approaches, such as wearable devices and camera-based systems, due\nto its unique privacy preservation and robustness advantages. However, existing\nsolutions based on convolutional and recurrent neural networks, although\neffective, are computationally demanding during deployment. This limits their\napplicability in scenarios with constrained resources or those requiring\nmultiple sensors. Advanced architectures, such as ViT and SSM architectures,\noffer improved modeling capabilities and have made efforts toward lightweight\ndesigns. However, their computational complexity remains relatively high. To\nleverage the strengths of transformer architectures while simultaneously\nenhancing accuracy and reducing computational complexity, this paper introduces\nRadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM\nspecifically tailored for radar-based HAR. Across three diverse datasets,\nRadMamba matches the top-performing previous model's 99.8% classification\naccuracy on Dataset DIAT with only 1/400 of its parameters and equals the\nleading models' 92.0% accuracy on Dataset CI4R with merely 1/10 of their\nparameters. In scenarios with continuous sequences of actions evaluated on\nDataset UoG2020, RadMamba surpasses other models with significantly higher\nparameter counts by at least 3%, achieving this with only 6.7k parameters. Our\ncode is available at: https://github.com/lab-emi/AIRHAR.\n","authors":["Yizhuo Wu","Francesco Fioranelli","Chang Gao"],"pdf_url":"https://arxiv.org/pdf/2504.12039v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2411.17461v3","updated":"2025-04-16T12:51:11Z","published":"2024-11-26T14:28:25Z","title":"SoK: Decentralized AI (DeAI)","summary":"  Centralization enhances the efficiency of Artificial Intelligence (AI), but\nit also brings critical challenges, such as single points of failure, inherent\nbiases, data privacy concerns, and scalability issues, for AI systems. These\nproblems are especially common in closed-source large language models (LLMs),\nwhere user data is collected and used with full transparency. To address these\nissues, blockchain-based decentralized AI (DeAI) has been introduced. DeAI\nleverages the strengths of blockchain technologies to enhance the transparency,\nsecurity, decentralization, as well as trustworthiness of AI systems. Although\nDeAI has been widely developed in industry, a comprehensive understanding of\nstate-of-the-art practical DeAI solutions is still lacking. In this work, we\npresent a Systematization of Knowledge (SoK) for blockchain-based DeAI\nsolutions. We propose a taxonomy to classify existing DeAI protocols based on\nthe model lifecycle. Based on this taxonomy, we provide a structured way to\nclarify the landscape of DeAI protocols and identify their similarities and\ndifferences. Specifically, we analyze the functionalities of blockchain in\nDeAI, investigate how blockchain features contribute to enhancing the security,\ntransparency, and trustworthiness of AI processes, and also ensure fair\nincentives for AI data and model contributors. In addition, we provide key\ninsights and research gaps in developing DeAI protocols for future research.\n","authors":["Zhipeng Wang","Rui Sun","Elizabeth Lui","Vatsal Shah","Xihan Xiong","Jiahao Sun","Davide Crapis","William Knottenbelt"],"pdf_url":"https://arxiv.org/pdf/2411.17461v3.pdf","comment":"This is a Systematization of Knowledge (SoK) for the rapidly evolving\n  field of Decentralized AI (DeAI). We welcome valuable comments, suggestions,\n  and collaboration to further refine and enhance this work. We hope our\n  contribution will help accelerate the advancement of DeAI"},{"id":"http://arxiv.org/abs/2410.15954v3","updated":"2025-04-16T12:39:13Z","published":"2024-10-21T12:34:02Z","title":"TS-ACL: Closed-Form Solution for Time Series-oriented Continual Learning","summary":"  Time series classification underpins critical applications such as healthcare\ndiagnostics and gesture-driven interactive systems in multimedia scenarios.\nHowever, time series class-incremental learning (TSCIL) faces two major\nchallenges: catastrophic forgetting and intra-class variations. Catastrophic\nforgetting occurs because gradient-based parameter update strategies inevitably\nerase past knowledge. And unlike images, time series data exhibits\nsubject-specific patterns, also known as intra-class variations, which refer to\ndifferences in patterns observed within the same class. While exemplar-based\nmethods fail to cover diverse variation with limited samples, existing\nexemplar-free methods lack explicit mechanisms to handle intra-class\nvariations. To address these two challenges, we propose TS-ACL, which leverages\na gradient-free closed-form solution to avoid the catastrophic forgetting\nproblem inherent in gradient-based optimization methods while simultaneously\nlearning global distributions to resolve intra-class variations. Additionally,\nit provides privacy protection and efficiency. Extensive experiments on five\nbenchmark datasets covering various sensor modalities and tasks demonstrate\nthat TS-ACL achieves performance close to joint training on four datasets,\noutperforming existing methods and establishing a new state-of-the-art (SOTA)\nfor TSCIL.\n","authors":["Jiaxu Li","Kejia Fan","Songning Lai","Linpu Lv","Jinfeng Xu","Jianheng Tang","Anfeng Liu","Houbing Herbert Song","Yutao Yue","Yunhuai Liu","Huiping Zhuang"],"pdf_url":"https://arxiv.org/pdf/2410.15954v3.pdf","comment":"12 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2504.12025v1","updated":"2025-04-16T12:32:37Z","published":"2025-04-16T12:32:37Z","title":"FedEPA: Enhancing Personalization and Modality Alignment in Multimodal\n  Federated Learning","summary":"  Federated Learning (FL) enables decentralized model training across multiple\nparties while preserving privacy. However, most FL systems assume clients hold\nonly unimodal data, limiting their real-world applicability, as institutions\noften possess multimodal data. Moreover, the lack of labeled data further\nconstrains the performance of most FL methods. In this work, we propose FedEPA,\na novel FL framework for multimodal learning. FedEPA employs a personalized\nlocal model aggregation strategy that leverages labeled data on clients to\nlearn personalized aggregation weights, thereby alleviating the impact of data\nheterogeneity. We also propose an unsupervised modality alignment strategy that\nworks effectively with limited labeled data. Specifically, we decompose\nmultimodal features into aligned features and context features. We then employ\ncontrastive learning to align the aligned features across modalities, ensure\nthe independence between aligned features and context features within each\nmodality, and promote the diversity of context features. A multimodal feature\nfusion strategy is introduced to obtain a joint embedding. The experimental\nresults show that FedEPA significantly outperforms existing FL methods in\nmultimodal classification tasks under limited labeled data conditions.\n","authors":["Yu Zhang","Qingfeng Du","Jiaqi Lv"],"pdf_url":"https://arxiv.org/pdf/2504.12025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06158v3","updated":"2025-04-16T12:31:41Z","published":"2025-03-08T10:47:27Z","title":"Invariant Federated Learning for Edge Intelligence: Mitigating\n  Heterogeneity and Asynchrony via Exit Strategy and Invariant Penalty","summary":"  This paper provides an invariant federated learning system for\nresource-constrained edge intelligence. This framework can mitigate the impact\nof heterogeneity and asynchrony via exit strategy and invariant penalty. We\nintroduce parameter orthogonality into edge intelligence to measure the\ncontribution or impact of heterogeneous and asynchronous clients. It is proved\nin this paper that the exit of abnormal edge clients can guarantee the effect\nof the model on most clients. Meanwhile, to ensure the models' performance on\nexited abnormal clients and those who lack training resources, we propose\nFederated Learning with Invariant Penalty for Generalization (FedIPG) by\nconstructing the approximate orthogonality of the invariant parameters and the\nheterogeneous parameters. Theoretical proof shows that FedIPG reduces the\nOut-Of-Distribution prediction loss without increasing the communication\nburden. The performance of FedIPG combined with an exit strategy is tested\nempirically in multiple scales using four datasets. It shows our system can\nenhance In-Distribution performance and outperform the state-of-the-art\nalgorithm in Out-Of-Distribution generalization while maintaining model\nconvergence. Additionally, the results of the visual experiment prove that\nFedIPG contains preliminary causality in terms of ignoring confounding\nfeatures.\n","authors":["Ziruo Hao","Zhenhua Cui","Tao Yang","Bo Hu","Xiaofeng Wu","Hui Feng"],"pdf_url":"https://arxiv.org/pdf/2503.06158v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12697v2","updated":"2025-04-16T12:29:45Z","published":"2024-11-19T18:06:06Z","title":"Attribute Inference Attacks for Federated Regression Tasks","summary":"  Federated Learning (FL) enables multiple clients, such as mobile phones and\nIoT devices, to collaboratively train a global machine learning model while\nkeeping their data localized. However, recent studies have revealed that the\ntraining phase of FL is vulnerable to reconstruction attacks, such as attribute\ninference attacks (AIA), where adversaries exploit exchanged messages and\nauxiliary public information to uncover sensitive attributes of targeted\nclients. While these attacks have been extensively studied in the context of\nclassification tasks, their impact on regression tasks remains largely\nunexplored. In this paper, we address this gap by proposing novel model-based\nAIAs specifically designed for regression tasks in FL environments. Our\napproach considers scenarios where adversaries can either eavesdrop on\nexchanged messages or directly interfere with the training process. We\nbenchmark our proposed attacks against state-of-the-art methods using\nreal-world datasets. The results demonstrate a significant increase in\nreconstruction accuracy, particularly in heterogeneous client datasets, a\ncommon scenario in FL. The efficacy of our model-based AIAs makes them better\ncandidates for empirically quantifying privacy leakage for federated regression\ntasks.\n","authors":["Francesco Diana","Othmane Marfoq","Chuan Xu","Giovanni Neglia","Frédéric Giroire","Eoin Thomas"],"pdf_url":"https://arxiv.org/pdf/2411.12697v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11383v2","updated":"2025-04-16T12:26:48Z","published":"2025-04-15T16:54:04Z","title":"Accelerating Multiscale Modeling with Hybrid Solvers: Coupling FEM and\n  Neural Operators with Domain Decomposition","summary":"  Numerical solvers for partial differential equations (PDEs) face challenges\nbalancing computational cost and accuracy, especially in multiscale and dynamic\nsystems. Neural operators can significantly speed up simulations; however, they\noften face challenges such as error accumulation and limited generalization in\nmultiphysics problems. This work introduces a novel hybrid framework that\nintegrates physics-informed DeepONet with FEM through domain decomposition. The\ncore innovation lies in adaptively coupling FEM and DeepONet subdomains via a\nSchwarz alternating method. This methodology strategically allocates\ncomputationally demanding regions to a pre-trained Deep Operator Network, while\nthe remaining computational domain is solved through FEM. To address dynamic\nsystems, we integrate the Newmark time-stepping scheme directly into the\nDeepONet, significantly mitigating error accumulation in long-term simulations.\nFurthermore, an adaptive subdomain evolution enables the ML-resolved region to\nexpand dynamically, capturing emerging fine-scale features without remeshing.\nThe framework's efficacy has been validated across a range of solid mechanics\nproblems, including static, quasi-static, and dynamic regimes, demonstrating\naccelerated convergence rates (up to 20% improvement compared to FE-FE\napproaches), while preserving solution fidelity with error < 1%. Our case\nstudies show that our proposed hybrid solver: (1) maintains solution continuity\nacross subdomain interfaces, (2) reduces computational costs by eliminating\nfine mesh requirements, (3) mitigates error accumulation in time-dependent\nsimulations, and (4) enables automatic adaptation to evolving physical\nphenomena. This work bridges the gap between numerical methods and AI-driven\nsurrogates, offering a scalable pathway for high-fidelity simulations in\nengineering and scientific applications.\n","authors":["Wei Wang","Maryam Hakimzadeh","Haihui Ruan","Somdatta Goswami"],"pdf_url":"https://arxiv.org/pdf/2504.11383v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12016v1","updated":"2025-04-16T12:16:10Z","published":"2025-04-16T12:16:10Z","title":"Active Human Feedback Collection via Neural Contextual Dueling Bandits","summary":"  Collecting human preference feedback is often expensive, leading recent works\nto develop principled algorithms to select them more efficiently. However,\nthese works assume that the underlying reward function is linear, an assumption\nthat does not hold in many real-life applications, such as online\nrecommendation and LLM alignment. To address this limitation, we propose\nNeural-ADB, an algorithm based on the neural contextual dueling bandit\nframework that provides a principled and practical method for collecting human\npreference feedback when the underlying latent reward function is non-linear.\nWe theoretically show that when preference feedback follows the\nBradley-Terry-Luce model, the worst sub-optimality gap of the policy learned by\nNeural-ADB decreases at a sub-linear rate as the preference dataset increases.\nOur experimental results on problem instances derived from synthetic preference\ndatasets further validate the effectiveness of Neural-ADB.\n","authors":["Arun Verma","Xiaoqiang Lin","Zhongxiang Dai","Daniela Rus","Bryan Kian Hsiang Low"],"pdf_url":"https://arxiv.org/pdf/2504.12016v1.pdf","comment":"Accepted at ICLR 2025 Workshop on Bidirectional Human-AI Alignment\n  (BiAlign)"},{"id":"http://arxiv.org/abs/2504.12011v1","updated":"2025-04-16T12:09:56Z","published":"2025-04-16T12:09:56Z","title":"Balancing Graph Embedding Smoothness in Self-Supervised Learning via\n  Information-Theoretic Decomposition","summary":"  Self-supervised learning (SSL) in graphs has garnered significant attention,\nparticularly in employing Graph Neural Networks (GNNs) with pretext tasks\ninitially designed for other domains, such as contrastive learning and feature\nreconstruction. However, it remains uncertain whether these methods effectively\nreflect essential graph properties, precisely representation similarity with\nits neighbors. We observe that existing methods position opposite ends of a\nspectrum driven by the graph embedding smoothness, with each end corresponding\nto outperformance on specific downstream tasks. Decomposing the SSL objective\ninto three terms via an information-theoretic framework with a neighbor\nrepresentation variable reveals that this polarization stems from an imbalance\namong the terms, which existing methods may not effectively maintain. Further\ninsights suggest that balancing between the extremes can lead to improved\nperformance across a wider range of downstream tasks. A framework, BSG\n(Balancing Smoothness in Graph SSL), introduces novel loss functions designed\nto supplement the representation quality in graph-based SSL by balancing the\nderived three terms: neighbor loss, minimal loss, and divergence loss. We\npresent a theoretical analysis of the effects of these loss functions,\nhighlighting their significance from both the SSL and graph smoothness\nperspectives. Extensive experiments on multiple real-world datasets across node\nclassification and link prediction consistently demonstrate that BSG achieves\nstate-of-the-art performance, outperforming existing methods. Our\nimplementation code is available at https://github.com/steve30572/BSG.\n","authors":["Heesoo Jung","Hogun Park"],"pdf_url":"https://arxiv.org/pdf/2504.12011v1.pdf","comment":"Accepted to the Web Conference (WWW) 2025"},{"id":"http://arxiv.org/abs/2504.12005v1","updated":"2025-04-16T11:59:56Z","published":"2025-04-16T11:59:56Z","title":"Voice Conversion with Diverse Intonation using Conditional Variational\n  Auto-Encoder","summary":"  Voice conversion is a task of synthesizing an utterance with target speaker's\nvoice while maintaining linguistic information of the source utterance. While a\nspeaker can produce varying utterances from a single script with different\nintonations, conventional voice conversion models were limited to producing\nonly one result per source input. To overcome this limitation, we propose a\nnovel approach for voice conversion with diverse intonations using conditional\nvariational autoencoder (CVAE). Experiments have shown that the speaker's style\nfeature can be mapped into a latent space with Gaussian distribution. We have\nalso been able to convert voices with more diverse intonation by making the\nposterior of the latent space more complex with inverse autoregressive flow\n(IAF). As a result, the converted voice not only has a diversity of\nintonations, but also has better sound quality than the model without CVAE.\n","authors":["Soobin Suh","Dabi Ahn","Heewoong Park","Jonghun Park"],"pdf_url":"https://arxiv.org/pdf/2504.12005v1.pdf","comment":"2 pages, Machine Learning in Speech and Language Processing Workshop\n  (MLSLP) 2018"},{"id":"http://arxiv.org/abs/2504.12000v1","updated":"2025-04-16T11:51:59Z","published":"2025-04-16T11:51:59Z","title":"Control of Rayleigh-Bénard Convection: Effectiveness of Reinforcement\n  Learning in the Turbulent Regime","summary":"  Data-driven flow control has significant potential for industry, energy\nsystems, and climate science. In this work, we study the effectiveness of\nReinforcement Learning (RL) for reducing convective heat transfer in the 2D\nRayleigh-B\\'enard Convection (RBC) system under increasing turbulence. We\ninvestigate the generalizability of control across varying initial conditions\nand turbulence levels and introduce a reward shaping technique to accelerate\nthe training. RL agents trained via single-agent Proximal Policy Optimization\n(PPO) are compared to linear proportional derivative (PD) controllers from\nclassical control theory. The RL agents reduced convection, measured by the\nNusselt Number, by up to 33% in moderately turbulent systems and 10% in highly\nturbulent settings, clearly outperforming PD control in all settings. The\nagents showed strong generalization performance across different initial\nconditions and to a significant extent, generalized to higher degrees of\nturbulence. The reward shaping improved sample efficiency and consistently\nstabilized the Nusselt Number to higher turbulence levels.\n","authors":["Thorben Markmann","Michiel Straat","Sebastian Peitz","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2504.12000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11997v1","updated":"2025-04-16T11:47:41Z","published":"2025-04-16T11:47:41Z","title":"A Computationally Efficient Algorithm for Infinite-Horizon\n  Average-Reward Linear MDPs","summary":"  We study reinforcement learning in infinite-horizon average-reward settings\nwith linear MDPs. Previous work addresses this problem by approximating the\naverage-reward setting by discounted setting and employing a value\niteration-based algorithm that uses clipping to constrain the span of the value\nfunction for improved statistical efficiency. However, the clipping procedure\nrequires computing the minimum of the value function over the entire state\nspace, which is prohibitive since the state space in linear MDP setting can be\nlarge or even infinite. In this paper, we introduce a value iteration method\nwith efficient clipping operation that only requires computing the minimum of\nvalue functions over the set of states visited by the algorithm. Our algorithm\nenjoys the same regret bound as the previous work while being computationally\nefficient, with computational complexity that is independent of the size of the\nstate space.\n","authors":["Kihyuk Hong","Ambuj Tewari"],"pdf_url":"https://arxiv.org/pdf/2504.11997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17112v2","updated":"2025-04-16T11:44:53Z","published":"2024-07-24T09:23:22Z","title":"Neural Dueling Bandits: Preference-Based Optimization with Human\n  Feedback","summary":"  Contextual dueling bandit is used to model the bandit problems, where a\nlearner's goal is to find the best arm for a given context using observed noisy\nhuman preference feedback over the selected arms for the past contexts.\nHowever, existing algorithms assume the reward function is linear, which can be\ncomplex and non-linear in many real-life applications like online\nrecommendations or ranking web search results. To overcome this challenge, we\nuse a neural network to estimate the reward function using preference feedback\nfor the previously selected arms. We propose upper confidence bound- and\nThompson sampling-based algorithms with sub-linear regret guarantees that\nefficiently select arms in each round. We also extend our theoretical results\nto contextual bandit problems with binary feedback, which is in itself a\nnon-trivial contribution. Experimental results on the problem instances derived\nfrom synthetic datasets corroborate our theoretical results.\n","authors":["Arun Verma","Zhongxiang Dai","Xiaoqiang Lin","Patrick Jaillet","Bryan Kian Hsiang Low"],"pdf_url":"https://arxiv.org/pdf/2407.17112v2.pdf","comment":"Accepted at ICLR 2025. Also, accepted at ICML 2024 Workshop on\n  Foundations of Reinforcement Learning and Control"},{"id":"http://arxiv.org/abs/2504.11992v1","updated":"2025-04-16T11:34:18Z","published":"2025-04-16T11:34:18Z","title":"Analysis of Pseudo-Labeling for Online Source-Free Universal Domain\n  Adaptation","summary":"  A domain (distribution) shift between training and test data often hinders\nthe real-world performance of deep neural networks, necessitating unsupervised\ndomain adaptation (UDA) to bridge this gap. Online source-free UDA has emerged\nas a solution for practical scenarios where access to source data is restricted\nand target data is received as a continuous stream. However, the open-world\nnature of many real-world applications additionally introduces category shifts\nmeaning that the source and target label spaces may differ. Online source-free\nuniversal domain adaptation (SF-UniDA) addresses this challenge. Existing\nmethods mainly rely on self-training with pseudo-labels, yet the relationship\nbetween pseudo-labeling and adaptation outcomes has not been studied yet. To\nbridge this gap, we conduct a systematic analysis through controlled\nexperiments with simulated pseudo-labeling, offering valuable insights into\npseudo-labeling for online SF-UniDA. Our findings reveal a substantial gap\nbetween the current state-of-the-art and the upper bound of adaptation achieved\nwith perfect pseudo-labeling. Moreover, we show that a contrastive loss enables\neffective adaptation even with moderate pseudo-label accuracy, while a\ncross-entropy loss, though less robust to pseudo-label errors, achieves\nsuperior results when pseudo-labeling approaches perfection. Lastly, our\nfindings indicate that pseudo-label accuracy is in general more crucial than\nquantity, suggesting that prioritizing fewer but high-confidence pseudo-labels\nis beneficial. Overall, our study highlights the critical role of\npseudo-labeling in (online) SF-UniDA and provides actionable insights to drive\nfuture advancements in the field. Our code is available at\nhttps://github.com/pascalschlachter/PLAnalysis.\n","authors":["Pascal Schlachter","Jonathan Fuss","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2504.11992v1.pdf","comment":"Submitted to the 33rd European Signal Processing Conference (EUSIPCO\n  2025)"},{"id":"http://arxiv.org/abs/2504.11990v1","updated":"2025-04-16T11:33:03Z","published":"2025-04-16T11:33:03Z","title":"Secure Transfer Learning: Training Clean Models Against Backdoor in\n  (Both) Pre-trained Encoders and Downstream Datasets","summary":"  Transfer learning from pre-trained encoders has become essential in modern\nmachine learning, enabling efficient model adaptation across diverse tasks.\nHowever, this combination of pre-training and downstream adaptation creates an\nexpanded attack surface, exposing models to sophisticated backdoor embeddings\nat both the encoder and dataset levels--an area often overlooked in prior\nresearch. Additionally, the limited computational resources typically available\nto users of pre-trained encoders constrain the effectiveness of generic\nbackdoor defenses compared to end-to-end training from scratch. In this work,\nwe investigate how to mitigate potential backdoor risks in resource-constrained\ntransfer learning scenarios. Specifically, we conduct an exhaustive analysis of\nexisting defense strategies, revealing that many follow a reactive workflow\nbased on assumptions that do not scale to unknown threats, novel attack types,\nor different training paradigms. In response, we introduce a proactive mindset\nfocused on identifying clean elements and propose the Trusted Core (T-Core)\nBootstrapping framework, which emphasizes the importance of pinpointing\ntrustworthy data and neurons to enhance model security. Our empirical\nevaluations demonstrate the effectiveness and superiority of T-Core,\nspecifically assessing 5 encoder poisoning attacks, 7 dataset poisoning\nattacks, and 14 baseline defenses across five benchmark datasets, addressing\nfour scenarios of 3 potential backdoor threats.\n","authors":["Yechao Zhang","Yuxuan Zhou","Tianyu Li","Minghui Li","Shengshan Hu","Wei Luo","Leo Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.11990v1.pdf","comment":"To appear at IEEE Symposium on Security and Privacy 2025, 20 pages"},{"id":"http://arxiv.org/abs/2504.09310v2","updated":"2025-04-16T11:25:54Z","published":"2025-04-12T19:05:00Z","title":"Conformal Calibration: Ensuring the Reliability of Black-Box AI in\n  Wireless Systems","summary":"  AI is poised to revolutionize telecommunication networks by boosting\nefficiency, automation, and decision-making. However, the black-box nature of\nmost AI models introduces substantial risk, possibly deterring adoption by\nnetwork operators. These risks are not addressed by the current prevailing\ndeployment strategy, which typically follows a best-effort train-and-deploy\nparadigm. This paper reviews conformal calibration, a general framework that\nmoves beyond the state of the art by adopting computationally lightweight,\nadvanced statistical tools that offer formal reliability guarantees without\nrequiring further training or fine-tuning. Conformal calibration encompasses\npre-deployment calibration via uncertainty quantification or hyperparameter\nselection; online monitoring to detect and mitigate failures in real time; and\ncounterfactual post-deployment performance analysis to address \"what if\"\ndiagnostic questions after deployment. By weaving conformal calibration into\nthe AI model lifecycle, network operators can establish confidence in black-box\nAI models as a dependable enabling technology for wireless systems.\n","authors":["Osvaldo Simeone","Sangwoo Park","Matteo Zecchin"],"pdf_url":"https://arxiv.org/pdf/2504.09310v2.pdf","comment":"submitted for a journal publication"},{"id":"http://arxiv.org/abs/2504.11982v1","updated":"2025-04-16T11:23:30Z","published":"2025-04-16T11:23:30Z","title":"Efficient identification of linear, parameter-varying, and nonlinear\n  systems with noise models","summary":"  We present a general system identification procedure capable of estimating of\na broad spectrum of state-space dynamical models, including linear\ntime-invariant (LTI), linear parameter-varying} (LPV), and nonlinear (NL)\ndynamics, along with rather general classes of noise models. Similar to the LTI\ncase, we show that for this general class of model structures, including the NL\ncase, the model dynamics can be separated into a deterministic process and a\nstochastic noise part, allowing to seamlessly tune the complexity of the\ncombined model both in terms of nonlinearity and noise modeling. We\nparameterize the involved nonlinear functional relations by means of artificial\nneural-networks (ANNs), although alternative parametric nonlinear mappings can\nalso be used. To estimate the resulting model structures, we optimize a\nprediction-error-based criterion using an efficient combination of a\nconstrained quasi-Newton approach and automatic differentiation, achieving\ntraining times in the order of seconds compared to existing state-of-the-art\nANN methods which may require hours for models of similar complexity. We\nformally establish the consistency guarantees for the proposed approach and\ndemonstrate its superior estimation accuracy and computational efficiency on\nseveral benchmark LTI, LPV, and NL system identification problems.\n","authors":["Alberto Bemporad","Roland Tóth"],"pdf_url":"https://arxiv.org/pdf/2504.11982v1.pdf","comment":"28 pages, 3 figures"},{"id":"http://arxiv.org/abs/2504.11981v1","updated":"2025-04-16T11:22:38Z","published":"2025-04-16T11:22:38Z","title":"Hardware-Friendly Delayed-Feedback Reservoir for Multivariate\n  Time-Series Classification","summary":"  Reservoir computing (RC) is attracting attention as a machine-learning\ntechnique for edge computing. In time-series classification tasks, the number\nof features obtained using a reservoir depends on the length of the input\nseries. Therefore, the features must be converted to a constant-length\nintermediate representation (IR), such that they can be processed by an output\nlayer. Existing conversion methods involve computationally expensive matrix\ninversion that significantly increases the circuit size and requires processing\npower when implemented in hardware. In this article, we propose a simple but\neffective IR, namely, dot-product-based reservoir representation (DPRR), for RC\nbased on the dot product of data features. Additionally, we propose a\nhardware-friendly delayed-feedback reservoir (DFR) consisting of a nonlinear\nelement and delayed feedback loop with DPRR. The proposed DFR successfully\nclassified multivariate time series data that has been considered particularly\ndifficult to implement efficiently in hardware. In contrast to conventional DFR\nmodels that require analog circuits, the proposed model can be implemented in a\nfully digital manner suitable for high-level syntheses. A comparison with\nexisting machine-learning methods via field-programmable gate array\nimplementation using 12 multivariate time-series classification tasks confirmed\nthe superior accuracy and small circuit size of the proposed method.\n","authors":["Sosei Ikeda","Hiromitsu Awano","Takashi Sato"],"pdf_url":"https://arxiv.org/pdf/2504.11981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12783v3","updated":"2025-04-16T11:16:31Z","published":"2024-12-17T10:43:26Z","title":"Noise-based Local Learning using Stochastic Magnetic Tunnel Junctions","summary":"  Brain-inspired learning in physical hardware has enormous potential to learn\nfast at minimal energy expenditure. One of the characteristics of biological\nlearning systems is their ability to learn in the presence of various noise\nsources. Inspired by this observation, we introduce a novel noise-based\nlearning approach for physical systems implementing multi-layer neural\nnetworks. Simulation results show that our approach allows for effective\nlearning whose performance approaches that of the conventional effective yet\nenergy-costly backpropagation algorithm. Using a spintronics hardware\nimplementation, we demonstrate experimentally that learning can be achieved in\na small network composed of physical stochastic magnetic tunnel junctions.\nThese results provide a path towards efficient learning in general physical\nsystems which embraces rather than mitigates the noise inherent in physical\ndevices.\n","authors":["Kees Koenders","Leo Schnitzpan","Fabian Kammerbauer","Sinan Shu","Gerhard Jakob","Mathis Kläui","Johan Mentink","Nasir Ahmad","Marcel van Gerven"],"pdf_url":"https://arxiv.org/pdf/2412.12783v3.pdf","comment":"20 pages, 5 figures, submitted to Physical Review Applied"},{"id":"http://arxiv.org/abs/2504.11966v1","updated":"2025-04-16T10:55:13Z","published":"2025-04-16T10:55:13Z","title":"Exploring Video-Based Driver Activity Recognition under Noisy Labels","summary":"  As an open research topic in the field of deep learning, learning with noisy\nlabels has attracted much attention and grown rapidly over the past ten years.\nLearning with label noise is crucial for driver distraction behavior\nrecognition, as real-world video data often contains mislabeled samples,\nimpacting model reliability and performance. However, label noise learning is\nbarely explored in the driver activity recognition field. In this paper, we\npropose the first label noise learning approach for the driver activity\nrecognition task. Based on the cluster assumption, we initially enable the\nmodel to learn clustering-friendly low-dimensional representations from given\nvideos and assign the resultant embeddings into clusters. We subsequently\nperform co-refinement within each cluster to smooth the classifier outputs.\nFurthermore, we propose a flexible sample selection strategy that combines two\nselection criteria without relying on any hyperparameters to filter clean\nsamples from the training dataset. We also incorporate a self-adaptive\nparameter into the sample selection process to enforce balancing across\nclasses. A comprehensive variety of experiments on the public Drive&Act dataset\nfor all granularity levels demonstrates the superior performance of our method\nin comparison with other label-denoising methods derived from the image\nclassification field. The source code is available at\nhttps://github.com/ilonafan/DAR-noisy-labels.\n","authors":["Linjuan Fan","Di Wen","Kunyu Peng","Kailun Yang","Jiaming Zhang","Ruiping Liu","Yufan Chen","Junwei Zheng","Jiamin Wu","Xudong Han","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2504.11966v1.pdf","comment":"The source code is available at\n  https://github.com/ilonafan/DAR-noisy-labels"},{"id":"http://arxiv.org/abs/2502.05151v2","updated":"2025-04-16T10:54:12Z","published":"2025-02-07T18:26:45Z","title":"Transforming Science with Large Language Models: A Survey on AI-assisted\n  Scientific Discovery, Experimentation, Content Generation, and Evaluation","summary":"  With the advent of large multimodal language models, science is now at a\nthreshold of an AI-based technological transformation. Recently, a plethora of\nnew AI models and tools has been proposed, promising to empower researchers and\nacademics worldwide to conduct their research more effectively and efficiently.\nThis includes all aspects of the research cycle, especially (1) searching for\nrelevant literature; (2) generating research ideas and conducting\nexperimentation; generating (3) text-based and (4) multimodal content (e.g.,\nscientific figures and diagrams); and (5) AI-based automatic peer review. In\nthis survey, we provide an in-depth overview over these exciting recent\ndevelopments, which promise to fundamentally alter the scientific research\nprocess for good. Our survey covers the five aspects outlined above, indicating\nrelevant datasets, methods and results (including evaluation) as well as\nlimitations and scope for future research. Ethical concerns regarding\nshortcomings of these tools and potential for misuse (fake science, plagiarism,\nharms to research integrity) take a particularly prominent place in our\ndiscussion. We hope that our survey will not only become a reference guide for\nnewcomers to the field but also a catalyst for new AI-based initiatives in the\narea of \"AI4Science\".\n","authors":["Steffen Eger","Yong Cao","Jennifer D'Souza","Andreas Geiger","Christian Greisinger","Stephanie Gross","Yufang Hou","Brigitte Krenn","Anne Lauscher","Yizhi Li","Chenghua Lin","Nafise Sadat Moosavi","Wei Zhao","Tristan Miller"],"pdf_url":"https://arxiv.org/pdf/2502.05151v2.pdf","comment":"44 pages, 7 figures, 8 tables"},{"id":"http://arxiv.org/abs/2404.07983v3","updated":"2025-04-16T10:50:18Z","published":"2024-04-11T17:58:06Z","title":"Two Effects, One Trigger: On the Modality Gap, Object Bias, and\n  Information Imbalance in Contrastive Vision-Language Models","summary":"  Contrastive vision-language models (VLMs), like CLIP, have gained popularity\nfor their versatile applicability to various downstream tasks. Despite their\nsuccesses in some tasks, like zero-shot object recognition, they perform\nsurprisingly poor on other tasks, like attribute recognition. Previous work has\nattributed these challenges to the modality gap, a separation of image and text\nin the shared representation space, and to a bias towards objects over other\nfactors, such as attributes. In this analysis paper, we investigate both\nphenomena thoroughly. We evaluated off-the-shelf VLMs and while the gap's\ninfluence on performance is typically overshadowed by other factors, we find\nindications that closing the gap indeed leads to improvements. Moreover, we\nfind that, contrary to intuition, only few embedding dimensions drive the gap\nand that the embedding spaces are differently organized. To allow for a clean\nstudy of object bias, we introduce a definition and a corresponding measure of\nit. Equipped with this tool, we find that object bias does not lead to worse\nperformance on other concepts, such as attributes per se. However, why do both\nphenomena, modality gap and object bias, emerge in the first place? To answer\nthis fundamental question and uncover some of the inner workings of contrastive\nVLMs, we conducted experiments that allowed us to control the amount of shared\ninformation between the modalities. These experiments revealed that the driving\nfactor behind both the modality gap and the object bias, is an information\nimbalance between images and captions, and unveiled an intriguing connection\nbetween the modality gap and entropy of the logits.\n","authors":["Simon Schrodi","David T. Hoffmann","Max Argus","Volker Fischer","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2404.07983v3.pdf","comment":"ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2410.00078v2","updated":"2025-04-16T10:42:51Z","published":"2024-09-30T16:26:40Z","title":"Shuffled Linear Regression via Spectral Matching","summary":"  Shuffled linear regression (SLR) seeks to estimate latent features through a\nlinear transformation, complicated by unknown permutations in the measurement\ndimensions. This problem extends traditional least-squares (LS) and Least\nAbsolute Shrinkage and Selection Operator (LASSO) approaches by jointly\nestimating the permutation, resulting in shuffled LS and shuffled LASSO\nformulations. Existing methods, constrained by the combinatorial complexity of\npermutation recovery, often address small-scale cases with limited\nmeasurements. In contrast, we focus on large-scale SLR, particularly suited for\nenvironments with abundant measurement samples. We propose a spectral matching\nmethod that efficiently resolves permutations by aligning spectral components\nof the measurement and feature covariances. Rigorous theoretical analyses\ndemonstrate that our method achieves accurate estimates in both shuffled LS and\nshuffled LASSO settings, given a sufficient number of samples. Furthermore, we\nextend our approach to address simultaneous pose and correspondence estimation\nin image registration tasks. Experiments on synthetic datasets and real-world\nimage registration scenarios show that our method outperforms existing\nalgorithms in both estimation accuracy and registration performance.\n","authors":["Hang Liu","Anna Scaglione"],"pdf_url":"https://arxiv.org/pdf/2410.00078v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2309.14073v3","updated":"2025-04-16T10:42:17Z","published":"2023-09-25T12:07:00Z","title":"Neural Network Parameter-optimization of Gaussian pmDAGs","summary":"  Finding the parameters of a latent variable causal model is central to causal\ninference and causal identification. In this article, we show that existing\ngraphical structures that are used in causal inference are not stable under\nmarginalization of Gaussian Bayesian networks, and present a graphical\nstructure that faithfully represent margins of Gaussian Bayesian networks. We\npresent the first duality between parameter optimization of a latent variable\nmodel and training a feed-forward neural network in the parameter space of the\nassumed family of distributions. Based on this observation, we develop an\nalgorithm for parameter optimization of these graphical structures based on a\ngiven observational distribution. Then, we provide conditions for causal effect\nidentifiability in the Gaussian setting. We propose an meta-algorithm that\nchecks whether a causal effect is identifiable or not. Moreover, we lay a\ngrounding for generalizing the duality between a neural network and a causal\nmodel from the Gaussian to other distributions.\n","authors":["Mehrzad Saremi"],"pdf_url":"https://arxiv.org/pdf/2309.14073v3.pdf","comment":"48 pages"},{"id":"http://arxiv.org/abs/2503.12793v3","updated":"2025-04-16T10:39:20Z","published":"2025-03-17T04:01:37Z","title":"Improving Generalization of Universal Adversarial Perturbation via\n  Dynamic Maximin Optimization","summary":"  Deep neural networks (DNNs) are susceptible to universal adversarial\nperturbations (UAPs). These perturbations are meticulously designed to fool the\ntarget model universally across all sample classes. Unlike instance-specific\nadversarial examples (AEs), generating UAPs is more complex because they must\nbe generalized across a wide range of data samples and models. Our research\nreveals that existing universal attack methods, which optimize UAPs using DNNs\nwith static model parameter snapshots, do not fully leverage the potential of\nDNNs to generate more effective UAPs. Rather than optimizing UAPs against\nstatic DNN models with a fixed training set, we suggest using dynamic\nmodel-data pairs to generate UAPs. In particular, we introduce a dynamic\nmaximin optimization strategy, aiming to optimize the UAP across a variety of\noptimal model-data pairs. We term this approach DM-UAP. DM-UAP utilizes an\niterative max-min-min optimization framework that refines the model-data pairs,\ncoupled with a curriculum UAP learning algorithm to examine the combined space\nof model parameters and data thoroughly. Comprehensive experiments on the\nImageNet dataset demonstrate that the proposed DM-UAP markedly enhances both\ncross-sample universality and cross-model transferability of UAPs. Using only\n500 samples for UAP generation, DM-UAP outperforms the state-of-the-art\napproach with an average increase in fooling ratio of 12.108%.\n","authors":["Yechao Zhang","Yingzhe Xu","Junyu Shi","Leo Yu Zhang","Shengshan Hu","Minghui Li","Yanjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.12793v3.pdf","comment":"Accepted in AAAI 2025"},{"id":"http://arxiv.org/abs/2504.11952v1","updated":"2025-04-16T10:29:30Z","published":"2025-04-16T10:29:30Z","title":"Robust and Fine-Grained Detection of AI Generated Texts","summary":"  An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.\n","authors":["Ram Mohan Rao Kadiyala","Siddartha Pullakhandam","Kanwal Mehreen","Drishti Sharma","Siddhant Gupta","Jebish Purbey","Ashay Srivastava","Subhasya TippaReddy","Arvind Reddy Bobbili","Suraj Telugara Chandrashekhar","Modabbir Adeeb","Srinadh Vura","Hamza Farooq"],"pdf_url":"https://arxiv.org/pdf/2504.11952v1.pdf","comment":"ACL 2025 Feb ARR Submission"},{"id":"http://arxiv.org/abs/2504.11944v1","updated":"2025-04-16T10:23:44Z","published":"2025-04-16T10:23:44Z","title":"VIPO: Value Function Inconsistency Penalized Offline Reinforcement\n  Learning","summary":"  Offline reinforcement learning (RL) learns effective policies from\npre-collected datasets, offering a practical solution for applications where\nonline interactions are risky or costly. Model-based approaches are\nparticularly advantageous for offline RL, owing to their data efficiency and\ngeneralizability. However, due to inherent model errors, model-based methods\noften artificially introduce conservatism guided by heuristic uncertainty\nestimation, which can be unreliable. In this paper, we introduce VIPO, a novel\nmodel-based offline RL algorithm that incorporates self-supervised feedback\nfrom value estimation to enhance model training. Specifically, the model is\nlearned by additionally minimizing the inconsistency between the value learned\ndirectly from the offline data and the one estimated from the model. We perform\ncomprehensive evaluations from multiple perspectives to show that VIPO can\nlearn a highly accurate model efficiently and consistently outperform existing\nmethods. It offers a general framework that can be readily integrated into\nexisting model-based offline RL algorithms to systematically enhance model\naccuracy. As a result, VIPO achieves state-of-the-art performance on almost all\ntasks in both D4RL and NeoRL benchmarks.\n","authors":["Xuyang Chen","Guojian Wang","Keyu Yan","Lin Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.11944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19935v3","updated":"2025-04-16T10:17:33Z","published":"2025-02-27T10:04:36Z","title":"Lotus at SemEval-2025 Task 11: RoBERTa with Llama-3 Generated\n  Explanations for Multi-Label Emotion Classification","summary":"  This paper presents a novel approach for multi-label emotion detection, where\nLlama-3 is used to generate explanatory content that clarifies ambiguous\nemotional expressions, thereby enhancing RoBERTa's emotion classification\nperformance. By incorporating explanatory context, our method improves\nF1-scores, particularly for emotions like fear, joy, and sadness, and\noutperforms text-only models. The addition of explanatory content helps resolve\nambiguity, addresses challenges like overlapping emotional cues, and enhances\nmulti-label classification, marking a significant advancement in emotion\ndetection tasks.\n","authors":["Niloofar Ranjbar","Hamed Baghbani"],"pdf_url":"https://arxiv.org/pdf/2502.19935v3.pdf","comment":"8 pages , submitted to SemEval 2025-Task 11"},{"id":"http://arxiv.org/abs/2504.11923v1","updated":"2025-04-16T09:58:04Z","published":"2025-04-16T09:58:04Z","title":"SemDiff: Generating Natural Unrestricted Adversarial Examples via\n  Semantic Attributes Optimization in Diffusion Models","summary":"  Unrestricted adversarial examples (UAEs), allow the attacker to create\nnon-constrained adversarial examples without given clean samples, posing a\nsevere threat to the safety of deep learning models. Recent works utilize\ndiffusion models to generate UAEs. However, these UAEs often lack naturalness\nand imperceptibility due to simply optimizing in intermediate latent noises. In\nlight of this, we propose SemDiff, a novel unrestricted adversarial attack that\nexplores the semantic latent space of diffusion models for meaningful\nattributes, and devises a multi-attributes optimization approach to ensure\nattack success while maintaining the naturalness and imperceptibility of\ngenerated UAEs. We perform extensive experiments on four tasks on three\nhigh-resolution datasets, including CelebA-HQ, AFHQ and ImageNet. The results\ndemonstrate that SemDiff outperforms state-of-the-art methods in terms of\nattack success rate and imperceptibility. The generated UAEs are natural and\nexhibit semantically meaningful changes, in accord with the attributes'\nweights. In addition, SemDiff is found capable of evading different defenses,\nwhich further validates its effectiveness and threatening.\n","authors":["Zeyu Dai","Shengcai Liu","Rui He","Jiahao Wu","Ning Lu","Wenqi Fan","Qing Li","Ke Tang"],"pdf_url":"https://arxiv.org/pdf/2504.11923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01329v2","updated":"2025-04-16T09:54:20Z","published":"2025-03-03T09:12:14Z","title":"Neural ODE Transformers: Analyzing Internal Dynamics and Adaptive\n  Fine-tuning","summary":"  Recent advancements in large language models (LLMs) based on transformer\narchitectures have sparked significant interest in understanding their inner\nworkings. In this paper, we introduce a novel approach to modeling transformer\narchitectures using highly flexible non-autonomous neural ordinary differential\nequations (ODEs). Our proposed model parameterizes all weights of attention and\nfeed-forward blocks through neural networks, expressing these weights as\nfunctions of a continuous layer index. Through spectral analysis of the model's\ndynamics, we uncover an increase in eigenvalue magnitude that challenges the\nweight-sharing assumption prevalent in existing theoretical studies. We also\nleverage the Lyapunov exponent to examine token-level sensitivity, enhancing\nmodel interpretability. Our neural ODE transformer demonstrates performance\ncomparable to or better than vanilla transformers across various configurations\nand datasets, while offering flexible fine-tuning capabilities that can adapt\nto different architectural constraints.\n","authors":["Anh Tong","Thanh Nguyen-Tang","Dongeun Lee","Duc Nguyen","Toan Tran","David Hall","Cheongwoong Kang","Jaesik Choi"],"pdf_url":"https://arxiv.org/pdf/2503.01329v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2411.07826v2","updated":"2025-04-16T09:51:33Z","published":"2024-11-12T14:22:16Z","title":"Efficient Federated Finetuning of Tiny Transformers with\n  Resource-Constrained Devices","summary":"  In recent years, Large Language Models (LLMs) through Transformer structures\nhave dominated many machine learning tasks, especially text processing.\nHowever, these models require massive amounts of data for training and induce\nhigh resource requirements, particularly in terms of the large number of\nFloating Point Operations (FLOPs) and the high amounts of memory needed. To\nfine-tune such a model in a parameter-efficient way, techniques like Adapter or\nLoRA have been developed. However, we observe that the application of LoRA,\nwhen used in federated learning (FL), while still being parameter-efficient, is\nmemory and FLOP inefficient. Based on that observation, we develop a novel\nlayer finetuning scheme that allows devices in cross-device FL to make use of\npretrained neural networks (NNs) while adhering to given resource constraints.\nWe show that our presented scheme outperforms the current state of the art when\ndealing with homogeneous or heterogeneous computation and memory constraints\nand is on par with LoRA regarding limited communication, thereby achieving\nsignificantly higher accuracies in FL training.\n","authors":["Kilian Pfeiffer","Mohamed Aboelenien Ahmed","Ramin Khalili","Jörg Henkel"],"pdf_url":"https://arxiv.org/pdf/2411.07826v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07157v3","updated":"2025-04-16T09:41:16Z","published":"2025-04-09T11:19:42Z","title":"GAAPO: Genetic Algorithmic Applied to Prompt Optimization","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, with their performance heavily dependent on the quality of input\nprompts. While prompt engineering has proven effective, it typically relies on\nmanual adjustments, making it time-consuming and potentially suboptimal. This\npaper introduces GAAPO (Genetic Algorithm Applied to Prompt Optimization), a\nnovel hybrid optimization framework that leverages genetic algorithm principles\nto evolve prompts through successive generations. Unlike traditional genetic\napproaches that rely solely on mutation and crossover operations, GAAPO\nintegrates multiple specialized prompt generation strategies within its\nevolutionary framework. Through extensive experimentation on diverse datasets\nincluding ETHOS, MMLU-Pro, and GPQA, our analysis reveals several important\npoint for the future development of automatic prompt optimization methods:\nimportance of the tradeoff between the population size and the number of\ngenerations, effect of selection methods on stability results, capacity of\ndifferent LLMs and especially reasoning models to be able to automatically\ngenerate prompts from similar queries... Furthermore, we provide insights into\nthe relative effectiveness of different prompt generation strategies and their\nevolution across optimization phases. These findings contribute to both the\ntheoretical understanding of prompt optimization and practical applications in\nimproving LLM performance.\n","authors":["Xavier Sécheresse","Jacques-Yves Guilbert--Ly","Antoine Villedieu de Torcy"],"pdf_url":"https://arxiv.org/pdf/2504.07157v3.pdf","comment":"26 pages, 9 figures"},{"id":"http://arxiv.org/abs/2504.11903v1","updated":"2025-04-16T09:28:26Z","published":"2025-04-16T09:28:26Z","title":"FedCanon: Non-Convex Composite Federated Learning with Efficient\n  Proximal Operation on Heterogeneous Data","summary":"  Composite federated learning offers a general framework for solving machine\nlearning problems with additional regularization terms. However, many existing\nmethods require clients to perform multiple proximal operations to handle\nnon-smooth terms and their performance are often susceptible to data\nheterogeneity. To overcome these limitations, we propose a novel composite\nfederated learning algorithm called \\textbf{FedCanon}, designed to solve the\noptimization problems comprising a possibly non-convex loss function and a\nweakly convex, potentially non-smooth regularization term. By decoupling\nproximal mappings from local updates, FedCanon requires only a single proximal\nevaluation on the server per iteration, thereby reducing the overall proximal\ncomputation cost. It also introduces control variables that incorporate global\ngradient information into client updates, which helps mitigate the effects of\ndata heterogeneity. Theoretical analysis demonstrates that FedCanon achieves\nsublinear convergence rates under general non-convex settings and linear\nconvergence under the Polyak-{\\L}ojasiewicz condition, without relying on\nbounded heterogeneity assumptions. Experiments demonstrate that FedCanon\noutperforms the state-of-the-art methods in terms of both accuracy and\ncomputational efficiency, particularly under heterogeneous data distributions.\n","authors":["Yuan Zhou","Jiachen Zhong","Xinli Shi","Guanghui Wen","Xinghuo Yu"],"pdf_url":"https://arxiv.org/pdf/2504.11903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10504v4","updated":"2025-04-16T09:25:06Z","published":"2024-02-16T08:27:55Z","title":"Resilience of Rademacher chaos of low degree","summary":"  The resilience of a Rademacher chaos is the maximum number of adversarial\nsign-flips that the chaos can sustain without having its largest atom\nprobability significantly altered. Inspired by probabilistic lower-bound\nguarantees for the resilience of linear Rademacher chaos, obtained by Bandeira,\nFerber, and Kwan (Advances in Mathematics, Vol. $319$, $2017$), we provide\nprobabilistic lower-bound guarantees for the resilience of Rademacher chaos of\narbitrary yet sufficiently low degree.\n  Our main results distinguish between Rademacher chaos of order two and those\nof higher order. In that, our first main result pertains to the resilience of\ndecoupled bilinear Rademacher forms where different asymptotic behaviour is\nobserved for sparse and dense matrices. For our second main result, we\nbootstrap our first result in order to provide resilience guarantees for\nquadratic Rademacher chaos. Our third main result, generalises the first and\nhandles the resilience of decoupled Rademacher chaos of arbitrary yet\nsufficiently low order.\n  Our results for decoupled Rademacher chaos of order two and that of higher\norder whilst are established through the same conceptual framework, differ\nsubstantially. A difference incurred due to the implementation of the same\nconceptual argument. The order two result is established using Dudley's maximal\ninequality for sub-Gaussian processes, the Hanson-Wright inequality, as well as\nthe Kolmogorov-Rogozin inequality. To handle higher order chaos, appeals to\nDudley's inequality as well as the Hanson-Wright inequality are replaced with\ntools suited for random tensors. Appeals to the Hanson-Wright inequality are\nreplaced with appeals to a concentration result for random tensors put forth by\nAdamczak and Wolff.\n  Our results are instance-dependent and thus allow for the efficient\ncomputation of resilience guarantees provided the order of the chaos is\nconstant.\n","authors":["Elad Aigner-Horev","Daniel Rosenberg","Roi Weiss"],"pdf_url":"https://arxiv.org/pdf/2402.10504v4.pdf","comment":"Small corrections from previous version"},{"id":"http://arxiv.org/abs/2504.11885v1","updated":"2025-04-16T09:11:16Z","published":"2025-04-16T09:11:16Z","title":"HyperSAT: Unsupervised Hypergraph Neural Networks for Weighted MaxSAT\n  Problems","summary":"  Graph neural networks (GNNs) have shown promising performance in solving both\nBoolean satisfiability (SAT) and Maximum Satisfiability (MaxSAT) problems due\nto their ability to efficiently model and capture the structural dependencies\nbetween literals and clauses. However, GNN methods for solving Weighted MaxSAT\nproblems remain underdeveloped. The challenges arise from the non-linear\ndependency and sensitive objective function, which are caused by the\nnon-uniform distribution of weights across clauses. In this paper, we present\nHyperSAT, a novel neural approach that employs an unsupervised hypergraph\nneural network model to solve Weighted MaxSAT problems. We propose a hypergraph\nrepresentation for Weighted MaxSAT instances and design a cross-attention\nmechanism along with a shared representation constraint loss function to\ncapture the logical interactions between positive and negative literal nodes in\nthe hypergraph. Extensive experiments on various Weighted MaxSAT datasets\ndemonstrate that HyperSAT achieves better performance than state-of-the-art\ncompetitors.\n","authors":["Qiyue Chen","Shaolin Tan","Suixiang Gao","Jinhu Lü"],"pdf_url":"https://arxiv.org/pdf/2504.11885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11877v1","updated":"2025-04-16T08:58:44Z","published":"2025-04-16T08:58:44Z","title":"Benchmarking Mutual Information-based Loss Functions in Federated\n  Learning","summary":"  Federated Learning (FL) has attracted considerable interest due to growing\nprivacy concerns and regulations like the General Data Protection Regulation\n(GDPR), which stresses the importance of privacy-preserving and fair machine\nlearning approaches. In FL, model training takes place on decentralized data,\nso as to allow clients to upload a locally trained model and receive a globally\naggregated model without exposing sensitive information. However, challenges\nrelated to fairness-such as biases, uneven performance among clients, and the\n\"free rider\" issue complicates its adoption. In this paper, we examine the use\nof Mutual Information (MI)-based loss functions to address these concerns. MI\nhas proven to be a powerful method for measuring dependencies between variables\nand optimizing deep learning models. By leveraging MI to extract essential\nfeatures and minimize biases, we aim to improve both the fairness and\neffectiveness of FL systems. Through extensive benchmarking, we assess the\nimpact of MI-based losses in reducing disparities among clients while enhancing\nthe overall performance of FL.\n","authors":["Sarang S","Harsh D. Chothani","Qilei Li","Ahmed M. Abdelmoniem","Arnab K. Paul"],"pdf_url":"https://arxiv.org/pdf/2504.11877v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.05602v2","updated":"2025-04-16T08:57:18Z","published":"2025-03-07T17:28:02Z","title":"On the similarity of bandwidth-tuned quantum kernels and classical\n  kernels","summary":"  Quantum kernels (QK) are widely used in quantum machine learning\napplications; yet, their potential to surpass classical machine learning\nmethods on classical datasets remains uncertain. This limitation can be\nattributed to the exponential concentration phenomenon, which can impair both\ntrainability and generalization. A common strategy to alleviate this is\nbandwidth tuning, which involves rescaling data points in the quantum model to\nimprove generalization. In this work, we numerically demonstrate that optimal\nbandwidth tuning results in QKs that closely resemble radial basis function\n(RBF) kernels, leading to a lack of quantum advantage over classical methods.\nMoreover, we reveal that the size of optimal bandwidth tuning parameters\nfurther simplifies QKs, causing them to behave like polynomial kernels,\ncorresponding to a low-order Taylor approximation of a RBF kernel. We\nthoroughly investigate this for fidelity quantum kernels and projected quantum\nkernels using various data encoding circuits across several classification\ndatasets. We provide numerical evidence and derive a simple analytical model\nthat elucidates how bandwidth tuning influences key quantities in\nclassification tasks. Overall, our findings shed light on the mechanisms that\nrender QK methods classically simulatable.\n","authors":["Roberto Flórez-Ablan","Marco Roth","Jan Schnabel"],"pdf_url":"https://arxiv.org/pdf/2503.05602v2.pdf","comment":"9 main pages with 5 figures, and 9 appendix pages with 12 figures.\n  Added reference to GitHub where code for reproduction is availabe; corrected\n  typos"},{"id":"http://arxiv.org/abs/2504.11874v1","updated":"2025-04-16T08:51:09Z","published":"2025-04-16T08:51:09Z","title":"Factor-MCLS: Multi-agent learning system with reward factor matrix and\n  multi-critic framework for dynamic portfolio optimization","summary":"  Typical deep reinforcement learning (DRL) agents for dynamic portfolio\noptimization learn the factors influencing portfolio return and risk by\nanalyzing the output values of the reward function while adjusting portfolio\nweights within the training environment. However, it faces a major limitation\nwhere it is difficult for investors to intervene in the training based on\ndifferent levels of risk aversion towards each portfolio asset. This difficulty\narises from another limitation: existing DRL agents may not develop a thorough\nunderstanding of the factors responsible for the portfolio return and risk by\nonly learning from the output of the reward function. As a result, the strategy\nfor determining the target portfolio weights is entirely dependent on the DRL\nagents themselves. To address these limitations, we propose a reward factor\nmatrix for elucidating the return and risk of each asset in the portfolio.\nAdditionally, we propose a novel learning system named Factor-MCLS using a\nmulti-critic framework that facilitates learning of the reward factor matrix.\nIn this way, our DRL-based learning system can effectively learn the factors\ninfluencing portfolio return and risk. Moreover, based on the critic networks\nwithin the multi-critic framework, we develop a risk constraint term in the\ntraining objective function of the policy function. This risk constraint term\nallows investors to intervene in the training of the DRL agent according to\ntheir individual levels of risk aversion towards the portfolio assets.\n","authors":["Ruoyu Sun","Angelos Stefanidis","Zhengyong Jiang","Jionglong Su"],"pdf_url":"https://arxiv.org/pdf/2504.11874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11170v2","updated":"2025-04-16T08:50:55Z","published":"2025-04-15T13:17:14Z","title":"A Real-time Anomaly Detection Method for Robots based on a Flexible and\n  Sparse Latent Space","summary":"  The growing demand for robots to operate effectively in diverse environments\nnecessitates the need for robust real-time anomaly detection techniques during\nrobotic operations. However, deep learning-based models in robotics face\nsignificant challenges due to limited training data and highly noisy signal\nfeatures. In this paper, we present Sparse Masked Autoregressive Flow-based\nAdversarial AutoEncoders model to address these problems. This approach\nintegrates Masked Autoregressive Flow model into Adversarial AutoEncoders to\nconstruct a flexible latent space and utilize Sparse autoencoder to efficiently\nfocus on important features, even in scenarios with limited feature space. Our\nexperiments demonstrate that the proposed model achieves a 4.96% to 9.75%\nhigher area under the receiver operating characteristic curve for\npick-and-place robotic operations with randomly placed cans, compared to\nexisting state-of-the-art methods. Notably, it showed up to 19.67% better\nperformance in scenarios involving collisions with lightweight objects.\nAdditionally, unlike the existing state-of-the-art model, our model performs\ninferences within 1 millisecond, ensuring real-time anomaly detection. These\ncapabilities make our model highly applicable to machine learning-based robotic\nsafety systems in dynamic environments. The code will be made publicly\navailable after acceptance.\n","authors":["Taewook Kang","Bum-Jae You","Juyoun Park","Yisoo Lee"],"pdf_url":"https://arxiv.org/pdf/2504.11170v2.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2504.11873v1","updated":"2025-04-16T08:50:51Z","published":"2025-04-16T08:50:51Z","title":"Transferable Deployment of Semantic Edge Inference Systems via\n  Unsupervised Domain Adaption","summary":"  This paper investigates deploying semantic edge inference systems for\nperforming a common image clarification task. In particular, each system\nconsists of multiple Internet of Things (IoT) devices that first locally encode\nthe sensing data into semantic features and then transmit them to an edge\nserver for subsequent data fusion and task inference. The inference accuracy is\ndetermined by efficient training of the feature encoder/decoder using labeled\ndata samples. Due to the difference in sensing data and communication channel\ndistributions, deploying the system in a new environment may induce high costs\nin annotating data labels and re-training the encoder/decoder models. To\nachieve cost-effective transferable system deployment, we propose an efficient\nDomain Adaptation method for Semantic Edge INference systems (DASEIN) that can\nmaintain high inference accuracy in a new environment without the need for\nlabeled samples. Specifically, DASEIN exploits the task-relevant data\ncorrelation between different deployment scenarios by leveraging the techniques\nof unsupervised domain adaptation and knowledge distillation. It devises an\nefficient two-step adaptation procedure that sequentially aligns the data\ndistributions and adapts to the channel variations. Numerical results show\nthat, under a substantial change in sensing data distributions, the proposed\nDASEIN outperforms the best-performing benchmark method by 7.09% and 21.33% in\ninference accuracy when the new environment has similar or 25 dB lower channel\nsignal to noise power ratios (SNRs), respectively. This verifies the\neffectiveness of the proposed method in adapting both data and channel\ndistributions in practical transfer deployment applications.\n","authors":["Weiqiang Jiao","Suzhi Bi","Xian Li","Cheng Guo","Hao Chen","Zhi Quan"],"pdf_url":"https://arxiv.org/pdf/2504.11873v1.pdf","comment":"14 pages, 14 figures, the paper is submitted for potential journal\n  publication"},{"id":"http://arxiv.org/abs/2504.11074v2","updated":"2025-04-16T08:48:34Z","published":"2025-04-15T11:16:13Z","title":"Dynamical errors in machine learning forecasts","summary":"  In machine learning forecasting, standard error metrics such as mean absolute\nerror (MAE) and mean squared error (MSE) quantify discrepancies between\npredictions and target values. However, these metrics do not directly evaluate\nthe physical and/or dynamical consistency of forecasts, an increasingly\ncritical concern in scientific and engineering applications.\n  Indeed, a fundamental yet often overlooked question is whether machine\nlearning forecasts preserve the dynamical behavior of the underlying system.\nAddressing this issue is essential for assessing the fidelity of machine\nlearning models and identifying potential failure modes, particularly in\napplications where maintaining correct dynamical behavior is crucial.\n  In this work, we investigate the relationship between standard forecasting\nerror metrics, such as MAE and MSE, and the dynamical properties of the\nunderlying system. To achieve this goal, we use two recently developed\ndynamical indices: the instantaneous dimension ($d$), and the inverse\npersistence ($\\theta$). Our results indicate that larger forecast errors --\ne.g., higher MSE -- tend to occur in states with higher $d$ (higher complexity)\nand higher $\\theta$ (lower persistence). To further assess dynamical\nconsistency, we propose error metrics based on the dynamical indices that\nmeasure the discrepancy of the forecasted $d$ and $\\theta$ versus their correct\nvalues. Leveraging these dynamical indices-based metrics, we analyze direct and\nrecursive forecasting strategies for three canonical datasets -- Lorenz,\nKuramoto-Sivashinsky equation, and Kolmogorov flow -- as well as a real-world\nweather forecasting task. Our findings reveal substantial distortions in\ndynamical properties in ML forecasts, especially for long forecast lead times\nor long recursive simulations, providing complementary information on ML\nforecast fidelity that can be used to improve ML models.\n","authors":["Zhou Fang","Gianmarco Mengaldo"],"pdf_url":"https://arxiv.org/pdf/2504.11074v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00036v3","updated":"2025-04-16T08:46:04Z","published":"2023-12-29T18:35:04Z","title":"Discrete Distribution Networks","summary":"  We introduce a novel generative model, the Discrete Distribution Networks\n(DDN), that approximates data distribution using hierarchical discrete\ndistributions. We posit that since the features within a network inherently\ncapture distributional information, enabling the network to generate multiple\nsamples simultaneously, rather than a single output, may offer an effective way\nto represent distributions. Therefore, DDN fits the target distribution,\nincluding continuous ones, by generating multiple discrete sample points. To\ncapture finer details of the target data, DDN selects the output that is\nclosest to the Ground Truth (GT) from the coarse results generated in the first\nlayer. This selected output is then fed back into the network as a condition\nfor the second layer, thereby generating new outputs more similar to the GT. As\nthe number of DDN layers increases, the representational space of the outputs\nexpands exponentially, and the generated samples become increasingly similar to\nthe GT. This hierarchical output pattern of discrete distributions endows DDN\nwith unique properties: more general zero-shot conditional generation and 1D\nlatent representation. We demonstrate the efficacy of DDN and its intriguing\nproperties through experiments on CIFAR-10 and FFHQ. The code is available at\nhttps://discrete-distribution-networks.github.io/\n","authors":["Lei Yang"],"pdf_url":"https://arxiv.org/pdf/2401.00036v3.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2504.11866v1","updated":"2025-04-16T08:41:20Z","published":"2025-04-16T08:41:20Z","title":"On the Problem of Best Arm Retention","summary":"  This paper presents a comprehensive study on the problem of Best Arm\nRetention (BAR), which has recently found applications in streaming algorithms\nfor multi-armed bandits. In the BAR problem, the goal is to retain $m$ arms\nwith the best arm included from $n$ after some trials, in stochastic\nmulti-armed bandit settings. We first investigate pure exploration for the BAR\nproblem under different criteria, and then minimize the regret with specific\nconstraints, in the context of further exploration in streaming algorithms.\n  - We begin by revisiting the lower bound for the $(\\varepsilon,\\delta)$-PAC\nalgorithm for Best Arm Identification (BAI) and adapt the classical\nKL-divergence argument to derive optimal bounds for $(\\varepsilon,\\delta)$-PAC\nalgorithms for BAR.\n  - We further study another variant of the problem, called $r$-BAR, which\nrequires the expected gap between the best arm and the optimal arm retained is\nless than $r$. We prove tight sample complexity for the problem.\n  - We explore the regret minimization problem for $r$-BAR and develop\nalgorithm beyond pure exploration. We conclude with a conjecture on the optimal\nregret in this setting.\n","authors":["Houshuang Chen","Yuchen He","Chihao Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.11866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13541v3","updated":"2025-04-16T08:39:39Z","published":"2024-12-18T06:40:53Z","title":"Spatio-Temporal Fuzzy-oriented Multi-Modal Meta-Learning for\n  Fine-grained Emotion Recognition","summary":"  Fine-grained emotion recognition (FER) plays a vital role in various fields,\nsuch as disease diagnosis, personalized recommendations, and multimedia mining.\nHowever, existing FER methods face three key challenges in real-world\napplications: (i) they rely on large amounts of continuously annotated data to\nensure accuracy since emotions are complex and ambiguous in reality, which is\ncostly and time-consuming; (ii) they cannot capture the temporal heterogeneity\ncaused by changing emotion patterns, because they usually assume that the\ntemporal correlation within sampling periods is the same; (iii) they do not\nconsider the spatial heterogeneity of different FER scenarios, that is, the\ndistribution of emotion information in different data may have bias or\ninterference. To address these challenges, we propose a Spatio-Temporal\nFuzzy-oriented Multi-modal Meta-learning framework (ST-F2M). Specifically,\nST-F2M first divides the multi-modal videos into multiple views, and each view\ncorresponds to one modality of one emotion. Multiple randomly selected views\nfor the same emotion form a meta-training task. Next, ST-F2M uses an integrated\nmodule with spatial and temporal convolutions to encode the data of each task,\nreflecting the spatial and temporal heterogeneity. Then it adds fuzzy semantic\ninformation to each task based on generalized fuzzy rules, which helps handle\nthe complexity and ambiguity of emotions. Finally, ST-F2M learns\nemotion-related general meta-knowledge through meta-recurrent neural networks\nto achieve fast and robust fine-grained emotion recognition. Extensive\nexperiments show that ST-F2M outperforms various state-of-the-art methods in\nterms of accuracy and model efficiency. In addition, we construct ablation\nstudies and further analysis to explore why ST-F2M performs well.\n","authors":["Jingyao Wang","Wenwen Qiang","Changwen Zheng","Fuchun Sun"],"pdf_url":"https://arxiv.org/pdf/2412.13541v3.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2504.11844v1","updated":"2025-04-16T08:07:08Z","published":"2025-04-16T08:07:08Z","title":"Evaluating the Goal-Directedness of Large Language Models","summary":"  To what extent do LLMs use their capabilities towards their given goal? We\ntake this as a measure of their goal-directedness. We evaluate\ngoal-directedness on tasks that require information gathering, cognitive\neffort, and plan execution, where we use subtasks to infer each model's\nrelevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI,\nand Anthropic show that goal-directedness is relatively consistent across\ntasks, differs from task performance, and is only moderately sensitive to\nmotivational prompts. Notably, most models are not fully goal-directed. We hope\nour goal-directedness evaluations will enable better monitoring of LLM\nprogress, and enable more deliberate design choices of agentic properties in\nLLMs.\n","authors":["Tom Everitt","Cristina Garbacea","Alexis Bellot","Jonathan Richens","Henry Papadatos","Siméon Campos","Rohin Shah"],"pdf_url":"https://arxiv.org/pdf/2504.11844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03042v2","updated":"2025-04-16T08:05:15Z","published":"2024-10-03T23:16:13Z","title":"FedPeWS: Personalized Warmup via Subnetworks for Enhanced Heterogeneous\n  Federated Learning","summary":"  Statistical data heterogeneity is a significant barrier to convergence in\nfederated learning (FL). While prior work has advanced heterogeneous FL through\nbetter optimization objectives, these methods fall short when there is extreme\ndata heterogeneity among collaborating participants. We hypothesize that\nconvergence under extreme data heterogeneity is primarily hindered due to the\naggregation of conflicting updates from the participants in the initial\ncollaboration rounds. To overcome this problem, we propose a warmup phase where\neach participant learns a personalized mask and updates only a subnetwork of\nthe full model. This personalized warmup allows the participants to focus\ninitially on learning specific subnetworks tailored to the heterogeneity of\ntheir data. After the warmup phase, the participants revert to standard\nfederated optimization, where all parameters are communicated. We empirically\ndemonstrate that the proposed personalized warmup via subnetworks (FedPeWS)\napproach improves accuracy and convergence speed over standard federated\noptimization methods.\n","authors":["Nurbek Tastan","Samuel Horvath","Martin Takac","Karthik Nandakumar"],"pdf_url":"https://arxiv.org/pdf/2410.03042v2.pdf","comment":"Accepted to CPAL 2025"},{"id":"http://arxiv.org/abs/2502.07005v6","updated":"2025-04-16T07:58:58Z","published":"2025-02-10T20:10:25Z","title":"Geometry-aware RL for Manipulation of Varying Shapes and Deformable\n  Objects","summary":"  Manipulating objects with varying geometries and deformable objects is a\nmajor challenge in robotics. Tasks such as insertion with different objects or\ncloth hanging require precise control and effective modelling of complex\ndynamics. In this work, we frame this problem through the lens of a\nheterogeneous graph that comprises smaller sub-graphs, such as actuators and\nobjects, accompanied by different edge types describing their interactions.\nThis graph representation serves as a unified structure for both rigid and\ndeformable objects tasks, and can be extended further to tasks comprising\nmultiple actuators. To evaluate this setup, we present a novel and challenging\nreinforcement learning benchmark, including rigid insertion of diverse objects,\nas well as rope and cloth manipulation with multiple end-effectors. These tasks\npresent a large search space, as both the initial and target configurations are\nuniformly sampled in 3D space. To address this issue, we propose a novel\ngraph-based policy model, dubbed Heterogeneous Equivariant Policy (HEPi),\nutilizing $SE(3)$ equivariant message passing networks as the main backbone to\nexploit the geometric symmetry. In addition, by modeling explicit\nheterogeneity, HEPi can outperform Transformer-based and non-heterogeneous\nequivariant policies in terms of average returns, sample efficiency, and\ngeneralization to unseen objects. Our project page is available at\nhttps://thobotics.github.io/hepi.\n","authors":["Tai Hoang","Huy Le","Philipp Becker","Vien Anh Ngo","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2502.07005v6.pdf","comment":"Accepted at ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2410.14466v2","updated":"2025-04-16T07:57:55Z","published":"2024-10-18T13:51:25Z","title":"Flow-Based Sampling for Entanglement Entropy and the Machine Learning of\n  Defects","summary":"  We introduce a novel technique to numerically calculate R\\'enyi entanglement\nentropies in lattice quantum field theory using generative models. We describe\nhow flow-based approaches can be combined with the replica trick using a custom\nneural-network architecture around a lattice defect connecting two replicas.\nNumerical tests for the $\\phi^4$ scalar field theory in two and three\ndimensions demonstrate that our technique outperforms state-of-the-art Monte\nCarlo calculations, and exhibit a promising scaling with the defect size.\n","authors":["Andrea Bulgarelli","Elia Cellini","Karl Jansen","Stefan Kühn","Alessandro Nada","Shinichi Nakajima","Kim A. Nicoli","Marco Panero"],"pdf_url":"https://arxiv.org/pdf/2410.14466v2.pdf","comment":"some discussions improved, matches the published version"},{"id":"http://arxiv.org/abs/2504.11840v1","updated":"2025-04-16T07:57:42Z","published":"2025-04-16T07:57:42Z","title":"GT-SVQ: A Linear-Time Graph Transformer for Node Classification Using\n  Spiking Vector Quantization","summary":"  Graph Transformers (GTs), which simultaneously integrate message-passing and\nself-attention mechanisms, have achieved promising empirical results in some\ngraph prediction tasks. Although these approaches show the potential of\nTransformers in capturing long-range graph topology information, issues\nconcerning the quadratic complexity and high computing energy consumption\nseverely limit the scalability of GTs on large-scale graphs. Recently, as\nbrain-inspired neural networks, Spiking Neural Networks (SNNs), facilitate the\ndevelopment of graph representation learning methods with lower computational\nand storage overhead through the unique event-driven spiking neurons. Inspired\nby these characteristics, we propose a linear-time Graph Transformer using\nSpiking Vector Quantization (GT-SVQ) for node classification. GT-SVQ\nreconstructs codebooks based on rate coding outputs from spiking neurons, and\ninjects the codebooks into self-attention blocks to aggregate global\ninformation in linear complexity. Besides, spiking vector quantization\neffectively alleviates codebook collapse and the reliance on complex machinery\n(distance measure, auxiliary loss, etc.) present in previous vector\nquantization-based graph learning methods. In experiments, we compare GT-SVQ\nwith other state-of-the-art baselines on node classification datasets ranging\nfrom small to large. Experimental results show that GT-SVQ has achieved\ncompetitive performances on most datasets while maintaining up to 130x faster\ninference speed compared to other GTs.\n","authors":["Huizhe Zhang","Jintang Li","Yuchang Zhu","Liang Chen","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2504.11840v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2410.20068v2","updated":"2025-04-16T07:55:46Z","published":"2024-10-26T04:19:52Z","title":"Understanding the Effect of GCN Convolutions in Regression Tasks","summary":"  Graph Convolutional Networks (GCNs) have become a pivotal method in machine\nlearning for modeling functions over graphs. Despite their widespread success\nacross various applications, their statistical properties (e.g., consistency,\nconvergence rates) remain ill-characterized. To begin addressing this knowledge\ngap, we consider networks for which the graph structure implies that\nneighboring nodes exhibit similar signals and provide statistical theory for\nthe impact of convolution operators. Focusing on estimators based solely on\nneighborhood aggregation, we examine how two common convolutions - the original\nGCN and GraphSAGE convolutions - affect the learning error as a function of the\nneighborhood topology and the number of convolutional layers. We explicitly\ncharacterize the bias-variance type trade-off incurred by GCNs as a function of\nthe neighborhood size and identify specific graph topologies where convolution\noperators are less effective. Our theoretical findings are corroborated by\nsynthetic experiments, and provide a start to a deeper quantitative\nunderstanding of convolutional effects in GCNs for offering rigorous guidelines\nfor practitioners.\n","authors":["Juntong Chen","Johannes Schmidt-Hieber","Claire Donnat","Olga Klopp"],"pdf_url":"https://arxiv.org/pdf/2410.20068v2.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2410.08244v2","updated":"2025-04-16T07:54:52Z","published":"2024-10-10T09:32:59Z","title":"RAB$^2$-DEF: Dynamic and explainable defense against adversarial attacks\n  in Federated Learning to fair poor clients","summary":"  At the same time that artificial intelligence is becoming popular, concern\nand the need for regulation is growing, including among other requirements the\ndata privacy. In this context, Federated Learning is proposed as a solution to\ndata privacy concerns derived from different source data scenarios due to its\ndistributed learning. The defense mechanisms proposed in literature are just\nfocused on defending against adversarial attacks and the performance, leaving\naside other important qualities such as explainability, fairness to poor\nquality clients, dynamism in terms of attacks configuration and generality in\nterms of being resilient against different kinds of attacks. In this work, we\npropose RAB$^2$-DEF, a $\\textbf{r}$esilient $\\textbf{a}$gainst\n$\\textbf{b}\\text{yzantine}$ and $\\textbf{b}$ackdoor attacks which is\n$\\textbf{d}$ynamic, $\\textbf{e}$xplainable and $\\textbf{f}$air to poor clients\nusing local linear explanations. We test the performance of RAB$^2$-DEF in\nimage datasets and both byzantine and backdoor attacks considering the\nstate-of-the-art defenses and achieve that RAB$^2$-DEF is a proper defense at\nthe same time that it boosts the other qualities towards trustworthy artificial\nintelligence.\n","authors":["Nuria Rodríguez-Barroso","M. Victoria Luzón","Francisco Herrera"],"pdf_url":"https://arxiv.org/pdf/2410.08244v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13729v4","updated":"2025-04-16T07:53:21Z","published":"2025-02-19T13:55:32Z","title":"Emergence of the Primacy Effect in Structured State-Space Models","summary":"  Human and animal memory for sequentially presented items is well-documented\nto be more accurate for those at the beginning and end of the sequence,\nphenomena known as the primacy and recency effects, respectively. By contrast,\nartificial neural network (ANN) models are typically designed with a memory\nthat decays monotonically over time. Accordingly, ANNs are expected to show the\nrecency effect but not the primacy effect. Contrary to this theoretical\nexpectation, however, the present study reveals a counterintuitive finding: a\nrecently developed ANN architecture, called structured state-space models,\nexhibits the primacy effect when trained and evaluated on a synthetic task that\nmirrors psychological memory experiments. Given that this model was originally\ndesigned for recovering neuronal activity patterns observed in biological\nbrains, this result provides a novel perspective on the psychological primacy\neffect while also posing a non-trivial puzzle for the current theories in\nmachine learning.\n","authors":["Takashi Morita"],"pdf_url":"https://arxiv.org/pdf/2502.13729v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11831v1","updated":"2025-04-16T07:41:40Z","published":"2025-04-16T07:41:40Z","title":"Support is All You Need for Certified VAE Training","summary":"  Variational Autoencoders (VAEs) have become increasingly popular and deployed\nin safety-critical applications. In such applications, we want to give\ncertified probabilistic guarantees on performance under adversarial attacks. We\npropose a novel method, CIVET, for certified training of VAEs. CIVET depends on\nthe key insight that we can bound worst-case VAE error by bounding the error on\ncarefully chosen support sets at the latent layer. We show this point\nmathematically and present a novel training algorithm utilizing this insight.\nWe show in an extensive evaluation across different datasets (in both the\nwireless and vision application areas), architectures, and perturbation\nmagnitudes that our method outperforms SOTA methods achieving good standard\nperformance with strong robustness guarantees.\n","authors":["Changming Xu","Debangshu Banerjee","Deepak Vasisht","Gagandeep Singh"],"pdf_url":"https://arxiv.org/pdf/2504.11831v1.pdf","comment":"21 pages, 3 figures, ICLR '25"},{"id":"http://arxiv.org/abs/2504.11830v1","updated":"2025-04-16T07:38:51Z","published":"2025-04-16T07:38:51Z","title":"Emergence of Computational Structure in a Neural Network Physics\n  Simulator","summary":"  Neural networks often have identifiable computational structures - components\nof the network which perform an interpretable algorithm or task - but the\nmechanisms by which these emerge and the best methods for detecting these\nstructures are not well understood. In this paper we investigate the emergence\nof computational structure in a transformer-like model trained to simulate the\nphysics of a particle system, where the transformer's attention mechanism is\nused to transfer information between particles. We show that (a) structures\nemerge in the attention heads of the transformer which learn to detect particle\ncollisions, (b) the emergence of these structures is associated to degenerate\ngeometry in the loss landscape, and (c) the dynamics of this emergence follows\na power law. This suggests that these components are governed by a degenerate\n\"effective potential\". These results have implications for the convergence time\nof computational structure within neural networks and suggest that the\nemergence of computational structure can be detected by studying the dynamics\nof network components.\n","authors":["Rohan Hitchcock","Gary W. Delaney","Jonathan H. Manton","Richard Scalzo","Jingge Zhu"],"pdf_url":"https://arxiv.org/pdf/2504.11830v1.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2501.08044v2","updated":"2025-04-16T07:34:25Z","published":"2025-01-14T11:52:16Z","title":"UFGraphFR: An attempt at a federated recommendation system based on user\n  text characteristics","summary":"  Federated learning has emerged as a key paradigm in privacy-preserving\ncomputing due to its \"data usable but not visible\" property, enabling users to\ncollaboratively train models without sharing raw data. Motivated by this,\nfederated recommendation systems offer a promising architecture that balances\nuser privacy with recommendation accuracy through distributed collaborative\nlearning. However, existing federated recommendation methods often neglect the\nunderlying semantic or behavioral relationships between users during parameter\naggregation, limiting their effectiveness. To address this, graph-based\nfederated recommendation systems have been proposed to leverage neighborhood\ninformation. Yet, conventional graph construction methods usually require\naccess to raw user data or explicit social links, which contradicts the strict\nprivacy requirements of federated learning. In this work, we propose UFGraphFR\n(User Text-feature-based Graph Federated Recommendation), a personalized\nfederated recommendation framework that constructs a user graph based on\nclients' locally embedded text features. Our core assumption is that users with\nsimilar textual descriptions exhibit similar preferences. UFGraphFR introduces\ntwo key components: a privacy-preserving user relationship graph built from the\njoint embedding layer's weight matrix without leaking raw user attributes, and\na Transformer-based architecture to model temporal dependencies in user-item\ninteraction sequences. Experimental results on benchmark datasets such as\nMovieLens and HetRec2011 demonstrate that UFGraphFR achieves competitive\naccuracy compared to centralized and state-of-the-art federated baselines while\npreserving user privacy. Code is available at\nhttps://github.com/trueWangSyutung/UFGraphFR\n","authors":["Xudong Wang"],"pdf_url":"https://arxiv.org/pdf/2501.08044v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21138v2","updated":"2025-04-16T07:26:19Z","published":"2025-03-27T04:00:49Z","title":"A Computational Framework for Efficient Model Evaluation with Causal\n  Guarantees","summary":"  In order to reduce the cost of experimental evaluation for models, we\nintroduce a computational theory of evaluation for prediction and decision\nmodels: build evaluation model to accelerate the evaluation procedures. We\nprove upper bounds of generalized error and generalized causal effect error of\ngiven evaluation models. We also prove efficiency, and consistency to estimated\ncausal effect from deployed subject to evaluation metric by prediction. To\nlearn evaluation models, we propose a meta-learner to handle heterogeneous\nevaluation subjects space problem. Comparing with existed evaluation\napproaches, our (conditional) evaluation model reduced 24.1\\%-99.0\\% evaluation\nerrors across 12 scenes, including individual medicine, scientific simulation,\nsocial experiment, business activity, and quantum trade. The evaluation time is\nreduced 3-7 order of magnitude comparing with experiments or simulations.\n","authors":["Hedong Yan"],"pdf_url":"https://arxiv.org/pdf/2503.21138v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18309v2","updated":"2025-04-16T07:24:54Z","published":"2025-03-24T03:19:45Z","title":"Efficient Transformed Gaussian Process State-Space Models for\n  Non-Stationary High-Dimensional Dynamical Systems","summary":"  Gaussian process state-space models (GPSSMs) offer a principled framework for\nlearning and inference in nonlinear dynamical systems with uncertainty\nquantification. However, existing GPSSMs are limited by the use of multiple\nindependent stationary Gaussian processes (GPs), leading to prohibitive\ncomputational and parametric complexity in high-dimensional settings and\nrestricted modeling capacity for non-stationary dynamics. To address these\nchallenges, we propose an efficient transformed Gaussian process state-space\nmodel (ETGPSSM) for scalable and flexible modeling of high-dimensional,\nnon-stationary dynamical systems. Specifically, our ETGPSSM integrates a single\nshared GP with input-dependent normalizing flows, yielding an expressive\nimplicit process prior that captures complex, non-stationary transition\ndynamics while significantly reducing model complexity. For the inference of\nthe implicit process, we develop a variational inference algorithm that jointly\napproximates the posterior over the underlying GP and the neural network\nparameters defining the normalizing flows. To avoid explicit variational\nparameterization of the latent states, we further incorporate the ensemble\nKalman filter (EnKF) into the variational framework, enabling accurate and\nefficient state estimation. Extensive empirical evaluations on synthetic and\nreal-world datasets demonstrate the superior performance of our ETGPSSM in\nsystem dynamics learning, high-dimensional state estimation, and time-series\nforecasting, outperforming existing GPSSMs and neural network-based SSMs in\nterms of computational efficiency and accuracy.\n","authors":["Zhidi Lin","Ying Li","Feng Yin","Juan Maroñas","Alexandre H. Thiéry"],"pdf_url":"https://arxiv.org/pdf/2503.18309v2.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.04820v3","updated":"2025-04-16T07:09:59Z","published":"2024-08-09T02:22:51Z","title":"Natural Language Outlines for Code: Literate Programming in the LLM Era","summary":"  We propose using natural language outlines as a novel modality and\ninteraction surface for providing AI assistance to developers throughout the\nsoftware development process. An NL outline for a code function comprises\nmultiple statements written in concise prose, which partition the code and\nsummarize its main ideas in the style of literate programming. Crucially, we\nfind that modern LLMs can generate accurate and high-quality NL outlines in\npractice. Moreover, NL outlines enable a bidirectional sync between code and\nNL: a developer can change one and the LLM automatically updates the other. We\ndiscuss many use cases for NL outlines: they can accelerate understanding and\nnavigation of code and diffs, simplify code maintenance, augment code search,\nsteer code generation, and more. We then propose and compare multiple LLM\nprompting techniques for generating outlines and ask professional developers to\njudge outline quality. Finally, we present two case studies applying NL\noutlines toward code review and malware detection.\n","authors":["Kensen Shi","Deniz Altınbüken","Saswat Anand","Mihai Christodorescu","Katja Grünwedel","Alexa Koenings","Sai Naidu","Anurag Pathak","Marc Rasi","Fredde Ribeiro","Brandon Ruffin","Siddhant Sanyam","Maxim Tabachnyk","Sara Toth","Roy Tu","Tobias Welp","Pengcheng Yin","Manzil Zaheer","Satish Chandra","Charles Sutton"],"pdf_url":"https://arxiv.org/pdf/2408.04820v3.pdf","comment":"Accepted to FSE'25 Industry Track"},{"id":"http://arxiv.org/abs/2311.05418v2","updated":"2025-04-16T07:07:16Z","published":"2023-11-09T14:54:28Z","title":"Generalization in medical AI: a perspective on developing scalable\n  models","summary":"  The scientific community is increasingly recognizing the importance of\ngeneralization in medical AI for translating research into practical clinical\napplications. A three-level scale is introduced to characterize\nout-of-distribution generalization performance of medical AI models. This scale\naddresses the diversity of real-world medical scenarios as well as whether\ntarget domain data and labels are available for model recalibration. It serves\nas a tool to help researchers characterize their development settings and\ndetermine the best approach to tackling the challenge of out-of-distribution\ngeneralization.\n","authors":["Eran Zvuloni","Leo Anthony Celi","Joachim A. Behar"],"pdf_url":"https://arxiv.org/pdf/2311.05418v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11816v1","updated":"2025-04-16T07:02:38Z","published":"2025-04-16T07:02:38Z","title":"Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading","summary":"  LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads.\n","authors":["Kihyun Kim","Jinwoo Kim","Hyunsun Chung","Myung-Hoon Cha","Hong-Yeon Kim","Youngjae Kim"],"pdf_url":"https://arxiv.org/pdf/2504.11816v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2504.11811v1","updated":"2025-04-16T06:49:56Z","published":"2025-04-16T06:49:56Z","title":"Manifold meta-learning for reduced-complexity neural system\n  identification","summary":"  System identification has greatly benefited from deep learning techniques,\nparticularly for modeling complex, nonlinear dynamical systems with partially\nunknown physics where traditional approaches may not be feasible. However, deep\nlearning models often require large datasets and significant computational\nresources at training and inference due to their high-dimensional\nparameterizations. To address this challenge, we propose a meta-learning\nframework that discovers a low-dimensional manifold within the parameter space\nof an over-parameterized neural network architecture. This manifold is learned\nfrom a meta-dataset of input-output sequences generated by a class of related\ndynamical systems, enabling efficient model training while preserving the\nnetwork's expressive power for the considered system class. Unlike bilevel\nmeta-learning approaches, our method employs an auxiliary neural network to map\ndatasets directly onto the learned manifold, eliminating the need for costly\nsecond-order gradient computations during meta-training and reducing the number\nof first-order updates required in inference, which could be expensive for\nlarge models. We validate our approach on a family of Bouc-Wen oscillators,\nwhich is a well-studied nonlinear system identification benchmark. We\ndemonstrate that we are able to learn accurate models even in small-data\nscenarios.\n","authors":["Marco Forgione","Ankush Chakrabarty","Dario Piga","Matteo Rufolo","Alberto Bemporad"],"pdf_url":"https://arxiv.org/pdf/2504.11811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11808v1","updated":"2025-04-16T06:43:20Z","published":"2025-04-16T06:43:20Z","title":"Federated Spectral Graph Transformers Meet Neural Ordinary Differential\n  Equations for Non-IID Graphs","summary":"  Graph Neural Network (GNN) research is rapidly advancing due to GNNs'\ncapacity to learn distributed representations from graph-structured data.\nHowever, centralizing large volumes of real-world graph data for GNN training\nis often impractical due to privacy concerns, regulatory restrictions, and\ncommercial competition. Federated learning (FL), a distributed learning\nparadigm, offers a solution by preserving data privacy with collaborative model\ntraining. Despite progress in training huge vision and language models,\nfederated learning for GNNs remains underexplored. To address this challenge,\nwe present a novel method for federated learning on GNNs based on spectral GNNs\nequipped with neural ordinary differential equations (ODE) for better\ninformation capture, showing promising results across both homophilic and\nheterophilic graphs. Our approach effectively handles non-Independent and\nIdentically Distributed (non-IID) data, while also achieving performance\ncomparable to existing methods that only operate on IID data. It is designed to\nbe privacy-preserving and bandwidth-optimized, making it suitable for\nreal-world applications such as social network analysis, recommendation\nsystems, and fraud detection, which often involve complex, non-IID, and\nheterophilic graph structures. Our results in the area of federated learning on\nnon-IID heterophilic graphs demonstrate significant improvements, while also\nachieving better performance on homophilic graphs. This work highlights the\npotential of federated learning in diverse and challenging graph settings.\nOpen-source code available on GitHub\n(https://github.com/SpringWiz11/Fed-GNODEFormer).\n","authors":["Kishan Gurumurthy","Himanshu Pal","Charu Sharma"],"pdf_url":"https://arxiv.org/pdf/2504.11808v1.pdf","comment":"The first two listed authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2206.10983v4","updated":"2025-04-16T06:36:22Z","published":"2022-06-22T11:17:46Z","title":"Traffic Congestion Prediction Using Machine Learning Techniques","summary":"  The prediction of traffic congestion can serve a crucial role in making\nfuture decisions. Although many studies have been conducted regarding\ncongestion, most of these could not cover all the important factors (e.g.,\nweather conditions). We proposed a prediction model for traffic congestion that\ncan predict congestion based on day, time and several weather data (e.g.,\ntemperature, humidity). To evaluate our model, it has been tested against the\ntraffic data of New Delhi. With this model, congestion of a road can be\npredicted one week ahead with an average RMSE of 1.12. Therefore, this model\ncan be used to take preventive measure beforehand.\n","authors":["Rafed Muhammad Yasir","Naushin Nower","Mohammad Shoyaib"],"pdf_url":"https://arxiv.org/pdf/2206.10983v4.pdf","comment":"This is an undergraduate research project and it isn't sufficiently\n  exhaustive"},{"id":"http://arxiv.org/abs/2210.06672v3","updated":"2025-04-16T06:23:46Z","published":"2022-10-13T01:58:06Z","title":"Variance-Aware Estimation of Kernel Mean Embedding","summary":"  An important feature of kernel mean embeddings (KME) is that the rate of\nconvergence of the empirical KME to the true distribution KME can be bounded\nindependently of the dimension of the space, properties of the distribution and\nsmoothness features of the kernel. We show how to speed-up convergence by\nleveraging variance information in the reproducing kernel Hilbert space.\nFurthermore, we show that even when such information is a priori unknown, we\ncan efficiently estimate it from the data, recovering the desiderata of a\ndistribution agnostic bound that enjoys acceleration in fortuitous settings. We\nfurther extend our results from independent data to stationary mixing sequences\nand illustrate our methods in the context of hypothesis testing and robust\nparametric estimation.\n","authors":["Geoffrey Wolfer","Pierre Alquier"],"pdf_url":"https://arxiv.org/pdf/2210.06672v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12716v2","updated":"2025-04-16T06:03:43Z","published":"2023-09-22T08:58:22Z","title":"H2O+: An Improved Framework for Hybrid Offline-and-Online RL with\n  Dynamics Gaps","summary":"  Solving real-world complex tasks using reinforcement learning (RL) without\nhigh-fidelity simulation environments or large amounts of offline data can be\nquite challenging. Online RL agents trained in imperfect simulation\nenvironments can suffer from severe sim-to-real issues. Offline RL approaches\nalthough bypass the need for simulators, often pose demanding requirements on\nthe size and quality of the offline datasets. The recently emerged hybrid\noffline-and-online RL provides an attractive framework that enables joint use\nof limited offline data and imperfect simulator for transferable policy\nlearning. In this paper, we develop a new algorithm, called H2O+, which offers\ngreat flexibility to bridge various choices of offline and online learning\nmethods, while also accounting for dynamics gaps between the real and\nsimulation environment. Through extensive simulation and real-world robotics\nexperiments, we demonstrate superior performance and flexibility over advanced\ncross-domain online and offline RL algorithms.\n","authors":["Haoyi Niu","Tianying Ji","Bingqi Liu","Haocheng Zhao","Xiangyu Zhu","Jianying Zheng","Pengfei Huang","Guyue Zhou","Jianming Hu","Xianyuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2309.12716v2.pdf","comment":"ICRA 2025"},{"id":"http://arxiv.org/abs/2504.11792v1","updated":"2025-04-16T05:52:22Z","published":"2025-04-16T05:52:22Z","title":"Large Language Models for Drug Overdose Prediction from Longitudinal\n  Medical Records","summary":"  The ability to predict drug overdose risk from a patient's medical records is\ncrucial for timely intervention and prevention. Traditional machine learning\nmodels have shown promise in analyzing longitudinal medical records for this\ntask. However, recent advancements in large language models (LLMs) offer an\nopportunity to enhance prediction performance by leveraging their ability to\nprocess long textual data and their inherent prior knowledge across diverse\ntasks. In this study, we assess the effectiveness of Open AI's GPT-4o LLM in\npredicting drug overdose events using patients' longitudinal insurance claims\nrecords. We evaluate its performance in both fine-tuned and zero-shot settings,\ncomparing them to strong traditional machine learning methods as baselines. Our\nresults show that LLMs not only outperform traditional models in certain\nsettings but can also predict overdose risk in a zero-shot setting without\ntask-specific training. These findings highlight the potential of LLMs in\nclinical decision support, particularly for drug overdose risk prediction.\n","authors":["Md Sultan Al Nahian","Chris Delcher","Daniel Harris","Peter Akpunonu","Ramakanth Kavuluru"],"pdf_url":"https://arxiv.org/pdf/2504.11792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11788v5","updated":"2025-04-16T05:47:53Z","published":"2024-04-17T22:44:22Z","title":"Understanding the Performance Horizon of the Latest ML Workloads with\n  NonGEMM Workloads","summary":"  Among ML operators today, GEneralMatrix Multiplication (GEMM)-based operators\nare known to be key operators that build the main backbone of ML models. As\ntheir computational overhead dominates the overall execution time (e.g., 42.8%\n- 96.6% in our results), GEMM operators have been the prime optimization\ntargets for fast ML inference. This led to advanced GPUs and accelerators\navailable today, which provided significant boost in the GEMM performance\ncompared to CPUs, aligned with the lesson from Amdahl's law. However,\naccelerating GEMM has significantly shifted the Amdahl's law's landscape for ML\ninference; due to the decreased GEMM execution time, the relative execution\ntime of non-GEMM operators is now significant. Although the importance of\nnon-GEMM performance is increasing, we have little knowledge about the non-GEMM\nperformance horizon in the latest hardware platforms and models. Therefore, to\nguide non-GEMM-oriented optimizations, we conduct a thorough performance\nanalysis of 17 widely adopted ML models in Hugging Face and Torchvision on\nworkstation and data center platforms with/without GPUs. We discover that\nnon-GEMM performance bottleneck is a considerable issue across all the\nplatforms and models, accounting for 11.3% to 73.6% of total latency, on\naverage. The challenge significantly aggravates when we apply quantization,\nwhich is a common model compression technique, due to the boosted GEMM\nperformance and extra non-GEMM operators for dequantization and requantization.\nTo provide insights into non-GEMM optimization targets, we demystify the most\ndominant non-GEMM operators for each model and deployment software. We also\nshow that widely adopted optimizations such as operator fusion do not\ncompletely address the non-GEMM performance bottleneck, where non-GEMM\noperators still account for 15% to 48% of total latency.\n","authors":["Rachid Karami","Sheng-Chun Kao","Hyoukjun Kwon"],"pdf_url":"https://arxiv.org/pdf/2404.11788v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09080v2","updated":"2025-04-16T05:45:31Z","published":"2024-10-04T21:39:30Z","title":"Leveraging Social Determinants of Health in Alzheimer's Research Using\n  LLM-Augmented Literature Mining and Knowledge Graphs","summary":"  Growing evidence suggests that social determinants of health (SDoH), a set of\nnonmedical factors, affect individuals' risks of developing Alzheimer's disease\n(AD) and related dementias. Nevertheless, the etiological mechanisms underlying\nsuch relationships remain largely unclear, mainly due to difficulties in\ncollecting relevant information. This study presents a novel, automated\nframework that leverages recent advancements of large language model (LLM) and\nnatural language processing techniques to mine SDoH knowledge from extensive\nliterature and integrate it with AD-related biological entities extracted from\nthe general-purpose knowledge graph PrimeKG. Utilizing graph neural networks,\nwe performed link prediction tasks to evaluate the resultant SDoH-augmented\nknowledge graph. Our framework shows promise for enhancing knowledge discovery\nin AD and can be generalized to other SDoH-related research areas, offering a\nnew tool for exploring the impact of social determinants on health outcomes.\nOur code is available at: https://github.com/hwq0726/SDoHenPKG\n","authors":["Tianqi Shang","Shu Yang","Weiqing He","Tianhua Zhai","Dawei Li","Bojian Hou","Tianlong Chen","Jason H. Moore","Marylyn D. Ritchie","Li Shen"],"pdf_url":"https://arxiv.org/pdf/2410.09080v2.pdf","comment":"Accepted by AMIA-IS'25: AMIA Informatics Summit"},{"id":"http://arxiv.org/abs/2504.11781v1","updated":"2025-04-16T05:33:42Z","published":"2025-04-16T05:33:42Z","title":"ACMamba: Fast Unsupervised Anomaly Detection via An Asymmetrical\n  Consensus State Space Model","summary":"  Unsupervised anomaly detection in hyperspectral images (HSI), aiming to\ndetect unknown targets from backgrounds, is challenging for earth surface\nmonitoring. However, current studies are hindered by steep computational costs\ndue to the high-dimensional property of HSI and dense sampling-based training\nparadigm, constraining their rapid deployment. Our key observation is that,\nduring training, not all samples within the same homogeneous area are\nindispensable, whereas ingenious sampling can provide a powerful substitute for\nreducing costs. Motivated by this, we propose an Asymmetrical Consensus State\nSpace Model (ACMamba) to significantly reduce computational costs without\ncompromising accuracy. Specifically, we design an asymmetrical anomaly\ndetection paradigm that utilizes region-level instances as an efficient\nalternative to dense pixel-level samples. In this paradigm, a low-cost\nMamba-based module is introduced to discover global contextual attributes of\nregions that are essential for HSI reconstruction. Additionally, we develop a\nconsensus learning strategy from the optimization perspective to simultaneously\nfacilitate background reconstruction and anomaly compression, further\nalleviating the negative impact of anomaly reconstruction. Theoretical analysis\nand extensive experiments across eight benchmarks verify the superiority of\nACMamba, demonstrating a faster speed and stronger performance over the\nstate-of-the-art.\n","authors":["Guanchun Wang","Xiangrong Zhang","Yifei Zhang","Zelin Peng","Tianyang Zhang","Xu Tang","Licheng Jiao"],"pdf_url":"https://arxiv.org/pdf/2504.11781v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2504.11777v1","updated":"2025-04-16T05:31:18Z","published":"2025-04-16T05:31:18Z","title":"Bridging the Semantic Gaps: Improving Medical VQA Consistency with\n  LLM-Augmented Question Sets","summary":"  Medical Visual Question Answering (MVQA) systems can interpret medical images\nin response to natural language queries. However, linguistic variability in\nquestion phrasing often undermines the consistency of these systems. To address\nthis challenge, we propose a Semantically Equivalent Question Augmentation\n(SEQA) framework, which leverages large language models (LLMs) to generate\ndiverse yet semantically equivalent rephrasings of questions. Specifically,\nthis approach enriches linguistic diversity while preserving semantic meaning.\nWe further introduce an evaluation metric, Total Agreement Rate with\nSemantically Equivalent Input and Correct Answer (TAR-SC), which assesses a\nmodel's capability to generate consistent and correct responses to semantically\nequivalent linguistic variations. In addition, we also propose three other\ndiversity metrics - average number of QA items per image (ANQI), average number\nof questions per image with the same answer (ANQA), and average number of\nopen-ended questions per image with the same semantics (ANQS). Using the SEQA\nframework, we augmented the benchmarked MVQA public datasets of SLAKE, VQA-RAD,\nand PathVQA. As a result, all three datasets achieved significant improvements\nby incorporating more semantically equivalent questions: ANQI increased by an\naverage of 86.1, ANQA by 85.1, and ANQS by 46. Subsequent experiments evaluate\nthree MVQA models (M2I2, MUMC, and BiomedGPT) under both zero-shot and\nfine-tuning settings on the enhanced datasets. Experimental results in MVQA\ndatasets show that fine-tuned models achieve an average accuracy improvement of\n19.35%, while our proposed TAR-SC metric shows an average improvement of 11.\n61%, indicating a substantial enhancement in model consistency.\n","authors":["Yongpei Ma","Pengyu Wang","Adam Dunn","Usman Naseem","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2504.11777v1.pdf","comment":"The first two listed authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2410.01795v2","updated":"2025-04-16T05:30:34Z","published":"2024-10-02T17:53:08Z","title":"Knowledge-Driven Feature Selection and Engineering for Genotype Data\n  with Large Language Models","summary":"  Predicting phenotypes with complex genetic bases based on a small,\ninterpretable set of variant features remains a challenging task.\nConventionally, data-driven approaches are utilized for this task, yet the high\ndimensional nature of genotype data makes the analysis and prediction\ndifficult. Motivated by the extensive knowledge encoded in pre-trained LLMs and\ntheir success in processing complex biomedical concepts, we set to examine the\nability of LLMs in feature selection and engineering for tabular genotype data,\nwith a novel knowledge-driven framework. We develop FREEFORM, Free-flow\nReasoning and Ensembling for Enhanced Feature Output and Robust Modeling,\ndesigned with chain-of-thought and ensembling principles, to select and\nengineer features with the intrinsic knowledge of LLMs. Evaluated on two\ndistinct genotype-phenotype datasets, genetic ancestry and hereditary hearing\nloss, we find this framework outperforms several data-driven methods,\nparticularly on low-shot regimes. FREEFORM is available as open-source\nframework at GitHub: https://github.com/PennShenLab/FREEFORM.\n","authors":["Joseph Lee","Shu Yang","Jae Young Baik","Xiaoxi Liu","Zhen Tan","Dawei Li","Zixuan Wen","Bojian Hou","Duy Duong-Tran","Tianlong Chen","Li Shen"],"pdf_url":"https://arxiv.org/pdf/2410.01795v2.pdf","comment":"accepted by AMIA-IS'25: AMIA Informatics Summit [Marco Ramoni\n  Distinguished Paper Award for Translational Bioinformatics]"},{"id":"http://arxiv.org/abs/2504.11775v1","updated":"2025-04-16T05:29:11Z","published":"2025-04-16T05:29:11Z","title":"Discrimination-free Insurance Pricing with Privatized Sensitive\n  Attributes","summary":"  Fairness has emerged as a critical consideration in the landscape of machine\nlearning algorithms, particularly as AI continues to transform decision-making\nacross societal domains. To ensure that these algorithms are free from bias and\ndo not discriminate against individuals based on sensitive attributes such as\ngender and race, the field of algorithmic bias has introduced various fairness\nconcepts, along with methodologies to achieve these notions in different\ncontexts. Despite the rapid advancement, not all sectors have embraced these\nfairness principles to the same extent. One specific sector that merits\nattention in this regard is insurance. Within the realm of insurance pricing,\nfairness is defined through a distinct and specialized framework. Consequently,\nachieving fairness according to established notions does not automatically\nensure fair pricing in insurance. In particular, regulators are increasingly\nemphasizing transparency in pricing algorithms and imposing constraints on\ninsurance companies on the collection and utilization of sensitive consumer\nattributes. These factors present additional challenges in the implementation\nof fairness in pricing algorithms. To address these complexities and comply\nwith regulatory demands, we propose an efficient method for constructing fair\nmodels that are tailored to the insurance domain, using only privatized\nsensitive attributes. Notably, our approach ensures statistical guarantees,\ndoes not require direct access to sensitive attributes, and adapts to varying\ntransparency requirements, addressing regulatory demands while ensuring\nfairness in insurance pricing.\n","authors":["Tianhe Zhang","Suhan Liu","Peng Shi"],"pdf_url":"https://arxiv.org/pdf/2504.11775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10143v2","updated":"2025-04-16T05:22:32Z","published":"2025-04-14T11:54:19Z","title":"Negate or Embrace: On How Misalignment Shapes Multimodal Representation\n  Learning","summary":"  Multimodal representation learning, exemplified by multimodal contrastive\nlearning (MMCL) using image-text pairs, aims to learn powerful representations\nby aligning cues across modalities. This approach relies on the core assumption\nthat the exemplar image-text pairs constitute two representations of an\nidentical concept. However, recent research has revealed that real-world\ndatasets often exhibit misalignment. There are two distinct viewpoints on how\nto address this issue: one suggests mitigating the misalignment, and the other\nleveraging it. We seek here to reconcile these seemingly opposing perspectives,\nand to provide a practical guide for practitioners. Using latent variable\nmodels we thus formalize misalignment by introducing two specific mechanisms:\nselection bias, where some semantic variables are missing, and perturbation\nbias, where semantic variables are distorted -- both affecting latent variables\nshared across modalities. Our theoretical analysis demonstrates that, under\nmild assumptions, the representations learned by MMCL capture exactly the\ninformation related to the subset of the semantic variables invariant to\nselection and perturbation biases. This provides a unified perspective for\nunderstanding misalignment. Based on this, we further offer actionable insights\ninto how misalignment should inform the design of real-world ML systems. We\nvalidate our theoretical findings through extensive empirical studies on both\nsynthetic data and real image-text datasets, shedding light on the nuanced\nimpact of misalignment on multimodal representation learning.\n","authors":["Yichao Cai","Yuhang Liu","Erdun Gao","Tianjiao Jiang","Zhen Zhang","Anton van den Hengel","Javen Qinfeng Shi"],"pdf_url":"https://arxiv.org/pdf/2504.10143v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08626v2","updated":"2025-04-16T04:31:06Z","published":"2024-07-11T16:05:56Z","title":"RoboMorph: Evolving Robot Morphology using Large Language Models","summary":"  We introduce RoboMorph, an automated approach for generating and optimizing\nmodular robot designs using large language models (LLMs) and evolutionary\nalgorithms. In this framework, we represent each robot design as a grammar and\nleverage the capabilities of LLMs to navigate the extensive robot design space,\nwhich is traditionally time-consuming and computationally demanding. By\nintroducing a best-shot prompting technique and a reinforcement learning-based\ncontrol algorithm, RoboMorph iteratively improves robot designs through\nfeedback loops. Experimental results demonstrate that RoboMorph successfully\ngenerates nontrivial robots optimized for different terrains while showcasing\nimprovements in robot morphology over successive evolutions. Our approach\nhighlights the potential of using LLMs for data-driven, modular robot design,\nproviding a promising methodology that can be extended to other domains with\nsimilar design frameworks.\n","authors":["Kevin Qiu","Władysław Pałucki","Krzysztof Ciebiera","Paweł Fijałkowski","Marek Cygan","Łukasz Kuciński"],"pdf_url":"https://arxiv.org/pdf/2407.08626v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11302v3","updated":"2025-04-16T04:30:56Z","published":"2024-12-15T20:27:45Z","title":"Sequence-Level Leakage Risk of Training Data in Large Language Models","summary":"  This work quantifies the risk of training data leakage from LLMs (Large\nLanguage Models) using sequence-level probabilities. Computing extraction\nprobabilities for individual sequences provides finer-grained information than\nhas been studied in prior benchmarking work. We re-analyze the effects of\ndecoding schemes, model sizes, prefix lengths, partial sequence leakages, and\ntoken positions to uncover new insights that were not possible in previous\nworks due to their choice of metrics. We perform this study on two pre-trained\nmodels, Llama and OPT, trained on the Common Crawl and The Pile respectively.\nWe discover that 1) Extraction Rate, the predominant metric used in prior\nquantification work, underestimates the threat of leakage of training data in\nrandomized LLMs by as much as 2.14X. 2) Although on average, larger models and\nlonger prefixes can extract more data, this is not true for a substantial\nportion of individual sequences. 30.4-41.5% of our sequences are easier to\nextract with either shorter prefixes or smaller models. 3) Contrary to previous\nbeliefs, partial leakage in commonly used decoding schemes like top-k and top-p\nis not easier than leaking verbatim training data. The aim of this work is to\nencourage the adoption of this metric for future work on quantification of\ntraining data extraction.\n","authors":["Trishita Tiwari","G. Edward Suh"],"pdf_url":"https://arxiv.org/pdf/2412.11302v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11757v1","updated":"2025-04-16T04:28:05Z","published":"2025-04-16T04:28:05Z","title":"Dynamics and Computational Principles of Echo State Networks: A\n  Mathematical Perspective","summary":"  Reservoir computing (RC) represents a class of state-space models (SSMs)\ncharacterized by a fixed state transition mechanism (the reservoir) and a\nflexible readout layer that maps from the state space. It is a paradigm of\ncomputational dynamical systems that harnesses the transient dynamics of\nhigh-dimensional state spaces for efficient processing of temporal data. Rooted\nin concepts from recurrent neural networks, RC achieves exceptional\ncomputational power by decoupling the training of the dynamic reservoir from\nthe linear readout layer, thereby circumventing the complexities of\ngradient-based optimization. This work presents a systematic exploration of RC,\naddressing its foundational properties such as the echo state property, fading\nmemory, and reservoir capacity through the lens of dynamical systems theory. We\nformalize the interplay between input signals and reservoir states,\ndemonstrating the conditions under which reservoirs exhibit stability and\nexpressive power. Further, we delve into the computational trade-offs and\nrobustness characteristics of RC architectures, extending the discussion to\ntheir applications in signal processing, time-series prediction, and control\nsystems. The analysis is complemented by theoretical insights into\noptimization, training methodologies, and scalability, highlighting open\nchallenges and potential directions for advancing the theoretical underpinnings\nof RC.\n","authors":["Pradeep Singh","Ashutosh Kumar","Sutirtha Ghosh","Hrishit B P","Balasubramanian Raman"],"pdf_url":"https://arxiv.org/pdf/2504.11757v1.pdf","comment":"100 pages, 17 tables, 41 figures"},{"id":"http://arxiv.org/abs/2504.11754v1","updated":"2025-04-16T04:13:53Z","published":"2025-04-16T04:13:53Z","title":"GrabS: Generative Embodied Agent for 3D Object Segmentation without\n  Scene Supervision","summary":"  We study the hard problem of 3D object segmentation in complex point clouds\nwithout requiring human labels of 3D scenes for supervision. By relying on the\nsimilarity of pretrained 2D features or external signals such as motion to\ngroup 3D points as objects, existing unsupervised methods are usually limited\nto identifying simple objects like cars or their segmented objects are often\ninferior due to the lack of objectness in pretrained features. In this paper,\nwe propose a new two-stage pipeline called GrabS. The core concept of our\nmethod is to learn generative and discriminative object-centric priors as a\nfoundation from object datasets in the first stage, and then design an embodied\nagent to learn to discover multiple objects by querying against the pretrained\ngenerative priors in the second stage. We extensively evaluate our method on\ntwo real-world datasets and a newly created synthetic dataset, demonstrating\nremarkable segmentation performance, clearly surpassing all existing\nunsupervised methods.\n","authors":["Zihui Zhang","Yafei Yang","Hongtao Wen","Bo Yang"],"pdf_url":"https://arxiv.org/pdf/2504.11754v1.pdf","comment":"ICLR 2025 Spotlight. Code and data are available at:\n  https://github.com/vLAR-group/GrabS"},{"id":"http://arxiv.org/abs/2502.06817v2","updated":"2025-04-16T03:40:29Z","published":"2025-02-05T03:08:17Z","title":"Diffusion-empowered AutoPrompt MedSAM","summary":"  MedSAM, a medical foundation model derived from the SAM architecture, has\ndemonstrated notable success across diverse medical domains. However, its\nclinical application faces two major challenges: the dependency on\nlabor-intensive manual prompt generation, which imposes a significant burden on\nclinicians, and the absence of semantic labeling in the generated segmentation\nmasks for organs or lesions, limiting its practicality for non-expert users. To\naddress these limitations, we propose AutoMedSAM, an end-to-end framework\nderived from SAM, designed to enhance usability and segmentation performance.\nAutoMedSAM retains MedSAM's image encoder and mask decoder structure while\nintroducing a novel diffusion-based class prompt encoder. The diffusion-based\nencoder employs a dual-decoder structure to collaboratively generate prompt\nembeddings guided by sparse and dense prompt definitions. These embeddings\nenhance the model's ability to understand and process clinical imagery\nautonomously. With this encoder, AutoMedSAM leverages class prompts to embed\nsemantic information into the model's predictions, transforming MedSAM's\nsemi-automated pipeline into a fully automated workflow. Furthermore,\nAutoMedSAM employs an uncertainty-aware joint optimization strategy during\ntraining to effectively inherit MedSAM's pre-trained knowledge while improving\ngeneralization by integrating multiple loss functions. Experimental results\nacross diverse datasets demonstrate that AutoMedSAM achieves superior\nperformance while broadening its applicability to both clinical settings and\nnon-expert users. Code is available at\nhttps://github.com/HP-ML/AutoPromptMedSAM.git.\n","authors":["Peng Huang","Shu Hu","Bo Peng","Xun Gong","Penghang Yin","Hongtu Zhu","Xi Wu","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06817v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11741v1","updated":"2025-04-16T03:39:38Z","published":"2025-04-16T03:39:38Z","title":"Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve\n  after SFT?","summary":"  Recent supervised fine-tuning (SFT) approaches have significantly improved\nlanguage models' performance on mathematical reasoning tasks, even when models\nare trained at a small scale. However, the specific capabilities enhanced\nthrough such fine-tuning remain poorly understood. In this paper, we conduct a\ndetailed analysis of model performance on the AIME24 dataset to understand how\nreasoning capabilities evolve. We discover a ladder-like structure in problem\ndifficulty, categorize questions into four tiers (Easy, Medium, Hard, and\nExtremely Hard (Exh)), and identify the specific requirements for advancing\nbetween tiers. We find that progression from Easy to Medium tier requires\nadopting an R1 reasoning style with minimal SFT (500-1K instances), while\nHard-level questions suffer from frequent model's errors at each step of the\nreasoning chain, with accuracy plateauing at around 65% despite logarithmic\nscaling. Exh-level questions present a fundamentally different challenge; they\nrequire unconventional problem-solving skills that current models uniformly\nstruggle with. Additional findings reveal that carefully curated small-scale\ndatasets offer limited advantage-scaling dataset size proves far more\neffective. Our analysis provides a clearer roadmap for advancing language model\ncapabilities in mathematical reasoning.\n","authors":["Yiyou Sun","Georgia Zhou","Hao Wang","Dacheng Li","Nouha Dziri","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2504.11741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11197v2","updated":"2025-04-16T03:32:23Z","published":"2025-04-15T13:53:08Z","title":"Efficient Distributed Retrieval-Augmented Generation for Enhancing\n  Language Model Performance","summary":"  Small language models (SLMs) support efficient deployments on\nresource-constrained edge devices, but their limited capacity compromises\ninference performance. Retrieval-augmented generation (RAG) is a promising\nsolution to enhance model performance by integrating external databases,\nwithout requiring intensive on-device model retraining. However, large-scale\npublic databases and user-specific private contextual documents are typically\nlocated on the cloud and the device separately, while existing RAG\nimplementations are primarily centralized. To bridge this gap, we propose\nDRAGON, a distributed RAG framework to enhance on-device SLMs through both\ngeneral and personal knowledge without the risk of leaking document privacy.\nSpecifically, DRAGON decomposes multi-document RAG into multiple parallel token\ngeneration processes performed independently and locally on the cloud and the\ndevice, and employs a newly designed Speculative Aggregation, a dual-side\nspeculative algorithm to avoid frequent output synchronization between the\ncloud and device. A new scheduling algorithm is further introduced to identify\nthe optimal aggregation side based on real-time network conditions. Evaluations\non real-world hardware testbed demonstrate a significant performance\nimprovement of DRAGON-up to 1.9x greater gains over standalone SLM compared to\nthe centralized RAG, substantial reduction in per-token latency, and negligible\nTime to First Token (TTFT) overhead.\n","authors":["Shangyu Liu","Zhenzhe Zheng","Xiaoyao Huang","Fan Wu","Guihai Chen","Jie Wu"],"pdf_url":"https://arxiv.org/pdf/2504.11197v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03098v3","updated":"2025-04-16T03:28:59Z","published":"2024-10-04T02:47:49Z","title":"Forest Proximities for Time Series","summary":"  RF-GAP has recently been introduced as an improved random forest proximity\nmeasure. In this paper, we present PF-GAP, an extension of RF-GAP proximities\nto proximity forests, an accurate and efficient time series classification\nmodel. We use the forest proximities in connection with Multi-Dimensional\nScaling to obtain vector embeddings of univariate time series, comparing the\nembeddings to those obtained using various time series distance measures. We\nalso use the forest proximities alongside Local Outlier Factors to investigate\nthe connection between misclassified points and outliers, comparing with\nnearest neighbor classifiers which use time series distance measures. We show\nthat the forest proximities seem to exhibit a stronger connection between\nmisclassified points and outliers than nearest neighbor classifiers.\n","authors":["Ben Shaw","Jake Rhodes","Soukaina Filali Boubrahimi","Kevin R. Moon"],"pdf_url":"https://arxiv.org/pdf/2410.03098v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11019v2","updated":"2025-04-16T03:22:41Z","published":"2025-02-16T07:06:17Z","title":"Unlocking the Power of Function Vectors for Characterizing and\n  Mitigating Catastrophic Forgetting in Continual Instruction Tuning","summary":"  Catastrophic forgetting (CF) poses a significant challenge in machine\nlearning, where a model forgets previously learned information upon learning\nnew tasks. Despite the advanced capabilities of Large Language Models (LLMs),\nthey continue to face challenges with CF during continual learning. The\nmajority of existing research focuses on analyzing forgetting patterns through\na singular training sequence, thereby overlooking the intricate effects that\ndiverse tasks have on model behavior. Our study explores CF across various\nsettings, discovering that model forgetting is influenced by both the specific\ntraining tasks and the models themselves. To this end, we interpret forgetting\nby examining the function vector (FV), a compact representation of functions in\nLLMs, offering a model-dependent indicator for the occurrence of CF. Through\ntheoretical and empirical analyses, we demonstrated that CF in LLMs primarily\nstems from biases in function activation rather than the overwriting of task\nprocessing functions. Leveraging these insights, we propose a novel function\nvector guided training methodology, incorporating a regularization technique to\nstabilize the FV and mitigate forgetting. Empirical tests on four benchmarks\nconfirm the effectiveness of our proposed training method, substantiating our\ntheoretical framework concerning CF and model function dynamics. We plan to\nmake our code publicly accessible in the near future.\n","authors":["Gangwei Jiang","Caigao Jiang","Zhaoyi Li","Siqiao Xue","Jun Zhou","Linqi Song","Defu Lian","Ying Wei"],"pdf_url":"https://arxiv.org/pdf/2502.11019v2.pdf","comment":"10pages"},{"id":"http://arxiv.org/abs/2410.20182v2","updated":"2025-04-16T03:19:29Z","published":"2024-10-26T13:40:13Z","title":"Chemical Language Model Linker: blending text and molecules with modular\n  adapters","summary":"  The development of large language models and multi-modal models has enabled\nthe appealing idea of generating novel molecules from text descriptions.\nGenerative modeling would shift the paradigm from relying on large-scale\nchemical screening to find molecules with desired properties to directly\ngenerating those molecules. However, multi-modal models combining text and\nmolecules are often trained from scratch, without leveraging existing\nhigh-quality pretrained models. Training from scratch consumes more\ncomputational resources and prohibits model scaling. In contrast, we propose a\nlightweight adapter-based strategy named Chemical Language Model Linker\n(ChemLML). ChemLML blends the two single domain models and obtains conditional\nmolecular generation from text descriptions while still operating in the\nspecialized embedding spaces of the molecular domain. ChemLML can tailor\ndiverse pretrained text models for molecule generation by training relatively\nfew adapter parameters. We find that the choice of molecular representation\nused within ChemLML, SMILES versus SELFIES, has a strong influence on\nconditional molecular generation performance. SMILES is often preferable\ndespite not guaranteeing valid molecules. We raise issues in using the entire\nPubChem dataset of molecules and their associated descriptions for evaluating\nmolecule generation and provide a filtered version of the dataset as a\ngeneration test set. To demonstrate how ChemLML could be used in practice, we\ngenerate candidate protein inhibitors and use docking to assess their quality\nand also generate candidate membrane permeable molecules.\n","authors":["Yifan Deng","Spencer S. Ericksen","Anthony Gitter"],"pdf_url":"https://arxiv.org/pdf/2410.20182v2.pdf","comment":"60 pages, 12 figures"},{"id":"http://arxiv.org/abs/2501.14700v4","updated":"2025-04-16T03:11:32Z","published":"2025-01-24T18:22:37Z","title":"An Attentive Graph Agent for Topology-Adaptive Cyber Defence","summary":"  As cyber threats grow increasingly sophisticated, reinforcement learning (RL)\nis emerging as a promising technique to create intelligent and adaptive cyber\ndefense systems. However, most existing autonomous defensive agents have\noverlooked the inherent graph structure of computer networks subject to cyber\nattacks, potentially missing critical information and constraining their\nadaptability. To overcome these limitations, we developed a custom version of\nthe Cyber Operations Research Gym (CybORG) environment, encoding network state\nas a directed graph with realistic low-level features. We employ a Graph\nAttention Network (GAT) architecture to process node, edge, and global\nfeatures, and adapt its output to be compatible with policy gradient methods in\nRL. Our GAT-based approach offers key advantages over flattened alternatives:\npolicies that demonstrate resilience to certain types of unexpected dynamic\nnetwork topology changes, reasonable generalisation to networks of varying\nsizes within the same structural distribution, and interpretable defensive\nactions grounded in tangible network properties. We demonstrate that GAT\ndefensive policies can be trained using our low-level directed graph\nobservations, even when unexpected connections arise during simulation.\nEvaluations across networks of different sizes, but consistent subnetwork\nstructure, show our policies achieve comparable performance to policies trained\nspecifically for each network configuration. Our study contributes to the\ndevelopment of robust cyber defence systems that can better adapt to real-world\nnetwork security challenges.\n","authors":["Ilya Orson Sandoval","Isaac Symes Thompson","Vasilios Mavroudis","Chris Hicks"],"pdf_url":"https://arxiv.org/pdf/2501.14700v4.pdf","comment":"Draft requires substantial revision"},{"id":"http://arxiv.org/abs/2504.11726v1","updated":"2025-04-16T03:03:42Z","published":"2025-04-16T03:03:42Z","title":"Saga: Capturing Multi-granularity Semantics from Massive Unlabelled IMU\n  Data for User Perception","summary":"  Inertial measurement units (IMUs), have been prevalently used in a wide range\nof mobile perception applications such as activity recognition and user\nauthentication, where a large amount of labelled data are normally required to\ntrain a satisfactory model. However, it is difficult to label micro-activities\nin massive IMU data due to the hardness of understanding raw IMU data and the\nlack of ground truth. In this paper, we propose a novel fine-grained user\nperception approach, called Saga, which only needs a small amount of labelled\nIMU data to achieve stunning user perception accuracy. The core idea of Saga is\nto first pre-train a backbone feature extraction model, utilizing the rich\nsemantic information of different levels embedded in the massive unlabelled IMU\ndata. Meanwhile, for a specific downstream user perception application,\nBayesian Optimization is employed to determine the optimal weights for\npre-training tasks involving different semantic levels. We implement Saga on\nfive typical mobile phones and evaluate Saga on three typical tasks on three\nIMU datasets. Results show that when only using about 100 training samples per\nclass, Saga can achieve over 90% accuracy of the full-fledged model trained on\nover ten thousands training samples with no additional system overhead.\n","authors":["Yunzhe Li","Facheng Hu","Hongzi Zhu","Shifan Zhang","Liang Zhang","Shan Chang","Minyi Guo"],"pdf_url":"https://arxiv.org/pdf/2504.11726v1.pdf","comment":"2025 IEEE 45th International Conference on Distributed Computing\n  Systems (ICDCS)"},{"id":"http://arxiv.org/abs/2406.05826v2","updated":"2025-04-16T02:39:44Z","published":"2024-06-09T15:31:00Z","title":"PSBD: Prediction Shift Uncertainty Unlocks Backdoor Detection","summary":"  Deep neural networks are susceptible to backdoor attacks, where adversaries\nmanipulate model predictions by inserting malicious samples into the training\ndata. Currently, there is still a significant challenge in identifying\nsuspicious training data to unveil potential backdoor samples. In this paper,\nwe propose a novel method, Prediction Shift Backdoor Detection (PSBD),\nleveraging an uncertainty-based approach requiring minimal unlabeled clean\nvalidation data. PSBD is motivated by an intriguing Prediction Shift (PS)\nphenomenon, where poisoned models' predictions on clean data often shift away\nfrom true labels towards certain other labels with dropout applied during\ninference, while backdoor samples exhibit less PS. We hypothesize PS results\nfrom the neuron bias effect, making neurons favor features of certain classes.\nPSBD identifies backdoor training samples by computing the Prediction Shift\nUncertainty (PSU), the variance in probability values when dropout layers are\ntoggled on and off during model inference. Extensive experiments have been\nconducted to verify the effectiveness and efficiency of PSBD, which achieves\nstate-of-the-art results among mainstream detection methods. The code is\navailable at https://github.com/WL-619/PSBD.\n","authors":["Wei Li","Pin-Yu Chen","Sijia Liu","Ren Wang"],"pdf_url":"https://arxiv.org/pdf/2406.05826v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11454v2","updated":"2025-04-16T02:35:11Z","published":"2025-04-15T17:59:43Z","title":"Elucidating the Design Space of Multimodal Protein Language Models","summary":"  Multimodal protein language models (PLMs) integrate sequence and token-based\nstructural information, serving as a powerful foundation for protein modeling,\ngeneration, and design. However, the reliance on tokenizing 3D structures into\ndiscrete tokens causes substantial loss of fidelity about fine-grained\nstructural details and correlations. In this paper, we systematically elucidate\nthe design space of multimodal PLMs to overcome their limitations. We identify\ntokenization loss and inaccurate structure token predictions by the PLMs as\nmajor bottlenecks. To address these, our proposed design space covers improved\ngenerative modeling, structure-aware architectures and representation learning,\nand data exploration. Our advancements approach finer-grained supervision,\ndemonstrating that token-based multimodal PLMs can achieve robust structural\nmodeling. The effective design methods dramatically improve the structure\ngeneration diversity, and notably, folding abilities of our 650M model by\nreducing the RMSD from 5.52 to 2.36 on PDB testset, even outperforming 3B\nbaselines and on par with the specialized folding models.\n","authors":["Cheng-Yen Hsieh","Xinyou Wang","Daiheng Zhang","Dongyu Xue","Fei Ye","Shujian Huang","Zaixiang Zheng","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2504.11454v2.pdf","comment":"Project Page: https://bytedance.github.io/dplm/dplm-2.1/"},{"id":"http://arxiv.org/abs/2504.11714v1","updated":"2025-04-16T02:20:56Z","published":"2025-04-16T02:20:56Z","title":"Unravelling Technical debt topics through Time, Programming Languages\n  and Repository","summary":"  This study explores the dynamic landscape of Technical Debt (TD) topics in\nsoftware engineering by examining its evolution across time, programming\nlanguages, and repositories. Despite the extensive research on identifying and\nquantifying TD, there remains a significant gap in understanding the diversity\nof TD topics and their temporal development. To address this, we have conducted\nan explorative analysis of TD data extracted from GitHub issues spanning from\n2015 to September 2023. We employed BERTopic for sophisticated topic modelling.\nThis study categorises the TD topics and tracks their progression over time.\nFurthermore, we have incorporated sentiment analysis for each identified topic,\nproviding a deeper insight into the perceptions and attitudes associated with\nthese topics. This offers a more nuanced understanding of the trends and shifts\nin TD topics through time, programming language, and repository.\n","authors":["Karthik Shivashankar","Antonio Martini"],"pdf_url":"https://arxiv.org/pdf/2504.11714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11713v1","updated":"2025-04-16T02:20:06Z","published":"2025-04-16T02:20:06Z","title":"Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint\n  Matching","summary":"  We introduce Adjoint Sampling, a highly scalable and efficient algorithm for\nlearning diffusion processes that sample from unnormalized densities, or energy\nfunctions. It is the first on-policy approach that allows significantly more\ngradient updates than the number of energy evaluations and model samples,\nallowing us to scale to much larger problem settings than previously explored\nby similar methods. Our framework is theoretically grounded in stochastic\noptimal control and shares the same theoretical guarantees as Adjoint Matching,\nbeing able to train without the need for corrective measures that push samples\ntowards the target distribution. We show how to incorporate key symmetries, as\nwell as periodic boundary conditions, for modeling molecules in both cartesian\nand torsional coordinates. We demonstrate the effectiveness of our approach\nthrough extensive experiments on classical energy functions, and further scale\nup to neural network-based energy models where we perform amortized conformer\ngeneration across many molecular systems. To encourage further research in\ndeveloping highly scalable sampling methods, we plan to open source these\nchallenging benchmarks, where successful methods can directly impact progress\nin computational chemistry.\n","authors":["Aaron Havens","Benjamin Kurt Miller","Bing Yan","Carles Domingo-Enrich","Anuroop Sriram","Brandon Wood","Daniel Levine","Bin Hu","Brandon Amos","Brian Karrer","Xiang Fu","Guan-Horng Liu","Ricky T. Q. Chen"],"pdf_url":"https://arxiv.org/pdf/2504.11713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11702v1","updated":"2025-04-16T01:57:33Z","published":"2025-04-16T01:57:33Z","title":"Clustering and analysis of user behaviour in blockchain: A case study of\n  Planet IX","summary":"  Decentralised applications (dApps) that run on public blockchains have the\nbenefit of trustworthiness and transparency as every activity that happens on\nthe blockchain can be publicly traced through the transaction data. However,\nthis introduces a potential privacy problem as this data can be tracked and\nanalysed, which can reveal user-behaviour information. A user behaviour\nanalysis pipeline was proposed to present how this type of information can be\nextracted and analysed to identify separate behavioural clusters that can\ndescribe how users behave in the game. The pipeline starts with the collection\nof transaction data, involving smart contracts, that is collected from a\nblockchain-based game called Planet IX. Both the raw transaction information\nand the transaction events are considered in the data collection. From this\ndata, separate game actions can be formed and those are leveraged to present\nhow and when the users conducted their in-game activities in the form of user\nflows. An extended version of these user flows also presents how the\nNon-Fungible Tokens (NFTs) are being leveraged in the user actions. The latter\nis given as input for a Graph Neural Network (GNN) model to provide graph\nembeddings for these flows which then can be leveraged by clustering algorithms\nto cluster user behaviours into separate behavioural clusters. We benchmark and\ncompare well-known clustering algorithms as a part of the proposed method. The\nuser behaviour clusters were analysed and visualised in a graph format. It was\nfound that behavioural information can be extracted regarding the users that\nbelong to these clusters. Such information can be exploited by malicious users\nto their advantage. To demonstrate this, a privacy threat model was also\npresented based on the results that correspond to multiple potentially affected\nareas.\n","authors":["Dorottya Zelenyanszki","Zhe Hou","Kamanashis Biswas","Vallipuram Muthukkumarasamy"],"pdf_url":"https://arxiv.org/pdf/2504.11702v1.pdf","comment":"15 pages, 8 figures, submitted to Blockchain: Research and\n  Applications"},{"id":"http://arxiv.org/abs/2504.11699v1","updated":"2025-04-16T01:51:25Z","published":"2025-04-16T01:51:25Z","title":"H$^3$GNNs: Harmonizing Heterophily and Homophily in GNNs via Joint\n  Structural Node Encoding and Self-Supervised Learning","summary":"  Graph Neural Networks (GNNs) struggle to balance heterophily and homophily in\nrepresentation learning, a challenge further amplified in self-supervised\nsettings. We propose H$^3$GNNs, an end-to-end self-supervised learning\nframework that harmonizes both structural properties through two key\ninnovations: (i) Joint Structural Node Encoding. We embed nodes into a unified\nspace combining linear and non-linear feature projections with K-hop structural\nrepresentations via a Weighted Graph Convolution Network(WGCN). A\ncross-attention mechanism enhances awareness and adaptability to heterophily\nand homophily. (ii) Self-Supervised Learning Using Teacher-Student Predictive\nArchitectures with Node-Difficulty Driven Dynamic Masking Strategies. We use a\nteacher-student model, the student sees the masked input graph and predicts\nnode features inferred by the teacher that sees the full input graph in the\njoint encoding space. To enhance learning difficulty, we introduce two novel\nnode-predictive-difficulty-based masking strategies. Experiments on seven\nbenchmarks (four heterophily datasets and three homophily datasets) confirm the\neffectiveness and efficiency of H$^3$GNNs across diverse graph types. Our\nH$^3$GNNs achieves overall state-of-the-art performance on the four heterophily\ndatasets, while retaining on-par performance to previous state-of-the-art\nmethods on the three homophily datasets.\n","authors":["Rui Xue","Tianfu Wu"],"pdf_url":"https://arxiv.org/pdf/2504.11699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10786v2","updated":"2025-04-16T01:27:42Z","published":"2025-04-15T01:04:56Z","title":"Visual Language Models show widespread visual deficits on\n  neuropsychological tests","summary":"  Visual Language Models (VLMs) show remarkable performance in visual reasoning\ntasks, successfully tackling college-level challenges that require high-level\nunderstanding of images. However, some recent reports of VLMs struggling to\nreason about elemental visual concepts like orientation, position, continuity,\nand occlusion suggest a potential gulf between human and VLM vision. Here we\nuse the toolkit of neuropsychology to systematically assess the capabilities of\nthree state-of-the-art VLMs across visual domains. Using 51 tests drawn from\nsix clinical and experimental batteries, we characterise the visual abilities\nof leading VLMs relative to normative performance in healthy adults. While the\nmodels excel in straightforward object recognition tasks, we find widespread\ndeficits in low- and mid-level visual abilities that would be considered\nclinically significant in humans. These selective deficits, profiled through\nvalidated test batteries, suggest that an artificial system can achieve complex\nobject recognition without developing foundational visual concepts that in\nhumans require no explicit training.\n","authors":["Gene Tangtartharakul","Katherine R. Storrs"],"pdf_url":"https://arxiv.org/pdf/2504.10786v2.pdf","comment":"31 pages, 3 figures, 1 supplementary document with 1 figure and 51\n  sample images; corrected typo in Fig 1"},{"id":"http://arxiv.org/abs/2501.08878v2","updated":"2025-04-16T01:21:23Z","published":"2025-01-15T15:49:46Z","title":"Incrementally Learning Multiple Diverse Data Domains via Multi-Source\n  Dynamic Expansion Model","summary":"  Continual Learning seeks to develop a model capable of incrementally\nassimilating new information while retaining prior knowledge. However, current\nresearch predominantly addresses a straightforward learning context, wherein\nall data samples originate from a singular data domain. This paper shifts focus\nto a more complex and realistic learning environment, characterized by data\nsamples sourced from multiple distinct domains. We tackle this intricate\nlearning challenge by introducing a novel methodology, termed the Multi-Source\nDynamic Expansion Model (MSDEM), which leverages various pre-trained models as\nbackbones and progressively establishes new experts based on them to adapt to\nemerging tasks. Additionally, we propose an innovative dynamic expandable\nattention mechanism designed to selectively harness knowledge from multiple\nbackbones, thereby accelerating the new task learning. Moreover, we introduce a\ndynamic graph weight router that strategically reuses all previously acquired\nparameters and representations for new task learning, maximizing the positive\nknowledge transfer effect, which further improves generalization performance.\nWe conduct a comprehensive series of experiments, and the empirical findings\nindicate that our proposed approach achieves state-of-the-art performance.\n","authors":["Runqing Wu","Fei Ye","Qihe Liu","Guoxi Huang","Jinyu Guo","Rongyao Hu"],"pdf_url":"https://arxiv.org/pdf/2501.08878v2.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.10561v2","updated":"2025-04-16T01:13:45Z","published":"2025-04-14T15:22:51Z","title":"Self-Controlled Dynamic Expansion Model for Continual Learning","summary":"  Continual Learning (CL) epitomizes an advanced training paradigm wherein\nprior data samples remain inaccessible during the acquisition of new tasks.\nNumerous investigations have delved into leveraging a pre-trained Vision\nTransformer (ViT) to enhance model efficacy in continual learning. Nonetheless,\nthese approaches typically utilize a singular, static backbone, which\ninadequately adapts to novel tasks, particularly when engaging with diverse\ndata domains, due to a substantial number of inactive parameters. This paper\naddresses this limitation by introducing an innovative Self-Controlled Dynamic\nExpansion Model (SCDEM), which orchestrates multiple distinct trainable\npre-trained ViT backbones to furnish diverse and semantically enriched\nrepresentations. Specifically, by employing the multi-backbone architecture as\na shared module, the proposed SCDEM dynamically generates a new expert with\nminimal parameters to accommodate a new task. A novel Collaborative\nOptimization Mechanism (COM) is introduced to synergistically optimize multiple\nbackbones by harnessing prediction signals from historical experts, thereby\nfacilitating new task learning without erasing previously acquired knowledge.\nAdditionally, a novel Feature Distribution Consistency (FDC) approach is\nproposed to align semantic similarity between previously and currently learned\nrepresentations through an optimal transport distance-based mechanism,\neffectively mitigating negative knowledge transfer effects. Furthermore, to\nalleviate over-regularization challenges, this paper presents a novel Dynamic\nLayer-Wise Feature Attention Mechanism (DLWFAM) to autonomously determine the\npenalization intensity on each trainable representation layer. An extensive\nseries of experiments have been conducted to evaluate the proposed\nmethodology's efficacy, with empirical results corroborating that the approach\nattains state-of-the-art performance.\n","authors":["Runqing Wu","Kaihui Huang","Hanyi Zhang","Fei Ye"],"pdf_url":"https://arxiv.org/pdf/2504.10561v2.pdf","comment":"10 pages, 3 figures, 6 tables, Continual Learning, Cross-Domain\n  Continual Learning, Mixture Model"}],"Computation and Language":[{"id":"http://arxiv.org/abs/2504.12285v1","updated":"2025-04-16T17:51:43Z","published":"2025-04-16T17:51:43Z","title":"BitNet b1.58 2B4T Technical Report","summary":"  We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.\n","authors":["Shuming Ma","Hongyu Wang","Shaohan Huang","Xingxing Zhang","Ying Hu","Ting Song","Yan Xia","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2504.12285v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.02108v2","updated":"2025-04-16T17:50:12Z","published":"2024-10-03T00:09:15Z","title":"ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement","summary":"  Post-training Large Language Models (LLMs) with explicit reasoning\ntrajectories can enhance their reasoning abilities. However, acquiring such\nhigh-quality trajectory data typically demands meticulous supervision from\nhumans or superior models, which can be either expensive or\nlicense-constrained. In this paper, we explore how far an LLM can improve its\nreasoning by self-synthesizing reasoning paths as training data without any\nadditional supervision. Existing self-synthesizing methods, such as STaR,\nsuffer from poor generalization to out-of-domain (OOD) reasoning tasks. We\nhypothesize it is due to that their self-synthesized reasoning paths are too\ntask-specific, lacking general task-agnostic reasoning guidance. To address\nthis, we propose Reasoning Generalist via Self-Improvement (ReGenesis), a\nmethod to self-synthesize reasoning paths as post-training data by progressing\nfrom abstract to concrete. More specifically, ReGenesis self-synthesizes\nreasoning paths by converting general reasoning guidelines into task-specific\nones, generating reasoning structures, and subsequently transforming these\nstructures into reasoning paths, without the need for human-designed\ntask-specific examples used in existing methods. We show that ReGenesis\nachieves superior performance on all in-domain and OOD settings tested compared\nto existing methods. For six OOD tasks specifically, while previous methods\nexhibited an average performance decrease of approximately 4.6% after post\ntraining, ReGenesis delivers around 6.1% performance improvement. We also\nconduct in-depth analysis of our framework and show ReGenesis is effective\nacross various LLMs and design choices.\n","authors":["Xiangyu Peng","Congying Xia","Xinyi Yang","Caiming Xiong","Chien-Sheng Wu","Chen Xing"],"pdf_url":"https://arxiv.org/pdf/2410.02108v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12279v1","updated":"2025-04-16T17:41:19Z","published":"2025-04-16T17:41:19Z","title":"Dysarthria Normalization via Local Lie Group Transformations for Robust\n  ASR","summary":"  We present a geometry-driven method for normalizing dysarthric speech using\nlocal Lie group transformations of spectrograms. Time, frequency, and amplitude\ndistortions are modeled as smooth, invertible deformations, parameterized by\nscalar fields and applied via exponential maps. A neural network is trained to\ninfer these fields from synthetic distortions of typical speech-without using\nany pathological data. At test time, the model applies an approximate inverse\nto real dysarthric inputs. Despite zero-shot generalization, we observe\nsubstantial ASR gains, including up to 16 percentage points WER reduction on\nchallenging TORGO samples, with no degradation on clean speech. This work\nintroduces a principled, interpretable approach for robust speech recognition\nunder motor speech disorders\n","authors":["Mikhail Osipov"],"pdf_url":"https://arxiv.org/pdf/2504.12279v1.pdf","comment":"Preprint. 11 pages, 3 figures, 2 tables, 8 appendices. Code and data\n  available upon request"},{"id":"http://arxiv.org/abs/2406.19388v4","updated":"2025-04-16T17:40:22Z","published":"2024-06-27T17:58:54Z","title":"Taming Data and Transformers for Audio Generation","summary":"  The scalability of ambient sound generators is hindered by data scarcity,\ninsufficient caption quality, and limited scalability in model architecture.\nThis work addresses these challenges by advancing both data and model scaling.\nFirst, we propose an efficient and scalable dataset collection pipeline\ntailored for ambient audio generation, resulting in AutoReCap-XL, the largest\nambient audio-text dataset with over 47 million clips. To provide high-quality\ntextual annotations, we propose AutoCap, a high-quality automatic audio\ncaptioning model. By adopting a Q-Former module and leveraging audio metadata,\nAutoCap substantially enhances caption quality, reaching a CIDEr score of\n$83.2$, a $3.2\\%$ improvement over previous captioning models. Finally, we\npropose GenAu, a scalable transformer-based audio generation architecture that\nwe scale up to 1.25B parameters. We demonstrate its benefits from data scaling\nwith synthetic captions as well as model size scaling. When compared to\nbaseline audio generators trained at similar size and data scale, GenAu obtains\nsignificant improvements of $4.7\\%$ in FAD score, $11.1\\%$ in IS, and $13.5\\%$\nin CLAP score. Our code, model checkpoints, and dataset are publicly available.\n","authors":["Moayed Haji-Ali","Willi Menapace","Aliaksandr Siarohin","Guha Balakrishnan","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2406.19388v4.pdf","comment":"Project Webpage: https://snap-research.github.io/GenAU/"},{"id":"http://arxiv.org/abs/2502.02696v2","updated":"2025-04-16T17:32:55Z","published":"2025-02-04T20:24:17Z","title":"How Inclusively do LMs Perceive Social and Moral Norms?","summary":"  This paper discusses and contains offensive content. Language models (LMs)\nare used in decision-making systems and as interactive assistants. However, how\nwell do these models making judgements align with the diversity of human\nvalues, particularly regarding social and moral norms? In this work, we\ninvestigate how inclusively LMs perceive norms across demographic groups (e.g.,\ngender, age, and income). We prompt 11 LMs on rules-of-thumb (RoTs) and compare\ntheir outputs with the existing responses of 100 human annotators. We introduce\nthe Absolute Distance Alignment Metric (ADA-Met) to quantify alignment on\nordinal questions. We find notable disparities in LM responses, with younger,\nhigher-income groups showing closer alignment, raising concerns about the\nrepresentation of marginalized perspectives. Our findings highlight the\nimportance of further efforts to make LMs more inclusive of diverse human\nvalues. The code and prompts are available on GitHub under the CC BY-NC 4.0\nlicense.\n","authors":["Michael Galarnyk","Agam Shah","Dipanwita Guhathakurta","Poojitha Nandigam","Sudheer Chava"],"pdf_url":"https://arxiv.org/pdf/2502.02696v2.pdf","comment":"Accepted at NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2409.02920v3","updated":"2025-04-16T17:31:39Z","published":"2024-09-04T17:59:52Z","title":"RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early\n  version)","summary":"  In the rapidly advancing field of robotics, dual-arm coordination and complex\nobject manipulation are essential capabilities for developing advanced\nautonomous systems. However, the scarcity of diverse, high-quality\ndemonstration data and real-world-aligned evaluation benchmarks severely limits\nsuch development. To address this, we introduce RoboTwin, a generative digital\ntwin framework that uses 3D generative foundation models and large language\nmodels to produce diverse expert datasets and provide a real-world-aligned\nevaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates\nvaried digital twins of objects from single 2D images, generating realistic and\ninteractive scenarios. It also introduces a spatial relation-aware code\ngeneration framework that combines object annotations with large language\nmodels to break down tasks, determine spatial constraints, and generate precise\nrobotic movement code. Our framework offers a comprehensive benchmark with both\nsimulated and real-world data, enabling standardized evaluation and better\nalignment between simulated training and real-world performance. We validated\nour approach using the open-source COBOT Magic Robot platform. Policies\npre-trained on RoboTwin-generated data and fine-tuned with limited real-world\nsamples improve the success rate of over 70% for single-arm tasks and over 40%\nfor dual-arm tasks compared to models trained solely on real-world data. This\nsignificant improvement demonstrates RoboTwin's potential to enhance the\ndevelopment and evaluation of dual-arm robotic manipulation systems. Project\nPage: https://robotwin-benchmark.github.io/early-version/.\n","authors":["Yao Mu","Tianxing Chen","Shijia Peng","Zanxin Chen","Zeyu Gao","Yude Zou","Lunkai Lin","Zhiqiang Xie","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2409.02920v3.pdf","comment":"Project page: https://robotwin-benchmark.github.io/early-version/"},{"id":"http://arxiv.org/abs/2504.12254v1","updated":"2025-04-16T17:05:14Z","published":"2025-04-16T17:05:14Z","title":"Advancing Arabic Speech Recognition Through Large-Scale Weakly\n  Supervised Learning","summary":"  Automatic speech recognition (ASR) is crucial for human-machine interaction\nin diverse applications like conversational agents, industrial robotics, call\ncenter automation, and automated subtitling. However, developing\nhigh-performance ASR models remains challenging, particularly for low-resource\nlanguages like Arabic, due to the scarcity of large, labeled speech datasets,\nwhich are costly and labor-intensive to produce. In this work, we employ weakly\nsupervised learning to train an Arabic ASR model using the Conformer\narchitecture. Our model is trained from scratch on 15,000 hours of weakly\nannotated speech data covering both Modern Standard Arabic (MSA) and Dialectal\nArabic (DA), eliminating the need for costly manual transcriptions. Despite the\nabsence of human-verified labels, our approach attains state-of-the-art (SOTA)\nperformance, exceeding all previous efforts in the field of Arabic ASR on the\nstandard benchmarks. By demonstrating the effectiveness of weak supervision as\na scalable, cost-efficient alternative to traditional supervised approaches,\npaving the way for improved ASR systems in low resource settings.\n","authors":["Mahmoud Salhab","Marwan Elghitany","Shameed Sait","Syed Sibghat Ullah","Mohammad Abusheikh","Hasan Abusheikh"],"pdf_url":"https://arxiv.org/pdf/2504.12254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12229v1","updated":"2025-04-16T16:25:26Z","published":"2025-04-16T16:25:26Z","title":"Watermarking Needs Input Repetition Masking","summary":"  Recent advancements in Large Language Models (LLMs) raised concerns over\npotential misuse, such as for spreading misinformation. In response two counter\nmeasures emerged: machine learning-based detectors that predict if text is\nsynthetic, and LLM watermarking, which subtly marks generated text for\nidentification and attribution. Meanwhile, humans are known to adjust language\nto their conversational partners both syntactically and lexically. By\nimplication, it is possible that humans or unwatermarked LLMs could\nunintentionally mimic properties of LLM generated text, making counter measures\nunreliable. In this work we investigate the extent to which such conversational\nadaptation happens. We call the concept $\\textit{mimicry}$ and demonstrate that\nboth humans and LLMs end up mimicking, including the watermarking signal even\nin seemingly improbable settings. This challenges current academic assumptions\nand suggests that for long-term watermarking to be reliable, the likelihood of\nfalse positives needs to be significantly lower, while longer word sequences\nshould be used for seeding watermarking mechanisms.\n","authors":["David Khachaturov","Robert Mullins","Ilia Shumailov","Sumanth Dathathri"],"pdf_url":"https://arxiv.org/pdf/2504.12229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17404v3","updated":"2025-04-16T16:21:29Z","published":"2024-11-26T13:05:53Z","title":"BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical\n  Modeling Problem Solving","summary":"  LLMs exhibit advanced reasoning capabilities, offering the potential to\ntransform natural language questions into mathematical models. However,\nexisting open-source datasets in operations research domain lack detailed\nannotations of the modeling process, such as variable definitions, focusing\nsolely on objective values, which hinders reinforcement learning applications.\nTo address this, we release the StructuredOR dataset, annotated with\ncomprehensive labels that capture the complete mathematical modeling process.\nWe further propose BPP-Search, an algorithm that integrates reinforcement\nlearning into a tree-of-thought structure using Beam search, a Process reward\nmodel, and a pairwise Preference algorithm. This approach enables efficient\nexploration of tree structures, avoiding exhaustive search while improving\naccuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLP\ndatasets show that BPP-Search significantly outperforms state-of-the-art\nmethods. In tree-based reasoning, BPP-Search excels in accuracy and efficiency,\nenabling faster retrieval of correct solutions. The StructuredOR dataset is\navailable at https://github.com/tengwang0318/StructuredOR.\n","authors":["Teng Wang","Wing-Yin Yu","Zhenqi He","Zehua Liu","Hailei Gong","Han Wu","Xiongwei Han","Wei Shi","Ruifeng She","Fangzhou Zhu","Tao Zhong"],"pdf_url":"https://arxiv.org/pdf/2411.17404v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12216v1","updated":"2025-04-16T16:08:45Z","published":"2025-04-16T16:08:45Z","title":"d1: Scaling Reasoning in Diffusion Large Language Models via\n  Reinforcement Learning","summary":"  Recent large language models (LLMs) have demonstrated strong reasoning\ncapabilities that benefits from online reinforcement learning (RL). These\ncapabilities have primarily been demonstrated within the left-to-right\nautoregressive (AR) generation paradigm. In contrast, non-autoregressive\nparadigms based on diffusion generate text in a coarse-to-fine manner. Although\nrecent diffusion-based large language models (dLLMs) have achieved competitive\nlanguage modeling performance compared to their AR counterparts, it remains\nunclear if dLLMs can also leverage recent advances in LLM reasoning. To this\nend, we propose d1, a framework to adapt pre-trained masked dLLMs into\nreasoning models via a combination of supervised finetuning (SFT) and RL.\nSpecifically, we develop and extend techniques to improve reasoning in\npretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge\nand instill self-improvement behavior directly from existing datasets, and (b)\nwe introduce a novel critic-free, policy-gradient based RL algorithm called\ndiffu-GRPO. Through empirical studies, we investigate the performance of\ndifferent post-training recipes on multiple mathematical and logical reasoning\nbenchmarks. We find that d1 yields the best performance and significantly\nimproves performance of a state-of-the-art dLLM.\n","authors":["Siyan Zhao","Devaansh Gupta","Qinqing Zheng","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2504.12216v1.pdf","comment":"25 pages, project page at https://dllm-reasoning.github.io/"},{"id":"http://arxiv.org/abs/2410.17088v2","updated":"2025-04-16T16:00:53Z","published":"2024-10-22T15:14:54Z","title":"Science Out of Its Ivory Tower: Improving Accessibility with\n  Reinforcement Learning","summary":"  A vast amount of scholarly work is published daily, yet much of it remains\ninaccessible to the general public due to dense jargon and complex language. To\naddress this challenge in science communication, we introduce a reinforcement\nlearning framework that fine-tunes a language model to rewrite scholarly\nabstracts into more comprehensible versions. Guided by a carefully balanced\ncombination of word- and sentence-level accessibility rewards, our language\nmodel effectively substitutes technical terms with more accessible\nalternatives, a task which models supervised fine-tuned or guided by\nconventional readability measures struggle to accomplish. Our best model\nadjusts the readability level of scholarly abstracts by approximately six U.S.\ngrade levels -- in other words, from a postgraduate to a high school level.\nThis translates to roughly a 90% relative boost over the supervised fine-tuning\nbaseline, all while maintaining factual accuracy and high-quality language. An\nin-depth analysis of our approach shows that balanced rewards lead to\nsystematic modifications in the base model, likely contributing to smoother\noptimization and superior performance. We envision this work as a step toward\nbridging the gap between scholarly research and the general public,\nparticularly younger readers and those without a college degree.\n","authors":["Haining Wang","Jason Clark","Hannah McKelvey","Leila Sterman","Zheng Gao","Zuoyu Tian","Sandra Kübler","Xiaozhong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.17088v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12144v3","updated":"2025-04-16T15:53:03Z","published":"2024-12-10T09:13:32Z","title":"Automatic Item Generation for Personality Situational Judgment Tests\n  with Large Language Models","summary":"  Personality assessment, particularly through situational judgment tests\n(SJTs), is a vital tool for psychological research, talent selection, and\neducational evaluation. This study explores the potential of GPT-4, a\nstate-of-the-art large language model (LLM), to automate the generation of\npersonality situational judgment tests (PSJTs) in Chinese. Traditional SJT\ndevelopment is labor-intensive and prone to biases, while GPT-4 offers a\nscalable, efficient alternative. Two studies were conducted: Study 1 evaluated\nthe impact of prompt design and temperature settings on content validity,\nfinding that optimized prompts with a temperature of 1.0 produced creative and\naccurate items. Study 2 assessed the psychometric properties of GPT-4-generated\nPSJTs, revealing that they demonstrated satisfactory reliability and validity,\nsurpassing the performance of manually developed tests in measuring the Big\nFive personality traits. This research highlights GPT-4's effectiveness in\ndeveloping high-quality PSJTs, providing a scalable and innovative method for\npsychometric test development. These findings expand the possibilities of\nautomatic item generation and the application of LLMs in psychology, and offer\npractical implications for streamlining test development processes in\nresource-limited settings.\n","authors":["Chang-Jin Li","Jiyuan Zhang","Yun Tang","Jian Li"],"pdf_url":"https://arxiv.org/pdf/2412.12144v3.pdf","comment":"Submitted to Psychological Methods. 56 pages (main text), 12 pages\n  (appendix), and 5 figures"},{"id":"http://arxiv.org/abs/2504.12187v1","updated":"2025-04-16T15:42:33Z","published":"2025-04-16T15:42:33Z","title":"What Do Large Language Models Know? Tacit Knowledge as a Potential\n  Causal-Explanatory Structure","summary":"  It is sometimes assumed that Large Language Models (LLMs) know language, or\nfor example that they know that Paris is the capital of France. But what -- if\nanything -- do LLMs actually know? In this paper, I argue that LLMs can acquire\ntacit knowledge as defined by Martin Davies (1990). Whereas Davies himself\ndenies that neural networks can acquire tacit knowledge, I demonstrate that\ncertain architectural features of LLMs satisfy the constraints of semantic\ndescription, syntactic structure, and causal systematicity. Thus, tacit\nknowledge may serve as a conceptual framework for describing, explaining, and\nintervening on LLMs and their behavior.\n","authors":["Céline Budding"],"pdf_url":"https://arxiv.org/pdf/2504.12187v1.pdf","comment":"Accepted for publication in Philosophy of Science"},{"id":"http://arxiv.org/abs/2504.12185v1","updated":"2025-04-16T15:40:10Z","published":"2025-04-16T15:40:10Z","title":"SALAD: Improving Robustness and Generalization through Contrastive\n  Learning with Structure-Aware and LLM-Driven Augmented Data","summary":"  In various natural language processing (NLP) tasks, fine-tuning Pre-trained\nLanguage Models (PLMs) often leads to the issue of spurious correlations, which\nnegatively impacts performance, particularly when dealing with\nout-of-distribution data. To address this problem, we propose SALAD}(Structure\nAware and LLM-driven Augmented Data), a novel approach designed to enhance\nmodel robustness and generalization by generating structure-aware and\ncounterfactually augmented data for contrastive learning. Our method leverages\na tagging-based approach to generate structure-aware positive samples and\nutilizes large language models (LLMs) to generate counterfactual negative\nsamples with diverse sentence patterns. By applying contrastive learning, SALAD\nenables the model to focus on learning the structural relationships between key\nsentence components while minimizing reliance on spurious correlations. We\nvalidate our approach through experiments on three tasks: Sentiment\nClassification, Sexism Detection, and Natural Language Inference. The results\ndemonstrate that SALAD not only improves model robustness and performance\nacross different environments but also enhances generalization to\nout-of-distribution datasets and cross-domain scenarios.\n","authors":["Suyoung Bae","Hyojun Kim","YunSeok Choi","Jee-Hyong Lee"],"pdf_url":"https://arxiv.org/pdf/2504.12185v1.pdf","comment":"Accepted to NAACL 2025 main. 15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.17287v2","updated":"2025-04-16T15:39:06Z","published":"2025-03-21T16:35:31Z","title":"FastCuRL: Curriculum Reinforcement Learning with Progressive Context\n  Extension for Efficient Training R1-like Reasoning Models","summary":"  Improving the training efficiency remains one of the most significant\nchallenges in large-scale reinforcement learning. In this paper, we investigate\nhow the model's context length and the complexity of the training dataset\ninfluence the training process of R1-like models. Our experiments reveal three\nkey insights: (1) adopting longer context lengths may not necessarily result in\nbetter performance; (2) selecting an appropriate context length helps mitigate\nentropy collapse; and (3) appropriately controlling the model's context length\nand curating training data based on input prompt length can effectively improve\nRL training efficiency, achieving better performance with shorter thinking\nlength. Inspired by these insights, we propose FastCuRL, a curriculum\nreinforcement learning framework with the progressive context extension\nstrategy, and successfully accelerate the training process of RL models.\nExperimental results demonstrate that FastCuRL-1.5B-Preview surpasses\nDeepScaleR-1.5B-Preview across all five benchmarks while only utilizing 50\\% of\ntraining steps. Furthermore, all training stages for FastCuRL-1.5B-Preview are\ncompleted using a single node with 8 GPUs.\n","authors":["Mingyang Song","Mao Zheng","Zheng Li","Wenjie Yang","Xuan Luo","Yue Pan","Feng Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.17287v2.pdf","comment":"Ongoing Work"},{"id":"http://arxiv.org/abs/2504.12180v1","updated":"2025-04-16T15:37:09Z","published":"2025-04-16T15:37:09Z","title":"Trusting CHATGPT: how minor tweaks in the prompts lead to major\n  differences in sentiment classification","summary":"  One fundamental question for the social sciences today is: how much can we\ntrust highly complex predictive models like ChatGPT? This study tests the\nhypothesis that subtle changes in the structure of prompts do not produce\nsignificant variations in the classification results of sentiment polarity\nanalysis generated by the Large Language Model GPT-4o mini. Using a dataset of\n100.000 comments in Spanish on four Latin American presidents, the model\nclassified the comments as positive, negative, or neutral on 10 occasions,\nvarying the prompts slightly each time. The experimental methodology included\nexploratory and confirmatory analyses to identify significant discrepancies\namong classifications.\n  The results reveal that even minor modifications to prompts such as lexical,\nsyntactic, or modal changes, or even their lack of structure impact the\nclassifications. In certain cases, the model produced inconsistent responses,\nsuch as mixing categories, providing unsolicited explanations, or using\nlanguages other than Spanish. Statistical analysis using Chi-square tests\nconfirmed significant differences in most comparisons between prompts, except\nin one case where linguistic structures were highly similar.\n  These findings challenge the robustness and trust of Large Language Models\nfor classification tasks, highlighting their vulnerability to variations in\ninstructions. Moreover, it was evident that the lack of structured grammar in\nprompts increases the frequency of hallucinations. The discussion underscores\nthat trust in Large Language Models is based not only on technical performance\nbut also on the social and institutional relationships underpinning their use.\n","authors":["Jaime E. Cuellar","Oscar Moreno-Martinez","Paula Sofia Torres-Rodriguez","Jaime Andres Pavlich-Mariscal","Andres Felipe Mican-Castiblanco","Juan Guillermo Torres-Hurtado"],"pdf_url":"https://arxiv.org/pdf/2504.12180v1.pdf","comment":"in Spanish language"},{"id":"http://arxiv.org/abs/2504.12177v1","updated":"2025-04-16T15:27:57Z","published":"2025-04-16T15:27:57Z","title":"Mapping Controversies Using Artificial Intelligence: An Analysis of the\n  Hamas-Israel Conflict on YouTube","summary":"  This article analyzes the Hamas-Israel controversy through 253,925\nSpanish-language YouTube comments posted between October 2023 and January 2024,\nfollowing the October 7 attack that escalated the conflict. Adopting an\ninterdisciplinary approach, the study combines the analysis of controversies\nfrom Science and Technology Studies (STS) with advanced computational\nmethodologies, specifically Natural Language Processing (NLP) using the BERT\n(Bidirectional Encoder Representations from Transformers) model. Using this\napproach, the comments were automatically classified into seven categories,\nreflecting pro-Palestinian, pro-Israeli, anti- Palestinian, anti-Israeli\npositions, among others. The results show a predominance of pro- Palestinian\ncomments, although pro-Israeli and anti-Palestinian comments received more\n\"likes.\" This study also applies the agenda-setting theory to demonstrate how\nmedia coverage significantly influences public perception, observing a notable\nshift in public opinion, transitioning from a pro- Palestinian stance to a more\ncritical position towards Israel. This work highlights the importance of\ncombining social science perspectives with technological tools in the analysis\nof controversies, presenting a methodological innovation by integrating\ncomputational analysis with critical social theories to address complex public\nopinion phenomena and media narratives.\n","authors":["Victor Manuel Hernandez Lopez","Jaime E. Cuellar"],"pdf_url":"https://arxiv.org/pdf/2504.12177v1.pdf","comment":"in Spanish language"},{"id":"http://arxiv.org/abs/2504.12172v1","updated":"2025-04-16T15:25:45Z","published":"2025-04-16T15:25:45Z","title":"Poem Meter Classification of Recited Arabic Poetry: Integrating\n  High-Resource Systems for a Low-Resource Task","summary":"  Arabic poetry is an essential and integral part of Arabic language and\nculture. It has been used by the Arabs to spot lights on their major events\nsuch as depicting brutal battles and conflicts. They also used it, as in many\nother languages, for various purposes such as romance, pride, lamentation, etc.\nArabic poetry has received major attention from linguistics over the decades.\nOne of the main characteristics of Arabic poetry is its special rhythmic\nstructure as opposed to prose. This structure is referred to as a meter.\nMeters, along with other poetic characteristics, are intensively studied in an\nArabic linguistic field called \"\\textit{Aroud}\". Identifying these meters for a\nverse is a lengthy and complicated process. It also requires technical\nknowledge in \\textit{Aruod}. For recited poetry, it adds an extra layer of\nprocessing. Developing systems for automatic identification of poem meters for\nrecited poems need large amounts of labelled data. In this study, we propose a\nstate-of-the-art framework to identify the poem meters of recited Arabic\npoetry, where we integrate two separate high-resource systems to perform the\nlow-resource task. To ensure generalization of our proposed architecture, we\npublish a benchmark for this task for future research.\n","authors":["Maged S. Al-Shaibani","Zaid Alyafeai","Irfan Ahmad"],"pdf_url":"https://arxiv.org/pdf/2504.12172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21169v4","updated":"2025-04-16T15:01:20Z","published":"2024-10-28T16:11:35Z","title":"Document Parsing Unveiled: Techniques, Challenges, and Prospects for\n  Structured Information Extraction","summary":"  Document parsing is essential for converting unstructured and semi-structured\ndocuments such as contracts, academic papers, and invoices into structured,\nmachine-readable data. Document parsing reliable structured data from\nunstructured inputs, providing huge convenience for numerous applications.\nEspecially with recent achievements in Large Language Models, document parsing\nplays an indispensable role in both knowledge base construction and training\ndata generation. This survey presents a comprehensive review of the current\nstate of document parsing, covering key methodologies, from modular pipeline\nsystems to end-to-end models driven by large vision-language models. Core\ncomponents such as layout detection, content extraction (including text,\ntables, and mathematical expressions), and multi-modal data integration are\nexamined in detail. Additionally, this paper discusses the challenges faced by\nmodular document parsing systems and vision-language models in handling complex\nlayouts, integrating multiple modules, and recognizing high-density text. It\noutlines future research directions and emphasizes the importance of developing\nlarger and more diverse datasets.\n","authors":["Qintong Zhang","Bin Wang","Victor Shea-Jay Huang","Junyuan Zhang","Zhengren Wang","Hao Liang","Conghui He","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.21169v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12140v1","updated":"2025-04-16T14:52:22Z","published":"2025-04-16T14:52:22Z","title":"Multilingual Contextualization of Large Language Models for\n  Document-Level Machine Translation","summary":"  Large language models (LLMs) have demonstrated strong performance in\nsentence-level machine translation, but scaling to document-level translation\nremains challenging, particularly in modeling long-range dependencies and\ndiscourse phenomena across sentences and paragraphs. In this work, we propose a\nmethod to improve LLM-based long-document translation through targeted\nfine-tuning on high-quality document-level data, which we curate and introduce\nas DocBlocks. Our approach supports multiple translation paradigms, including\ndirect document-to-document and chunk-level translation, by integrating\ninstructions both with and without surrounding context. This enables models to\nbetter capture cross-sentence dependencies while maintaining strong\nsentence-level translation performance. Experimental results show that\nincorporating multiple translation paradigms improves document-level\ntranslation quality and inference speed compared to prompting and agent-based\nmethods.\n","authors":["Miguel Moura Ramos","Patrick Fernandes","Sweta Agrawal","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2504.12140v1.pdf","comment":"9 pages, work-in-progress"},{"id":"http://arxiv.org/abs/2504.12137v1","updated":"2025-04-16T14:50:25Z","published":"2025-04-16T14:50:25Z","title":"Efficient Contrastive Decoding with Probabilistic Hallucination\n  Detection - Mitigating Hallucinations in Large Vision Language Models -","summary":"  Despite recent advances in Large Vision Language Models (LVLMs), these models\nstill suffer from generating hallucinatory responses that do not align with the\nvisual input provided. To mitigate such hallucinations, we introduce Efficient\nContrastive Decoding (ECD), a simple method that leverages probabilistic\nhallucination detection to shift the output distribution towards contextually\naccurate answers at inference time. By contrasting token probabilities and\nhallucination scores, ECD subtracts hallucinated concepts from the original\ndistribution, effectively suppressing hallucinations. Notably, our proposed\nmethod can be applied to any open-source LVLM and does not require additional\nLVLM training. We evaluate our method on several benchmark datasets and across\ndifferent LVLMs. Our experiments show that ECD effectively mitigates\nhallucinations, outperforming state-of-the-art methods with respect to\nperformance on LVLM benchmarks and computation time.\n","authors":["Laura Fieback","Nishilkumar Balar","Jakob Spiegelberg","Hanno Gottschalk"],"pdf_url":"https://arxiv.org/pdf/2504.12137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08525v3","updated":"2025-04-16T14:48:13Z","published":"2025-04-11T13:38:36Z","title":"Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware\n  Extensions for Multi-Step LLM Agent Tasks","summary":"  Large Language Models (LLMs) are increasingly used as autonomous agents for\nmulti-step tasks. However, most existing frameworks fail to maintain a\nstructured understanding of the task state, often relying on linear prompt\nconcatenation or shallow memory buffers. This leads to brittle performance,\nfrequent hallucinations, and poor long-range coherence. In this work, we\npropose the Task Memory Engine (TME), a lightweight and structured memory\nmodule that tracks task execution using a hierarchical Task Memory Tree (TMT).\nEach node in the tree corresponds to a task step, storing relevant input,\noutput, status, and sub-task relationships. We introduce a prompt synthesis\nmethod that dynamically generates LLM prompts based on the active node path,\nsignificantly improving execution consistency and contextual grounding. Through\ncase studies and comparative experiments on multi-step agent tasks, we\ndemonstrate that TME leads to better task completion accuracy and more\ninterpretable behavior with minimal implementation overhead. A reference\nimplementation of the core TME components is available at\nhttps://github.com/biubiutomato/TME-Agent, including basic examples and\nstructured memory integration. While the current implementation uses a\ntree-based structure, TME is designed to be graph-aware, supporting reusable\nsubsteps, converging task paths, and shared dependencies. This lays the\ngroundwork for future DAG-based memory architectures.\n","authors":["Ye Ye"],"pdf_url":"https://arxiv.org/pdf/2504.08525v3.pdf","comment":"14 pages, 5 figures. Preprint prepared for future submission.\n  Includes implementation and token-efficiency analysis. Code at\n  https://github.com/biubiutomato/TME-Agent"},{"id":"http://arxiv.org/abs/2504.10185v2","updated":"2025-04-16T14:45:55Z","published":"2025-04-14T12:38:37Z","title":"LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in\n  Current Benchmarks","summary":"  Large language model unlearning has become a critical challenge in ensuring\nsafety and controlled model behavior by removing undesired data-model\ninfluences from the pretrained model while preserving general utility.\nSignificant recent efforts have been dedicated to developing LLM unlearning\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\nUnlearning Six-way Evaluation), facilitating standardized unlearning\nperformance assessment and method comparison. Despite their usefulness, we\nuncover for the first time a novel coreset effect within these benchmarks.\nSpecifically, we find that LLM unlearning achieved with the original (full)\nforget set can be effectively maintained using a significantly smaller subset\n(functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\ncan be performed surprisingly easily, even in an extremely low-data regime. We\ndemonstrate that this coreset effect remains strong, regardless of the LLM\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\nThe surprisingly strong coreset effect is also robust across various data\nselection methods, ranging from random selection to more sophisticated\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\nkeyword-based perspective, showing that keywords extracted from the forget set\nalone contribute significantly to unlearning effectiveness and indicating that\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\nmodels along additional dimensions, such as mode connectivity and robustness to\njailbreaking attacks. Codes are available at\nhttps://github.com/OPTML-Group/MU-Coreset.\n","authors":["Soumyadeep Pal","Changsheng Wang","James Diffenderfer","Bhavya Kailkhura","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2504.10185v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12108v1","updated":"2025-04-16T14:16:38Z","published":"2025-04-16T14:16:38Z","title":"Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust\n  and Traceable Text Generation","summary":"  The rapid development of Large Language Models (LLMs) has intensified\nconcerns about content traceability and potential misuse. Existing watermarking\nschemes for sampled text often face trade-offs between maintaining text quality\nand ensuring robust detection against various attacks. To address these issues,\nwe propose a novel watermarking scheme that improves both detectability and\ntext quality by introducing a cumulative watermark entropy threshold. Our\napproach is compatible with and generalizes existing sampling functions,\nenhancing adaptability. Experimental results across multiple LLMs show that our\nscheme significantly outperforms existing methods, achieving over 80\\%\nimprovements on widely-used datasets, e.g., MATH and GSM8K, while maintaining\nhigh detection accuracy.\n","authors":["Shizhan Cai","Liang Ding","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2504.12108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12098v1","updated":"2025-04-16T14:02:21Z","published":"2025-04-16T14:02:21Z","title":"Gauging Overprecision in LLMs: An Empirical Study","summary":"  Recently, overconfidence in large language models (LLMs) has garnered\nconsiderable attention due to its fundamental importance in quantifying the\ntrustworthiness of LLM generation. However, existing approaches prompt the\n\\textit{black box LLMs} to produce their confidence (\\textit{verbalized\nconfidence}), which can be subject to many biases and hallucinations. Inspired\nby a different aspect of overconfidence in cognitive science called\n\\textit{overprecision}, we designed a framework for its study in black box\nLLMs. This framework contains three main phases: 1) generation, 2) refinement\nand 3) evaluation. In the generation phase we prompt the LLM to generate\nanswers to numerical questions in the form of intervals with a certain level of\nconfidence. This confidence level is imposed in the prompt and not required for\nthe LLM to generate as in previous approaches. We use various prompting\ntechniques and use the same prompt multiple times to gauge the effects of\nrandomness in the generation process. In the refinement phase, answers from the\nprevious phase are refined to generate better answers. The LLM answers are\nevaluated and studied in the evaluation phase to understand its internal\nworkings. This study allowed us to gain various insights into LLM\noverprecision: 1) LLMs are highly uncalibrated for numerical tasks 2)\n{\\color{blue}there is no correlation between the length of the interval and the\nimposed confidence level, which can be symptomatic of a a) lack of\nunderstanding of the concept of confidence or b) inability to adjust\nself-confidence by following instructions}, {\\color{blue}3)} LLM numerical\nprecision differs depending on the task, scale of answer and prompting\ntechnique {\\color{blue}4) Refinement of answers doesn't improve precision in\nmost cases}. We believe this study offers new perspectives on LLM\noverconfidence and serves as a strong baseline for overprecision in LLMs.\n","authors":["Adil Bahaj","Hamed Rahimi","Mohamed Chetouani","Mounir Ghogho"],"pdf_url":"https://arxiv.org/pdf/2504.12098v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2504.11290v2","updated":"2025-04-16T13:49:21Z","published":"2025-04-15T15:30:22Z","title":"Automated Python Translation","summary":"  Python is one of the most commonly used programming languages in industry and\neducation. Its English keywords and built-in functions/modules allow it to come\nclose to pseudo-code in terms of its readability and ease of writing. However,\nthose who do not speak English may not experience these advantages. In fact,\nthey may even be hindered in their ability to understand Python code, as the\nEnglish nature of its terms creates an additional layer of overhead. To that\nend, we introduce the task of automatically translating Python's natural\nmodality (keywords, error types, identifiers, etc.) into other human languages.\nThis presents a unique challenge, considering the abbreviated nature of these\nforms, as well as potential untranslatability of advanced\nmathematical/programming concepts across languages. We therefore create an\nautomated pipeline to translate Python into other human languages, comparing\nstrategies using machine translation and large language models. We then use\nthis pipeline to acquire translations from five common Python libraries\n(pytorch, pandas, tensorflow, numpy, and random) in seven languages, and do a\nquality test on a subset of these terms in French, Greek, and Bengali. We hope\nthis will provide a clearer path forward towards creating a universal Python,\naccessible to anyone regardless of nationality or language background.\n","authors":["Joshua Otten","Antonios Anastasopoulos","Kevin Moran"],"pdf_url":"https://arxiv.org/pdf/2504.11290v2.pdf","comment":"15 pages, 4 figures, 17 tables"},{"id":"http://arxiv.org/abs/2504.12082v1","updated":"2025-04-16T13:43:23Z","published":"2025-04-16T13:43:23Z","title":"Selective Demonstration Retrieval for Improved Implicit Hate Speech\n  Detection","summary":"  Hate speech detection is a crucial area of research in natural language\nprocessing, essential for ensuring online community safety. However, detecting\nimplicit hate speech, where harmful intent is conveyed in subtle or indirect\nways, remains a major challenge. Unlike explicit hate speech, implicit\nexpressions often depend on context, cultural subtleties, and hidden biases,\nmaking them more challenging to identify consistently. Additionally, the\ninterpretation of such speech is influenced by external knowledge and\ndemographic biases, resulting in varied detection results across different\nlanguage models. Furthermore, Large Language Models often show heightened\nsensitivity to toxic language and references to vulnerable groups, which can\nlead to misclassifications. This over-sensitivity results in false positives\n(incorrectly identifying harmless statements as hateful) and false negatives\n(failing to detect genuinely harmful content). Addressing these issues requires\nmethods that not only improve detection precision but also reduce model biases\nand enhance robustness. To address these challenges, we propose a novel method,\nwhich utilizes in-context learning without requiring model fine-tuning. By\nadaptively retrieving demonstrations that focus on similar groups or those with\nthe highest similarity scores, our approach enhances contextual comprehension.\nExperimental results show that our method outperforms current state-of-the-art\ntechniques. Implementation details and code are available at TBD.\n","authors":["Yumin Kim","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2504.12082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14773v2","updated":"2025-04-16T13:38:58Z","published":"2024-03-21T18:27:29Z","title":"StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation\n  from Text","summary":"  Text-to-video diffusion models enable the generation of high-quality videos\nthat follow text instructions, making it easy to create diverse and individual\ncontent. However, existing approaches mostly focus on high-quality short video\ngeneration (typically 16 or 24 frames), ending up with hard-cuts when naively\nextended to the case of long video synthesis. To overcome these limitations, we\nintroduce StreamingT2V, an autoregressive approach for long video generation of\n80, 240, 600, 1200 or more frames with smooth transitions. The key components\nare:(i) a short-term memory block called conditional attention module (CAM),\nwhich conditions the current generation on the features extracted from the\nprevious chunk via an attentional mechanism, leading to consistent chunk\ntransitions, (ii) a long-term memory block called appearance preservation\nmodule, which extracts high-level scene and object features from the first\nvideo chunk to prevent the model from forgetting the initial scene, and (iii) a\nrandomized blending approach that enables to apply a video enhancer\nautoregressively for infinitely long videos without inconsistencies between\nchunks. Experiments show that StreamingT2V generates high motion amount. In\ncontrast, all competing image-to-video methods are prone to video stagnation\nwhen applied naively in an autoregressive manner. Thus, we propose with\nStreamingT2V a high-quality seamless text-to-long video generator that\noutperforms competitors with consistency and motion. Our code will be available\nat: https://github.com/Picsart-AI-Research/StreamingT2V\n","authors":["Roberto Henschel","Levon Khachatryan","Hayk Poghosyan","Daniil Hayrapetyan","Vahram Tadevosyan","Zhangyang Wang","Shant Navasardyan","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.14773v2.pdf","comment":"https://github.com/Picsart-AI-Research/StreamingT2V"},{"id":"http://arxiv.org/abs/2411.05986v2","updated":"2025-04-16T13:31:12Z","published":"2024-11-08T21:55:37Z","title":"Fine-Grained Reward Optimization for Machine Translation using Error\n  Severity Mappings","summary":"  Reinforcement learning (RL) has been proven to be an effective and robust\nmethod for training neural machine translation systems, especially when paired\nwith powerful reward models that accurately assess translation quality.\nHowever, most research has focused on RL methods that use sentence-level\nfeedback, leading to inefficient learning signals due to the reward sparsity\nproblem -- the model receives a single score for the entire sentence. To\naddress this, we propose a novel approach that leverages fine-grained,\ntoken-level quality assessments along with error severity levels using RL\nmethods. Specifically, we use xCOMET, a state-of-the-art quality estimation\nsystem, as our token-level reward model. We conduct experiments on small and\nlarge translation datasets with standard encoder-decoder and large language\nmodels-based machine translation systems, comparing the impact of\nsentence-level versus fine-grained reward signals on translation quality. Our\nresults show that training with token-level rewards improves translation\nquality across language pairs over baselines according to both automatic and\nhuman evaluation. Furthermore, token-level reward optimization improves\ntraining stability, evidenced by a steady increase in mean rewards over\ntraining epochs.\n","authors":["Miguel Moura Ramos","Tomás Almeida","Daniel Vareta","Filipe Azevedo","Sweta Agrawal","Patrick Fernandes","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2411.05986v2.pdf","comment":"12 pages, work-in-progress"},{"id":"http://arxiv.org/abs/2504.12052v1","updated":"2025-04-16T13:06:24Z","published":"2025-04-16T13:06:24Z","title":"Bayesian dynamic borrowing considering semantic similarity between\n  outcomes for disproportionality analysis in FAERS","summary":"  We present a Bayesian dynamic borrowing (BDB) approach to enhance the\nquantitative identification of adverse events (AEs) in spontaneous reporting\nsystems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior\nwithin a Bayesian hierarchical model and incorporates semantic similarity\nmeasures (SSMs) to enable weighted information sharing from MedDRA Preferred\nTerms (PTs) that are clinical similar to the target PT. This continuous\nsimilarity-based borrowing addresses limitation of rigid hierarchical grouping\nin current disproportionality analysis (DPA).\n  Using data from the FDA Adverse Event Reporting System (FAERS) between 2015\nand 2019, we evalute this approach - termed IC SSM - against standard\nInformation Component (IC) analysis and IC with borrowing at the MedDRA\nhigh-level group term (HLGT) level. A novel references set (PVLens), derived\nfrom FDA product label updates, enabled prospective evaluation of method\nperformance in identifying AEs prior to official labeling.\n  The IC SSM approach demonstrated improved sensitivity compared to both\ntraditional IC and HLGT-based borrowing, with minor trade-offs in F1 scores and\nYouden's index. IC SSM consistently identified more true positives and detected\nsignals over 5 months sooner than traditional IC. Despite a marginally lower\naggregate Youden's index, IC SSM showed higher performance in the early\npost-marketing period, providing more stable and relevant estimates than\nHLGT-based borrowing and traditional IC.\n  These findings support the use of SSM-informed Bayesian borrowing as a\nscalable and context-aware enhancement to traditional DPA methods. Future\nresearch should validate this approach across other datasets and explore\nadditional similarity metrics and Bayesian inference strategies using\ncase-level data.\n","authors":["François Haguinet","Jeffery L Painter","Gregory E Powell","Andrea Callegaro","Andrew Bate"],"pdf_url":"https://arxiv.org/pdf/2504.12052v1.pdf","comment":"30 pages, 7 figures, 5 supplementary figures"},{"id":"http://arxiv.org/abs/2504.11986v1","updated":"2025-04-16T11:27:47Z","published":"2025-04-16T11:27:47Z","title":"Language Models as Quasi-Crystalline Thought: Structure, Constraint, and\n  Emergence in Generative Systems","summary":"  This essay proposes an analogy between large language models (LLMs) and\nquasicrystals: systems that exhibit global coherence without periodic\nrepetition and that are generated through local constraints. While LLMs are\noften evaluated in terms of predictive accuracy, factuality, or alignment, this\nstructural perspective suggests that their most characteristic behavior is the\nproduction of internally resonant linguistic patterns. Just as quasicrystals\nforced a redefinition of order in physical systems, viewing LLMs as generators\nof quasi-structured language opens new paths for evaluation and design:\nprivileging propagation of constraint over token-level accuracy, and coherence\nof form over fixed meaning. LLM outputs should be read not only for what they\nsay, but for the patterns of constraint and coherence that organize them. This\nshift reframes generative language as a space of emergent patterning: LLMs are\nneither fully random nor strictly rule-based, but defined by a logic of\nconstraint, resonance, and structural depth.\n","authors":["Jose Manuel Guevara-Vela"],"pdf_url":"https://arxiv.org/pdf/2504.11986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11975v1","updated":"2025-04-16T11:15:26Z","published":"2025-04-16T11:15:26Z","title":"SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on\n  Hallucinations and Related Observable Overgeneration Mistakes","summary":"  We present the Mu-SHROOM shared task which is focused on detecting\nhallucinations and other overgeneration mistakes in the output of\ninstruction-tuned large language models (LLMs). Mu-SHROOM addresses\ngeneral-purpose LLMs in 14 languages, and frames the hallucination detection\nproblem as a span-labeling task. We received 2,618 submissions from 43\nparticipating teams employing diverse methodologies. The large number of\nsubmissions underscores the interest of the community in hallucination\ndetection. We present the results of the participating systems and conduct an\nempirical analysis to identify key factors contributing to strong performance\nin this task. We also emphasize relevant current challenges, notably the\nvarying degree of hallucinations across languages and the high annotator\ndisagreement when labeling hallucination spans.\n","authors":["Raúl Vázquez","Timothee Mickus","Elaine Zosa","Teemu Vahtola","Jörg Tiedemann","Aman Sinha","Vincent Segonne","Fernando Sánchez-Vega","Alessandro Raganato","Jindřich Libovický","Jussi Karlgren","Shaoxiong Ji","Jindřich Helcl","Liane Guillou","Ona de Gibert","Jaione Bengoetxea","Joseph Attieh","Marianna Apidianaki"],"pdf_url":"https://arxiv.org/pdf/2504.11975v1.pdf","comment":"Mu-SHROOM is part of SemEval-2025 (Task 3). TBP: Proceedings of the\n  19th International Workshop on Semantic Evaluation (SemEval-2025)"},{"id":"http://arxiv.org/abs/2504.11972v1","updated":"2025-04-16T11:08:46Z","published":"2025-04-16T11:08:46Z","title":"LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA","summary":"  Extractive reading comprehension question answering (QA) datasets are\ntypically evaluated using Exact Match (EM) and F1-score, but these metrics\noften fail to fully capture model performance. With the success of large\nlanguage models (LLMs), they have been employed in various tasks, including\nserving as judges (LLM-as-a-judge). In this paper, we reassess the performance\nof QA models using LLM-as-a-judge across four reading comprehension QA\ndatasets. We examine different families of LLMs and various answer types to\nevaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show\nthat LLM-as-a-judge is highly correlated with human judgments and can replace\ntraditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human\njudgments improves significantly, from 0.17 (EM) and 0.36 (F1-score) to 0.85.\nThese findings confirm that EM and F1 metrics underestimate the true\nperformance of the QA models. While LLM-as-a-judge is not perfect for more\ndifficult answer types (e.g., job), it still outperforms EM/F1, and we observe\nno bias issues, such as self-preference, when the same model is used for both\nthe QA and judgment tasks.\n","authors":["Xanh Ho","Jiahao Huang","Florian Boudin","Akiko Aizawa"],"pdf_url":"https://arxiv.org/pdf/2504.11972v1.pdf","comment":"17 pages; code and data are available at\n  https://github.com/Alab-NII/llm-judge-extract-qa"},{"id":"http://arxiv.org/abs/2502.05151v2","updated":"2025-04-16T10:54:12Z","published":"2025-02-07T18:26:45Z","title":"Transforming Science with Large Language Models: A Survey on AI-assisted\n  Scientific Discovery, Experimentation, Content Generation, and Evaluation","summary":"  With the advent of large multimodal language models, science is now at a\nthreshold of an AI-based technological transformation. Recently, a plethora of\nnew AI models and tools has been proposed, promising to empower researchers and\nacademics worldwide to conduct their research more effectively and efficiently.\nThis includes all aspects of the research cycle, especially (1) searching for\nrelevant literature; (2) generating research ideas and conducting\nexperimentation; generating (3) text-based and (4) multimodal content (e.g.,\nscientific figures and diagrams); and (5) AI-based automatic peer review. In\nthis survey, we provide an in-depth overview over these exciting recent\ndevelopments, which promise to fundamentally alter the scientific research\nprocess for good. Our survey covers the five aspects outlined above, indicating\nrelevant datasets, methods and results (including evaluation) as well as\nlimitations and scope for future research. Ethical concerns regarding\nshortcomings of these tools and potential for misuse (fake science, plagiarism,\nharms to research integrity) take a particularly prominent place in our\ndiscussion. We hope that our survey will not only become a reference guide for\nnewcomers to the field but also a catalyst for new AI-based initiatives in the\narea of \"AI4Science\".\n","authors":["Steffen Eger","Yong Cao","Jennifer D'Souza","Andreas Geiger","Christian Greisinger","Stephanie Gross","Yufang Hou","Brigitte Krenn","Anne Lauscher","Yizhi Li","Chenghua Lin","Nafise Sadat Moosavi","Wei Zhao","Tristan Miller"],"pdf_url":"https://arxiv.org/pdf/2502.05151v2.pdf","comment":"44 pages, 7 figures, 8 tables"},{"id":"http://arxiv.org/abs/2504.11952v1","updated":"2025-04-16T10:29:30Z","published":"2025-04-16T10:29:30Z","title":"Robust and Fine-Grained Detection of AI Generated Texts","summary":"  An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.\n","authors":["Ram Mohan Rao Kadiyala","Siddartha Pullakhandam","Kanwal Mehreen","Drishti Sharma","Siddhant Gupta","Jebish Purbey","Ashay Srivastava","Subhasya TippaReddy","Arvind Reddy Bobbili","Suraj Telugara Chandrashekhar","Modabbir Adeeb","Srinadh Vura","Hamza Farooq"],"pdf_url":"https://arxiv.org/pdf/2504.11952v1.pdf","comment":"ACL 2025 Feb ARR Submission"},{"id":"http://arxiv.org/abs/2503.22675v2","updated":"2025-04-16T10:20:11Z","published":"2025-03-28T17:59:03Z","title":"Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation","summary":"  Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose \\textbf{ReaRec}, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation.\n","authors":["Jiakai Tang","Sunhao Dai","Teng Shi","Jun Xu","Xu Chen","Wen Chen","Wu Jian","Yuning Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.22675v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11942v1","updated":"2025-04-16T10:20:11Z","published":"2025-04-16T10:20:11Z","title":"ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign\n  Language Translation","summary":"  Current sign language machine translation systems rely on recognizing hand\nmovements, facial expressions and body postures, and natural language\nprocessing, to convert signs into text. Recent approaches use Transformer\narchitectures to model long-range dependencies via positional encoding.\nHowever, they lack accuracy in recognizing fine-grained, short-range temporal\ndependencies between gestures captured at high frame rates. Moreover, their\nhigh computational complexity leads to inefficient training. To mitigate these\nissues, we propose an Adaptive Transformer (ADAT), which incorporates\ncomponents for enhanced feature extraction and adaptive feature weighting\nthrough a gating mechanism to emphasize contextually relevant features while\nreducing training overhead and maintaining translation accuracy. To evaluate\nADAT, we introduce MedASL, the first public medical American Sign Language\ndataset. In sign-to-gloss-to-text experiments, ADAT outperforms the\nencoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing\ntraining time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text\nexperiments, it improves accuracy by 8.7% and reduces training time by 2.8% on\nPHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on\nMedASL. Compared to encoder-only and decoder-only baselines in sign-to-text,\nADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its\ndual-stream structure.\n","authors":["Nada Shahin","Leila Ismail"],"pdf_url":"https://arxiv.org/pdf/2504.11942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11934v1","updated":"2025-04-16T10:14:27Z","published":"2025-04-16T10:14:27Z","title":"An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation\n  Evaluation","summary":"  Gender-neutral translation (GNT) aims to avoid expressing the gender of human\nreferents when the source text lacks explicit cues about the gender of those\nreferents. Evaluating GNT automatically is particularly challenging, with\ncurrent solutions being limited to monolingual classifiers. Such solutions are\nnot ideal because they do not factor in the source sentence and require\ndedicated data and fine-tuning to scale to new languages. In this work, we\naddress such limitations by investigating the use of large language models\n(LLMs) as evaluators of GNT. Specifically, we explore two prompting approaches:\none in which LLMs generate sentence-level assessments only, and another, akin\nto a chain-of-thought approach, where they first produce detailed phrase-level\nannotations before a sentence-level judgment. Through extensive experiments on\nmultiple languages with five models, both open and proprietary, we show that\nLLMs can serve as evaluators of GNT. Moreover, we find that prompting for\nphrase-level annotations before sentence-level assessments consistently\nimproves the accuracy of all models, providing a better and more scalable\nalternative to current solutions.\n","authors":["Andrea Piergentili","Beatrice Savoldi","Matteo Negri","Luisa Bentivogli"],"pdf_url":"https://arxiv.org/pdf/2504.11934v1.pdf","comment":"Accepted at GITT 2025"},{"id":"http://arxiv.org/abs/2412.00127v2","updated":"2025-04-16T10:04:24Z","published":"2024-11-28T13:00:38Z","title":"Orthus: Autoregressive Interleaved Image-Text Generation with\n  Modality-Specific Heads","summary":"  We introduce Orthus, an autoregressive (AR) transformer that excels in\ngenerating images given textual prompts, answering questions based on visual\ninputs, and even crafting lengthy image-text interleaved contents. Unlike prior\narts on unified multimodal modeling, Orthus simultaneously copes with discrete\ntext tokens and continuous image features under the AR modeling principle. The\ncontinuous treatment of visual signals minimizes the information loss for both\nimage understanding and generation while the fully AR formulation renders the\ncharacterization of the correlation between modalities straightforward. The key\nmechanism enabling Orthus to leverage these advantages lies in its\nmodality-specific heads -- one regular language modeling (LM) head predicts\ndiscrete text tokens and one diffusion head generates continuous image features\nconditioning on the output of the backbone. We devise an efficient strategy for\nbuilding Orthus -- by substituting the Vector Quantization (VQ) operation in\nthe existing unified AR model with a soft alternative, introducing a diffusion\nhead, and tuning the added modules to reconstruct images, we can create an\nOrthus-base model effortlessly (e.g., within mere 72 A100 GPU hours).\nOrthus-base can further embrace post-training to better model interleaved\nimages and texts. Empirically, Orthus surpasses competing baselines including\nShow-o and Chameleon across standard benchmarks, achieving a GenEval score of\n0.58 and an MME-P score of 1265.8 using 7B parameters. Orthus also shows\nexceptional mixed-modality generation capabilities, reflecting the potential\nfor handling intricate practical generation tasks.\n","authors":["Siqi Kou","Jiachun Jin","Zhihong Liu","Chang Liu","Ye Ma","Jian Jia","Quan Chen","Peng Jiang","Zhijie Deng"],"pdf_url":"https://arxiv.org/pdf/2412.00127v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11900v1","updated":"2025-04-16T09:25:54Z","published":"2025-04-16T09:25:54Z","title":"Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models\n  via Plot Hole Detection","summary":"  Stories are a fundamental aspect of human experience. Engaging deeply with\nstories and spotting plot holes -- inconsistencies in a storyline that break\nthe internal logic or rules of a story's world -- requires nuanced reasoning\nskills, including tracking entities and events and their interplay, abstract\nthinking, pragmatic narrative understanding, commonsense and social reasoning,\nand theory of mind. As Large Language Models (LLMs) increasingly generate,\ninterpret, and modify text, rigorously assessing their narrative consistency\nand deeper language understanding becomes critical. However, existing\nbenchmarks focus mainly on surface-level comprehension. In this work, we\npropose plot hole detection in stories as a proxy to evaluate language\nunderstanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel\nalgorithm to controllably and carefully synthesize plot holes in human-written\nstories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot\nhole detection abilities in stories -- FlawedFictions -- , which is robust to\ncontamination, with human filtering ensuring high quality. We find that\nstate-of-the-art LLMs struggle in accurately solving FlawedFictions regardless\nof the reasoning effort allowed, with performance significantly degrading as\nstory length increases. Finally, we show that LLM-based story summarization and\nstory generation are prone to introducing plot holes, with more than 50% and\n100% increases in plot hole detection rates with respect to human-written\noriginals.\n","authors":["Kabir Ahuja","Melanie Sclar","Yulia Tsvetkov"],"pdf_url":"https://arxiv.org/pdf/2504.11900v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2504.11889v1","updated":"2025-04-16T09:17:45Z","published":"2025-04-16T09:17:45Z","title":"Rethinking LLM-Based Recommendations: A Query Generation-Based,\n  Training-Free Approach","summary":"  Existing large language model LLM-based recommendation methods face several\nchallenges, including inefficiency in handling large candidate pools,\nsensitivity to item order within prompts (\"lost in the middle\" phenomenon) poor\nscalability, and unrealistic evaluation due to random negative sampling. To\naddress these issues, we propose a Query-to-Recommendation approach that\nleverages LLMs to generate personalized queries for retrieving relevant items\nfrom the entire candidate pool, eliminating the need for candidate\npre-selection. This method can be integrated into an ID-based recommendation\nsystem without additional training, enhances recommendation performance and\ndiversity through LLMs' world knowledge, and performs well even for less\npopular item groups. Experiments on three datasets show up to 57 percent\nimprovement, with an average gain of 31 percent, demonstrating strong zero-shot\nperformance and further gains when ensembled with existing models.\n","authors":["Donghee Han","Hwanjun Song","Mun Yong Yi"],"pdf_url":"https://arxiv.org/pdf/2504.11889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.13636v3","updated":"2025-04-16T08:23:43Z","published":"2022-09-27T19:05:22Z","title":"Local Grammar-Based Coding Revisited","summary":"  In the setting of minimal local grammar-based coding, the input string is\nrepresented as a grammar with the minimal output length defined via simple\nsymbol-by-symbol encoding. This paper discusses four contributions to this\nfield. First, we invoke a simple harmonic bound on ranked probabilities, which\nreminds Zipf's law and simplifies universality proofs for minimal local\ngrammar-based codes. Second, we refine known bounds on the vocabulary size,\nshowing its partial power-law equivalence with mutual information and\nredundancy. These bounds are relevant for linking Zipf's law with the neural\nscaling law for large language models. Third, we develop a framework for\nuniversal codes with fixed infinite vocabularies, recasting universal coding as\nmatching ranked patterns that are independent of empirical data. Finally, we\nanalyze grammar-based codes with finite vocabularies being empirical rank\nlists, proving that that such codes are also universal. These results extend\nfoundations of universal grammar-based coding and reaffirm previously stated\nconnections to power laws for human language and language models.\n","authors":["Łukasz Dębowski"],"pdf_url":"https://arxiv.org/pdf/2209.13636v3.pdf","comment":"41 pages, no figures"},{"id":"http://arxiv.org/abs/2504.11844v1","updated":"2025-04-16T08:07:08Z","published":"2025-04-16T08:07:08Z","title":"Evaluating the Goal-Directedness of Large Language Models","summary":"  To what extent do LLMs use their capabilities towards their given goal? We\ntake this as a measure of their goal-directedness. We evaluate\ngoal-directedness on tasks that require information gathering, cognitive\neffort, and plan execution, where we use subtasks to infer each model's\nrelevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI,\nand Anthropic show that goal-directedness is relatively consistent across\ntasks, differs from task performance, and is only moderately sensitive to\nmotivational prompts. Notably, most models are not fully goal-directed. We hope\nour goal-directedness evaluations will enable better monitoring of LLM\nprogress, and enable more deliberate design choices of agentic properties in\nLLMs.\n","authors":["Tom Everitt","Cristina Garbacea","Alexis Bellot","Jonathan Richens","Henry Papadatos","Siméon Campos","Rohin Shah"],"pdf_url":"https://arxiv.org/pdf/2504.11844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11837v1","updated":"2025-04-16T07:52:06Z","published":"2025-04-16T07:52:06Z","title":"FiSMiness: A Finite State Machine Based Paradigm for Emotional Support\n  Conversations","summary":"  Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Finite State Machine (FSM) on LLMs, and propose a framework called\nFiSMiness. Our framework allows a single LLM to bootstrap the planning during\nESC, and self-reason the seeker's emotion, support strategy and the final\nresponse upon each conversational turn. Substantial experiments on ESC datasets\nsuggest that FiSMiness outperforms many baselines, including direct inference,\nself-refine, chain of thought, finetuning, and external-assisted methods, even\nthose with many more parameters.\n","authors":["Yue Zhao","Qingqing Gu","Xiaoyu Wang","Teng Chen","Zhonglin Jiang","Yong Chen","Luo Ji"],"pdf_url":"https://arxiv.org/pdf/2504.11837v1.pdf","comment":"accepted by CMCL"},{"id":"http://arxiv.org/abs/2504.11833v1","updated":"2025-04-16T07:45:10Z","published":"2025-04-16T07:45:10Z","title":"Could Thinking Multilingually Empower LLM Reasoning?","summary":"  Previous work indicates that large language models exhibit a significant\n\"English bias\", i.e. they often perform better when tasks are presented in\nEnglish. Interestingly, we have observed that using certain other languages in\nreasoning tasks can yield better performance than English. However, this\nphenomenon remains under-explored. In this paper, we explore the upper bound of\nharnessing multilingualism in reasoning tasks, suggesting that multilingual\nreasoning promises significantly (by nearly 10 Acc@$k$ points) and robustly\n(tolerance for variations in translation quality and language choice) higher\nupper bounds than English-only reasoning. Besides analyzing the reason behind\nthe upper bound and challenges in reaching it, we also find that common answer\nselection methods cannot achieve this upper bound, due to their limitations and\nbiases. These insights could pave the way for future research aimed at fully\nharnessing the potential of multilingual reasoning in LLMs.\n","authors":["Changjiang Gao","Xu Huang","Wenhao Zhu","Shujian Huang","Lei Li","Fei Yuan"],"pdf_url":"https://arxiv.org/pdf/2504.11833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11829v1","updated":"2025-04-16T07:38:19Z","published":"2025-04-16T07:38:19Z","title":"Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine\n  Translation Evaluation","summary":"  Generation capabilities and language coverage of multilingual large language\nmodels (mLLMs) are advancing rapidly. However, evaluation practices for\ngenerative abilities of mLLMs are still lacking comprehensiveness, scientific\nrigor, and consistent adoption across research labs, which undermines their\npotential to meaningfully guide mLLM development. We draw parallels with\nmachine translation (MT) evaluation, a field that faced similar challenges and\nhas, over decades, developed transparent reporting standards and reliable\nevaluations for multilingual generative models. Through targeted experiments\nacross key stages of the generative evaluation pipeline, we demonstrate how\nbest practices from MT evaluation can deepen the understanding of quality\ndifferences between models. Additionally, we identify essential components for\nrobust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are\nrigorously assessed. We distill these insights into a checklist of actionable\nrecommendations for mLLM research and development.\n","authors":["Julia Kreutzer","Eleftheria Briakou","Sweta Agrawal","Marzieh Fadaee","Kocmi Tom"],"pdf_url":"https://arxiv.org/pdf/2504.11829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04820v3","updated":"2025-04-16T07:09:59Z","published":"2024-08-09T02:22:51Z","title":"Natural Language Outlines for Code: Literate Programming in the LLM Era","summary":"  We propose using natural language outlines as a novel modality and\ninteraction surface for providing AI assistance to developers throughout the\nsoftware development process. An NL outline for a code function comprises\nmultiple statements written in concise prose, which partition the code and\nsummarize its main ideas in the style of literate programming. Crucially, we\nfind that modern LLMs can generate accurate and high-quality NL outlines in\npractice. Moreover, NL outlines enable a bidirectional sync between code and\nNL: a developer can change one and the LLM automatically updates the other. We\ndiscuss many use cases for NL outlines: they can accelerate understanding and\nnavigation of code and diffs, simplify code maintenance, augment code search,\nsteer code generation, and more. We then propose and compare multiple LLM\nprompting techniques for generating outlines and ask professional developers to\njudge outline quality. Finally, we present two case studies applying NL\noutlines toward code review and malware detection.\n","authors":["Kensen Shi","Deniz Altınbüken","Saswat Anand","Mihai Christodorescu","Katja Grünwedel","Alexa Koenings","Sai Naidu","Anurag Pathak","Marc Rasi","Fredde Ribeiro","Brandon Ruffin","Siddhant Sanyam","Maxim Tabachnyk","Sara Toth","Roy Tu","Tobias Welp","Pengcheng Yin","Manzil Zaheer","Satish Chandra","Charles Sutton"],"pdf_url":"https://arxiv.org/pdf/2408.04820v3.pdf","comment":"Accepted to FSE'25 Industry Track"},{"id":"http://arxiv.org/abs/2504.11814v1","updated":"2025-04-16T07:00:47Z","published":"2025-04-16T07:00:47Z","title":"ARWI: Arabic Write and Improve","summary":"  Although Arabic is spoken by over 400 million people, advanced Arabic writing\nassistance tools remain limited. To address this gap, we present ARWI, a new\nwriting assistant that helps learners improve essay writing in Modern Standard\nArabic. ARWI is the first publicly available Arabic writing assistant to\ninclude a prompt database for different proficiency levels, an Arabic text\neditor, state-of-the-art grammatical error detection and correction, and\nautomated essay scoring aligned with the Common European Framework of Reference\nstandards for language attainment. Moreover, ARWI can be used to gather a\ngrowing auto-annotated corpus, facilitating further research on Arabic grammar\ncorrection and essay scoring, as well as profiling patterns of errors made by\nnative speakers and non-native learners. A preliminary user study shows that\nARWI provides actionable feedback, helping learners identify grammatical gaps,\nassess language proficiency, and guide improvement.\n","authors":["Kirill Chirkunov","Bashar Alhafni","Chatrine Qwaider","Nizar Habash","Ted Briscoe"],"pdf_url":"https://arxiv.org/pdf/2504.11814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11809v1","updated":"2025-04-16T06:46:15Z","published":"2025-04-16T06:46:15Z","title":"Efficient and Adaptive Simultaneous Speech Translation with Fully\n  Unidirectional Architecture","summary":"  Simultaneous speech translation (SimulST) produces translations incrementally\nwhile processing partial speech input. Although large language models (LLMs)\nhave showcased strong capabilities in offline translation tasks, applying them\nto SimulST poses notable challenges. Existing LLM-based SimulST approaches\neither incur significant computational overhead due to repeated encoding of\nbidirectional speech encoder, or they depend on a fixed read/write policy,\nlimiting the efficiency and performance. In this work, we introduce Efficient\nand Adaptive Simultaneous Speech Translation (EASiST) with fully unidirectional\narchitecture, including both speech encoder and LLM. EASiST includes a\nmulti-latency data curation strategy to generate semantically aligned SimulST\ntraining samples and redefines SimulST as an interleaved generation task with\nexplicit read/write tokens. To facilitate adaptive inference, we incorporate a\nlightweight policy head that dynamically predicts read/write actions.\nAdditionally, we employ a multi-stage training strategy to align speech-text\nmodalities and optimize both translation and policy behavior. Experiments on\nthe MuST-C En$\\rightarrow$De and En$\\rightarrow$Es datasets demonstrate that\nEASiST offers superior latency-quality trade-offs compared to several strong\nbaselines.\n","authors":["Biao Fu","Donglei Yu","Minpeng Liao","Chengxi Li","Yidong Chen","Kai Fan","Xiaodong Shi"],"pdf_url":"https://arxiv.org/pdf/2504.11809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11793v1","updated":"2025-04-16T05:59:29Z","published":"2025-04-16T05:59:29Z","title":"Selective Attention Federated Learning: Improving Privacy and Efficiency\n  for Clinical Text Classification","summary":"  Federated Learning (FL) faces major challenges regarding communication\noverhead and model privacy when training large language models (LLMs),\nespecially in healthcare applications. To address these, we introduce Selective\nAttention Federated Learning (SAFL), a novel approach that dynamically\nfine-tunes only those transformer layers identified as attention-critical. By\nemploying attention patterns to determine layer importance, SAFL significantly\nreduces communication bandwidth and enhances differential privacy resilience.\nEvaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and\nMIMIC-III discharge summaries) demonstrate that SAFL achieves competitive\nperformance with centralized models while substantially improving communication\nefficiency and privacy preservation.\n","authors":["Yue Li","Lihong Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.11793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09080v2","updated":"2025-04-16T05:45:31Z","published":"2024-10-04T21:39:30Z","title":"Leveraging Social Determinants of Health in Alzheimer's Research Using\n  LLM-Augmented Literature Mining and Knowledge Graphs","summary":"  Growing evidence suggests that social determinants of health (SDoH), a set of\nnonmedical factors, affect individuals' risks of developing Alzheimer's disease\n(AD) and related dementias. Nevertheless, the etiological mechanisms underlying\nsuch relationships remain largely unclear, mainly due to difficulties in\ncollecting relevant information. This study presents a novel, automated\nframework that leverages recent advancements of large language model (LLM) and\nnatural language processing techniques to mine SDoH knowledge from extensive\nliterature and integrate it with AD-related biological entities extracted from\nthe general-purpose knowledge graph PrimeKG. Utilizing graph neural networks,\nwe performed link prediction tasks to evaluate the resultant SDoH-augmented\nknowledge graph. Our framework shows promise for enhancing knowledge discovery\nin AD and can be generalized to other SDoH-related research areas, offering a\nnew tool for exploring the impact of social determinants on health outcomes.\nOur code is available at: https://github.com/hwq0726/SDoHenPKG\n","authors":["Tianqi Shang","Shu Yang","Weiqing He","Tianhua Zhai","Dawei Li","Bojian Hou","Tianlong Chen","Jason H. Moore","Marylyn D. Ritchie","Li Shen"],"pdf_url":"https://arxiv.org/pdf/2410.09080v2.pdf","comment":"Accepted by AMIA-IS'25: AMIA Informatics Summit"},{"id":"http://arxiv.org/abs/2504.11788v1","updated":"2025-04-16T05:41:20Z","published":"2025-04-16T05:41:20Z","title":"Enhancing Web Agents with Explicit Rollback Mechanisms","summary":"  With recent advancements in large language models, web agents have been\ngreatly improved. However, dealing with complex and dynamic web environments\nrequires more advanced planning and search abilities. Previous studies usually\nadopt a greedy one-way search strategy, which may struggle to recover from\nerroneous states. In this work, we enhance web agents with an explicit rollback\nmechanism, enabling the agent to revert back to a previous state in its\nnavigation trajectory. This mechanism gives the model the flexibility to\ndirectly control the search process, leading to an effective and efficient web\nnavigation method. We conduct experiments on two live web navigation benchmarks\nwith zero-shot and fine-tuning settings. The results demonstrate the\neffectiveness of our proposed approach.\n","authors":["Zhisong Zhang","Tianqing Fang","Kaixin Ma","Wenhao Yu","Hongming Zhang","Haitao Mi","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2504.11788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01795v2","updated":"2025-04-16T05:30:34Z","published":"2024-10-02T17:53:08Z","title":"Knowledge-Driven Feature Selection and Engineering for Genotype Data\n  with Large Language Models","summary":"  Predicting phenotypes with complex genetic bases based on a small,\ninterpretable set of variant features remains a challenging task.\nConventionally, data-driven approaches are utilized for this task, yet the high\ndimensional nature of genotype data makes the analysis and prediction\ndifficult. Motivated by the extensive knowledge encoded in pre-trained LLMs and\ntheir success in processing complex biomedical concepts, we set to examine the\nability of LLMs in feature selection and engineering for tabular genotype data,\nwith a novel knowledge-driven framework. We develop FREEFORM, Free-flow\nReasoning and Ensembling for Enhanced Feature Output and Robust Modeling,\ndesigned with chain-of-thought and ensembling principles, to select and\nengineer features with the intrinsic knowledge of LLMs. Evaluated on two\ndistinct genotype-phenotype datasets, genetic ancestry and hereditary hearing\nloss, we find this framework outperforms several data-driven methods,\nparticularly on low-shot regimes. FREEFORM is available as open-source\nframework at GitHub: https://github.com/PennShenLab/FREEFORM.\n","authors":["Joseph Lee","Shu Yang","Jae Young Baik","Xiaoxi Liu","Zhen Tan","Dawei Li","Zixuan Wen","Bojian Hou","Duy Duong-Tran","Tianlong Chen","Li Shen"],"pdf_url":"https://arxiv.org/pdf/2410.01795v2.pdf","comment":"accepted by AMIA-IS'25: AMIA Informatics Summit [Marco Ramoni\n  Distinguished Paper Award for Translational Bioinformatics]"},{"id":"http://arxiv.org/abs/2504.11770v1","updated":"2025-04-16T05:20:08Z","published":"2025-04-16T05:20:08Z","title":"Unsupervised Classification of English Words Based on Phonological\n  Information: Discovery of Germanic and Latinate Clusters","summary":"  Cross-linguistically, native words and loanwords follow different\nphonological rules. In English, for example, words of Germanic and Latinate\norigin exhibit different stress patterns, and a certain syntactic structure is\nexclusive to Germanic verbs. When seeing them as a cognitive model, however,\nsuch etymology-based generalizations face challenges in terms of learnability,\nsince the historical origins of words are presumably inaccessible information\nfor general language learners. In this study, we present computational evidence\nindicating that the Germanic-Latinate distinction in the English lexicon is\nlearnable from the phonotactic information of individual words. Specifically,\nwe performed an unsupervised clustering on corpus-extracted words, and the\nresulting word clusters largely aligned with the etymological distinction. The\nmodel-discovered clusters also recovered various linguistic generalizations\ndocumented in the previous literature regarding the corresponding etymological\nclasses. Moreover, our findings also uncovered previously unrecognized features\nof the quasi-etymological clusters, offering novel hypotheses for future\nexperimental studies.\n","authors":["Takashi Morita","Timothy J. O'Donnell"],"pdf_url":"https://arxiv.org/pdf/2504.11770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09566v2","updated":"2025-04-16T05:02:01Z","published":"2025-04-13T13:35:41Z","title":"Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution","summary":"  Chain-of-Thought (CoT) prompting enhances the reasoning of large language\nmodels (LLMs) by decomposing problems into sequential steps, mimicking human\nlogic and reducing errors. However, complex tasks with vast solution spaces and\nvague constraints often exceed the capacity of a single reasoning chain.\nInspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic\ngeometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends\nCoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper\nlogical dependencies, enabling more robust and structured problem-solving. MFR\ndecomposes a module into a sequence of free modules with minimal rank,\nproviding a structured analytical approach to complex systems. This method\nintroduces the concepts of \"Module\", \"Betti numbers\",\"Freeness\", \"Mapping\",\n\"Exactness\" and \"Minimality\", enabling the systematic decomposition of the\noriginal complex problem into logically complete minimal subproblems while\npreserving key problem features and reducing reasoning length. We tested SoT\nacross diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini,\nQwen2.5), achieving inference accuracy that matches or surpasses mainstream\nCoTs standards. Additionally, by aligning the sampling process with algebraic\nconstraints, our approach enhances the scalability of inference time in LLMs,\nensuring both transparent reasoning and high performance. Our code will be\npublicly available at https://github.com/dlMARiA/Syzygy-of-thoughts.\n","authors":["Chenghao Li","Chaoning Zhang","Yi Lu","Jiaquan Zhang","Qigan Sun","Xudong Wang","Jiwei Wei","Guoqing Wang","Yang Yang","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2504.09566v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.24235v2","updated":"2025-04-16T04:32:24Z","published":"2025-03-31T15:46:15Z","title":"What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models","summary":"  As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions. Our repository is available on\nhttps://github.com/testtimescaling/testtimescaling.github.io/\n","authors":["Qiyuan Zhang","Fuyuan Lyu","Zexu Sun","Lei Wang","Weixu Zhang","Zhihan Guo","Yufei Wang","Niklas Muennighoff","Irwin King","Xue Liu","Chen Ma"],"pdf_url":"https://arxiv.org/pdf/2503.24235v2.pdf","comment":"v2: Creating the GitHub repository, Citing some missed works,\n  Incorporating two new domains (agentic and evaluation) in where to scale,\n  Incorporating one direction (thoughtology research) in challenge and future\n  work"},{"id":"http://arxiv.org/abs/2412.11302v3","updated":"2025-04-16T04:30:56Z","published":"2024-12-15T20:27:45Z","title":"Sequence-Level Leakage Risk of Training Data in Large Language Models","summary":"  This work quantifies the risk of training data leakage from LLMs (Large\nLanguage Models) using sequence-level probabilities. Computing extraction\nprobabilities for individual sequences provides finer-grained information than\nhas been studied in prior benchmarking work. We re-analyze the effects of\ndecoding schemes, model sizes, prefix lengths, partial sequence leakages, and\ntoken positions to uncover new insights that were not possible in previous\nworks due to their choice of metrics. We perform this study on two pre-trained\nmodels, Llama and OPT, trained on the Common Crawl and The Pile respectively.\nWe discover that 1) Extraction Rate, the predominant metric used in prior\nquantification work, underestimates the threat of leakage of training data in\nrandomized LLMs by as much as 2.14X. 2) Although on average, larger models and\nlonger prefixes can extract more data, this is not true for a substantial\nportion of individual sequences. 30.4-41.5% of our sequences are easier to\nextract with either shorter prefixes or smaller models. 3) Contrary to previous\nbeliefs, partial leakage in commonly used decoding schemes like top-k and top-p\nis not easier than leaking verbatim training data. The aim of this work is to\nencourage the adoption of this metric for future work on quantification of\ntraining data extraction.\n","authors":["Trishita Tiwari","G. Edward Suh"],"pdf_url":"https://arxiv.org/pdf/2412.11302v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11741v1","updated":"2025-04-16T03:39:38Z","published":"2025-04-16T03:39:38Z","title":"Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve\n  after SFT?","summary":"  Recent supervised fine-tuning (SFT) approaches have significantly improved\nlanguage models' performance on mathematical reasoning tasks, even when models\nare trained at a small scale. However, the specific capabilities enhanced\nthrough such fine-tuning remain poorly understood. In this paper, we conduct a\ndetailed analysis of model performance on the AIME24 dataset to understand how\nreasoning capabilities evolve. We discover a ladder-like structure in problem\ndifficulty, categorize questions into four tiers (Easy, Medium, Hard, and\nExtremely Hard (Exh)), and identify the specific requirements for advancing\nbetween tiers. We find that progression from Easy to Medium tier requires\nadopting an R1 reasoning style with minimal SFT (500-1K instances), while\nHard-level questions suffer from frequent model's errors at each step of the\nreasoning chain, with accuracy plateauing at around 65% despite logarithmic\nscaling. Exh-level questions present a fundamentally different challenge; they\nrequire unconventional problem-solving skills that current models uniformly\nstruggle with. Additional findings reveal that carefully curated small-scale\ndatasets offer limited advantage-scaling dataset size proves far more\neffective. Our analysis provides a clearer roadmap for advancing language model\ncapabilities in mathematical reasoning.\n","authors":["Yiyou Sun","Georgia Zhou","Hao Wang","Dacheng Li","Nouha Dziri","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2504.11741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11107v2","updated":"2025-04-16T03:33:29Z","published":"2025-01-19T16:35:09Z","title":"ChaosEater: Fully Automating Chaos Engineering with Large Language\n  Models","summary":"  Chaos Engineering (CE) is an engineering technique aimed at improving the\nresiliency of distributed systems. It involves artificially injecting specific\nfailures into a distributed system and observing its behavior in response.\nBased on the observation, the system can be proactively improved to handle\nthose failures. Recent CE tools implement the automated execution of predefined\nCE experiments. However, defining these experiments and improving the system\nbased on the experimental results still remain manual. To reduce the costs of\nthe manual operations, we propose ChaosEater, a system for automating the\nentire CE operations with Large Language Models (LLMs). It predefines the\nagentic workflow according to a systematic CE cycle and assigns subdivided\noperations within the workflow to LLMs. ChaosEater targets CE for Kubernetes\nsystems, which are managed through code (i.e., Infrastructure as Code).\nTherefore, the LLMs in ChaosEater perform software engineering tasks to\ncomplete CE cycles, including requirement definition, code generation,\ndebugging, and testing. We evaluate ChaosEater through case studies on both\nsmall and large Kubernetes systems. The results demonstrate that it stably\ncompletes reasonable single CE cycles with significantly low time and monetary\ncosts. The CE cycles are also qualitatively validated by human engineers and\nLLMs.\n","authors":["Daisuke Kikuta","Hiroki Ikeuchi","Kengo Tajiri"],"pdf_url":"https://arxiv.org/pdf/2501.11107v2.pdf","comment":"114 pages (7 main), 11 figures. Project page:\n  https://ntt-dkiku.github.io/chaos-eater"},{"id":"http://arxiv.org/abs/2504.11739v1","updated":"2025-04-16T03:33:25Z","published":"2025-04-16T03:33:25Z","title":"The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for\n  Text-to-Video Generation","summary":"  The evolution of Text-to-video (T2V) generative models, trained on\nlarge-scale datasets, has been marked by significant progress. However, the\nsensitivity of T2V generative models to input prompts highlights the critical\nrole of prompt design in influencing generative outcomes. Prior research has\npredominantly relied on Large Language Models (LLMs) to align user-provided\nprompts with the distribution of training prompts, albeit without tailored\nguidance encompassing prompt vocabulary and sentence structure nuances. To this\nend, we introduce \\textbf{RAPO}, a novel \\textbf{R}etrieval-\\textbf{A}ugmented\n\\textbf{P}rompt \\textbf{O}ptimization framework. In order to address potential\ninaccuracies and ambiguous details generated by LLM-generated prompts. RAPO\nrefines the naive prompts through dual optimization branches, selecting the\nsuperior prompt for T2V generation. The first branch augments user prompts with\ndiverse modifiers extracted from a learned relational graph, refining them to\nalign with the format of training prompts via a fine-tuned LLM. Conversely, the\nsecond branch rewrites the naive prompt using a pre-trained LLM following a\nwell-defined instruction set. Extensive experiments demonstrate that RAPO can\neffectively enhance both the static and dynamic dimensions of generated videos,\ndemonstrating the significance of prompt optimization for user-provided\nprompts. Project website:\n\\href{https://whynothaha.github.io/Prompt_optimizer/RAPO.html}{GitHub}.\n","authors":["Bingjie Gao","Xinyu Gao","Xiaoxue Wu","Yujie Zhou","Yu Qiao","Li Niu","Xinyuan Chen","Yaohui Wang"],"pdf_url":"https://arxiv.org/pdf/2504.11739v1.pdf","comment":"accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2504.11257v2","updated":"2025-04-16T02:29:33Z","published":"2025-04-15T14:56:21Z","title":"UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction\n  Synthesis","summary":"  Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation. In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://colmon46.github.io/i2e-bench-leaderboard/ .\n","authors":["Xinyi Liu","Xiaoyi Zhang","Ziyun Zhang","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2504.11257v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00342v7","updated":"2025-04-16T02:18:10Z","published":"2024-06-29T07:01:51Z","title":"KPC-cF: Aspect-Based Sentiment Analysis via Implicit-Feature Alignment\n  with Corpus Filtering","summary":"  Investigations into Aspect-Based Sentiment Analysis (ABSA) for Korean\nindustrial reviews are notably lacking in the existing literature. Our research\nproposes an intuitive and effective framework for ABSA in low-resource\nlanguages such as Korean. It optimizes prediction labels by integrating\ntranslated benchmark and unlabeled Korean data. Using a model fine-tuned on\ntranslated data, we pseudo-labeled the actual Korean NLI set. Subsequently, we\napplied LaBSE and \\MSP{}-based filtering to this pseudo-NLI set as implicit\nfeature, enhancing Aspect Category Detection and Polarity determination through\nadditional training. Incorporating dual filtering, this model bridged dataset\ngaps and facilitates feature alignment with minimal resources. By implementing\nalignment pipelines, our approach aims to leverage high-resource datasets to\ndevelop reliable predictive and refined models within corporate or individual\ncommunities in low-resource language countries. Compared to English ABSA, our\nframework showed an approximately 3\\% difference in F1 scores and accuracy. We\nwill release our dataset and code for Korean ABSA, at this link.\n","authors":["Kibeom Nam"],"pdf_url":"https://arxiv.org/pdf/2407.00342v7.pdf","comment":"Work in Progress, DMLR@ICML 2024"},{"id":"http://arxiv.org/abs/2504.10982v2","updated":"2025-04-16T01:42:26Z","published":"2025-04-15T08:46:39Z","title":"Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical\n  Question Answering with Small-Scale LLMs","summary":"  Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages.\n","authors":["Yingjian Chen","Feiyang Li","Xingyu Song","Tianxiao Li","Issey Sukeda","Irene Li"],"pdf_url":"https://arxiv.org/pdf/2504.10982v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2407.12879v4","updated":"2025-04-16T00:26:13Z","published":"2024-07-16T09:28:23Z","title":"Large Visual-Language Models Are Also Good Classifiers: A Study of\n  In-Context Multimodal Fake News Detection","summary":"  Large visual-language models (LVLMs) exhibit exceptional performance in\nvisual-language reasoning across diverse cross-modal benchmarks. Despite these\nadvances, recent research indicates that Large Language Models (LLMs), like\nGPT-3.5-turbo, underachieve compared to well-trained smaller models, such as\nBERT, in Fake News Detection (FND), prompting inquiries into LVLMs' efficacy in\nFND tasks. Although performance could improve through fine-tuning LVLMs, the\nsubstantial parameters and requisite pre-trained weights render it a\nresource-heavy endeavor for FND applications. This paper initially assesses the\nFND capabilities of two notable LVLMs, CogVLM and GPT4V, in comparison to a\nsmaller yet adeptly trained CLIP model in a zero-shot context. The findings\ndemonstrate that LVLMs can attain performance competitive with that of the\nsmaller model. Next, we integrate standard in-context learning (ICL) with\nLVLMs, noting improvements in FND performance, though limited in scope and\nconsistency. To address this, we introduce the \\textbf{I}n-context\n\\textbf{M}ultimodal \\textbf{F}ake \\textbf{N}ews \\textbf{D}etection (IMFND)\nframework, enriching in-context examples and test inputs with predictions and\ncorresponding probabilities from a well-trained smaller model. This strategic\nintegration directs the LVLMs' focus towards news segments associated with\nhigher probabilities, thereby improving their analytical accuracy. The\nexperimental results suggest that the IMFND framework significantly boosts the\nFND efficiency of LVLMs, achieving enhanced accuracy over the standard ICL\napproach across three publicly available FND datasets.\n","authors":["Ye Jiang","Yimin Wang"],"pdf_url":"https://arxiv.org/pdf/2407.12879v4.pdf","comment":"Withdraw for new experiments"},{"id":"http://arxiv.org/abs/2504.11673v1","updated":"2025-04-16T00:10:34Z","published":"2025-04-16T00:10:34Z","title":"Higher-Order Binding of Language Model Virtual Personas: a Study on\n  Approximating Political Partisan Misperceptions","summary":"  Large language models (LLMs) are increasingly capable of simulating human\nbehavior, offering cost-effective ways to estimate user responses during the\nearly phases of survey design. While previous studies have examined whether\nmodels can reflect individual opinions or attitudes, we argue that a\n\\emph{higher-order} binding of virtual personas requires successfully\napproximating not only the opinions of a user as an identified member of a\ngroup, but also the nuanced ways in which that user perceives and evaluates\nthose outside the group. In particular, faithfully simulating how humans\nperceive different social groups is critical for applying LLMs to various\npolitical science studies, including timely topics on polarization dynamics,\ninter-group conflict, and democratic backsliding. To this end, we propose a\nnovel methodology for constructing virtual personas with synthetic user\n``backstories\" generated as extended, multi-turn interview transcripts. Our\ngenerated backstories are longer, rich in detail, and consistent in\nauthentically describing a singular individual, compared to previous methods.\nWe show that virtual personas conditioned on our backstories closely replicate\nhuman response distributions (up to an 87\\% improvement as measured by\nWasserstein Distance) and produce effect sizes that closely match those\nobserved in the original studies. Altogether, our work extends the\napplicability of LLMs beyond estimating individual self-opinions, enabling\ntheir use in a broader range of human studies.\n","authors":["Minwoo Kang","Suhong Moon","Seung Hyeong Lee","Ayush Raj","Joseph Suh","David M. Chan"],"pdf_url":"https://arxiv.org/pdf/2504.11673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12526v1","updated":"2025-04-16T23:15:09Z","published":"2025-04-16T23:15:09Z","title":"MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models","summary":"  Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency.\n","authors":["Junyang Zhang","Tianyi Zhu","Cheng Luo","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2504.12526v1.pdf","comment":"Submitted to COLM"},{"id":"http://arxiv.org/abs/2504.12523v1","updated":"2025-04-16T23:03:40Z","published":"2025-04-16T23:03:40Z","title":"Memorization vs. Reasoning: Updating LLMs with New Knowledge","summary":"  Large language models (LLMs) encode vast amounts of pre-trained knowledge in\ntheir parameters, but updating them as real-world information evolves remains a\nchallenge. Existing methodologies and benchmarks primarily target entity\nsubstitutions, failing to capture the full breadth of complex real-world\ndynamics. In this paper, we introduce Knowledge Update Playground (KUP), an\nautomatic pipeline for simulating realistic knowledge updates reflected in an\nevidence corpora. KUP's evaluation framework includes direct and indirect\nprobes to both test memorization of updated facts and reasoning over them, for\nany update learning methods. Next, we present a lightweight method called\nmemory conditioned training (MCT), which conditions tokens in the update corpus\non self-generated \"memory\" tokens during training. Our strategy encourages LLMs\nto surface and reason over newly memorized knowledge at inference. Our results\non two strong LLMs show that (1) KUP benchmark is highly challenging, with the\nbest CPT models achieving $<2\\%$ in indirect probing setting (reasoning) and\n(2) MCT training significantly outperforms prior continued pre-training (CPT)\nbaselines, improving direct probing (memorization) results by up to $25.4\\%$.\n","authors":["Aochong Oliver Li","Tanya Goyal"],"pdf_url":"https://arxiv.org/pdf/2504.12523v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2504.12522v1","updated":"2025-04-16T23:02:23Z","published":"2025-04-16T23:02:23Z","title":"Evaluating the Diversity and Quality of LLM Generated Content","summary":"  Recent work suggests that preference-tuning techniques--including\nReinforcement Learning from Human Preferences (RLHF) methods like PPO and GRPO,\nas well as alternatives like DPO--reduce diversity, creating a dilemma given\nthat such models are widely deployed in applications requiring diverse outputs.\nTo address this, we introduce a framework for measuring effective semantic\ndiversity--diversity among outputs that meet quality thresholds--which better\nreflects the practical utility of large language models (LLMs). Using\nopen-ended tasks that require no human intervention, we find counterintuitive\nresults: although preference-tuned models--especially those trained via\nRL--exhibit reduced lexical and syntactic diversity, they produce greater\neffective semantic diversity than SFT or base models, not from increasing\ndiversity among high-quality outputs, but from generating more high-quality\noutputs overall. We discover that preference tuning reduces syntactic diversity\nwhile preserving semantic diversity--revealing a distinction between diversity\nin form and diversity in content that traditional metrics often overlook. Our\nanalysis further shows that smaller models are consistently more\nparameter-efficient at generating unique content within a fixed sampling\nbudget, offering insights into the relationship between model scaling and\ndiversity. These findings have important implications for applications that\nrequire diverse yet high-quality outputs, from creative assistance to synthetic\ndata generation.\n","authors":["Alexander Shypula","Shuo Li","Botong Zhang","Vishakh Padmakumar","Kayo Yin","Osbert Bastani"],"pdf_url":"https://arxiv.org/pdf/2504.12522v1.pdf","comment":"ICLR 2025 Third Workshop on Deep Learning for Code"},{"id":"http://arxiv.org/abs/2504.12516v1","updated":"2025-04-16T22:27:45Z","published":"2025-04-16T22:27:45Z","title":"BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents","summary":"  We present BrowseComp, a simple yet challenging benchmark for measuring the\nability for agents to browse the web. BrowseComp comprises 1,266 questions that\nrequire persistently navigating the internet in search of hard-to-find,\nentangled information. Despite the difficulty of the questions, BrowseComp is\nsimple and easy-to-use, as predicted answers are short and easily verifiable\nagainst reference answers. BrowseComp for browsing agents can be seen as\nanalogous to how programming competitions are an incomplete but useful\nbenchmark for coding agents. While BrowseComp sidesteps challenges of a true\nuser query distribution, like generating long answers or resolving ambiguity,\nit measures the important core capability of exercising persistence and\ncreativity in finding information. BrowseComp can be found at\nhttps://github.com/openai/simple-evals.\n","authors":["Jason Wei","Zhiqing Sun","Spencer Papay","Scott McKinney","Jeffrey Han","Isa Fulford","Hyung Won Chung","Alex Tachard Passos","William Fedus","Amelia Glaese"],"pdf_url":"https://arxiv.org/pdf/2504.12516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04682v2","updated":"2025-04-16T22:20:21Z","published":"2024-08-08T05:45:42Z","title":"ToolSandbox: A Stateful, Conversational, Interactive Evaluation\n  Benchmark for LLM Tool Use Capabilities","summary":"  Recent large language models (LLMs) advancements sparked a growing research\ninterest in tool assisted LLMs solving real-world challenges, which calls for\ncomprehensive evaluation of tool-use capabilities. While previous works focused\non either evaluating over stateless web services (RESTful API), based on a\nsingle turn user prompt, or an off-policy dialog trajectory, ToolSandbox\nincludes stateful tool execution, implicit state dependencies between tools, a\nbuilt-in user simulator supporting on-policy conversational evaluation and a\ndynamic evaluation strategy for intermediate and final milestones over an\narbitrary trajectory. We show that open source and proprietary models have a\nsignificant performance gap, and complex tasks like State Dependency,\nCanonicalization and Insufficient Information defined in ToolSandbox are\nchallenging even the most capable SOTA LLMs, providing brand-new insights into\ntool-use LLM capabilities. ToolSandbox evaluation framework is released at\nhttps://github.com/apple/ToolSandbox\n","authors":["Jiarui Lu","Thomas Holleis","Yizhe Zhang","Bernhard Aumayer","Feng Nan","Felix Bai","Shuang Ma","Shen Ma","Mengyu Li","Guoli Yin","Zirui Wang","Ruoming Pang"],"pdf_url":"https://arxiv.org/pdf/2408.04682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09586v2","updated":"2025-04-16T21:50:45Z","published":"2024-09-15T02:13:03Z","title":"ValueCompass: A Framework for Measuring Contextual Value Alignment\n  Between Human and LLMs","summary":"  As AI systems become more advanced, ensuring their alignment with a diverse\nrange of individuals and societal values becomes increasingly critical. But how\ncan we capture fundamental human values and assess the degree to which AI\nsystems align with them? We introduce ValueCompass, a framework of fundamental\nvalues, grounded in psychological theory and a systematic review, to identify\nand evaluate human-AI alignment. We apply ValueCompass to measure the value\nalignment of humans and large language models (LLMs) across four real-world\nscenarios: collaborative writing, education, public sectors, and healthcare.\nOur findings reveal concerning misalignments between humans and LLMs, such as\nhumans frequently endorse values like \"National Security\" which were largely\nrejected by LLMs. We also observe that values differ across scenarios,\nhighlighting the need for context-aware AI alignment strategies. This work\nprovides valuable insights into the design space of human-AI alignment, laying\nthe foundations for developing AI systems that responsibly reflect societal\nvalues and ethics.\n","authors":["Hua Shen","Tiffany Knearem","Reshmi Ghosh","Yu-Ju Yang","Nicholas Clark","Tanushree Mitra","Yun Huang"],"pdf_url":"https://arxiv.org/pdf/2409.09586v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10816v2","updated":"2025-04-16T21:45:16Z","published":"2025-04-15T02:31:34Z","title":"CSPLADE: Learned Sparse Retrieval with Causal Language Models","summary":"  In recent years, dense retrieval has been the focus of information retrieval\n(IR) research. While effective, dense retrieval produces uninterpretable dense\nvectors, and suffers from the drawback of large index size. Learned sparse\nretrieval (LSR) has emerged as promising alternative, achieving competitive\nretrieval performance while also being able to leverage the classical inverted\nindex data structure for efficient retrieval. However, limited works have\nexplored scaling LSR beyond BERT scale. In this work, we identify two\nchallenges in training large language models (LLM) for LSR: (1) training\ninstability during the early stage of contrastive training; (2) suboptimal\nperformance due to pre-trained LLM's unidirectional attention. To address these\nchallenges, we propose two corresponding techniques: (1) a lightweight\nadaptation training phase to eliminate training instability; (2) two model\nvariants to enable bidirectional information. With these techniques, we are\nable to train LSR models with 8B scale LLM, and achieve competitive retrieval\nperformance with reduced index size. Furthermore, we are among the first to\nanalyze the performance-efficiency tradeoff of LLM-based LSR model through the\nlens of model quantization. Our findings provide insights into adapting LLMs\nfor efficient retrieval modeling.\n","authors":["Zhichao Xu","Aosong Feng","Yijun Tian","Haibo Ding","Lin Lee Cheong"],"pdf_url":"https://arxiv.org/pdf/2504.10816v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12495v1","updated":"2025-04-16T21:24:41Z","published":"2025-04-16T21:24:41Z","title":"Beyond Text: Characterizing Domain Expert Needs in Document Research","summary":"  Working with documents is a key part of almost any knowledge work, from\ncontextualizing research in a literature review to reviewing legal precedent.\nRecently, as their capabilities have expanded, primarily text-based NLP systems\nhave often been billed as able to assist or even automate this kind of work.\nBut to what extent are these systems able to model these tasks as experts\nconceptualize and perform them now? In this study, we interview sixteen domain\nexperts across two domains to understand their processes of document research,\nand compare it to the current state of NLP systems. We find that our\nparticipants processes are idiosyncratic, iterative, and rely extensively on\nthe social context of a document in addition its content; existing approaches\nin NLP and adjacent fields that explicitly center the document as an object,\nrather than as merely a container for text, tend to better reflect our\nparticipants' priorities, though they are often less accessible outside their\nresearch communities. We call on the NLP community to more carefully consider\nthe role of the document in building useful tools that are accessible,\npersonalizable, iterative, and socially aware.\n","authors":["Sireesh Gururaja","Nupoor Gandhi","Jeremiah Milbauer","Emma Strubell"],"pdf_url":"https://arxiv.org/pdf/2504.12495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12494v1","updated":"2025-04-16T21:24:38Z","published":"2025-04-16T21:24:38Z","title":"Accelerating Clinical NLP at Scale with a Hybrid Framework with Reduced\n  GPU Demands: A Case Study in Dementia Identification","summary":"  Clinical natural language processing (NLP) is increasingly in demand in both\nclinical research and operational practice. However, most of the\nstate-of-the-art solutions are transformers-based and require high\ncomputational resources, limiting their accessibility. We propose a hybrid NLP\nframework that integrates rule-based filtering, a Support Vector Machine (SVM)\nclassifier, and a BERT-based model to improve efficiency while maintaining\naccuracy. We applied this framework in a dementia identification case study\ninvolving 4.9 million veterans with incident hypertension, analyzing 2.1\nbillion clinical notes. At the patient level, our method achieved a precision\nof 0.90, a recall of 0.84, and an F1-score of 0.87. Additionally, this NLP\napproach identified over three times as many dementia cases as structured data\nmethods. All processing was completed in approximately two weeks using a single\nmachine with dual A40 GPUs. This study demonstrates the feasibility of hybrid\nNLP solutions for large-scale clinical text analysis, making state-of-the-art\nmethods more accessible to healthcare organizations with limited computational\nresources.\n","authors":["Jianlin Shi","Qiwei Gan","Elizabeth Hanchrow","Annie Bowles","John Stanley","Adam P. Bress","Jordana B. Cohen","Patrick R. Alba"],"pdf_url":"https://arxiv.org/pdf/2504.12494v1.pdf","comment":"This manuscript has been submitted to AMIA 2025 annual symposium\n  (https://amia.org/education-events/amia-2025-annual-symposium)"},{"id":"http://arxiv.org/abs/2504.12491v1","updated":"2025-04-16T21:19:09Z","published":"2025-04-16T21:19:09Z","title":"Can Pre-training Indicators Reliably Predict Fine-tuning Outcomes of\n  LLMs?","summary":"  While metrics available during pre-training, such as perplexity, correlate\nwell with model performance at scaling-laws studies, their predictive\ncapacities at a fixed model size remain unclear, hindering effective model\nselection and development. To address this gap, we formulate the task of\nselecting pre-training checkpoints to maximize downstream fine-tuning\nperformance as a pairwise classification problem: predicting which of two LLMs,\ndiffering in their pre-training, will perform better after supervised\nfine-tuning (SFT). We construct a dataset using 50 1B parameter LLM variants\nwith systematically varied pre-training configurations, e.g., objectives or\ndata, and evaluate them on diverse downstream tasks after SFT. We first conduct\na study and demonstrate that the conventional perplexity is a misleading\nindicator. As such, we introduce novel unsupervised and supervised proxy\nmetrics derived from pre-training that successfully reduce the relative\nperformance prediction error rate by over 50%. Despite the inherent complexity\nof this task, we demonstrate the practical utility of our proposed proxies in\nspecific scenarios, paving the way for more efficient design of pre-training\nschemes optimized for various downstream tasks.\n","authors":["Hansi Zeng","Kai Hui","Honglei Zhuang","Zhen Qin","Zhenrui Yue","Hamed Zamani","Dana Alon"],"pdf_url":"https://arxiv.org/pdf/2504.12491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12477v1","updated":"2025-04-16T20:28:50Z","published":"2025-04-16T20:28:50Z","title":"Towards Conversational AI for Human-Machine Collaborative MLOps","summary":"  This paper presents a Large Language Model (LLM) based conversational agent\nsystem designed to enhance human-machine collaboration in Machine Learning\nOperations (MLOps). We introduce the Swarm Agent, an extensible architecture\nthat integrates specialized agents to create and manage ML workflows through\nnatural language interactions. The system leverages a hierarchical, modular\ndesign incorporating a KubeFlow Pipelines (KFP) Agent for ML pipeline\norchestration, a MinIO Agent for data management, and a Retrieval-Augmented\nGeneration (RAG) Agent for domain-specific knowledge integration. Through\niterative reasoning loops and context-aware processing, the system enables\nusers with varying technical backgrounds to discover, execute, and monitor ML\npipelines; manage datasets and artifacts; and access relevant documentation,\nall via intuitive conversational interfaces. Our approach addresses the\naccessibility gap in complex MLOps platforms like Kubeflow, making advanced ML\ntools broadly accessible while maintaining the flexibility to extend to other\nplatforms. The paper describes the architecture, implementation details, and\ndemonstrates how this conversational MLOps assistant reduces complexity and\nlowers barriers to entry for users across diverse technical skill levels.\n","authors":["George Fatouros","Georgios Makridis","George Kousiouris","John Soldatos","Anargyros Tsadimas","Dimosthenis Kyriazis"],"pdf_url":"https://arxiv.org/pdf/2504.12477v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.12474v1","updated":"2025-04-16T20:25:11Z","published":"2025-04-16T20:25:11Z","title":"Integrating Structural and Semantic Signals in Text-Attributed Graphs\n  with BiGTex","summary":"  Text-attributed graphs (TAGs) present unique challenges in representation\nlearning by requiring models to capture both the semantic richness of\nnode-associated texts and the structural dependencies of the graph. While graph\nneural networks (GNNs) excel at modeling topological information, they lack the\ncapacity to process unstructured text. Conversely, large language models (LLMs)\nare proficient in text understanding but are typically unaware of graph\nstructure. In this work, we propose BiGTex (Bidirectional Graph Text), a novel\narchitecture that tightly integrates GNNs and LLMs through stacked Graph-Text\nFusion Units. Each unit allows for mutual attention between textual and\nstructural representations, enabling information to flow in both directions,\ntext influencing structure and structure guiding textual interpretation. The\nproposed architecture is trained using parameter-efficient fine-tuning (LoRA),\nkeeping the LLM frozen while adapting to task-specific signals. Extensive\nexperiments on five benchmark datasets demonstrate that BiGTex achieves\nstate-of-the-art performance in node classification and generalizes effectively\nto link prediction. An ablation study further highlights the importance of soft\nprompting and bi-directional attention in the model's success.\n","authors":["Azadeh Beiranvand","Seyed Mehdi Vahidipour"],"pdf_url":"https://arxiv.org/pdf/2504.12474v1.pdf","comment":"20 pages, 3 figures"},{"id":"http://arxiv.org/abs/2504.12466v1","updated":"2025-04-16T20:03:47Z","published":"2025-04-16T20:03:47Z","title":"SLURG: Investigating the Feasibility of Generating Synthetic Online\n  Fallacious Discourse","summary":"  In our paper we explore the definition, and extrapolation of fallacies as\nthey pertain to the automatic detection of manipulation on social media. In\nparticular we explore how these logical fallacies might appear in the real\nworld i.e internet forums. We discovered a prevalence of misinformation /\nmisguided intention in discussion boards specifically centered around the\nUkrainian Russian Conflict which serves to narrow the domain of our task.\nAlthough automatic fallacy detection has gained attention recently, most\ndatasets use unregulated fallacy taxonomies or are limited to formal linguistic\ndomains like political debates or news reports. Online discourse, however,\noften features non-standardized and diverse language not captured in these\ndomains. We present Shady Linguistic Utterance Replication-Generation (SLURG)\nto address these limitations, exploring the feasibility of generating synthetic\nfallacious forum-style comments using large language models (LLMs),\nspecifically DeepHermes-3-Mistral-24B. Our findings indicate that LLMs can\nreplicate the syntactic patterns of real data} and that high-quality few-shot\nprompts enhance LLMs' ability to mimic the vocabulary diversity of online\nforums.\n","authors":["Cal Blanco","Gavin Dsouza","Hugo Lin","Chelsey Rush"],"pdf_url":"https://arxiv.org/pdf/2504.12466v1.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2504.12459v1","updated":"2025-04-16T19:50:03Z","published":"2025-04-16T19:50:03Z","title":"On Linear Representations and Pretraining Data Frequency in Language\n  Models","summary":"  Pretraining data has a direct impact on the behaviors and quality of language\nmodels (LMs), but we only understand the most basic principles of this\nrelationship. While most work focuses on pretraining data's effect on\ndownstream task behavior, we investigate its relationship to LM\nrepresentations. Previous work has discovered that, in language models, some\nconcepts are encoded `linearly' in the representations, but what factors cause\nthese representations to form? We study the connection between pretraining data\nfrequency and models' linear representations of factual relations. We find\nevidence that the formation of linear representations is strongly connected to\npretraining term frequencies; specifically for subject-relation-object fact\ntriplets, both subject-object co-occurrence frequency and in-context learning\naccuracy for the relation are highly correlated with linear representations.\nThis is the case across all phases of pretraining. In OLMo-7B and GPT-J, we\ndiscover that a linear representation consistently (but not exclusively) forms\nwhen the subjects and objects within a relation co-occur at least 1k and 2k\ntimes, respectively, regardless of when these occurrences happen during\npretraining. Finally, we train a regression model on measurements of linear\nrepresentation quality in fully-trained LMs that can predict how often a term\nwas seen in pretraining. Our model achieves low error even on inputs from a\ndifferent model with a different pretraining dataset, providing a new method\nfor estimating properties of the otherwise-unknown training data of closed-data\nmodels. We conclude that the strength of linear representations in LMs contains\nsignal about the models' pretraining corpora that may provide new avenues for\ncontrolling and improving model behavior: particularly, manipulating the\nmodels' training data to meet specific frequency thresholds.\n","authors":["Jack Merullo","Noah A. Smith","Sarah Wiegreffe","Yanai Elazar"],"pdf_url":"https://arxiv.org/pdf/2504.12459v1.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2405.01744v2","updated":"2025-04-16T19:45:24Z","published":"2024-05-02T21:27:45Z","title":"ALCM: Autonomous LLM-Augmented Causal Discovery Framework","summary":"  To perform effective causal inference in high-dimensional datasets,\ninitiating the process with causal discovery is imperative, wherein a causal\ngraph is generated based on observational data. However, obtaining a complete\nand accurate causal graph poses a formidable challenge, recognized as an NP-\nhard problem. Recently, the advent of Large Language Models (LLMs) has ushered\nin a new era, indicating their emergent capabilities and widespread\napplicability in facilitating causal reasoning across diverse domains, such as\nmedicine, finance, and science. The expansive knowledge base of LLMs holds the\npotential to elevate the field of causal reasoning by offering\ninterpretability, making inferences, generalizability, and uncovering novel\ncausal structures. In this paper, we introduce a new framework, named\nAutonomous LLM-Augmented Causal Discovery Framework (ALCM), to synergize\ndata-driven causal discovery algorithms and LLMs, automating the generation of\na more resilient, accurate, and explicable causal graph. The ALCM consists of\nthree integral components: causal structure learning, causal wrapper, and\nLLM-driven causal refiner. These components autonomously collaborate within a\ndynamic environment to address causal discovery questions and deliver plausible\ncausal graphs. We evaluate the ALCM framework by implementing two\ndemonstrations on seven well-known datasets. Experimental results demonstrate\nthat ALCM outperforms existing LLM methods and conventional data-driven causal\nreasoning mechanisms. This study not only shows the effectiveness of the ALCM\nbut also underscores new research directions in leveraging the causal reasoning\ncapabilities of LLMs.\n","authors":["Elahe Khatibi","Mahyar Abbasian","Zhongqi Yang","Iman Azimi","Amir M. Rahmani"],"pdf_url":"https://arxiv.org/pdf/2405.01744v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07466v2","updated":"2025-04-16T19:20:42Z","published":"2024-11-12T01:05:55Z","title":"IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark for\n  LLMs","summary":"  Recent evaluations of LLMs on coreference resolution have revealed that\ntraditional output formats and evaluation metrics do not fully capture the\nmodels' referential understanding. To address this, we introduce IdentifyMe, a\nnew benchmark for mention resolution presented in a multiple-choice question\n(MCQ) format, commonly used for evaluating LLMs. IdentifyMe features long\nnarratives and employs heuristics to exclude easily identifiable mentions,\ncreating a more challenging task. The benchmark also consists of a curated\nmixture of different mention types and corresponding entities, allowing for a\nfine-grained analysis of model performance. We evaluate both closed- and open\nsource LLMs on IdentifyMe and observe a significant performance gap (20-30%)\nbetween the state-of-the-art sub-10B open models vs. closed ones. We observe\nthat pronominal mentions, which have limited surface information, are typically\nmuch harder for models to resolve than nominal mentions. Additionally, we find\nthat LLMs often confuse entities when their mentions overlap in nested\nstructures. The highest-scoring model, GPT-4o, achieves 81.9% accuracy,\nhighlighting the strong referential capabilities of state-of-the-art LLMs while\nalso indicating room for further improvement.\n","authors":["Kawshik Manikantan","Makarand Tapaswi","Vineet Gandhi","Shubham Toshniwal"],"pdf_url":"https://arxiv.org/pdf/2411.07466v2.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.08672v3","updated":"2025-04-16T18:59:52Z","published":"2024-04-05T05:14:46Z","title":"Taxonomy and Analysis of Sensitive User Queries in Generative AI Search","summary":"  Although there has been a growing interest among industries in integrating\ngenerative LLMs into their services, limited experience and scarcity of\nresources act as a barrier in launching and servicing large-scale LLM-based\nservices. In this paper, we share our experiences in developing and operating\ngenerative AI models within a national-scale search engine, with a specific\nfocus on the sensitiveness of user queries. We propose a taxonomy for sensitive\nsearch queries, outline our approaches, and present a comprehensive analysis\nreport on sensitive queries from actual users. We believe that our experiences\nin launching generative AI search systems can contribute to reducing the\nbarrier in building generative LLM-based services.\n","authors":["Hwiyeol Jo","Taiwoo Park","Hyunwoo Lee","Nayoung Choi","Changbong Kim","Ohjoon Kwon","Donghyeon Jeon","Eui-Hyeon Lee","Kyoungho Shin","Sun Suk Lim","Kyungmi Kim","Jihye Lee","Sun Kim"],"pdf_url":"https://arxiv.org/pdf/2404.08672v3.pdf","comment":"NAACL2025(Findings), corrected typo in co-corresponding authors"},{"id":"http://arxiv.org/abs/2504.12427v1","updated":"2025-04-16T18:56:14Z","published":"2025-04-16T18:56:14Z","title":"Position: The Most Expensive Part of an LLM should be its Training Data","summary":"  Training a state-of-the-art Large Language Model (LLM) is an increasingly\nexpensive endeavor due to growing computational, hardware, energy, and\nengineering demands. Yet, an often-overlooked (and seldom paid) expense is the\nhuman labor behind these models' training data. Every LLM is built on an\nunfathomable amount of human effort: trillions of carefully written words\nsourced from books, academic papers, codebases, social media, and more. This\nposition paper aims to assign a monetary value to this labor and argues that\nthe most expensive part of producing an LLM should be the compensation provided\nto training data producers for their work. To support this position, we study\n64 LLMs released between 2016 and 2024, estimating what it would cost to pay\npeople to produce their training datasets from scratch. Even under highly\nconservative estimates of wage rates, the costs of these models' training\ndatasets are 10-1000 times larger than the costs to train the models\nthemselves, representing a significant financial liability for LLM providers.\nIn the face of the massive gap between the value of training data and the lack\nof compensation for its creation, we highlight and discuss research directions\nthat could enable fairer practices in the future.\n","authors":["Nikhil Kandpal","Colin Raffel"],"pdf_url":"https://arxiv.org/pdf/2504.12427v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.18368v2","updated":"2025-04-16T18:54:05Z","published":"2024-11-27T14:16:51Z","title":"AMPS: ASR with Multimodal Paraphrase Supervision","summary":"  Spontaneous or conversational multilingual speech presents many challenges\nfor state-of-the-art automatic speech recognition (ASR) systems. In this work,\nwe present a new technique AMPS that augments a multilingual multimodal ASR\nsystem with paraphrase-based supervision for improved conversational ASR in\nmultiple languages, including Hindi, Marathi, Malayalam, Kannada, and Nyanja.\nWe use paraphrases of the reference transcriptions as additional supervision\nwhile training the multimodal ASR model and selectively invoke this paraphrase\nobjective for utterances with poor ASR performance. Using AMPS with a\nstate-of-the-art multimodal model SeamlessM4T, we obtain significant relative\nreductions in word error rates (WERs) of up to 5%. We present detailed analyses\nof our system using both objective and human evaluation metrics.\n","authors":["Abhishek Gupta","Amruta Parulekar","Sameep Chattopadhyay","Preethi Jyothi"],"pdf_url":"https://arxiv.org/pdf/2411.18368v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.02894v2","updated":"2025-04-16T18:42:12Z","published":"2025-04-02T18:44:53Z","title":"OnRL-RAG: Real-Time Personalized Mental Health Dialogue System","summary":"  Large language models (LLMs) have been widely used for various tasks and\napplications. However, LLMs and fine-tuning are limited to the pre-trained\ndata. For example, ChatGPT's world knowledge until 2021 can be outdated or\ninaccurate. To enhance the capabilities of LLMs, Retrieval-Augmented Generation\n(RAG), is proposed to augment LLMs with additional, new, latest details and\ninformation to LLMs. While RAG offers the correct information, it may not best\npresent it, especially to different population groups with personalizations.\nReinforcement Learning from Human Feedback (RLHF) adapts to user needs by\naligning model responses with human preference through feedback loops. In\nreal-life applications, such as mental health problems, a dynamic and\nfeedback-based model would continuously adapt to new information and offer\npersonalized assistance due to complex factors fluctuating in a daily\nenvironment. Thus, we propose an Online Reinforcement Learning-based\nRetrieval-Augmented Generation (OnRL-RAG) system to detect and personalize the\nresponding systems to mental health problems, such as stress, anxiety, and\ndepression. We use an open-source dataset collected from 2028 College Students\nwith 28 survey questions for each student to demonstrate the performance of our\nproposed system with the existing systems. Our system achieves superior\nperformance compared to standard RAG and simple LLM via GPT-4o, GPT-4o-mini,\nGemini-1.5, and GPT-3.5. This work would open up the possibilities of real-life\napplications of LLMs for personalized services in the everyday environment. The\nresults will also help researchers in the fields of sociology, psychology, and\nneuroscience to align their theories more closely with the actual human daily\nenvironment.\n","authors":["Ahsan Bilal","Beiyu Lin"],"pdf_url":"https://arxiv.org/pdf/2504.02894v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11700v2","updated":"2025-04-16T18:41:28Z","published":"2024-02-18T20:47:10Z","title":"Why Lift so Heavy? Slimming Large Language Models by Cutting Off the\n  Layers","summary":"  Large Language Models (LLMs) possess outstanding capabilities in addressing\nvarious natural language processing (NLP) tasks. However, the sheer size of\nthese models poses challenges in terms of storage, training and inference due\nto the inclusion of billions of parameters through layer stacking. While\ntraditional approaches such as model pruning or distillation offer ways for\nreducing model size, they often come at the expense of performance retention.\nIn our investigation, we systematically explore the approach of reducing the\nnumber of layers in LLMs. Surprisingly, we observe that even with fewer layers,\nLLMs maintain similar or better performance levels, particularly in\nprompt-based fine-tuning for text classification tasks. Remarkably, in certain\ncases, models with a single layer outperform their fully layered counterparts.\nThese findings offer valuable insights for future work aimed at mitigating the\nsize constraints of LLMs while preserving their performance, thereby opening\navenues for significantly more efficient use of LLMs.\n","authors":["Shuzhou Yuan","Ercong Nie","Bolei Ma","Michael Färber"],"pdf_url":"https://arxiv.org/pdf/2402.11700v2.pdf","comment":"IJCNN 2025"},{"id":"http://arxiv.org/abs/2410.20056v2","updated":"2025-04-16T18:17:24Z","published":"2024-10-26T03:07:22Z","title":"Multi-Field Adaptive Retrieval","summary":"  Document retrieval for tasks such as search and retrieval-augmented\ngeneration typically involves datasets that are unstructured: free-form text\nwithout explicit internal structure in each document. However, documents can\nhave a structured form, consisting of fields such as an article title, message\nbody, or HTML header. To address this gap, we introduce Multi-Field Adaptive\nRetrieval (MFAR), a flexible framework that accommodates any number of and any\ntype of document indices on structured data. Our framework consists of two main\nsteps: (1) the decomposition of an existing document into fields, each indexed\nindependently through dense and lexical methods, and (2) learning a model which\nadaptively predicts the importance of a field by conditioning on the document\nquery, allowing on-the-fly weighting of the most likely field(s). We find that\nour approach allows for the optimized use of dense versus lexical\nrepresentations across field types, significantly improves in document ranking\nover a number of existing retrievers, and achieves state-of-the-art performance\nfor multi-field structured data.\n","authors":["Millicent Li","Tongfei Chen","Benjamin Van Durme","Patrick Xia"],"pdf_url":"https://arxiv.org/pdf/2410.20056v2.pdf","comment":"ICLR 2025, Spotlight"},{"id":"http://arxiv.org/abs/2504.12408v1","updated":"2025-04-16T18:17:19Z","published":"2025-04-16T18:17:19Z","title":"A Human-AI Comparative Analysis of Prompt Sensitivity in LLM-Based\n  Relevance Judgment","summary":"  Large Language Models (LLMs) are increasingly used to automate relevance\njudgments for information retrieval (IR) tasks, often demonstrating agreement\nwith human labels that approaches inter-human agreement. To assess the\nrobustness and reliability of LLM-based relevance judgments, we systematically\ninvestigate impact of prompt sensitivity on the task. We collected prompts for\nrelevance assessment from 15 human experts and 15 LLMs across three tasks~ --\n~binary, graded, and pairwise~ -- ~yielding 90 prompts in total. After\nfiltering out unusable prompts from three humans and three LLMs, we employed\nthe remaining 72 prompts with three different LLMs as judges to label\ndocument/query pairs from two TREC Deep Learning Datasets (2020 and 2021). We\ncompare LLM-generated labels with TREC official human labels using Cohen's\n$\\kappa$ and pairwise agreement measures. In addition to investigating the\nimpact of prompt variations on agreement with human labels, we compare human-\nand LLM-generated prompts and analyze differences among different LLMs as\njudges. We also compare human- and LLM-generated prompts with the standard\nUMBRELA prompt used for relevance assessment by Bing and TREC 2024 Retrieval\nAugmented Generation (RAG) Track. To support future research in LLM-based\nevaluation, we release all data and prompts at\nhttps://github.com/Narabzad/prompt-sensitivity-relevance-judgements/.\n","authors":["Negar Arabzadeh","Charles L. A . Clarke"],"pdf_url":"https://arxiv.org/pdf/2504.12408v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2504.12299v1","updated":"2025-04-16T17:59:54Z","published":"2025-04-16T17:59:54Z","title":"Adapting a World Model for Trajectory Following in a 3D Game","summary":"  Imitation learning is a powerful tool for training agents by leveraging\nexpert knowledge, and being able to replicate a given trajectory is an integral\npart of it. In complex environments, like modern 3D video games, distribution\nshift and stochasticity necessitate robust approaches beyond simple action\nreplay. In this study, we apply Inverse Dynamics Models (IDM) with different\nencoders and policy heads to trajectory following in a modern 3D video game --\nBleeding Edge. Additionally, we investigate several future alignment strategies\nthat address the distribution shift caused by the aleatoric uncertainty and\nimperfections of the agent. We measure both the trajectory deviation distance\nand the first significant deviation point between the reference and the agent's\ntrajectory and show that the optimal configuration depends on the chosen\nsetting. Our results show that in a diverse data setting, a GPT-style policy\nhead with an encoder trained from scratch performs the best, DINOv2 encoder\nwith the GPT-style policy head gives the best results in the low data regime,\nand both GPT-style and MLP-style policy heads had comparable results when\npre-trained on a diverse setting and fine-tuned for a specific behaviour\nsetting.\n","authors":["Marko Tot","Shu Ishida","Abdelhak Lemkhenter","David Bignell","Pallavi Choudhury","Chris Lovett","Luis França","Matheus Ribeiro Furtado de Mendonça","Tarun Gupta","Darren Gehring","Sam Devlin","Sergio Valcarcel Macua","Raluca Georgescu"],"pdf_url":"https://arxiv.org/pdf/2504.12299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12292v1","updated":"2025-04-16T17:55:02Z","published":"2025-04-16T17:55:02Z","title":"SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians","summary":"  Accurate, real-time 3D reconstruction of human heads from monocular images\nand videos underlies numerous visual applications. As 3D ground truth data is\nhard to come by at scale, previous methods have sought to learn from abundant\n2D videos in a self-supervised manner. Typically, this involves the use of\ndifferentiable mesh rendering, which is effective but faces limitations. To\nimprove on this, we propose SHeaP (Self-supervised Head Geometry Predictor\nLearned via 2D Gaussians). Given a source image, we predict a 3DMM mesh and a\nset of Gaussians that are rigged to this mesh. We then reanimate this rigged\nhead avatar to match a target frame, and backpropagate photometric losses to\nboth the 3DMM and Gaussian prediction networks. We find that using Gaussians\nfor rendering substantially improves the effectiveness of this self-supervised\napproach. Training solely on 2D data, our method surpasses existing\nself-supervised approaches in geometric evaluations on the NoW benchmark for\nneutral faces and a new benchmark for non-neutral expressions. Our method also\nproduces highly expressive meshes, outperforming state-of-the-art in emotion\nclassification.\n","authors":["Liam Schoneveld","Zhe Chen","Davide Davoli","Jiapeng Tang","Saimon Terazawa","Ko Nishino","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2504.12292v1.pdf","comment":"For video demonstrations and additional materials please see\n  https://nlml.github.io/sheap/"},{"id":"http://arxiv.org/abs/2504.12284v1","updated":"2025-04-16T17:48:12Z","published":"2025-04-16T17:48:12Z","title":"How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday\n  Interactions","summary":"  We tackle the novel problem of predicting 3D hand motion and contact maps (or\nInteraction Trajectories) given a single RGB view, action text, and a 3D\ncontact point on the object as input. Our approach consists of (1) Interaction\nCodebook: a VQVAE model to learn a latent codebook of hand poses and contact\npoints, effectively tokenizing interaction trajectories, (2) Interaction\nPredictor: a transformer-decoder module to predict the interaction trajectory\nfrom test time inputs by using an indexer module to retrieve a latent\naffordance from the learned codebook. To train our model, we develop a data\nengine that extracts 3D hand poses and contact trajectories from the diverse\nHoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger\nthan existing works, in terms of diversity of objects and interactions\nobserved, and test for generalization of the model across object categories,\naction categories, tasks, and scenes. Experimental results show the\neffectiveness of our approach over transformer & diffusion baselines across all\nsettings.\n","authors":["Aditya Prakash","Benjamin Lundell","Dmitry Andreychuk","David Forsyth","Saurabh Gupta","Harpreet Sawhney"],"pdf_url":"https://arxiv.org/pdf/2504.12284v1.pdf","comment":"CVPR 2025, Project page:\n  https://ap229997.github.io/projects/latentact"},{"id":"http://arxiv.org/abs/2406.19388v4","updated":"2025-04-16T17:40:22Z","published":"2024-06-27T17:58:54Z","title":"Taming Data and Transformers for Audio Generation","summary":"  The scalability of ambient sound generators is hindered by data scarcity,\ninsufficient caption quality, and limited scalability in model architecture.\nThis work addresses these challenges by advancing both data and model scaling.\nFirst, we propose an efficient and scalable dataset collection pipeline\ntailored for ambient audio generation, resulting in AutoReCap-XL, the largest\nambient audio-text dataset with over 47 million clips. To provide high-quality\ntextual annotations, we propose AutoCap, a high-quality automatic audio\ncaptioning model. By adopting a Q-Former module and leveraging audio metadata,\nAutoCap substantially enhances caption quality, reaching a CIDEr score of\n$83.2$, a $3.2\\%$ improvement over previous captioning models. Finally, we\npropose GenAu, a scalable transformer-based audio generation architecture that\nwe scale up to 1.25B parameters. We demonstrate its benefits from data scaling\nwith synthetic captions as well as model size scaling. When compared to\nbaseline audio generators trained at similar size and data scale, GenAu obtains\nsignificant improvements of $4.7\\%$ in FAD score, $11.1\\%$ in IS, and $13.5\\%$\nin CLAP score. Our code, model checkpoints, and dataset are publicly available.\n","authors":["Moayed Haji-Ali","Willi Menapace","Aliaksandr Siarohin","Guha Balakrishnan","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2406.19388v4.pdf","comment":"Project Webpage: https://snap-research.github.io/GenAU/"},{"id":"http://arxiv.org/abs/2504.12276v1","updated":"2025-04-16T17:35:09Z","published":"2025-04-16T17:35:09Z","title":"The Tenth NTIRE 2025 Image Denoising Challenge Report","summary":"  This paper presents an overview of the NTIRE 2025 Image Denoising Challenge\n({\\sigma} = 50), highlighting the proposed methodologies and corresponding\nresults. The primary objective is to develop a network architecture capable of\nachieving high-quality denoising performance, quantitatively evaluated using\nPSNR, without constraints on computational complexity or model size. The task\nassumes independent additive white Gaussian noise (AWGN) with a fixed noise\nlevel of 50. A total of 290 participants registered for the challenge, with 20\nteams successfully submitting valid results, providing insights into the\ncurrent state-of-the-art in image denoising.\n","authors":["Lei Sun","Hang Guo","Bin Ren","Luc Van Gool","Radu Timofte","Yawei Li","Xiangyu Kong","Hyunhee Park","Xiaoxuan Yu","Suejin Han","Hakjae Jeon","Jia Li","Hyung-Ju Chun","Donghun Ryou","Inju Ha","Bohyung Han","Jingyu Ma","Zhijuan Huang","Huiyuan Fu","Hongyuan Yu","Boqi Zhang","Jiawei Shi","Heng Zhang","Huadong Ma","Deepak Kumar Tyagi","Aman Kukretti","Gajender Sharma","Sriharsha Koundinya","Asim Manna","Jun Cheng","Shan Tan","Jun Liu","Jiangwei Hao","Jianping Luo","Jie Lu","Satya Narayan Tazi","Arnim Gautam","Aditi Pawar","Aishwarya Joshi","Akshay Dudhane","Praful Hambadre","Sachin Chaudhary","Santosh Kumar Vipparthi","Subrahmanyam Murala","Jiachen Tu","Nikhil Akalwadi","Vijayalaxmi Ashok Aralikatti","Dheeraj Damodar Hegde","G Gyaneshwar Rao","Jatin Kalal","Chaitra Desai","Ramesh Ashok Tabib","Uma Mudenagudi","Zhenyuan Lin","Yubo Dong","Weikun Li","Anqi Li","Ang Gao","Weijun Yuan","Zhan Li","Ruting Deng","Yihang Chen","Yifan Deng","Zhanglu Chen","Boyang Yao","Shuling Zheng","Feng Zhang","Zhiheng Fu","Anas M. Ali","Bilel Benjdira","Wadii Boulila","Jan Seny","Pei Zhou","Jianhua Hu","K. L. Eddie Law","Jaeho Lee","M. J. Aashik Rasool","Abdur Rehman","SMA Sharif","Seongwan Kim","Alexandru Brateanu","Raul Balmez","Ciprian Orhei","Cosmin Ancuti","Zeyu Xiao","Zhuoyuan Li","Ziqi Wang","Yanyan Wei","Fei Wang","Kun Li","Shengeng Tang","Yunkai Zhang","Weirun Zhou","Haoxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2504.12276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12273v1","updated":"2025-04-16T17:32:50Z","published":"2025-04-16T17:32:50Z","title":"Beyond Reconstruction: A Physics Based Neural Deferred Shader for\n  Photo-realistic Rendering","summary":"  Deep learning based rendering has demonstrated major improvements for\nphoto-realistic image synthesis, applicable to various applications including\nvisual effects in movies and photo-realistic scene building in video games.\nHowever, a significant limitation is the difficulty of decomposing the\nillumination and material parameters, which limits such methods to reconstruct\nan input scene, without any possibility to control these parameters. This paper\nintroduces a novel physics based neural deferred shading pipeline to decompose\nthe data-driven rendering process, learn a generalizable shading function to\nproduce photo-realistic results for shading and relighting tasks, we also\nprovide a shadow estimator to efficiently mimic shadowing effect. Our model\nachieves improved performance compared to classical models and a state-of-art\nneural shading model, and enables generalizable photo-realistic shading from\narbitrary illumination input.\n","authors":["Zhuo He","Paul Henderson","Nicolas Pugeault"],"pdf_url":"https://arxiv.org/pdf/2504.12273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.06220v3","updated":"2025-04-16T17:23:59Z","published":"2025-04-08T17:09:33Z","title":"Earth-Adapter: Bridge the Geospatial Domain Gaps with Mixture of\n  Frequency Adaptation","summary":"  Parameter-Efficient Fine-Tuning (PEFT) is a technique that allows us to adapt\npowerful Foundation Models (FMs) to diverse downstream tasks while preserving\nand unleashing their inherent capabilities. However, we have observed that\nexisting PEFT methods, which are often designed with natural imagery in mind,\nstruggle when applied to Remote Sensing (RS) scenarios. This is primarily due\nto their inability to handle artifact influences, a problem particularly severe\nin RS image features. To tackle this challenge, we introduce Earth-Adapter, the\nfirst PEFT method specifically designed for RS artifacts conquering.\nEarth-Adapter introduces a novel Mixture of Frequency Adaptation process that\ncombines a Mixture of Adapter (MoA) with Discrete Fourier Transformation (DFT).\nBy utilizing DFT, Earth-Adapter can decompose features into different frequency\ncomponents, precisely separating artifacts from original features. The MoA then\ndynamically assigns weights to each adapter expert, allowing for the\ncombination of features across various frequency domains. These\nsimple-yet-effective approaches enable Earth-Adapter to more efficiently\novercome the disturbances caused by artifacts than previous PEFT methods,\nsignificantly enhancing the FMs' performance on RS scenarios. Experiments on\nDomain Adaptation (DA), and Domain Generalization (DG) semantic segmentation\nbenchmarks showcase the Earth-Adapter's effectiveness. Compared with baseline\nRein, Earth-Adapter significantly improves 9.0% mIoU in DA and 3.1% mIoU in DG\nbenchmarks. Our code will be released at\nhttps://github.com/VisionXLab/Earth-Adapter.\n","authors":["Xiaoxing Hu","Ziyang Gong","Yupei Wang","Yuru Jia","Gen Luo","Xue Yang"],"pdf_url":"https://arxiv.org/pdf/2504.06220v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12264v1","updated":"2025-04-16T17:21:55Z","published":"2025-04-16T17:21:55Z","title":"Towards Learning to Complete Anything in Lidar","summary":"  We propose CAL (Complete Anything in Lidar) for Lidar-based shape-completion\nin-the-wild. This is closely related to Lidar-based semantic/panoptic scene\ncompletion. However, contemporary methods can only complete and recognize\nobjects from a closed vocabulary labeled in existing Lidar datasets. Different\nto that, our zero-shot approach leverages the temporal context from multi-modal\nsensor sequences to mine object shapes and semantic features of observed\nobjects. These are then distilled into a Lidar-only instance-level completion\nand recognition model. Although we only mine partial shape completions, we find\nthat our distilled model learns to infer full object shapes from multiple such\npartial observations across the dataset. We show that our model can be prompted\non standard benchmarks for Semantic and Panoptic Scene Completion, localize\nobjects as (amodal) 3D bounding boxes, and recognize objects beyond fixed class\nvocabularies. Our project page is\nhttps://research.nvidia.com/labs/dvl/projects/complete-anything-lidar\n","authors":["Ayca Takmaz","Cristiano Saltori","Neehar Peri","Tim Meinhardt","Riccardo de Lutio","Laura Leal-Taixé","Aljoša Ošep"],"pdf_url":"https://arxiv.org/pdf/2504.12264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12259v1","updated":"2025-04-16T17:09:13Z","published":"2025-04-16T17:09:13Z","title":"VGDFR: Diffusion-based Video Generation with Dynamic Latent Frame Rate","summary":"  Diffusion Transformer(DiT)-based generation models have achieved remarkable\nsuccess in video generation. However, their inherent computational demands pose\nsignificant efficiency challenges. In this paper, we exploit the inherent\ntemporal non-uniformity of real-world videos and observe that videos exhibit\ndynamic information density, with high-motion segments demanding greater detail\npreservation than static scenes. Inspired by this temporal non-uniformity, we\npropose VGDFR, a training-free approach for Diffusion-based Video Generation\nwith Dynamic Latent Frame Rate. VGDFR adaptively adjusts the number of elements\nin latent space based on the motion frequency of the latent space content,\nusing fewer tokens for low-frequency segments while preserving detail in\nhigh-frequency segments. Specifically, our key contributions are: (1) A dynamic\nframe rate scheduler for DiT video generation that adaptively assigns frame\nrates for video segments. (2) A novel latent-space frame merging method to\nalign latent representations with their denoised counterparts before merging\nthose redundant in low-resolution space. (3) A preference analysis of Rotary\nPositional Embeddings (RoPE) across DiT layers, informing a tailored RoPE\nstrategy optimized for semantic and local information capture. Experiments show\nthat VGDFR can achieve a speedup up to 3x for video generation with minimal\nquality degradation.\n","authors":["Zhihang Yuan","Rui Xie","Yuzhang Shang","Hanling Zhang","Siyuan Wang","Shengen Yan","Guohao Dai","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2504.12259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12256v1","updated":"2025-04-16T17:07:16Z","published":"2025-04-16T17:07:16Z","title":"FLIP Reasoning Challenge","summary":"  Over the past years, advances in artificial intelligence (AI) have\ndemonstrated how AI can solve many perception and generation tasks, such as\nimage classification and text writing, yet reasoning remains a challenge. This\npaper introduces the FLIP dataset, a benchmark for evaluating AI reasoning\ncapabilities based on human verification tasks on the Idena blockchain. FLIP\nchallenges present users with two orderings of 4 images, requiring them to\nidentify the logically coherent one. By emphasizing sequential reasoning,\nvisual storytelling, and common sense, FLIP provides a unique testbed for\nmultimodal AI systems. Our experiments evaluate state-of-the-art models,\nleveraging both vision-language models (VLMs) and large language models (LLMs).\nResults reveal that even the best open-sourced and closed-sourced models\nachieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot\nsettings, compared to human performance of 95.3%. Captioning models aid\nreasoning models by providing text descriptions of images, yielding better\nresults than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5\nPro. Combining the predictions from 15 models in an ensemble increases the\naccuracy to 85.2%. These findings highlight the limitations of existing\nreasoning models and the need for robust multimodal benchmarks like FLIP. The\nfull codebase and dataset will be available at\nhttps://github.com/aplesner/FLIP-Reasoning-Challenge.\n","authors":["Andreas Plesner","Turlan Kuzhagaliyev","Roger Wattenhofer"],"pdf_url":"https://arxiv.org/pdf/2504.12256v1.pdf","comment":"Published at First Workshop on Open Science for Foundation Models at\n  ICLR 2025"},{"id":"http://arxiv.org/abs/2504.12255v1","updated":"2025-04-16T17:05:58Z","published":"2025-04-16T17:05:58Z","title":"Human Aligned Compression for Robust Models","summary":"  Adversarial attacks on image models threaten system robustness by introducing\nimperceptible perturbations that cause incorrect predictions. We investigate\nhuman-aligned learned lossy compression as a defense mechanism, comparing two\nlearned models (HiFiC and ELIC) against traditional JPEG across various quality\nlevels. Our experiments on ImageNet subsets demonstrate that learned\ncompression methods outperform JPEG, particularly for Vision Transformer\narchitectures, by preserving semantically meaningful content while removing\nadversarial noise. Even in white-box settings where attackers can access the\ndefense, these methods maintain substantial effectiveness. We also show that\nsequential compression--applying rounds of\ncompression/decompression--significantly enhances defense efficacy while\nmaintaining classification performance. Our findings reveal that human-aligned\ncompression provides an effective, computationally efficient defense that\nprotects the image features most relevant to human and machine understanding.\nIt offers a practical approach to improving model robustness against\nadversarial threats.\n","authors":["Samuel Räber","Andreas Plesner","Till Aczel","Roger Wattenhofer"],"pdf_url":"https://arxiv.org/pdf/2504.12255v1.pdf","comment":"Presented at the Workshop AdvML at CVPR 2025"},{"id":"http://arxiv.org/abs/2504.12249v1","updated":"2025-04-16T16:54:37Z","published":"2025-04-16T16:54:37Z","title":"Comparative Evaluation of Radiomics and Deep Learning Models for Disease\n  Detection in Chest Radiography","summary":"  The application of artificial intelligence (AI) in medical imaging has\nrevolutionized diagnostic practices, enabling advanced analysis and\ninterpretation of radiological data. This study presents a comprehensive\nevaluation of radiomics-based and deep learning-based approaches for disease\ndetection in chest radiography, focusing on COVID-19, lung opacity, and viral\npneumonia. While deep learning models, particularly convolutional neural\nnetworks (CNNs) and vision transformers (ViTs), learn directly from image data,\nradiomics-based models extract and analyze quantitative features, potentially\nproviding advantages in data-limited scenarios. This study systematically\ncompares the diagnostic accuracy and robustness of various AI models, including\nDecision Trees, Gradient Boosting, Random Forests, Support Vector Machines\n(SVM), and Multi-Layer Perceptrons (MLP) for radiomics, against\nstate-of-the-art computer vision deep learning architectures. Performance\nmetrics across varying sample sizes reveal insights into each model's efficacy,\nhighlighting the contexts in which specific AI approaches may offer enhanced\ndiagnostic capabilities. The results aim to inform the integration of AI-driven\ndiagnostic tools in clinical practice, particularly in automated and\nhigh-throughput environments where timely, reliable diagnosis is critical. This\ncomparative study addresses an essential gap, establishing guidance for the\nselection of AI models based on clinical and operational needs.\n","authors":["Zhijin He","Alan B. McMillan"],"pdf_url":"https://arxiv.org/pdf/2504.12249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12245v1","updated":"2025-04-16T16:50:41Z","published":"2025-04-16T16:50:41Z","title":"SIDME: Self-supervised Image Demoiréing via Masked Encoder-Decoder\n  Reconstruction","summary":"  Moir\\'e patterns, resulting from aliasing between object light signals and\ncamera sampling frequencies, often degrade image quality during capture.\nTraditional demoir\\'eing methods have generally treated images as a whole for\nprocessing and training, neglecting the unique signal characteristics of\ndifferent color channels. Moreover, the randomness and variability of moir\\'e\npattern generation pose challenges to the robustness of existing methods when\napplied to real-world data. To address these issues, this paper presents SIDME\n(Self-supervised Image Demoir\\'eing via Masked Encoder-Decoder Reconstruction),\na novel model designed to generate high-quality visual images by effectively\nprocessing moir\\'e patterns. SIDME combines a masked encoder-decoder\narchitecture with self-supervised learning, allowing the model to reconstruct\nimages using the inherent properties of camera sampling frequencies. A key\ninnovation is the random masked image reconstructor, which utilizes an\nencoder-decoder structure to handle the reconstruction task. Furthermore, since\nthe green channel in camera sampling has a higher sampling frequency compared\nto red and blue channels, a specialized self-supervised loss function is\ndesigned to improve the training efficiency and effectiveness. To ensure the\ngeneralization ability of the model, a self-supervised moir\\'e image generation\nmethod has been developed to produce a dataset that closely mimics real-world\nconditions. Extensive experiments demonstrate that SIDME outperforms existing\nmethods in processing real moir\\'e pattern data, showing its superior\ngeneralization performance and robustness.\n","authors":["Xia Wang","Haiyang Sun","Tiantian Cao","Yueying Sun","Min Feng"],"pdf_url":"https://arxiv.org/pdf/2504.12245v1.pdf","comment":"21 pages, 13 figures"},{"id":"http://arxiv.org/abs/2504.12240v1","updated":"2025-04-16T16:45:19Z","published":"2025-04-16T16:45:19Z","title":"Cobra: Efficient Line Art COlorization with BRoAder References","summary":"  The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.\n","authors":["Junhao Zhuang","Lingen Li","Xuan Ju","Zhaoyang Zhang","Chun Yuan","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2504.12240v1.pdf","comment":"Project page with code: https://zhuang2002.github.io/Cobra/"},{"id":"http://arxiv.org/abs/2411.06481v2","updated":"2025-04-16T16:45:12Z","published":"2024-11-10T14:41:38Z","title":"KMM: Key Frame Mask Mamba for Extended Motion Generation","summary":"  Human motion generation is a cut-edge area of research in generative computer\nvision, with promising applications in video creation, game development, and\nrobotic manipulation. The recent Mamba architecture shows promising results in\nefficiently modeling long and complex sequences, yet two significant challenges\nremain: Firstly, directly applying Mamba to extended motion generation is\nineffective, as the limited capacity of the implicit memory leads to memory\ndecay. Secondly, Mamba struggles with multimodal fusion compared to\nTransformers, and lack alignment with textual queries, often confusing\ndirections (left or right) or omitting parts of longer text queries. To address\nthese challenges, our paper presents three key contributions: Firstly, we\nintroduce KMM, a novel architecture featuring Key frame Masking Modeling,\ndesigned to enhance Mamba's focus on key actions in motion segments. This\napproach addresses the memory decay problem and represents a pioneering method\nin customizing strategic frame-level masking in SSMs. Additionally, we designed\na contrastive learning paradigm for addressing the multimodal fusion problem in\nMamba and improving the motion-text alignment. Finally, we conducted extensive\nexperiments on the go-to dataset, BABEL, achieving state-of-the-art performance\nwith a reduction of more than 57% in FID and 70% parameters compared to\nprevious state-of-the-art methods. See project website:\nhttps://steve-zeyu-zhang.github.io/KMM\n","authors":["Zeyu Zhang","Hang Gao","Akide Liu","Qi Chen","Feng Chen","Yiran Wang","Danning Li","Rui Zhao","Zhenming Li","Zhongwen Zhou","Hao Tang","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2411.06481v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11346v2","updated":"2025-04-16T16:23:31Z","published":"2025-04-15T16:19:07Z","title":"Seedream 3.0 Technical Report","summary":"  We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality.\n","authors":["Yu Gao","Lixue Gong","Qiushan Guo","Xiaoxia Hou","Zhichao Lai","Fanshi Li","Liang Li","Xiaochen Lian","Chao Liao","Liyang Liu","Wei Liu","Yichun Shi","Shiqi Sun","Yu Tian","Zhi Tian","Peng Wang","Rui Wang","Xuanda Wang","Xun Wang","Ye Wang","Guofeng Wu","Jie Wu","Xin Xia","Xuefeng Xiao","Zhonghua Zhai","Xinyu Zhang","Qi Zhang","Yuwei Zhang","Shijia Zhao","Jianchao Yang","Weilin Huang"],"pdf_url":"https://arxiv.org/pdf/2504.11346v2.pdf","comment":"Seedream 3.0 Technical Report"},{"id":"http://arxiv.org/abs/2504.12222v1","updated":"2025-04-16T16:14:43Z","published":"2025-04-16T16:14:43Z","title":"Coding-Prior Guided Diffusion Network for Video Deblurring","summary":"  While recent video deblurring methods have advanced significantly, they often\noverlook two valuable prior information: (1) motion vectors (MVs) and coding\nresiduals (CRs) from video codecs, which provide efficient inter-frame\nalignment cues, and (2) the rich real-world knowledge embedded in pre-trained\ndiffusion generative models. We present CPGDNet, a novel two-stage framework\nthat effectively leverages both coding priors and generative diffusion priors\nfor high-quality deblurring. First, our coding-prior feature propagation (CPFP)\nmodule utilizes MVs for efficient frame alignment and CRs to generate attention\nmasks, addressing motion inaccuracies and texture variations. Second, a\ncoding-prior controlled generation (CPC) module network integrates coding\npriors into a pretrained diffusion model, guiding it to enhance critical\nregions and synthesize realistic details. Experiments demonstrate our method\nachieves state-of-the-art perceptual quality with up to 30% improvement in IQA\nmetrics. Both the code and the codingprior-augmented dataset will be\nopen-sourced.\n","authors":["Yike Liu","Jianhui Zhang","Haipeng Li","Shuaicheng Liu","Bing Zeng"],"pdf_url":"https://arxiv.org/pdf/2504.12222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.08763v3","updated":"2025-04-16T16:12:59Z","published":"2022-09-19T05:06:57Z","title":"Decentralized Vehicle Coordination: The Berkeley DeepDrive Drone Dataset\n  and Consensus-Based Models","summary":"  A significant portion of roads, particularly in densely populated developing\ncountries, lacks explicitly defined right-of-way rules. These understructured\nroads pose substantial challenges for autonomous vehicle motion planning, where\nefficient and safe navigation relies on understanding decentralized human\ncoordination for collision avoidance. This coordination, often termed \"social\ndriving etiquette,\" remains underexplored due to limited open-source empirical\ndata and suitable modeling frameworks. In this paper, we present a novel\ndataset and modeling framework designed to study motion planning in these\nunderstructured environments. The dataset includes 20 aerial videos of\nrepresentative scenarios, an image dataset for training vehicle detection\nmodels, and a development kit for vehicle trajectory estimation. We demonstrate\nthat a consensus-based modeling approach can effectively explain the emergence\nof priority orders observed in our dataset, and is therefore a viable framework\nfor decentralized collision avoidance planning.\n","authors":["Fangyu Wu","Dequan Wang","Minjune Hwang","Chenhui Hao","Jiawei Lu","Jiamu Zhang","Christopher Chou","Trevor Darrell","Alexandre Bayen"],"pdf_url":"https://arxiv.org/pdf/2209.08763v3.pdf","comment":"7 pages, 7 figures, 1 table"},{"id":"http://arxiv.org/abs/2504.12215v1","updated":"2025-04-16T16:08:38Z","published":"2025-04-16T16:08:38Z","title":"Uncertainty-Guided Coarse-to-Fine Tumor Segmentation with Anatomy-Aware\n  Post-Processing","summary":"  Reliable tumor segmentation in thoracic computed tomography (CT) remains\nchallenging due to boundary ambiguity, class imbalance, and anatomical\nvariability. We propose an uncertainty-guided, coarse-to-fine segmentation\nframework that combines full-volume tumor localization with refined\nregion-of-interest (ROI) segmentation, enhanced by anatomically aware\npost-processing. The first-stage model generates a coarse prediction, followed\nby anatomically informed filtering based on lung overlap, proximity to lung\nsurfaces, and component size. The resulting ROIs are segmented by a\nsecond-stage model trained with uncertainty-aware loss functions to improve\naccuracy and boundary calibration in ambiguous regions. Experiments on private\nand public datasets demonstrate improvements in Dice and Hausdorff scores, with\nfewer false positives and enhanced spatial interpretability. These results\nhighlight the value of combining uncertainty modeling and anatomical priors in\ncascaded segmentation pipelines for robust and clinically meaningful tumor\ndelineation. On the Orlando dataset, our framework improved Swin UNETR Dice\nfrom 0.4690 to 0.6447. Reduction in spurious components was strongly correlated\nwith segmentation gains, underscoring the value of anatomically informed\npost-processing.\n","authors":["Ilkin Sevgi Isler","David Mohaisen","Curtis Lisle","Damla Turgut","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2504.12215v1.pdf","comment":"6 pages, 2 figures, to appear in IEEE ADSCA 2025"},{"id":"http://arxiv.org/abs/2407.19546v4","updated":"2025-04-16T16:00:52Z","published":"2024-07-28T17:38:21Z","title":"MMCLIP: Cross-modal Attention Masked Modelling for Medical\n  Language-Image Pre-Training","summary":"  Vision-and-language pretraining (VLP) in the medical field utilizes\ncontrastive learning on image-text pairs to achieve effective transfer across\ntasks. Yet, current VLP approaches with the masked modeling strategy face two\nchallenges when applied to the medical domain. First, current models struggle\nto accurately reconstruct key pathological features due to the scarcity of\nmedical data. Second, most methods only adopt either paired image-text or\nimage-only data, failing to exploit the combination of both paired and unpaired\ndata. To this end, this paper proposes the MMCLIP (Masked Medical Contrastive\nLanguage-Image Pre-Training) framework to enhance pathological learning and\nfeature learning via unpaired data. First, we introduce the attention-masked\nimage modeling (AttMIM) and entity-driven masked language modeling module\n(EntMLM), which learns to reconstruct pathological visual and textual tokens\nvia multi-modal feature interaction, thus improving medical-enhanced features.\nThe AttMIM module masks a portion of the image features that are highly\nresponsive to textual features. This allows MMCLIP to improve the\nreconstruction of highly similar image data in medicine efficiency. Second, our\nMMCLIP capitalizes unpaired data to enhance multimodal learning by introducing\ndisease-kind prompts. The experimental results show that MMCLIP achieves SOTA\nfor zero-shot and fine-tuning classification performance on five datasets. Our\ncode will be available at https://github.com/AIGeeksGroup/MMCLIP.\n","authors":["Biao Wu","Yutong Xie","Zeyu Zhang","Minh Hieu Phan","Qi Chen","Ling Chen","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2407.19546v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10974v2","updated":"2025-04-16T15:58:55Z","published":"2025-04-15T08:34:56Z","title":"Self-Supervised Enhancement of Forward-Looking Sonar Images: Bridging\n  Cross-Modal Degradation Gaps through Feature Space Transformation and\n  Multi-Frame Fusion","summary":"  Enhancing forward-looking sonar images is critical for accurate underwater\ntarget detection. Current deep learning methods mainly rely on supervised\ntraining with simulated data, but the difficulty in obtaining high-quality\nreal-world paired data limits their practical use and generalization. Although\nself-supervised approaches from remote sensing partially alleviate data\nshortages, they neglect the cross-modal degradation gap between sonar and\nremote sensing images. Directly transferring pretrained weights often leads to\noverly smooth sonar images, detail loss, and insufficient brightness. To\naddress this, we propose a feature-space transformation that maps sonar images\nfrom the pixel domain to a robust feature domain, effectively bridging the\ndegradation gap. Additionally, our self-supervised multi-frame fusion strategy\nleverages complementary inter-frame information to naturally remove speckle\nnoise and enhance target-region brightness. Experiments on three self-collected\nreal-world forward-looking sonar datasets show that our method significantly\noutperforms existing approaches, effectively suppressing noise, preserving\ndetailed edges, and substantially improving brightness, demonstrating strong\npotential for underwater target detection applications.\n","authors":["Zhisheng Zhang","Peng Zhang","Fengxiang Wang","Liangli Ma","Fuchun Sun"],"pdf_url":"https://arxiv.org/pdf/2504.10974v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12204v1","updated":"2025-04-16T15:53:53Z","published":"2025-04-16T15:53:53Z","title":"Towards Realistic Low-Light Image Enhancement via ISP Driven Data\n  Modeling","summary":"  Deep neural networks (DNNs) have recently become the leading method for\nlow-light image enhancement (LLIE). However, despite significant progress,\ntheir outputs may still exhibit issues such as amplified noise, incorrect white\nbalance, or unnatural enhancements when deployed in real world applications. A\nkey challenge is the lack of diverse, large scale training data that captures\nthe complexities of low-light conditions and imaging pipelines. In this paper,\nwe propose a novel image signal processing (ISP) driven data synthesis pipeline\nthat addresses these challenges by generating unlimited paired training data.\nSpecifically, our pipeline begins with easily collected high-quality\nnormal-light images, which are first unprocessed into the RAW format using a\nreverse ISP. We then synthesize low-light degradations directly in the RAW\ndomain. The resulting data is subsequently processed through a series of ISP\nstages, including white balance adjustment, color space conversion, tone\nmapping, and gamma correction, with controlled variations introduced at each\nstage. This broadens the degradation space and enhances the diversity of the\ntraining data, enabling the generated data to capture a wide range of\ndegradations and the complexities inherent in the ISP pipeline. To demonstrate\nthe effectiveness of our synthetic pipeline, we conduct extensive experiments\nusing a vanilla UNet model consisting solely of convolutional layers, group\nnormalization, GeLU activation, and convolutional block attention modules\n(CBAMs). Extensive testing across multiple datasets reveals that the vanilla\nUNet model trained with our data synthesis pipeline delivers high fidelity,\nvisually appealing enhancement results, surpassing state-of-the-art (SOTA)\nmethods both quantitatively and qualitatively.\n","authors":["Zhihua Wang","Yu Long","Qinghua Lin","Kai Zhang","Yazhu Zhang","Yuming Fang","Li Liu","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2504.12204v1.pdf","comment":"17 pages, 11 tables, 10 figures"},{"id":"http://arxiv.org/abs/2504.12203v1","updated":"2025-04-16T15:53:40Z","published":"2025-04-16T15:53:40Z","title":"Modality-Independent Explainable Detection of Inaccurate Organ\n  Segmentations Using Denoising Autoencoders","summary":"  In radiation therapy planning, inaccurate segmentations of organs at risk can\nresult in suboptimal treatment delivery, if left undetected by the clinician.\nTo address this challenge, we developed a denoising autoencoder-based method to\ndetect inaccurate organ segmentations. We applied noise to ground truth organ\nsegmentations, and the autoencoders were tasked to denoise them. Through the\napplication of our method to organ segmentations generated on both MR and CT\nscans, we demonstrated that the method is independent of imaging modality. By\nproviding reconstructions, our method offers visual information about\ninaccurate regions of the organ segmentations, leading to more explainable\ndetection of suboptimal segmentations. We compared our method to existing\napproaches in the literature and demonstrated that it achieved superior\nperformance for the majority of organs.\n","authors":["Levente Lippenszky","István Megyeri","Krisztian Koos","Zsófia Karancsi","Borbála Deák-Karancsi","András Frontó","Árpád Makk","Attila Rádics","Erhan Bas","László Ruskó"],"pdf_url":"https://arxiv.org/pdf/2504.12203v1.pdf","comment":"Short version of this paper was accepted for poster presentation at\n  IEEE ISBI 2025"},{"id":"http://arxiv.org/abs/2504.12197v1","updated":"2025-04-16T15:48:21Z","published":"2025-04-16T15:48:21Z","title":"Beyond Patches: Mining Interpretable Part-Prototypes for Explainable AI","summary":"  Deep learning has provided considerable advancements for multimedia systems,\nyet the interpretability of deep models remains a challenge. State-of-the-art\npost-hoc explainability methods, such as GradCAM, provide visual interpretation\nbased on heatmaps but lack conceptual clarity. Prototype-based approaches, like\nProtoPNet and PIPNet, offer a more structured explanation but rely on fixed\npatches, limiting their robustness and semantic consistency.\n  To address these limitations, a part-prototypical concept mining network\n(PCMNet) is proposed that dynamically learns interpretable prototypes from\nmeaningful regions. PCMNet clusters prototypes into concept groups, creating\nsemantically grounded explanations without requiring additional annotations.\nThrough a joint process of unsupervised part discovery and concept activation\nvector extraction, PCMNet effectively captures discriminative concepts and\nmakes interpretable classification decisions.\n  Our extensive experiments comparing PCMNet against state-of-the-art methods\non multiple datasets show that it can provide a high level of interpretability,\nstability, and robustness under clean and occluded scenarios.\n","authors":["Mahdi Alehdaghi","Rajarshi Bhattacharya","Pourya Shamsolmoali","Rafael M. O. Cruz","Maguelonne Heritier","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2504.12197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12186v1","updated":"2025-04-16T15:40:15Z","published":"2025-04-16T15:40:15Z","title":"CoMotion: Concurrent Multi-person 3D Motion","summary":"  We introduce an approach for detecting and tracking detailed 3D poses of\nmultiple people from a single monocular camera stream. Our system maintains\ntemporally coherent predictions in crowded scenes filled with difficult poses\nand occlusions. Our model performs both strong per-frame detection and a\nlearned pose update to track people from frame to frame. Rather than match\ndetections across time, poses are updated directly from a new input image,\nwhich enables online tracking through occlusion. We train on numerous image and\nvideo datasets leveraging pseudo-labeled annotations to produce a model that\nmatches state-of-the-art systems in 3D pose estimation accuracy while being\nfaster and more accurate in tracking multiple people through time. Code and\nweights are provided at https://github.com/apple/ml-comotion\n","authors":["Alejandro Newell","Peiyun Hu","Lahav Lipson","Stephan R. Richter","Vladlen Koltun"],"pdf_url":"https://arxiv.org/pdf/2504.12186v1.pdf","comment":"Accepted at ICLR 2025, for code and weights go to\n  https://github.com/apple/ml-comotion"},{"id":"http://arxiv.org/abs/2406.05131v2","updated":"2025-04-16T15:37:56Z","published":"2024-06-07T17:58:36Z","title":"A Semi-Self-Supervised Approach for Dense-Pattern Video Object\n  Segmentation","summary":"  Video object segmentation (VOS) -- predicting pixel-level regions for objects\nwithin each frame of a video -- is particularly challenging in agricultural\nscenarios, where videos of crops include hundreds of small, dense, and occluded\nobjects (stems, leaves, flowers, pods) that sway and move unpredictably in the\nwind. Supervised training is the state-of-the-art for VOS, but it requires\nlarge, pixel-accurate, human-annotated videos, which are costly to produce for\nvideos with many densely packed objects in each frame. To address these\nchallenges, we proposed a semi-self-supervised spatiotemporal approach for\ndense-VOS (DVOS) using a diffusion-based method through multi-task\n(reconstruction and segmentation) learning. We train the model first with\nsynthetic data that mimics the camera and object motion of real videos and then\nwith pseudo-labeled videos. We evaluate our DVOS method for wheat head\nsegmentation from a diverse set of videos (handheld, drone-captured, different\nfield locations, and different growth stages -- spanning from Boot-stage to\nWheat-mature and Harvest-ready). Despite using only a few manually annotated\nvideo frames, the proposed approach yielded a high-performing model, achieving\na Dice score of 0.79 when tested on a drone-captured external test set. While\nour method was evaluated on wheat head segmentation, it can be extended to\nother crops and domains, such as crowd analysis or microscopic image analysis.\n","authors":["Keyhan Najafian","Farhad Maleki","Lingling Jin","Ian Stavness"],"pdf_url":"https://arxiv.org/pdf/2406.05131v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03196v2","updated":"2025-04-16T15:34:51Z","published":"2025-03-05T05:30:22Z","title":"SpiritSight Agent: Advanced GUI Agent with One Look","summary":"  Graphical User Interface (GUI) agents show amazing abilities in assisting\nhuman-computer interaction, automating human user's navigation on digital\ndevices. An ideal GUI agent is expected to achieve high accuracy, low latency,\nand compatibility for different GUI platforms. Recent vision-based approaches\nhave shown promise by leveraging advanced Vision Language Models (VLMs). While\nthey generally meet the requirements of compatibility and low latency, these\nvision-based GUI agents tend to have low accuracy due to their limitations in\nelement grounding. To address this issue, we propose $\\textbf{SpiritSight}$, a\nvision-based, end-to-end GUI agent that excels in GUI navigation tasks across\nvarious GUI platforms. First, we create a multi-level, large-scale,\nhigh-quality GUI dataset called $\\textbf{GUI-Lasagne}$ using scalable methods,\nempowering SpiritSight with robust GUI understanding and grounding\ncapabilities. Second, we introduce the $\\textbf{Universal Block Parsing (UBP)}$\nmethod to resolve the ambiguity problem in dynamic high-resolution of visual\ninputs, further enhancing SpiritSight's ability to ground GUI objects. Through\nthese efforts, SpiritSight agent outperforms other advanced methods on diverse\nGUI benchmarks, demonstrating its superior capability and compatibility in GUI\nnavigation tasks. Models and datasets are available at\nhttps://hzhiyuan.github.io/SpiritSight-Agent.\n","authors":["Zhiyuan Huang","Ziming Cheng","Junting Pan","Zhaohui Hou","Mingjie Zhan"],"pdf_url":"https://arxiv.org/pdf/2503.03196v2.pdf","comment":"Paper accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2407.00143v2","updated":"2025-04-16T15:26:14Z","published":"2024-06-28T16:08:26Z","title":"InfoNCE: Identifying the Gap Between Theory and Practice","summary":"  Prior theory work on Contrastive Learning via the InfoNCE loss showed that,\nunder certain assumptions, the learned representations recover the ground-truth\nlatent factors. We argue that these theories overlook crucial aspects of how CL\nis deployed in practice. Specifically, they either assume equal variance across\nall latents or that certain latents are kept invariant. However, in practice,\npositive pairs are often generated using augmentations such as strong cropping\nto just a few pixels. Hence, a more realistic assumption is that all latent\nfactors change with a continuum of variability across all factors. We introduce\nAnInfoNCE, a generalization of InfoNCE that can provably uncover the latent\nfactors in this anisotropic setting, broadly generalizing previous\nidentifiability results in CL. We validate our identifiability results in\ncontrolled experiments and show that AnInfoNCE increases the recovery of\npreviously collapsed information in CIFAR10 and ImageNet, albeit at the cost of\ndownstream accuracy. Finally, we discuss the remaining mismatches between\ntheoretical assumptions and practical implementations.\n","authors":["Evgenia Rusak","Patrik Reizinger","Attila Juhos","Oliver Bringmann","Roland S. Zimmermann","Wieland Brendel"],"pdf_url":"https://arxiv.org/pdf/2407.00143v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12169v1","updated":"2025-04-16T15:19:11Z","published":"2025-04-16T15:19:11Z","title":"Towards a General-Purpose Zero-Shot Synthetic Low-Light Image and Video\n  Pipeline","summary":"  Low-light conditions pose significant challenges for both human and machine\nannotation. This in turn has led to a lack of research into machine\nunderstanding for low-light images and (in particular) videos. A common\napproach is to apply annotations obtained from high quality datasets to\nsynthetically created low light versions. In addition, these approaches are\noften limited through the use of unrealistic noise models. In this paper, we\npropose a new Degradation Estimation Network (DEN), which synthetically\ngenerates realistic standard RGB (sRGB) noise without the requirement for\ncamera metadata. This is achieved by estimating the parameters of\nphysics-informed noise distributions, trained in a self-supervised manner. This\nzero-shot approach allows our method to generate synthetic noisy content with a\ndiverse range of realistic noise characteristics, unlike other methods which\nfocus on recreating the noise characteristics of the training data. We evaluate\nour proposed synthetic pipeline using various methods trained on its synthetic\ndata for typical low-light tasks including synthetic noise replication, video\nenhancement, and object detection, showing improvements of up to 24\\% KLD, 21\\%\nLPIPS, and 62\\% AP$_{50-95}$, respectively.\n","authors":["Joanne Lin","Crispian Morris","Ruirui Lin","Fan Zhang","David Bull","Nantheera Anantrasirichai"],"pdf_url":"https://arxiv.org/pdf/2504.12169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12167v1","updated":"2025-04-16T15:18:56Z","published":"2025-04-16T15:18:56Z","title":"RADLER: Radar Object Detection Leveraging Semantic 3D City Models and\n  Self-Supervised Radar-Image Learning","summary":"  Semantic 3D city models are worldwide easy-accessible, providing accurate,\nobject-oriented, and semantic-rich 3D priors. To date, their potential to\nmitigate the noise impact on radar object detection remains under-explored. In\nthis paper, we first introduce a unique dataset, RadarCity, comprising 54K\nsynchronized radar-image pairs and semantic 3D city models. Moreover, we\npropose a novel neural network, RADLER, leveraging the effectiveness of\ncontrastive self-supervised learning (SSL) and semantic 3D city models to\nenhance radar object detection of pedestrians, cyclists, and cars.\nSpecifically, we first obtain the robust radar features via a SSL network in\nthe radar-image pretext task. We then use a simple yet effective feature fusion\nstrategy to incorporate semantic-depth features from semantic 3D city models.\nHaving prior 3D information as guidance, RADLER obtains more fine-grained\ndetails to enhance radar object detection. We extensively evaluate RADLER on\nthe collected RadarCity dataset and demonstrate average improvements of 5.46%\nin mean avarage precision (mAP) and 3.51% in mean avarage recall (mAR) over\nprevious radar object detection methods. We believe this work will foster\nfurther research on semantic-guided and map-supported radar object detection.\nOur project page is publicly available\nathttps://gpp-communication.github.io/RADLER .\n","authors":["Yuan Luo","Rudolf Hoffmann","Yan Xia","Olaf Wysocki","Benedikt Schwab","Thomas H. Kolbe","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2504.12167v1.pdf","comment":"The paper accepted for CVPRW '25 (PBVS 2025 - the Perception Beyond\n  the Visible Spectrum)"},{"id":"http://arxiv.org/abs/2504.12165v1","updated":"2025-04-16T15:18:11Z","published":"2025-04-16T15:18:11Z","title":"CodingHomo: Bootstrapping Deep Homography With Video Coding","summary":"  Homography estimation is a fundamental task in computer vision with\napplications in diverse fields. Recent advances in deep learning have improved\nhomography estimation, particularly with unsupervised learning approaches,\noffering increased robustness and generalizability. However, accurately\npredicting homography, especially in complex motions, remains a challenge. In\nresponse, this work introduces a novel method leveraging video coding,\nparticularly by harnessing inherent motion vectors (MVs) present in videos. We\npresent CodingHomo, an unsupervised framework for homography estimation. Our\nframework features a Mask-Guided Fusion (MGF) module that identifies and\nutilizes beneficial features among the MVs, thereby enhancing the accuracy of\nhomography prediction. Additionally, the Mask-Guided Homography Estimation\n(MGHE) module is presented for eliminating undesired features in the\ncoarse-to-fine homography refinement process. CodingHomo outperforms existing\nstate-of-the-art unsupervised methods, delivering good robustness and\ngeneralizability. The code and dataset are available at:\n\\href{github}{https://github.com/liuyike422/CodingHomo\n","authors":["Yike Liu","Haipeng Li","Shuaicheng Liu","Bing Zeng"],"pdf_url":"https://arxiv.org/pdf/2504.12165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.06352v3","updated":"2025-04-16T15:17:51Z","published":"2023-02-13T13:35:09Z","title":"Deep Anatomical Federated Network (Dafne): An open client-server\n  framework for the continuous, collaborative improvement of deep\n  learning-based medical image segmentation","summary":"  Purpose: To present and evaluate Dafne (deep anatomical federated network), a\nfreely available decentralized, collaborative deep learning system for the\nsemantic segmentation of radiological images through federated incremental\nlearning. Materials and Methods: Dafne is free software with a client-server\narchitecture. The client side is an advanced user interface that applies the\ndeep learning models stored on the server to the user's data and allows the\nuser to check and refine the prediction. Incremental learning is then performed\nat the client's side and sent back to the server, where it is integrated into\nthe root model. Dafne was evaluated locally, by assessing the performance gain\nacross model generations on 38 MRI datasets of the lower legs, and through the\nanalysis of real-world usage statistics (n = 639 use-cases). Results: Dafne\ndemonstrated a statistically improvement in the accuracy of semantic\nsegmentation over time (average increase of the Dice Similarity Coefficient by\n0.007 points/generation on the local validation set, p < 0.001). Qualitatively,\nthe models showed enhanced performance on various radiologic image types,\nincluding those not present in the initial training sets, indicating good model\ngeneralizability. Conclusion: Dafne showed improvement in segmentation quality\nover time, demonstrating potential for learning and generalization.\n","authors":["Francesco Santini","Jakob Wasserthal","Abramo Agosti","Xeni Deligianni","Kevin R. Keene","Hermien E. Kan","Stefan Sommer","Fengdan Wang","Claudia Weidensteiner","Giulia Manco","Matteo Paoletti","Valentina Mazzoli","Arjun Desai","Anna Pichiecchio"],"pdf_url":"https://arxiv.org/pdf/2302.06352v3.pdf","comment":"In this new version: accepted version in Radiology: Artificial\n  Intelligence. Note regarding the license/copyright: This submission is\n  conforming with the RSNA Preprint policy available here:\n  https://pubs.rsna.org/page/ai/author-instructions, which REQUIRES authors to\n  update the version on preprint servers with the accepted version and the\n  copyright notice as indicated in the PDF"},{"id":"http://arxiv.org/abs/2401.05308v2","updated":"2025-04-16T15:14:34Z","published":"2024-01-10T18:22:00Z","title":"Strategic Client Selection to Address Non-IIDness in HAPS-enabled FL\n  Networks","summary":"  The deployment of federated learning (FL) in non-terrestrial networks (NTN)\nthat are supported by high-altitude platform stations (HAPS) offers numerous\nadvantages. Due to its large footprint, it facilitates interaction with a large\nnumber of line-of-sight (LoS) ground clients, each possessing diverse datasets\nalong with distinct communication and computational capabilities. The presence\nof many clients enhances the accuracy of the FL model and speeds up\nconvergence. However, the variety of datasets among these clients poses a\nsignificant challenge, as it leads to pervasive non-independent and identically\ndistributed (non-IID) data. The data non-IIDness results in markedly reduced\ntraining accuracy and slower convergence rates. To address this issue, we\npropose a novel weighted attribute-based client selection strategy that\nleverages multiple user-specific attributes, including historical traffic\npatterns, instantaneous channel conditions, computational capabilities, and\nprevious-round learning performance. By combining these attributes into a\ncomposite score for each user at every FL round and selecting users with higher\nscores as FL clients, the framework ensures more uniform and representative\ndata distributions, effectively mitigating the adverse effects of non-IID data.\nSimulation results corroborate the effectiveness of the proposed client\nselection strategy in enhancing FL model accuracy and convergence rate, as well\nas reducing training loss, by effectively addressing the critical challenge of\ndata non-IIDness in large-scale FL system implementations.\n","authors":["Amin Farajzadeh","Animesh Yadav","Halim Yanikomeroglu"],"pdf_url":"https://arxiv.org/pdf/2401.05308v2.pdf","comment":"Submitted to IEEE for possible publication"},{"id":"http://arxiv.org/abs/2405.01533v2","updated":"2025-04-16T15:12:21Z","published":"2024-05-02T17:59:24Z","title":"OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving\n  with Counterfactual Reasoning","summary":"  The advances in vision-language models (VLMs) have led to a growing interest\nin autonomous driving to leverage their strong reasoning capabilities. However,\nextending these capabilities from 2D to full 3D understanding is crucial for\nreal-world applications. To address this challenge, we propose OmniDrive, a\nholistic vision-language dataset that aligns agent models with 3D driving tasks\nthrough counterfactual reasoning. This approach enhances decision-making by\nevaluating potential scenarios and their outcomes, similar to human drivers\nconsidering alternative actions. Our counterfactual-based synthetic data\nannotation process generates large-scale, high-quality datasets, providing\ndenser supervision signals that bridge planning trajectories and language-based\nreasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely\nOmni-L and Omni-Q, to assess the importance of vision-language alignment versus\n3D perception, revealing critical insights into designing effective LLM-agents.\nSignificant improvements on the DriveLM Q\\&A benchmark and nuScenes open-loop\nplanning demonstrate the effectiveness of our dataset and methods.\n","authors":["Shihao Wang","Zhiding Yu","Xiaohui Jiang","Shiyi Lan","Min Shi","Nadine Chang","Jan Kautz","Ying Li","Jose M. Alvarez"],"pdf_url":"https://arxiv.org/pdf/2405.01533v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12029v3","updated":"2025-04-16T15:12:05Z","published":"2023-09-21T12:51:11Z","title":"Exploring Self-supervised Skeleton-based Action Recognition in Occluded\n  Environments","summary":"  To integrate action recognition into autonomous robotic systems, it is\nessential to address challenges such as person occlusions-a common yet often\noverlooked scenario in existing self-supervised skeleton-based action\nrecognition methods. In this work, we propose IosPSTL, a simple and effective\nself-supervised learning framework designed to handle occlusions. IosPSTL\ncombines a cluster-agnostic KNN imputer with an Occluded Partial\nSpatio-Temporal Learning (OPSTL) strategy. First, we pre-train the model on\noccluded skeleton sequences. Then, we introduce a cluster-agnostic KNN imputer\nthat performs semantic grouping using k-means clustering on sequence\nembeddings. It imputes missing skeleton data by applying K-Nearest Neighbors in\nthe latent space, leveraging nearby sample representations to restore occluded\njoints. This imputation generates more complete skeleton sequences, which\nsignificantly benefits downstream self-supervised models. To further enhance\nlearning, the OPSTL module incorporates Adaptive Spatial Masking (ASM) to make\nbetter use of intact, high-quality skeleton sequences during training. Our\nmethod achieves state-of-the-art performance on the occluded versions of the\nNTU-60 and NTU-120 datasets, demonstrating its robustness and effectiveness\nunder challenging conditions. Code is available at\nhttps://github.com/cyfml/OPSTL.\n","authors":["Yifei Chen","Kunyu Peng","Alina Roitberg","David Schneider","Jiaming Zhang","Junwei Zheng","Yufan Chen","Ruiping Liu","Kailun Yang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2309.12029v3.pdf","comment":"Accepted to IJCNN 2025. Code is available at\n  https://github.com/cyfml/OPSTL"},{"id":"http://arxiv.org/abs/2412.20110v3","updated":"2025-04-16T15:07:11Z","published":"2024-12-28T10:40:21Z","title":"Cross-Modal Mapping: Mitigating the Modality Gap for Few-Shot Image\n  Classification","summary":"  Few-shot image classification remains a critical challenge in the field of\ncomputer vision, particularly in data-scarce environments. Existing methods\ntypically rely on pre-trained visual-language models, such as CLIP. However,\ndue to the modality gap, which is the inconsistent distribution of image and\ntext features in the joint embedding space, directly using these features as\nclass prototypes often leads to suboptimal performance. To address this issue,\nwe propose a novel Cross-Modal Mapping (CMM) method. This method globally\naligns image features with the text feature space through linear transformation\nand optimizes their local spatial relationships using triplet loss, thereby\nsignificantly enhancing cross-modal consistency. Experimental results show that\ncompared to other methods, CMM simplifies the training process and demonstrates\nhigher efficiency. Furthermore, CMM improves the average Top-1 accuracy by\n1.06% on 11 benchmark datasets compared to methods that partially fine-tune the\nbackbone, and it performs excellently on 4 distribution shift datasets.\nNotably, CMM effectively mitigates the modality gap in pre-trained models,\nenabling text features to serve as effective class prototypes for image\nfeatures, thus providing an efficient and highly generalizable solution for\nfew-shot learning.\n","authors":["Xi Yang","Pai Peng","Wulin Xie","Xiaohuan Lu","Jie Wen"],"pdf_url":"https://arxiv.org/pdf/2412.20110v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17510v3","updated":"2025-04-16T15:04:28Z","published":"2023-11-29T10:35:00Z","title":"StructRe: Rewriting for Structured Shape Modeling","summary":"  Man-made 3D shapes are naturally organized in parts and hierarchies; such\nstructures provide important constraints for shape reconstruction and\ngeneration. Modeling shape structures is difficult, because there can be\nmultiple hierarchies for a given shape, causing ambiguity, and across different\ncategories the shape structures are correlated with semantics, limiting\ngeneralization. We present StructRe, a structure rewriting system, as a novel\napproach to structured shape modeling. Given a 3D object represented by points\nand components, StructRe can rewrite it upward into more concise structures, or\ndownward into more detailed structures; by iterating the rewriting process,\nhierarchies are obtained. Such a localized rewriting process enables\nprobabilistic modeling of ambiguous structures and robust generalization across\nobject categories. We train StructRe on PartNet data and show its\ngeneralization to cross-category and multiple object hierarchies, and test its\nextension to ShapeNet. We also demonstrate the benefits of probabilistic and\ngeneralizable structure modeling for shape reconstruction, generation and\nediting tasks.\n","authors":["Jiepeng Wang","Hao Pan","Yang Liu","Xin Tong","Taku Komura","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2311.17510v3.pdf","comment":"Our project page: https://jiepengwang.github.io/StructRe/"},{"id":"http://arxiv.org/abs/2504.12157v1","updated":"2025-04-16T15:04:14Z","published":"2025-04-16T15:04:14Z","title":"FocusedAD: Character-centric Movie Audio Description","summary":"  Movie Audio Description (AD) aims to narrate visual content during\ndialogue-free segments, particularly benefiting blind and visually impaired\n(BVI) audiences. Compared with general video captioning, AD demands\nplot-relevant narration with explicit character name references, posing unique\nchallenges in movie understanding.To identify active main characters and focus\non storyline-relevant regions, we propose FocusedAD, a novel framework that\ndelivers character-centric movie audio descriptions. It includes: (i) a\nCharacter Perception Module(CPM) for tracking character regions and linking\nthem to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues\nfrom prior ADs and subtitles via learnable soft prompts; and (iii) a Focused\nCaption Module(FCM) that generates narrations enriched with plot-relevant\ndetails and named characters. To overcome limitations in character\nidentification, we also introduce an automated pipeline for building character\nquery banks. FocusedAD achieves state-of-the-art performance on multiple\nbenchmarks, including strong zero-shot results on MAD-eval-Named and our newly\nproposed Cinepile-AD dataset. Code and data will be released at\nhttps://github.com/Thorin215/FocusedAD .\n","authors":["Xiaojun Ye","Chun Wang","Yiren Song","Sheng Zhou","Liangcheng Li","Jiajun Bu"],"pdf_url":"https://arxiv.org/pdf/2504.12157v1.pdf","comment":"Code and Demo link: https://github.com/Thorin215/FocusedAD"},{"id":"http://arxiv.org/abs/2410.21169v4","updated":"2025-04-16T15:01:20Z","published":"2024-10-28T16:11:35Z","title":"Document Parsing Unveiled: Techniques, Challenges, and Prospects for\n  Structured Information Extraction","summary":"  Document parsing is essential for converting unstructured and semi-structured\ndocuments such as contracts, academic papers, and invoices into structured,\nmachine-readable data. Document parsing reliable structured data from\nunstructured inputs, providing huge convenience for numerous applications.\nEspecially with recent achievements in Large Language Models, document parsing\nplays an indispensable role in both knowledge base construction and training\ndata generation. This survey presents a comprehensive review of the current\nstate of document parsing, covering key methodologies, from modular pipeline\nsystems to end-to-end models driven by large vision-language models. Core\ncomponents such as layout detection, content extraction (including text,\ntables, and mathematical expressions), and multi-modal data integration are\nexamined in detail. Additionally, this paper discusses the challenges faced by\nmodular document parsing systems and vision-language models in handling complex\nlayouts, integrating multiple modules, and recognizing high-density text. It\noutlines future research directions and emphasizes the importance of developing\nlarger and more diverse datasets.\n","authors":["Qintong Zhang","Bin Wang","Victor Shea-Jay Huang","Junyuan Zhang","Zhengren Wang","Hao Liang","Conghui He","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.21169v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04348v2","updated":"2025-04-16T15:00:11Z","published":"2025-04-06T03:54:21Z","title":"OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving\n  with Counterfactual Reasoning","summary":"  The advances in vision-language models (VLMs) have led to a growing interest\nin autonomous driving to leverage their strong reasoning capabilities. However,\nextending these capabilities from 2D to full 3D understanding is crucial for\nreal-world applications. To address this challenge, we propose OmniDrive, a\nholistic vision-language dataset that aligns agent models with 3D driving tasks\nthrough counterfactual reasoning. This approach enhances decision-making by\nevaluating potential scenarios and their outcomes, similar to human drivers\nconsidering alternative actions. Our counterfactual-based synthetic data\nannotation process generates large-scale, high-quality datasets, providing\ndenser supervision signals that bridge planning trajectories and language-based\nreasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely\nOmni-L and Omni-Q, to assess the importance of vision-language alignment versus\n3D perception, revealing critical insights into designing effective LLM-agents.\nSignificant improvements on the DriveLM Q\\&A benchmark and nuScenes open-loop\nplanning demonstrate the effectiveness of our dataset and methods.\n","authors":["Shihao Wang","Zhiding Yu","Xiaohui Jiang","Shiyi Lan","Min Shi","Nadine Chang","Jan Kautz","Ying Li","Jose M. Alvarez"],"pdf_url":"https://arxiv.org/pdf/2504.04348v2.pdf","comment":"Mistaken resubmission. The original version is at arXiv:2405.01533"},{"id":"http://arxiv.org/abs/2504.12137v1","updated":"2025-04-16T14:50:25Z","published":"2025-04-16T14:50:25Z","title":"Efficient Contrastive Decoding with Probabilistic Hallucination\n  Detection - Mitigating Hallucinations in Large Vision Language Models -","summary":"  Despite recent advances in Large Vision Language Models (LVLMs), these models\nstill suffer from generating hallucinatory responses that do not align with the\nvisual input provided. To mitigate such hallucinations, we introduce Efficient\nContrastive Decoding (ECD), a simple method that leverages probabilistic\nhallucination detection to shift the output distribution towards contextually\naccurate answers at inference time. By contrasting token probabilities and\nhallucination scores, ECD subtracts hallucinated concepts from the original\ndistribution, effectively suppressing hallucinations. Notably, our proposed\nmethod can be applied to any open-source LVLM and does not require additional\nLVLM training. We evaluate our method on several benchmark datasets and across\ndifferent LVLMs. Our experiments show that ECD effectively mitigates\nhallucinations, outperforming state-of-the-art methods with respect to\nperformance on LVLM benchmarks and computation time.\n","authors":["Laura Fieback","Nishilkumar Balar","Jakob Spiegelberg","Hanno Gottschalk"],"pdf_url":"https://arxiv.org/pdf/2504.12137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12132v1","updated":"2025-04-16T14:45:26Z","published":"2025-04-16T14:45:26Z","title":"Weakly Semi-supervised Whole Slide Image Classification by Two-level\n  Cross Consistency Supervision","summary":"  Computer-aided Whole Slide Image (WSI) classification has the potential to\nenhance the accuracy and efficiency of clinical pathological diagnosis. It is\ncommonly formulated as a Multiple Instance Learning (MIL) problem, where each\nWSI is treated as a bag and the small patches extracted from the WSI are\nconsidered instances within that bag. However, obtaining labels for a large\nnumber of bags is a costly and time-consuming process, particularly when\nutilizing existing WSIs for new classification tasks. This limitation renders\nmost existing WSI classification methods ineffective. To address this issue, we\npropose a novel WSI classification problem setting, more aligned with clinical\npractice, termed Weakly Semi-supervised Whole slide image Classification\n(WSWC). In WSWC, a small number of bags are labeled, while a significant number\nof bags remain unlabeled. The MIL nature of the WSWC problem, coupled with the\nabsence of patch labels, distinguishes it from typical semi-supervised image\nclassification problems, making existing algorithms for natural images\nunsuitable for directly solving the WSWC problem. In this paper, we present a\nconcise and efficient framework, named CroCo, to tackle the WSWC problem\nthrough two-level Cross Consistency supervision. CroCo comprises two\nheterogeneous classifier branches capable of performing both instance\nclassification and bag classification. The fundamental idea is to establish\ncross-consistency supervision at both the bag-level and instance-level between\nthe two branches during training. Extensive experiments conducted on four\ndatasets demonstrate that CroCo achieves superior bag classification and\ninstance classification performance compared to other comparative methods when\nlimited WSIs with bag labels are available. To the best of our knowledge, this\npaper presents for the first time the WSWC problem and gives a successful\nresolution.\n","authors":["Linhao Qu","Shiman Li","Xiaoyuan Luo","Shaolei Liu","Qinhao Guo","Manning Wang","Zhijian Song"],"pdf_url":"https://arxiv.org/pdf/2504.12132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12129v1","updated":"2025-04-16T14:44:00Z","published":"2025-04-16T14:44:00Z","title":"Anti-Aesthetics: Protecting Facial Privacy against Customized\n  Text-to-Image Synthesis","summary":"  The rise of customized diffusion models has spurred a boom in personalized\nvisual content creation, but also poses risks of malicious misuse, severely\nthreatening personal privacy and copyright protection. Some studies show that\nthe aesthetic properties of images are highly positively correlated with human\nperception of image quality. Inspired by this, we approach the problem from a\nnovel and intriguing aesthetic perspective to degrade the generation quality of\nmaliciously customized models, thereby achieving better protection of facial\nidentity. Specifically, we propose a Hierarchical Anti-Aesthetic (HAA)\nframework to fully explore aesthetic cues, which consists of two key branches:\n1) Global Anti-Aesthetics: By establishing a global anti-aesthetic reward\nmechanism and a global anti-aesthetic loss, it can degrade the overall\naesthetics of the generated content; 2) Local Anti-Aesthetics: A local\nanti-aesthetic reward mechanism and a local anti-aesthetic loss are designed to\nguide adversarial perturbations to disrupt local facial identity. By seamlessly\nintegrating both branches, our HAA effectively achieves the goal of\nanti-aesthetics from a global to a local level during customized generation.\nExtensive experiments show that HAA outperforms existing SOTA methods largely\nin identity removal, providing a powerful tool for protecting facial privacy\nand copyright.\n","authors":["Songping Wang","Yueming Lyu","Shiqi Liu","Ning Li","Tong Tong","Hao Sun","Caifeng Shan"],"pdf_url":"https://arxiv.org/pdf/2504.12129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04196v2","updated":"2025-04-16T14:37:31Z","published":"2024-09-06T11:34:24Z","title":"GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers","summary":"  Reconstructing posed 3D human models from monocular images has important\napplications in the sports industry, including performance tracking, injury\nprevention and virtual training. In this work, we combine 3D human pose and\nshape estimation with 3D Gaussian Splatting (3DGS), a representation of the\nscene composed of a mixture of Gaussians. This allows training or fine-tuning a\nhuman model predictor on multi-view images alone, without 3D ground truth.\nPredicting such mixtures for a human from a single input image is challenging\ndue to self-occlusions and dependence on articulations, while also needing to\nretain enough flexibility to accommodate a variety of clothes and poses. Our\nkey observation is that the vertices of standardized human meshes (such as\nSMPL) can provide an adequate spatial density and approximate initial position\nfor the Gaussians. We can then train a transformer model to jointly predict\ncomparatively small adjustments to these positions, as well as the other 3DGS\nattributes and the SMPL parameters. We show empirically that this combination\n(using only multi-view supervision) can achieve near real-time inference of 3D\nhuman models from a single image without expensive diffusion models or 3D\npoints supervision, thus making it ideal for the sport industry at any level.\nMore importantly, rendering is an effective auxiliary objective to refine 3D\npose estimation by accounting for clothes and other geometric variations. The\ncode is available at https://github.com/prosperolo/GST.\n","authors":["Lorenza Prospero","Abdullah Hamdi","Joao F. Henriques","Christian Rupprecht"],"pdf_url":"https://arxiv.org/pdf/2409.04196v2.pdf","comment":"Camera ready for CVSports workshop at CVPR 2025"},{"id":"http://arxiv.org/abs/2504.12121v1","updated":"2025-04-16T14:33:57Z","published":"2025-04-16T14:33:57Z","title":"Remote sensing colour image semantic segmentation of trails created by\n  large herbivorous Mammals","summary":"  Detection of spatial areas where biodiversity is at risk is of paramount\nimportance for the conservation and monitoring of ecosystems. Large terrestrial\nmammalian herbivores are keystone species as their activity not only has deep\neffects on soils, plants, and animals, but also shapes landscapes, as large\nherbivores act as allogenic ecosystem engineers. One key landscape feature that\nindicates intense herbivore activity and potentially impacts biodiversity is\nthe formation of grazing trails. Grazing trails are formed by the continuous\ntrampling activity of large herbivores that can produce complex networks of\ntracks of bare soil. Here, we evaluated different algorithms based on machine\nlearning techniques to identify grazing trails. Our goal is to automatically\ndetect potential areas with intense herbivory activity, which might be\nbeneficial for conservation and management plans.\n  We have applied five semantic segmentation methods combined with fourteen\nencoders aimed at mapping grazing trails on aerial images. Our results indicate\nthat in most cases the chosen methodology successfully mapped the trails,\nalthough there were a few instances where the actual trail structure was\nunderestimated. The UNet architecture with the MambaOut encoder was the best\narchitecture for mapping trails. The proposed approach could be applied to\ndevelop tools for mapping and monitoring temporal changes in these landscape\nstructures to support habitat conservation and land management programs. This\nis the first time, to the best of our knowledge, that competitive image\nsegmentation results are obtained for the detection and delineation of trails\nof large herbivorous mammals.\n","authors":["Jose Francisco Diez-Pastor","Francisco Javier Gonzalez-Moya","Pedro Latorre-Carmona","Francisco Javier Perez-Barbería","Ludmila I. Kuncheva","Antonio Canepa-Oneto","Alvar Arnaiz-González","Cesar Garcia-Osorio"],"pdf_url":"https://arxiv.org/pdf/2504.12121v1.pdf","comment":"24 pages, 6 figures. Submitted to Computers and Geosciences"},{"id":"http://arxiv.org/abs/2504.12112v1","updated":"2025-04-16T14:19:57Z","published":"2025-04-16T14:19:57Z","title":"A Diffusion-Based Framework for Terrain-Aware Remote Sensing Image\n  Reconstruction","summary":"  Remote sensing imagery is essential for environmental monitoring,\nagricultural management, and disaster response. However, data loss due to cloud\ncover, sensor failures, or incomplete acquisition-especially in high-resolution\nand high-frequency tasks-severely limits satellite imagery's effectiveness.\nTraditional interpolation methods struggle with large missing areas and complex\nstructures. Remote sensing imagery consists of multiple bands, each with\ndistinct meanings, and ensuring consistency across bands is critical to avoid\nanomalies in the combined images. This paper proposes SatelliteMaker, a\ndiffusion-based method that reconstructs missing data across varying levels of\ndata loss while maintaining spatial, spectral, and temporal consistency. We\nalso propose Digital Elevation Model (DEM) as a conditioning input and use\ntailored prompts to generate realistic images, making diffusion models\napplicable to quantitative remote sensing tasks. Additionally, we propose a\nVGG-Adapter module based on Distribution Loss, which reduces distribution\ndiscrepancy and ensures style consistency. Extensive experiments show that\nSatelliteMaker achieves state-of-the-art performance across multiple tasks.\n","authors":["Zhenyu Yu","Mohd Yamani Inda Idris","Pei Wang"],"pdf_url":"https://arxiv.org/pdf/2504.12112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12104v1","updated":"2025-04-16T14:12:56Z","published":"2025-04-16T14:12:56Z","title":"Logits DeConfusion with CLIP for Few-Shot Learning","summary":"  With its powerful visual-language alignment capability, CLIP performs well in\nzero-shot and few-shot learning tasks. However, we found in experiments that\nCLIP's logits suffer from serious inter-class confusion problems in downstream\ntasks, and the ambiguity between categories seriously affects the accuracy. To\naddress this challenge, we propose a novel method called Logits DeConfusion,\nwhich effectively learns and eliminates inter-class confusion in logits by\ncombining our Multi-level Adapter Fusion (MAF) module with our Inter-Class\nDeconfusion (ICD) module. Our MAF extracts features from different levels and\nfuses them uniformly to enhance feature representation. Our ICD learnably\neliminates inter-class confusion in logits with a residual structure.\nExperimental results show that our method can significantly improve the\nclassification performance and alleviate the inter-class confusion problem. The\ncode is available at https://github.com/LiShuo1001/LDC.\n","authors":["Shuo Li","Fang Liu","Zehua Hao","Xinyi Wang","Lingling Li","Xu Liu","Puhua Chen","Wenping Ma"],"pdf_url":"https://arxiv.org/pdf/2504.12104v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2504.12103v1","updated":"2025-04-16T14:12:25Z","published":"2025-04-16T14:12:25Z","title":"Metric-Solver: Sliding Anchored Metric Depth Estimation from a Single\n  Image","summary":"  Accurate and generalizable metric depth estimation is crucial for various\ncomputer vision applications but remains challenging due to the diverse depth\nscales encountered in indoor and outdoor environments. In this paper, we\nintroduce Metric-Solver, a novel sliding anchor-based metric depth estimation\nmethod that dynamically adapts to varying scene scales. Our approach leverages\nan anchor-based representation, where a reference depth serves as an anchor to\nseparate and normalize the scene depth into two components: scaled near-field\ndepth and tapered far-field depth. The anchor acts as a normalization factor,\nenabling the near-field depth to be normalized within a consistent range while\nmapping far-field depth smoothly toward zero. Through this approach, any depth\nfrom zero to infinity in the scene can be represented within a unified\nrepresentation, effectively eliminating the need to manually account for scene\nscale variations. More importantly, for the same scene, the anchor can slide\nalong the depth axis, dynamically adjusting to different depth scales. A\nsmaller anchor provides higher resolution in the near-field, improving depth\nprecision for closer objects while a larger anchor improves depth estimation in\nfar regions. This adaptability enables the model to handle depth predictions at\nvarying distances and ensure strong generalization across datasets. Our design\nenables a unified and adaptive depth representation across diverse\nenvironments. Extensive experiments demonstrate that Metric-Solver outperforms\nexisting methods in both accuracy and cross-dataset generalization.\n","authors":["Tao Wen","Jiepeng Wang","Yabo Chen","Shugong Xu","Chi Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2504.12103v1.pdf","comment":"Our project page: https://tele-ai.github.io/MetricSolver/"},{"id":"http://arxiv.org/abs/2409.02699v2","updated":"2025-04-16T14:03:37Z","published":"2024-09-04T13:35:15Z","title":"Collaborative Learning for Enhanced Unsupervised Domain Adaptation","summary":"  Unsupervised Domain Adaptation (UDA) endeavors to bridge the gap between a\nmodel trained on a labeled source domain and its deployment in an unlabeled\ntarget domain. However, current high-performance models demand significant\nresources, making deployment costs prohibitive and highlighting the need for\ncompact, yet effective models. For UDA of lightweight models, Knowledge\nDistillation (KD) leveraging a Teacher-Student framework could be a common\napproach, but we found that domain shift in UDA leads to a significant increase\nin non-salient parameters in the teacher model, degrading model's\ngeneralization ability and transferring misleading information to the student\nmodel. Interestingly, we observed that this phenomenon occurs considerably less\nin the student model. Driven by this insight, we introduce Collaborative\nLearning for UDA (CLDA), a method that updates the teacher's non-salient\nparameters using the student model and at the same time utilizes the updated\nteacher model to improve UDA performance of the student model. Experiments show\nconsistent performance improvements for both student and teacher models. For\nexample, in semantic segmentation, CLDA achieves an improvement of +0.7% mIoU\nfor the teacher model and +1.4% mIoU for the student model compared to the\nbaseline model in the GTA-to-Cityscapes datasets. In the Synthia-to-Cityscapes\ndataset, it achieves an improvement of +0.8% mIoU and +2.0% mIoU for the\nteacher and student models, respectively.\n","authors":["Minhee Cho","Hyesong Choi","Hayeon Jo","Dongbo Min"],"pdf_url":"https://arxiv.org/pdf/2409.02699v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12100v1","updated":"2025-04-16T14:03:24Z","published":"2025-04-16T14:03:24Z","title":"Generalized Visual Relation Detection with Diffusion Models","summary":"  Visual relation detection (VRD) aims to identify relationships (or\ninteractions) between object pairs in an image. Although recent VRD models have\nachieved impressive performance, they are all restricted to pre-defined\nrelation categories, while failing to consider the semantic ambiguity\ncharacteristic of visual relations. Unlike objects, the appearance of visual\nrelations is always subtle and can be described by multiple predicate words\nfrom different perspectives, e.g., ``ride'' can be depicted as ``race'' and\n``sit on'', from the sports and spatial position views, respectively. To this\nend, we propose to model visual relations as continuous embeddings, and design\ndiffusion models to achieve generalized VRD in a conditional generative manner,\ntermed Diff-VRD. We model the diffusion process in a latent space and generate\nall possible relations in the image as an embedding sequence. During the\ngeneration, the visual and text embeddings of subject-object pairs serve as\nconditional signals and are injected via cross-attention. After the generation,\nwe design a subsequent matching stage to assign the relation words to\nsubject-object pairs by considering their semantic similarities. Benefiting\nfrom the diffusion-based generative process, our Diff-VRD is able to generate\nvisual relations beyond the pre-defined category labels of datasets. To\nproperly evaluate this generalized VRD task, we introduce two evaluation\nmetrics, i.e., text-to-image retrieval and SPICE PR Curve inspired by image\ncaptioning. Extensive experiments in both human-object interaction (HOI)\ndetection and scene graph generation (SGG) benchmarks attest to the superiority\nand effectiveness of Diff-VRD.\n","authors":["Kaifeng Gao","Siqi Chen","Hanwang Zhang","Jun Xiao","Yueting Zhuang","Qianru Sun"],"pdf_url":"https://arxiv.org/pdf/2504.12100v1.pdf","comment":"Under review at IEEE TCSVT. The Appendix is provided additionally"},{"id":"http://arxiv.org/abs/2504.09530v2","updated":"2025-04-16T13:59:54Z","published":"2025-04-13T11:30:57Z","title":"Trajectory-guided Motion Perception for Facial Expression Quality\n  Assessment in Neurological Disorders","summary":"  Automated facial expression quality assessment (FEQA) in neurological\ndisorders is critical for enhancing diagnostic accuracy and improving patient\ncare, yet effectively capturing the subtle motions and nuances of facial muscle\nmovements remains a challenge. We propose to analyse facial landmark\ntrajectories, a compact yet informative representation, that encodes these\nsubtle motions from a high-level structural perspective. Hence, we introduce\nTrajectory-guided Motion Perception Transformer (TraMP-Former), a novel FEQA\nframework that fuses landmark trajectory features for fine-grained motion\ncapture with visual semantic cues from RGB frames, ultimately regressing the\ncombined features into a quality score. Extensive experiments demonstrate that\nTraMP-Former achieves new state-of-the-art performance on benchmark datasets\nwith neurological disorders, including PFED5 (up by 6.51%) and an augmented\nToronto NeuroFace (up by 7.62%). Our ablation studies further validate the\nefficiency and effectiveness of landmark trajectories in FEQA. Our code is\navailable at https://github.com/shuchaoduan/TraMP-Former.\n","authors":["Shuchao Duan","Amirhossein Dadashzadeh","Alan Whone","Majid Mirmehdi"],"pdf_url":"https://arxiv.org/pdf/2504.09530v2.pdf","comment":"Accepted to IEEE FG 2025 (preprint)"},{"id":"http://arxiv.org/abs/2504.12088v1","updated":"2025-04-16T13:51:16Z","published":"2025-04-16T13:51:16Z","title":"AttentionDrop: A Novel Regularization Method for Transformer Models","summary":"  Transformer-based architectures achieve state-of-the-art performance across a\nwide range of tasks in natural language processing, computer vision, and\nspeech. However, their immense capacity often leads to overfitting, especially\nwhen training data is limited or noisy. We propose AttentionDrop, a unified\nfamily of stochastic regularization techniques that operate directly on the\nself-attention distributions. We introduces three variants: 1. Hard Attention\nMasking: randomly zeroes out top-k attention logits per query to encourage\ndiverse context utilization. 2. Blurred Attention Smoothing: applies a dynamic\nGaussian convolution over attention logits to diffuse overly peaked\ndistributions. 3. Consistency-Regularized AttentionDrop: enforces output\nstability under multiple independent AttentionDrop perturbations via a KL-based\nconsistency loss.\n","authors":["Mirza Samad Ahmed Baig","Syeda Anshrah Gillani","Abdul Akbar Khan","Shahid Munir Shah"],"pdf_url":"https://arxiv.org/pdf/2504.12088v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2407.03653v4","updated":"2025-04-16T13:44:46Z","published":"2024-07-04T05:48:28Z","title":"reBEN: Refined BigEarthNet Dataset for Remote Sensing Image Analysis","summary":"  This paper presents refined BigEarthNet (reBEN) that is a large-scale,\nmulti-modal remote sensing dataset constructed to support deep learning (DL)\nstudies for remote sensing image analysis. The reBEN dataset consists of\n549,488 pairs of Sentinel-1 and Sentinel-2 image patches. To construct reBEN,\nwe initially consider the Sentinel-1 and Sentinel-2 tiles used to construct the\nBigEarthNet dataset and then divide them into patches of size 1200 m x 1200 m.\nWe apply atmospheric correction to the Sentinel-2 patches using the latest\nversion of the sen2cor tool, resulting in higher-quality patches compared to\nthose present in BigEarthNet. Each patch is then associated with a pixel-level\nreference map and scene-level multi-labels. This makes reBEN suitable for\npixel- and scene-based learning tasks. The labels are derived from the most\nrecent CORINE Land Cover (CLC) map of 2018 by utilizing the 19-class\nnomenclature as in BigEarthNet. The use of the most recent CLC map results in\novercoming the label noise present in BigEarthNet. Furthermore, we introduce a\nnew geographical-based split assignment algorithm that significantly reduces\nthe spatial correlation among the train, validation, and test sets with respect\nto those present in BigEarthNet. This increases the reliability of the\nevaluation of DL models. To minimize the DL model training time, we introduce\nsoftware tools that convert the reBEN dataset into a DL-optimized data format.\nIn our experiments, we show the potential of reBEN for multi-modal multi-label\nimage classification problems by considering several state-of-the-art DL\nmodels. The pre-trained model weights, associated code, and complete dataset\nare available at https://bigearth.net.\n","authors":["Kai Norman Clasen","Leonard Hackel","Tom Burgert","Gencer Sumbul","Begüm Demir","Volker Markl"],"pdf_url":"https://arxiv.org/pdf/2407.03653v4.pdf","comment":"Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS) 2025. Our code is available at\n  https://github.com/rsim-tu-berlin/bigearthnet-pipeline"},{"id":"http://arxiv.org/abs/2504.12083v1","updated":"2025-04-16T13:43:56Z","published":"2025-04-16T13:43:56Z","title":"Self-alignment of Large Video Language Models with Refined Regularized\n  Preference Optimization","summary":"  Despite recent advances in Large Video Language Models (LVLMs), they still\nstruggle with fine-grained temporal understanding, hallucinate, and often make\nsimple mistakes on even simple video question-answering tasks, all of which\npose significant challenges to their safe and reliable deployment in real-world\napplications. To address these limitations, we propose a self-alignment\nframework that enables LVLMs to learn from their own errors. Our proposed\nframework first obtains a training set of preferred and non-preferred response\npairs, where non-preferred responses are generated by incorporating common\nerror patterns that often occur due to inadequate spatio-temporal\nunderstanding, spurious correlations between co-occurring concepts, and\nover-reliance on linguistic cues while neglecting the vision modality, among\nothers. To facilitate self-alignment of LVLMs with the constructed preferred\nand non-preferred response pairs, we introduce Refined Regularized Preference\nOptimization (RRPO), a novel preference optimization method that utilizes\nsub-sequence-level refined rewards and token-wise KL regularization to address\nthe limitations of Direct Preference Optimization (DPO). We demonstrate that\nRRPO achieves more precise alignment and more stable training compared to DPO.\nOur experiments and analysis validate the effectiveness of our approach across\ndiverse video tasks, including video hallucination, short- and long-video\nunderstanding, and fine-grained temporal reasoning.\n","authors":["Pritam Sarkar","Ali Etemad"],"pdf_url":"https://arxiv.org/pdf/2504.12083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12080v1","updated":"2025-04-16T13:41:59Z","published":"2025-04-16T13:41:59Z","title":"DC-SAM: In-Context Segment Anything in Images and Videos via Dual\n  Consistency","summary":"  Given a single labeled example, in-context segmentation aims to segment\ncorresponding objects. This setting, known as one-shot segmentation in few-shot\nlearning, explores the segmentation model's generalization ability and has been\napplied to various vision tasks, including scene understanding and image/video\nediting. While recent Segment Anything Models have achieved state-of-the-art\nresults in interactive segmentation, these approaches are not directly\napplicable to in-context segmentation. In this work, we propose the Dual\nConsistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2\nfor in-context segmentation of both images and videos. Our key insights are to\nenhance the features of the SAM's prompt encoder in segmentation by providing\nhigh-quality visual prompts. When generating a mask prior, we fuse the SAM\nfeatures to better align the prompt encoder. Then, we design a cycle-consistent\ncross-attention on fused features and initial visual prompts. Next, a\ndual-branch design is provided by using the discriminative positive and\nnegative prompts in the prompt encoder. Furthermore, we design a simple\nmask-tube training strategy to adopt our proposed dual consistency method into\nthe mask tube. Although the proposed DC-SAM is primarily designed for images,\nit can be seamlessly extended to the video domain with the support of SAM2.\nGiven the absence of in-context segmentation in the video domain, we manually\ncurate and construct the first benchmark from existing video segmentation\ndatasets, named In-Context Video Object Segmentation (IC-VOS), to better assess\nthe in-context capability of the model. Extensive experiments demonstrate that\nour method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on\nPASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our\nsource code and benchmark are available at https://github.com/zaplm/DC-SAM.\n","authors":["Mengshi Qi","Pengfei Zhu","Xiangtai Li","Xiaoyang Bi","Lu Qi","Huadong Ma","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2504.12080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12078v1","updated":"2025-04-16T13:41:02Z","published":"2025-04-16T13:41:02Z","title":"Single-shot Star-convex Polygon-based Instance Segmentation for\n  Spatially-correlated Biomedical Objects","summary":"  Biomedical images often contain objects known to be spatially correlated or\nnested due to their inherent properties, leading to semantic relations.\nExamples include cell nuclei being nested within eukaryotic cells and colonies\ngrowing exclusively within their culture dishes. While these semantic relations\nbear key importance, detection tasks are often formulated independently,\nrequiring multi-shot analysis pipelines. Importantly, spatial correlation could\nconstitute a fundamental prior facilitating learning of more meaningful\nrepresentations for tasks like instance segmentation. This knowledge has, thus\nfar, not been utilised by the biomedical computer vision community. We argue\nthat the instance segmentation of two or more categories of objects can be\nachieved in parallel. We achieve this via two architectures HydraStarDist (HSD)\nand the novel (HSD-WBR) based on the widely-used StarDist (SD), to take\nadvantage of the star-convexity of our target objects. HSD and HSD-WBR are\nconstructed to be capable of incorporating their interactions as constraints\ninto account. HSD implicitly incorporates spatial correlation priors based on\nobject interaction through a joint encoder. HSD-WBR further enforces the prior\nin a regularisation layer with the penalty we proposed named Within Boundary\nRegularisation Penalty (WBR). Both architectures achieve nested instance\nsegmentation in a single shot. We demonstrate their competitiveness based on\n$IoU_R$ and AP and superiority in a new, task-relevant criteria, Joint TP rate\n(JTPR) compared to their baseline SD and Cellpose. Our approach can be further\nmodified to capture partial-inclusion/-exclusion in multi-object interactions\nin fluorescent or brightfield microscopy or digital imaging. Finally, our\nstrategy suggests gains by making this learning single-shot and computationally\nefficient.\n","authors":["Trina De","Adrian Urbanski","Artur Yakimovich"],"pdf_url":"https://arxiv.org/pdf/2504.12078v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.14773v2","updated":"2025-04-16T13:38:58Z","published":"2024-03-21T18:27:29Z","title":"StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation\n  from Text","summary":"  Text-to-video diffusion models enable the generation of high-quality videos\nthat follow text instructions, making it easy to create diverse and individual\ncontent. However, existing approaches mostly focus on high-quality short video\ngeneration (typically 16 or 24 frames), ending up with hard-cuts when naively\nextended to the case of long video synthesis. To overcome these limitations, we\nintroduce StreamingT2V, an autoregressive approach for long video generation of\n80, 240, 600, 1200 or more frames with smooth transitions. The key components\nare:(i) a short-term memory block called conditional attention module (CAM),\nwhich conditions the current generation on the features extracted from the\nprevious chunk via an attentional mechanism, leading to consistent chunk\ntransitions, (ii) a long-term memory block called appearance preservation\nmodule, which extracts high-level scene and object features from the first\nvideo chunk to prevent the model from forgetting the initial scene, and (iii) a\nrandomized blending approach that enables to apply a video enhancer\nautoregressively for infinitely long videos without inconsistencies between\nchunks. Experiments show that StreamingT2V generates high motion amount. In\ncontrast, all competing image-to-video methods are prone to video stagnation\nwhen applied naively in an autoregressive manner. Thus, we propose with\nStreamingT2V a high-quality seamless text-to-long video generator that\noutperforms competitors with consistency and motion. Our code will be available\nat: https://github.com/Picsart-AI-Research/StreamingT2V\n","authors":["Roberto Henschel","Levon Khachatryan","Hayk Poghosyan","Daniil Hayrapetyan","Vahram Tadevosyan","Zhangyang Wang","Shant Navasardyan","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.14773v2.pdf","comment":"https://github.com/Picsart-AI-Research/StreamingT2V"},{"id":"http://arxiv.org/abs/2503.02247v2","updated":"2025-04-16T13:23:05Z","published":"2025-03-04T03:51:36Z","title":"WMNav: Integrating Vision-Language Models into World Models for Object\n  Goal Navigation","summary":"  Object Goal Navigation-requiring an agent to locate a specific object in an\nunseen environment-remains a core challenge in embodied AI. Although recent\nprogress in Vision-Language Model (VLM)-based agents has demonstrated promising\nperception and decision-making abilities through prompting, none has yet\nestablished a fully modular world model design that reduces risky and costly\ninteractions with the environment by predicting the future state of the world.\nWe introduce WMNav, a novel World Model-based Navigation framework powered by\nVision-Language Models (VLMs). It predicts possible outcomes of decisions and\nbuilds memories to provide feedback to the policy module. To retain the\npredicted state of the environment, WMNav proposes the online maintained\nCuriosity Value Map as part of the world model memory to provide dynamic\nconfiguration for navigation policy. By decomposing according to a human-like\nthinking process, WMNav effectively alleviates the impact of model\nhallucination by making decisions based on the feedback difference between the\nworld model plan and observation. To further boost efficiency, we implement a\ntwo-stage action proposer strategy: broad exploration followed by precise\nlocalization. Extensive evaluation on HM3D and MP3D validates WMNav surpasses\nexisting zero-shot benchmarks in both success rate and exploration efficiency\n(absolute improvement: +3.2% SR and +3.2% SPL on HM3D, +13.5% SR and +1.1% SPL\non MP3D). Project page: https://b0b8k1ng.github.io/WMNav/.\n","authors":["Dujun Nie","Xianda Guo","Yiqun Duan","Ruijun Zhang","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2503.02247v2.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.14331v2","updated":"2025-04-16T13:22:12Z","published":"2025-03-18T15:03:28Z","title":"ADAPT: An Autonomous Forklift for Construction Site Operation","summary":"  Efficient material logistics play a critical role in controlling costs and\nschedules in the construction industry. However, manual material handling\nremains prone to inefficiencies, delays, and safety risks. Autonomous forklifts\noffer a promising solution to streamline on-site logistics, reducing reliance\non human operators and mitigating labor shortages. This paper presents the\ndevelopment and evaluation of ADAPT (Autonomous Dynamic All-terrain Pallet\nTransporter), a fully autonomous off-road forklift designed for construction\nenvironments. Unlike structured warehouse settings, construction sites pose\nsignificant challenges, including dynamic obstacles, unstructured terrain, and\nvarying weather conditions. To address these challenges, our system integrates\nAI-driven perception techniques with traditional approaches for decision\nmaking, planning, and control, enabling reliable operation in complex\nenvironments. We validate the system through extensive real-world testing,\ncomparing its continuous performance against an experienced human operator\nacross various weather conditions. Our findings demonstrate that autonomous\noutdoor forklifts can operate near human-level performance, offering a viable\npath toward safer and more efficient construction logistics.\n","authors":["Johannes Huemer","Markus Murschitz","Matthias Schörghuber","Lukas Reisinger","Thomas Kadiofsky","Christoph Weidinger","Mario Niedermeyer","Benedikt Widy","Marcel Zeilinger","Csaba Beleznai","Tobias Glück","Andreas Kugi","Patrik Zips"],"pdf_url":"https://arxiv.org/pdf/2503.14331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12048v1","updated":"2025-04-16T13:04:01Z","published":"2025-04-16T13:04:01Z","title":"Modular-Cam: Modular Dynamic Camera-view Video Generation with LLM","summary":"  Text-to-Video generation, which utilizes the provided text prompt to generate\nhigh-quality videos, has drawn increasing attention and achieved great success\ndue to the development of diffusion models recently. Existing methods mainly\nrely on a pre-trained text encoder to capture the semantic information and\nperform cross attention with the encoded text prompt to guide the generation of\nvideo. However, when it comes to complex prompts that contain dynamic scenes\nand multiple camera-view transformations, these methods can not decompose the\noverall information into separate scenes, as well as fail to smoothly change\nscenes based on the corresponding camera-views. To solve these problems, we\npropose a novel method, i.e., Modular-Cam. Specifically, to better understand a\ngiven complex prompt, we utilize a large language model to analyze user\ninstructions and decouple them into multiple scenes together with transition\nactions. To generate a video containing dynamic scenes that match the given\ncamera-views, we incorporate the widely-used temporal transformer into the\ndiffusion model to ensure continuity within a single scene and propose\nCamOperator, a modular network based module that well controls the camera\nmovements. Moreover, we propose AdaControlNet, which utilizes ControlNet to\nensure consistency across scenes and adaptively adjusts the color tone of the\ngenerated video. Extensive qualitative and quantitative experiments prove our\nproposed Modular-Cam's strong capability of generating multi-scene videos\ntogether with its ability to achieve fine-grained control of camera movements.\nGenerated results are available at https://modular-cam.github.io.\n","authors":["Zirui Pan","Xin Wang","Yipeng Zhang","Hong Chen","Kwan Man Cheng","Yaofei Wu","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2504.12048v1.pdf","comment":"AAAI 2025 Poster"},{"id":"http://arxiv.org/abs/2408.14744v3","updated":"2025-04-16T13:02:25Z","published":"2024-08-27T02:45:26Z","title":"RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with\n  Rich Linguistic Semantics from Openly Available Data and Large Language\n  Models","summary":"  Abundant, well-annotated multimodal data in remote sensing are pivotal for\naligning complex visual remote sensing (RS) scenes with human language,\nenabling the development of specialized vision language models across diverse\nRS interpretation tasks. However, annotating RS images with rich linguistic\nsemantics at scale demands expertise in RS and substantial human labor, making\nit costly and often impractical. In this study, we propose a workflow that\nleverages large language models (LLMs) to generate multimodal datasets with\nsemantically rich captions at scale from plain OpenStreetMap (OSM) data for\nimages sourced from the Google Earth Engine (GEE) platform. This approach\nfacilitates the generation of paired remote sensing data and can be readily\nscaled up using openly available data. Within this framework, we present\nRSTeller, a multimodal dataset comprising over 1.3 million RS images, each\naccompanied by two descriptive captions. Extensive experiments demonstrate that\nRSTeller enhances the performance of multiple existing vision language models\nfor RS scene understanding through continual pre-training. Our methodology\nsignificantly reduces the manual effort and expertise needed for annotating\nremote sensing imagery while democratizing access to high-quality annotated\ndata. This advancement fosters progress in visual language modeling and\nencourages broader participation in remote sensing research and applications.\nThe RSTeller dataset is available at https://github.com/SlytherinGe/RSTeller.\n","authors":["Junyao Ge","Xu Zhang","Yang Zheng","Kaitai Guo","Jimin Liang"],"pdf_url":"https://arxiv.org/pdf/2408.14744v3.pdf","comment":"Submitted to ISPRS"},{"id":"http://arxiv.org/abs/2504.12045v1","updated":"2025-04-16T13:01:44Z","published":"2025-04-16T13:01:44Z","title":"pix2pockets: Shot Suggestions in 8-Ball Pool from a Single Image in the\n  Wild","summary":"  Computer vision models have seen increased usage in sports, and reinforcement\nlearning (RL) is famous for beating humans in strategic games such as Chess and\nGo. In this paper, we are interested in building upon these advances and\nexamining the game of classic 8-ball pool. We introduce pix2pockets, a\nfoundation for an RL-assisted pool coach. Given a single image of a pool table,\nwe first aim to detect the table and the balls and then propose the optimal\nshot suggestion. For the first task, we build a dataset with 195 diverse images\nwhere we manually annotate all balls and table dots, leading to 5748 object\nsegmentation masks. For the second task, we build a standardized RL environment\nthat allows easy development and benchmarking of any RL algorithm. Our object\ndetection model yields an AP50 of 91.2 while our ball location pipeline obtains\nan error of only 0.4 cm. Furthermore, we compare standard RL algorithms to set\na baseline for the shot suggestion task and we show that all of them fail to\npocket all balls without making a foul move. We also present a simple baseline\nthat achieves a per-shot success rate of 94.7% and clears a full game in a\nsingle turn 30% of the time.\n","authors":["Jonas Myhre Schiøtt","Viktor Sebastian Petersen","Dimitrios P. Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2504.12045v1.pdf","comment":"15 pages, 7 figures, to be published in SCIA 2025"},{"id":"http://arxiv.org/abs/2504.12039v1","updated":"2025-04-16T12:54:11Z","published":"2025-04-16T12:54:11Z","title":"RadMamba: Efficient Human Activity Recognition through Radar-based\n  Micro-Doppler-Oriented Mamba State-Space Model","summary":"  Radar-based HAR has emerged as a promising alternative to conventional\nmonitoring approaches, such as wearable devices and camera-based systems, due\nto its unique privacy preservation and robustness advantages. However, existing\nsolutions based on convolutional and recurrent neural networks, although\neffective, are computationally demanding during deployment. This limits their\napplicability in scenarios with constrained resources or those requiring\nmultiple sensors. Advanced architectures, such as ViT and SSM architectures,\noffer improved modeling capabilities and have made efforts toward lightweight\ndesigns. However, their computational complexity remains relatively high. To\nleverage the strengths of transformer architectures while simultaneously\nenhancing accuracy and reducing computational complexity, this paper introduces\nRadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM\nspecifically tailored for radar-based HAR. Across three diverse datasets,\nRadMamba matches the top-performing previous model's 99.8% classification\naccuracy on Dataset DIAT with only 1/400 of its parameters and equals the\nleading models' 92.0% accuracy on Dataset CI4R with merely 1/10 of their\nparameters. In scenarios with continuous sequences of actions evaluated on\nDataset UoG2020, RadMamba surpasses other models with significantly higher\nparameter counts by at least 3%, achieving this with only 6.7k parameters. Our\ncode is available at: https://github.com/lab-emi/AIRHAR.\n","authors":["Yizhuo Wu","Francesco Fioranelli","Chang Gao"],"pdf_url":"https://arxiv.org/pdf/2504.12039v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2504.12029v1","updated":"2025-04-16T12:39:00Z","published":"2025-04-16T12:39:00Z","title":"Object Placement for Anything","summary":"  Object placement aims to determine the appropriate placement (\\emph{e.g.},\nlocation and size) of a foreground object when placing it on the background\nimage. Most previous works are limited by small-scale labeled dataset, which\nhinders the real-world application of object placement. In this work, we devise\na semi-supervised framework which can exploit large-scale unlabeled dataset to\npromote the generalization ability of discriminative object placement models.\nThe discriminative models predict the rationality label for each foreground\nplacement given a foreground-background pair. To better leverage the labeled\ndata, under the semi-supervised framework, we further propose to transfer the\nknowledge of rationality variation, \\emph{i.e.}, whether the change of\nforeground placement would result in the change of rationality label, from\nlabeled data to unlabeled data. Extensive experiments demonstrate that our\nframework can effectively enhance the generalization ability of discriminative\nobject placement models.\n","authors":["Bingjie Gao","Bo Zhang","Li Niu"],"pdf_url":"https://arxiv.org/pdf/2504.12029v1.pdf","comment":"accepted by ICME 2025"},{"id":"http://arxiv.org/abs/2504.12027v1","updated":"2025-04-16T12:37:08Z","published":"2025-04-16T12:37:08Z","title":"Understanding Attention Mechanism in Video Diffusion Models","summary":"  Text-to-video (T2V) synthesis models, such as OpenAI's Sora, have garnered\nsignificant attention due to their ability to generate high-quality videos from\na text prompt. In diffusion-based T2V models, the attention mechanism is a\ncritical component. However, it remains unclear what intermediate features are\nlearned and how attention blocks in T2V models affect various aspects of video\nsynthesis, such as image quality and temporal consistency. In this paper, we\nconduct an in-depth perturbation analysis of the spatial and temporal attention\nblocks of T2V models using an information-theoretic approach. Our results\nindicate that temporal and spatial attention maps affect not only the timing\nand layout of the videos but also the complexity of spatiotemporal elements and\nthe aesthetic quality of the synthesized videos. Notably, high-entropy\nattention maps are often key elements linked to superior video quality, whereas\nlow-entropy attention maps are associated with the video's intra-frame\nstructure. Based on our findings, we propose two novel methods to enhance video\nquality and enable text-guided video editing. These methods rely entirely on\nlightweight manipulation of the attention matrices in T2V models. The efficacy\nand effectiveness of our methods are further validated through experimental\nevaluation across multiple datasets.\n","authors":["Bingyan Liu","Chengyu Wang","Tongtong Su","Huan Ten","Jun Huang","Kailing Guo","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2504.12027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15451v2","updated":"2025-04-16T12:35:53Z","published":"2025-03-19T17:32:24Z","title":"MotionStreamer: Streaming Motion Generation via Diffusion-based\n  Autoregressive Model in Causal Latent Space","summary":"  This paper addresses the challenge of text-conditioned streaming motion\ngeneration, which requires us to predict the next-step human pose based on\nvariable-length historical motions and incoming texts. Existing methods\nstruggle to achieve streaming motion generation, e.g., diffusion models are\nconstrained by pre-defined motion lengths, while GPT-based methods suffer from\ndelayed response and error accumulation problem due to discretized non-causal\ntokenization. To solve these problems, we propose MotionStreamer, a novel\nframework that incorporates a continuous causal latent space into a\nprobabilistic autoregressive model. The continuous latents mitigate information\nloss caused by discretization and effectively reduce error accumulation during\nlong-term autoregressive generation. In addition, by establishing temporal\ncausal dependencies between current and historical motion latents, our model\nfully utilizes the available information to achieve accurate online motion\ndecoding. Experiments show that our method outperforms existing approaches\nwhile offering more applications, including multi-round generation, long-term\ngeneration, and dynamic motion composition. Project Page:\nhttps://zju3dv.github.io/MotionStreamer/\n","authors":["Lixing Xiao","Shunlin Lu","Huaijin Pi","Ke Fan","Liang Pan","Yueer Zhou","Ziyong Feng","Xiaowei Zhou","Sida Peng","Jingbo Wang"],"pdf_url":"https://arxiv.org/pdf/2503.15451v2.pdf","comment":"Project Page: https://zju3dv.github.io/MotionStreamer/"},{"id":"http://arxiv.org/abs/2406.01658v3","updated":"2025-04-16T12:32:09Z","published":"2024-06-03T17:36:36Z","title":"Proxy Denoising for Source-Free Domain Adaptation","summary":"  Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model\nto an unlabeled target domain with no access to the source data. Inspired by\nthe success of large Vision-Language (ViL) models in many applications, the\nlatest research has validated ViL's benefit for SFDA by using their predictions\nas pseudo supervision. However, we observe that ViL's supervision could be\nnoisy and inaccurate at an unknown rate, introducing additional negative\neffects during adaption. To address this thus-far ignored challenge, we\nintroduce a novel Proxy Denoising (ProDe) approach. The key idea is to leverage\nthe ViL model as a proxy to facilitate the adaptation process towards the\nlatent domain-invariant space. We design a proxy denoising mechanism to correct\nViL's predictions, grounded on a proxy confidence theory that models the\ndynamic effect of proxy's divergence against the domain-invariant space during\nadaptation. To capitalize on the corrected proxy, we derive a mutual knowledge\ndistilling regularization. Extensive experiments show that ProDe significantly\noutperforms current state-of-the-art alternatives under the conventional closed\nset setting and more challenging open set, partial set, generalized SFDA,\nmulti-target, multi-source, and test-time settings. Our code and data are\navailable at https://github.com/tntek/source-free-domain-adaptation.\n","authors":["Song Tang","Wenxin Su","Yan Gan","Mao Ye","Jianwei Zhang","Xiatian Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.01658v3.pdf","comment":"This paper is accepted by ICLR 2025 (Oral, Top 1.8%)"},{"id":"http://arxiv.org/abs/2504.12021v1","updated":"2025-04-16T12:24:33Z","published":"2025-04-16T12:24:33Z","title":"Action Anticipation from SoccerNet Football Video Broadcasts","summary":"  Artificial intelligence has revolutionized the way we analyze sports videos,\nwhether to understand the actions of games in long untrimmed videos or to\nanticipate the player's motion in future frames. Despite these efforts, little\nattention has been given to anticipating game actions before they occur. In\nthis work, we introduce the task of action anticipation for football broadcast\nvideos, which consists in predicting future actions in unobserved future\nframes, within a five- or ten-second anticipation window. To benchmark this\ntask, we release a new dataset, namely the SoccerNet Ball Action Anticipation\ndataset, based on SoccerNet Ball Action Spotting. Additionally, we propose a\nFootball Action ANticipation TRAnsformer (FAANTRA), a baseline method that\nadapts FUTR, a state-of-the-art action anticipation model, to predict\nball-related actions. To evaluate action anticipation, we introduce new\nmetrics, including mAP@$\\delta$, which evaluates the temporal precision of\npredicted future actions, as well as mAP@$\\infty$, which evaluates their\noccurrence within the anticipation window. We also conduct extensive ablation\nstudies to examine the impact of various task settings, input configurations,\nand model architectures. Experimental results highlight both the feasibility\nand challenges of action anticipation in football videos, providing valuable\ninsights into the design of predictive models for sports analytics. By\nforecasting actions before they unfold, our work will enable applications in\nautomated broadcasting, tactical analysis, and player decision-making. Our\ndataset and code are publicly available at\nhttps://github.com/MohamadDalal/FAANTRA.\n","authors":["Mohamad Dalal","Artur Xarles","Anthony Cioppa","Silvio Giancola","Marc Van Droogenbroeck","Bernard Ghanem","Albert Clapés","Sergio Escalera","Thomas B. Moeslund"],"pdf_url":"https://arxiv.org/pdf/2504.12021v1.pdf","comment":"15 pages, 14 figures. To be published in the CVSports CVPR workshop"},{"id":"http://arxiv.org/abs/2504.12020v1","updated":"2025-04-16T12:23:30Z","published":"2025-04-16T12:23:30Z","title":"MixSignGraph: A Sign Sequence is Worth Mixed Graphs of Nodes","summary":"  Recent advances in sign language research have benefited from CNN-based\nbackbones, which are primarily transferred from traditional computer vision\ntasks (\\eg object identification, image recognition). However, these CNN-based\nbackbones usually excel at extracting features like contours and texture, but\nmay struggle with capturing sign-related features. In fact, sign language tasks\nrequire focusing on sign-related regions, including the collaboration between\ndifferent regions (\\eg left hand region and right hand region) and the\neffective content in a single region. To capture such region-related features,\nwe introduce MixSignGraph, which represents sign sequences as a group of mixed\ngraphs and designs the following three graph modules for feature extraction,\n\\ie Local Sign Graph (LSG) module, Temporal Sign Graph (TSG) module and\nHierarchical Sign Graph (HSG) module. Specifically, the LSG module learns the\ncorrelation of intra-frame cross-region features within one frame, \\ie focusing\non spatial features. The TSG module tracks the interaction of inter-frame\ncross-region features among adjacent frames, \\ie focusing on temporal features.\nThe HSG module aggregates the same-region features from different-granularity\nfeature maps of a frame, \\ie focusing on hierarchical features. In addition, to\nfurther improve the performance of sign language tasks without gloss\nannotations, we propose a simple yet counter-intuitive Text-driven CTC\nPre-training (TCP) method, which generates pseudo gloss labels from text labels\nfor model pre-training. Extensive experiments conducted on current five public\nsign language datasets demonstrate the superior performance of the proposed\nmodel. Notably, our model surpasses the SOTA models on multiple sign language\ntasks across several datasets, without relying on any additional cues.\n","authors":["Shiwei Gan","Yafeng Yin","Zhiwei Jiang","Hongkai Wen","Lei Xie","Sanglu Lu"],"pdf_url":"https://arxiv.org/pdf/2504.12020v1.pdf","comment":"17 pages, 9 figures, submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence (T-PAMI). This is a regular paper\n  submission"},{"id":"http://arxiv.org/abs/2411.16443v3","updated":"2025-04-16T12:21:59Z","published":"2024-11-25T14:46:17Z","title":"SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting\n  Synthesis","summary":"  Text-based generation and editing of 3D scenes hold significant potential for\nstreamlining content creation through intuitive user interactions. While recent\nadvances leverage 3D Gaussian Splatting (3DGS) for high-fidelity and real-time\nrendering, existing methods are often specialized and task-focused, lacking a\nunified framework for both generation and editing. In this paper, we introduce\nSplatFlow, a comprehensive framework that addresses this gap by enabling direct\n3DGS generation and editing. SplatFlow comprises two main components: a\nmulti-view rectified flow (RF) model and a Gaussian Splatting Decoder\n(GSDecoder). The multi-view RF model operates in latent space, generating\nmulti-view images, depths, and camera poses simultaneously, conditioned on text\nprompts, thus addressing challenges like diverse scene scales and complex\ncamera trajectories in real-world settings. Then, the GSDecoder efficiently\ntranslates these latent outputs into 3DGS representations through a\nfeed-forward 3DGS method. Leveraging training-free inversion and inpainting\ntechniques, SplatFlow enables seamless 3DGS editing and supports a broad range\nof 3D tasks-including object editing, novel view synthesis, and camera pose\nestimation-within a unified framework without requiring additional complex\npipelines. We validate SplatFlow's capabilities on the MVImgNet and DL3DV-7K\ndatasets, demonstrating its versatility and effectiveness in various 3D\ngeneration, editing, and inpainting-based tasks.\n","authors":["Hyojun Go","Byeongjun Park","Jiho Jang","Jin-Young Kim","Soonwoo Kwon","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2411.16443v3.pdf","comment":"Project Page: https://gohyojun15.github.io/SplatFlow/"},{"id":"http://arxiv.org/abs/2504.12018v1","updated":"2025-04-16T12:21:49Z","published":"2025-04-16T12:21:49Z","title":"Instruction-augmented Multimodal Alignment for Image-Text and Element\n  Matching","summary":"  With the rapid advancement of text-to-image (T2I) generation models,\nassessing the semantic alignment between generated images and text descriptions\nhas become a significant research challenge. Current methods, including those\nbased on Visual Question Answering (VQA), still struggle with fine-grained\nassessments and precise quantification of image-text alignment. This paper\npresents an improved evaluation method named Instruction-augmented Multimodal\nAlignment for Image-Text and Element Matching (iMatch), which evaluates\nimage-text semantic alignment by fine-tuning multimodal large language models.\nWe introduce four innovative augmentation strategies: First, the QAlign\nstrategy creates a precise probabilistic mapping to convert discrete scores\nfrom multimodal large language models into continuous matching scores. Second,\na validation set augmentation strategy uses pseudo-labels from model\npredictions to expand training data, boosting the model's generalization\nperformance. Third, an element augmentation strategy integrates element\ncategory labels to refine the model's understanding of image-text matching.\nFourth, an image augmentation strategy employs techniques like random lighting\nto increase the model's robustness. Additionally, we propose prompt type\naugmentation and score perturbation strategies to further enhance the accuracy\nof element assessments. Our experimental results show that the iMatch method\nsignificantly surpasses existing methods, confirming its effectiveness and\npractical value. Furthermore, our iMatch won first place in the CVPR NTIRE 2025\nText to Image Generation Model Quality Assessment - Track 1 Image-Text\nAlignment.\n","authors":["Xinli Yue","JianHui Sun","Junda Lu","Liangchao Yao","Fan Xia","Tianyi Wang","Fengyun Rao","Jing Lyu","Yuetang Deng"],"pdf_url":"https://arxiv.org/pdf/2504.12018v1.pdf","comment":"Accepted to CVPR 2025 Workshop"},{"id":"http://arxiv.org/abs/2504.11999v1","updated":"2025-04-16T11:51:34Z","published":"2025-04-16T11:51:34Z","title":"A Complex-valued SAR Foundation Model Based on Physically Inspired\n  Representation Learning","summary":"  Vision foundation models in remote sensing have been extensively studied due\nto their superior generalization on various downstream tasks. Synthetic\nAperture Radar (SAR) offers all-day, all-weather imaging capabilities,\nproviding significant advantages for Earth observation. However, establishing a\nfoundation model for SAR image interpretation inevitably encounters the\nchallenges of insufficient information utilization and poor interpretability.\nIn this paper, we propose a remote sensing foundation model based on\ncomplex-valued SAR data, which simulates the polarimetric decomposition process\nfor pre-training, i.e., characterizing pixel scattering intensity as a weighted\ncombination of scattering bases and scattering coefficients, thereby endowing\nthe foundation model with physical interpretability. Specifically, we construct\na series of scattering queries, each representing an independent and meaningful\nscattering basis, which interact with SAR features in the scattering query\ndecoder and output the corresponding scattering coefficient. To guide the\npre-training process, polarimetric decomposition loss and power\nself-supervision loss are constructed. The former aligns the predicted\ncoefficients with Yamaguchi coefficients, while the latter reconstructs power\nfrom the predicted coefficients and compares it to the input image's power. The\nperformance of our foundation model is validated on six typical downstream\ntasks, achieving state-of-the-art results. Notably, the foundation model can\nextract stable feature representations and exhibits strong generalization, even\nin data-scarce conditions.\n","authors":["Mengyu Wang","Hanbo Bi","Yingchao Feng","Linlin Xin","Shuo Gong","Tianqi Wang","Zhiyuan Yan","Peijin Wang","Wenhui Diao","Xian Sun"],"pdf_url":"https://arxiv.org/pdf/2504.11999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11995v1","updated":"2025-04-16T11:40:55Z","published":"2025-04-16T11:40:55Z","title":"A Review of YOLOv12: Attention-Based Enhancements vs. Previous Versions","summary":"  The YOLO (You Only Look Once) series has been a leading framework in\nreal-time object detection, consistently improving the balance between speed\nand accuracy. However, integrating attention mechanisms into YOLO has been\nchallenging due to their high computational overhead. YOLOv12 introduces a\nnovel approach that successfully incorporates attention-based enhancements\nwhile preserving real-time performance. This paper provides a comprehensive\nreview of YOLOv12's architectural innovations, including Area Attention for\ncomputationally efficient self-attention, Residual Efficient Layer Aggregation\nNetworks for improved feature aggregation, and FlashAttention for optimized\nmemory access. Additionally, we benchmark YOLOv12 against prior YOLO versions\nand competing object detectors, analyzing its improvements in accuracy,\ninference speed, and computational efficiency. Through this analysis, we\ndemonstrate how YOLOv12 advances real-time object detection by refining the\nlatency-accuracy trade-off and optimizing computational resources.\n","authors":["Rahima Khanam","Muhammad Hussain"],"pdf_url":"https://arxiv.org/pdf/2504.11995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11992v1","updated":"2025-04-16T11:34:18Z","published":"2025-04-16T11:34:18Z","title":"Analysis of Pseudo-Labeling for Online Source-Free Universal Domain\n  Adaptation","summary":"  A domain (distribution) shift between training and test data often hinders\nthe real-world performance of deep neural networks, necessitating unsupervised\ndomain adaptation (UDA) to bridge this gap. Online source-free UDA has emerged\nas a solution for practical scenarios where access to source data is restricted\nand target data is received as a continuous stream. However, the open-world\nnature of many real-world applications additionally introduces category shifts\nmeaning that the source and target label spaces may differ. Online source-free\nuniversal domain adaptation (SF-UniDA) addresses this challenge. Existing\nmethods mainly rely on self-training with pseudo-labels, yet the relationship\nbetween pseudo-labeling and adaptation outcomes has not been studied yet. To\nbridge this gap, we conduct a systematic analysis through controlled\nexperiments with simulated pseudo-labeling, offering valuable insights into\npseudo-labeling for online SF-UniDA. Our findings reveal a substantial gap\nbetween the current state-of-the-art and the upper bound of adaptation achieved\nwith perfect pseudo-labeling. Moreover, we show that a contrastive loss enables\neffective adaptation even with moderate pseudo-label accuracy, while a\ncross-entropy loss, though less robust to pseudo-label errors, achieves\nsuperior results when pseudo-labeling approaches perfection. Lastly, our\nfindings indicate that pseudo-label accuracy is in general more crucial than\nquantity, suggesting that prioritizing fewer but high-confidence pseudo-labels\nis beneficial. Overall, our study highlights the critical role of\npseudo-labeling in (online) SF-UniDA and provides actionable insights to drive\nfuture advancements in the field. Our code is available at\nhttps://github.com/pascalschlachter/PLAnalysis.\n","authors":["Pascal Schlachter","Jonathan Fuss","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2504.11992v1.pdf","comment":"Submitted to the 33rd European Signal Processing Conference (EUSIPCO\n  2025)"},{"id":"http://arxiv.org/abs/2504.11967v1","updated":"2025-04-16T10:58:33Z","published":"2025-04-16T10:58:33Z","title":"Securing the Skies: A Comprehensive Survey on Anti-UAV Methods,\n  Benchmarking, and Future Directions","summary":"  Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure\ninspection, surveillance, and related tasks, yet they also introduce critical\nsecurity challenges. This survey provides a wide-ranging examination of the\nanti-UAV domain, centering on three core objectives-classification, detection,\nand tracking-while detailing emerging methodologies such as diffusion-based\ndata synthesis, multi-modal fusion, vision-language modeling, self-supervised\nlearning, and reinforcement learning. We systematically evaluate\nstate-of-the-art solutions across both single-modality and multi-sensor\npipelines (spanning RGB, infrared, audio, radar, and RF) and discuss\nlarge-scale as well as adversarially oriented benchmarks. Our analysis reveals\npersistent gaps in real-time performance, stealth detection, and swarm-based\nscenarios, underscoring pressing needs for robust, adaptive anti-UAV systems.\nBy highlighting open research directions, we aim to foster innovation and guide\nthe development of next-generation defense strategies in an era marked by the\nextensive use of UAVs.\n","authors":["Yifei Dong","Fengyi Wu","Sanjian Zhang","Guangyu Chen","Yuzhi Hu","Masumi Yano","Jingdong Sun","Siyu Huang","Feng Liu","Qi Dai","Zhi-Qi Cheng"],"pdf_url":"https://arxiv.org/pdf/2504.11967v1.pdf","comment":"Accepted at CVPR Workshop Anti-UAV 2025. 15 pages"},{"id":"http://arxiv.org/abs/2504.11966v1","updated":"2025-04-16T10:55:13Z","published":"2025-04-16T10:55:13Z","title":"Exploring Video-Based Driver Activity Recognition under Noisy Labels","summary":"  As an open research topic in the field of deep learning, learning with noisy\nlabels has attracted much attention and grown rapidly over the past ten years.\nLearning with label noise is crucial for driver distraction behavior\nrecognition, as real-world video data often contains mislabeled samples,\nimpacting model reliability and performance. However, label noise learning is\nbarely explored in the driver activity recognition field. In this paper, we\npropose the first label noise learning approach for the driver activity\nrecognition task. Based on the cluster assumption, we initially enable the\nmodel to learn clustering-friendly low-dimensional representations from given\nvideos and assign the resultant embeddings into clusters. We subsequently\nperform co-refinement within each cluster to smooth the classifier outputs.\nFurthermore, we propose a flexible sample selection strategy that combines two\nselection criteria without relying on any hyperparameters to filter clean\nsamples from the training dataset. We also incorporate a self-adaptive\nparameter into the sample selection process to enforce balancing across\nclasses. A comprehensive variety of experiments on the public Drive&Act dataset\nfor all granularity levels demonstrates the superior performance of our method\nin comparison with other label-denoising methods derived from the image\nclassification field. The source code is available at\nhttps://github.com/ilonafan/DAR-noisy-labels.\n","authors":["Linjuan Fan","Di Wen","Kunyu Peng","Kailun Yang","Jiaming Zhang","Ruiping Liu","Yufan Chen","Junwei Zheng","Jiamin Wu","Xudong Han","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2504.11966v1.pdf","comment":"The source code is available at\n  https://github.com/ilonafan/DAR-noisy-labels"},{"id":"http://arxiv.org/abs/2502.05151v2","updated":"2025-04-16T10:54:12Z","published":"2025-02-07T18:26:45Z","title":"Transforming Science with Large Language Models: A Survey on AI-assisted\n  Scientific Discovery, Experimentation, Content Generation, and Evaluation","summary":"  With the advent of large multimodal language models, science is now at a\nthreshold of an AI-based technological transformation. Recently, a plethora of\nnew AI models and tools has been proposed, promising to empower researchers and\nacademics worldwide to conduct their research more effectively and efficiently.\nThis includes all aspects of the research cycle, especially (1) searching for\nrelevant literature; (2) generating research ideas and conducting\nexperimentation; generating (3) text-based and (4) multimodal content (e.g.,\nscientific figures and diagrams); and (5) AI-based automatic peer review. In\nthis survey, we provide an in-depth overview over these exciting recent\ndevelopments, which promise to fundamentally alter the scientific research\nprocess for good. Our survey covers the five aspects outlined above, indicating\nrelevant datasets, methods and results (including evaluation) as well as\nlimitations and scope for future research. Ethical concerns regarding\nshortcomings of these tools and potential for misuse (fake science, plagiarism,\nharms to research integrity) take a particularly prominent place in our\ndiscussion. We hope that our survey will not only become a reference guide for\nnewcomers to the field but also a catalyst for new AI-based initiatives in the\narea of \"AI4Science\".\n","authors":["Steffen Eger","Yong Cao","Jennifer D'Souza","Andreas Geiger","Christian Greisinger","Stephanie Gross","Yufang Hou","Brigitte Krenn","Anne Lauscher","Yizhi Li","Chenghua Lin","Nafise Sadat Moosavi","Wei Zhao","Tristan Miller"],"pdf_url":"https://arxiv.org/pdf/2502.05151v2.pdf","comment":"44 pages, 7 figures, 8 tables"},{"id":"http://arxiv.org/abs/2404.07983v3","updated":"2025-04-16T10:50:18Z","published":"2024-04-11T17:58:06Z","title":"Two Effects, One Trigger: On the Modality Gap, Object Bias, and\n  Information Imbalance in Contrastive Vision-Language Models","summary":"  Contrastive vision-language models (VLMs), like CLIP, have gained popularity\nfor their versatile applicability to various downstream tasks. Despite their\nsuccesses in some tasks, like zero-shot object recognition, they perform\nsurprisingly poor on other tasks, like attribute recognition. Previous work has\nattributed these challenges to the modality gap, a separation of image and text\nin the shared representation space, and to a bias towards objects over other\nfactors, such as attributes. In this analysis paper, we investigate both\nphenomena thoroughly. We evaluated off-the-shelf VLMs and while the gap's\ninfluence on performance is typically overshadowed by other factors, we find\nindications that closing the gap indeed leads to improvements. Moreover, we\nfind that, contrary to intuition, only few embedding dimensions drive the gap\nand that the embedding spaces are differently organized. To allow for a clean\nstudy of object bias, we introduce a definition and a corresponding measure of\nit. Equipped with this tool, we find that object bias does not lead to worse\nperformance on other concepts, such as attributes per se. However, why do both\nphenomena, modality gap and object bias, emerge in the first place? To answer\nthis fundamental question and uncover some of the inner workings of contrastive\nVLMs, we conducted experiments that allowed us to control the amount of shared\ninformation between the modalities. These experiments revealed that the driving\nfactor behind both the modality gap and the object bias, is an information\nimbalance between images and captions, and unveiled an intriguing connection\nbetween the modality gap and entropy of the logits.\n","authors":["Simon Schrodi","David T. Hoffmann","Max Argus","Volker Fischer","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2404.07983v3.pdf","comment":"ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2504.04519v2","updated":"2025-04-16T10:49:02Z","published":"2025-04-06T15:32:08Z","title":"SAM2MOT: A Novel Paradigm of Multi-Object Tracking by Segmentation","summary":"  Segment Anything 2 (SAM2) enables robust single-object tracking using\nsegmentation. To extend this to multi-object tracking (MOT), we propose\nSAM2MOT, introducing a novel Tracking by Segmentation paradigm. Unlike Tracking\nby Detection or Tracking by Query, SAM2MOT directly generates tracking boxes\nfrom segmentation masks, reducing reliance on detection accuracy. SAM2MOT has\ntwo key advantages: zero-shot generalization, allowing it to work across\ndatasets without fine-tuning, and strong object association, inherited from\nSAM2. To further improve performance, we integrate a trajectory manager system\nfor precise object addition and removal, and a cross-object interaction module\nto handle occlusions. Experiments on DanceTrack, UAVDT, and BDD100K show\nstate-of-the-art results. Notably, SAM2MOT outperforms existing methods on\nDanceTrack by +2.1 HOTA and +4.5 IDF1, highlighting its effectiveness in MOT.\nCode is available at https://github.com/TripleJoy/SAM2MOT.\n","authors":["Junjie Jiang","Zelin Wang","Manqi Zhao","Yin Li","DongSheng Jiang"],"pdf_url":"https://arxiv.org/pdf/2504.04519v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12793v3","updated":"2025-04-16T10:39:20Z","published":"2025-03-17T04:01:37Z","title":"Improving Generalization of Universal Adversarial Perturbation via\n  Dynamic Maximin Optimization","summary":"  Deep neural networks (DNNs) are susceptible to universal adversarial\nperturbations (UAPs). These perturbations are meticulously designed to fool the\ntarget model universally across all sample classes. Unlike instance-specific\nadversarial examples (AEs), generating UAPs is more complex because they must\nbe generalized across a wide range of data samples and models. Our research\nreveals that existing universal attack methods, which optimize UAPs using DNNs\nwith static model parameter snapshots, do not fully leverage the potential of\nDNNs to generate more effective UAPs. Rather than optimizing UAPs against\nstatic DNN models with a fixed training set, we suggest using dynamic\nmodel-data pairs to generate UAPs. In particular, we introduce a dynamic\nmaximin optimization strategy, aiming to optimize the UAP across a variety of\noptimal model-data pairs. We term this approach DM-UAP. DM-UAP utilizes an\niterative max-min-min optimization framework that refines the model-data pairs,\ncoupled with a curriculum UAP learning algorithm to examine the combined space\nof model parameters and data thoroughly. Comprehensive experiments on the\nImageNet dataset demonstrate that the proposed DM-UAP markedly enhances both\ncross-sample universality and cross-model transferability of UAPs. Using only\n500 samples for UAP generation, DM-UAP outperforms the state-of-the-art\napproach with an average increase in fooling ratio of 12.108%.\n","authors":["Yechao Zhang","Yingzhe Xu","Junyu Shi","Leo Yu Zhang","Shengshan Hu","Minghui Li","Yanjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.12793v3.pdf","comment":"Accepted in AAAI 2025"},{"id":"http://arxiv.org/abs/2504.11953v1","updated":"2025-04-16T10:30:08Z","published":"2025-04-16T10:30:08Z","title":"Novel-view X-ray Projection Synthesis through Geometry-Integrated Deep\n  Learning","summary":"  X-ray imaging plays a crucial role in the medical field, providing essential\ninsights into the internal anatomy of patients for diagnostics, image-guided\nprocedures, and clinical decision-making. Traditional techniques often require\nmultiple X-ray projections from various angles to obtain a comprehensive view,\nleading to increased radiation exposure and more complex clinical processes.\nThis paper explores an innovative approach using the DL-GIPS model, which\nsynthesizes X-ray projections from new viewpoints by leveraging a single\nexisting projection. The model strategically manipulates geometry and texture\nfeatures extracted from an initial projection to match new viewing angles. It\nthen synthesizes the final projection by merging these modified geometry\nfeatures with consistent texture information through an advanced image\ngeneration process. We demonstrate the effectiveness and broad applicability of\nthe DL-GIPS framework through lung imaging examples, highlighting its potential\nto revolutionize stereoscopic and volumetric imaging by minimizing the need for\nextensive data acquisition.\n","authors":["Daiqi Liu","Fuxin Fan","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2504.11953v1.pdf","comment":"6 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2504.11949v1","updated":"2025-04-16T10:25:20Z","published":"2025-04-16T10:25:20Z","title":"Flow Intelligence: Robust Feature Matching via Temporal Signature\n  Correlation","summary":"  Feature matching across video streams remains a cornerstone challenge in\ncomputer vision. Increasingly, robust multimodal matching has garnered interest\nin robotics, surveillance, remote sensing, and medical imaging. While\ntraditional rely on detecting and matching spatial features, they break down\nwhen faced with noisy, misaligned, or cross-modal data. Recent deep learning\nmethods have improved robustness through learned representations, but remain\nconstrained by their dependence on extensive training data and computational\ndemands. We present Flow Intelligence, a paradigm-shifting approach that moves\nbeyond spatial features by focusing on temporal motion patterns exclusively.\nInstead of detecting traditional keypoints, our method extracts motion\nsignatures from pixel blocks across consecutive frames and extract temporal\nmotion signatures between videos. These motion-based descriptors achieve\nnatural invariance to translation, rotation, and scale variations while\nremaining robust across different imaging modalities. This novel approach also\nrequires no pretraining data, eliminates the need for spatial feature\ndetection, enables cross-modal matching using only temporal motion, and it\noutperforms existing methods in challenging scenarios where traditional\napproaches fail. By leveraging motion rather than appearance, Flow Intelligence\nenables robust, real-time video feature matching in diverse environments.\n","authors":["Jie Wang","Chen Ye Gan","Caoqi Wei","Jiangtao Wen","Yuxing Han"],"pdf_url":"https://arxiv.org/pdf/2504.11949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11946v1","updated":"2025-04-16T10:23:59Z","published":"2025-04-16T10:23:59Z","title":"R-Meshfusion: Reinforcement Learning Powered Sparse-View Mesh\n  Reconstruction with Diffusion Priors","summary":"  Mesh reconstruction from multi-view images is a fundamental problem in\ncomputer vision, but its performance degrades significantly under sparse-view\nconditions, especially in unseen regions where no ground-truth observations are\navailable. While recent advances in diffusion models have demonstrated strong\ncapabilities in synthesizing novel views from limited inputs, their outputs\noften suffer from visual artifacts and lack 3D consistency, posing challenges\nfor reliable mesh optimization. In this paper, we propose a novel framework\nthat leverages diffusion models to enhance sparse-view mesh reconstruction in a\nprincipled and reliable manner. To address the instability of diffusion\noutputs, we propose a Consensus Diffusion Module that filters unreliable\ngenerations via interquartile range (IQR) analysis and performs variance-aware\nimage fusion to produce robust pseudo-supervision. Building on this, we design\nan online reinforcement learning strategy based on the Upper Confidence Bound\n(UCB) to adaptively select the most informative viewpoints for enhancement,\nguided by diffusion loss. Finally, the fused images are used to jointly\nsupervise a NeRF-based model alongside sparse-view ground truth, ensuring\nconsistency across both geometry and appearance. Extensive experiments\ndemonstrate that our method achieves significant improvements in both geometric\nquality and rendering quality.\n","authors":["Haoyang Wang","Liming Liu","Peiheng Wang","Junlin Hao","Jiangkai Wu","Xinggong Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.11946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11942v1","updated":"2025-04-16T10:20:11Z","published":"2025-04-16T10:20:11Z","title":"ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign\n  Language Translation","summary":"  Current sign language machine translation systems rely on recognizing hand\nmovements, facial expressions and body postures, and natural language\nprocessing, to convert signs into text. Recent approaches use Transformer\narchitectures to model long-range dependencies via positional encoding.\nHowever, they lack accuracy in recognizing fine-grained, short-range temporal\ndependencies between gestures captured at high frame rates. Moreover, their\nhigh computational complexity leads to inefficient training. To mitigate these\nissues, we propose an Adaptive Transformer (ADAT), which incorporates\ncomponents for enhanced feature extraction and adaptive feature weighting\nthrough a gating mechanism to emphasize contextually relevant features while\nreducing training overhead and maintaining translation accuracy. To evaluate\nADAT, we introduce MedASL, the first public medical American Sign Language\ndataset. In sign-to-gloss-to-text experiments, ADAT outperforms the\nencoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing\ntraining time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text\nexperiments, it improves accuracy by 8.7% and reduces training time by 2.8% on\nPHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on\nMedASL. Compared to encoder-only and decoder-only baselines in sign-to-text,\nADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its\ndual-stream structure.\n","authors":["Nada Shahin","Leila Ismail"],"pdf_url":"https://arxiv.org/pdf/2504.11942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11930v1","updated":"2025-04-16T10:09:45Z","published":"2025-04-16T10:09:45Z","title":"Beyond Words: Augmenting Discriminative Richness via Diffusions in\n  Unsupervised Prompt Learning","summary":"  Fine-tuning vision-language models (VLMs) with large amounts of unlabeled\ndata has recently garnered significant interest. However, a key challenge\nremains the lack of high-quality pseudo-labeled data. Current pseudo-labeling\nstrategies often struggle with mismatches between semantic and visual\ninformation, leading to sub-optimal performance of unsupervised prompt learning\n(UPL) methods. In this paper, we introduce a simple yet effective approach\ncalled \\textbf{A}ugmenting D\\textbf{i}scriminative \\textbf{R}ichness via\nDiffusions (AiR), toward learning a richer discriminating way to represent the\nclass comprehensively and thus facilitate classification. Specifically, our\napproach includes a pseudo-label generation module that leverages high-fidelity\nsynthetic samples to create an auxiliary classifier, which captures richer\nvisual variation, bridging text-image-pair classification to a more robust\nimage-image-pair classification. Additionally, we exploit the diversity of\ndiffusion-based synthetic samples to enhance prompt learning, providing greater\ninformation for semantic-visual alignment. Extensive experiments on five public\nbenchmarks, including RESISC45 and Flowers102, and across three learning\nparadigms-UL, SSL, and TRZSL-demonstrate that AiR achieves substantial and\nconsistent performance improvements over state-of-the-art unsupervised prompt\nlearning methods.\n","authors":["Hairui Ren","Fan Tang","He Zhao","Zixuan Wang","Dandan Guo","Yi Chang"],"pdf_url":"https://arxiv.org/pdf/2504.11930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00127v2","updated":"2025-04-16T10:04:24Z","published":"2024-11-28T13:00:38Z","title":"Orthus: Autoregressive Interleaved Image-Text Generation with\n  Modality-Specific Heads","summary":"  We introduce Orthus, an autoregressive (AR) transformer that excels in\ngenerating images given textual prompts, answering questions based on visual\ninputs, and even crafting lengthy image-text interleaved contents. Unlike prior\narts on unified multimodal modeling, Orthus simultaneously copes with discrete\ntext tokens and continuous image features under the AR modeling principle. The\ncontinuous treatment of visual signals minimizes the information loss for both\nimage understanding and generation while the fully AR formulation renders the\ncharacterization of the correlation between modalities straightforward. The key\nmechanism enabling Orthus to leverage these advantages lies in its\nmodality-specific heads -- one regular language modeling (LM) head predicts\ndiscrete text tokens and one diffusion head generates continuous image features\nconditioning on the output of the backbone. We devise an efficient strategy for\nbuilding Orthus -- by substituting the Vector Quantization (VQ) operation in\nthe existing unified AR model with a soft alternative, introducing a diffusion\nhead, and tuning the added modules to reconstruct images, we can create an\nOrthus-base model effortlessly (e.g., within mere 72 A100 GPU hours).\nOrthus-base can further embrace post-training to better model interleaved\nimages and texts. Empirically, Orthus surpasses competing baselines including\nShow-o and Chameleon across standard benchmarks, achieving a GenEval score of\n0.58 and an MME-P score of 1265.8 using 7B parameters. Orthus also shows\nexceptional mixed-modality generation capabilities, reflecting the potential\nfor handling intricate practical generation tasks.\n","authors":["Siqi Kou","Jiachun Jin","Zhihong Liu","Chang Liu","Ye Ma","Jian Jia","Quan Chen","Peng Jiang","Zhijie Deng"],"pdf_url":"https://arxiv.org/pdf/2412.00127v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11923v1","updated":"2025-04-16T09:58:04Z","published":"2025-04-16T09:58:04Z","title":"SemDiff: Generating Natural Unrestricted Adversarial Examples via\n  Semantic Attributes Optimization in Diffusion Models","summary":"  Unrestricted adversarial examples (UAEs), allow the attacker to create\nnon-constrained adversarial examples without given clean samples, posing a\nsevere threat to the safety of deep learning models. Recent works utilize\ndiffusion models to generate UAEs. However, these UAEs often lack naturalness\nand imperceptibility due to simply optimizing in intermediate latent noises. In\nlight of this, we propose SemDiff, a novel unrestricted adversarial attack that\nexplores the semantic latent space of diffusion models for meaningful\nattributes, and devises a multi-attributes optimization approach to ensure\nattack success while maintaining the naturalness and imperceptibility of\ngenerated UAEs. We perform extensive experiments on four tasks on three\nhigh-resolution datasets, including CelebA-HQ, AFHQ and ImageNet. The results\ndemonstrate that SemDiff outperforms state-of-the-art methods in terms of\nattack success rate and imperceptibility. The generated UAEs are natural and\nexhibit semantically meaningful changes, in accord with the attributes'\nweights. In addition, SemDiff is found capable of evading different defenses,\nwhich further validates its effectiveness and threatening.\n","authors":["Zeyu Dai","Shengcai Liu","Rui He","Jiahao Wu","Ning Lu","Wenqi Fan","Qing Li","Ke Tang"],"pdf_url":"https://arxiv.org/pdf/2504.11923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11922v1","updated":"2025-04-16T09:57:23Z","published":"2025-04-16T09:57:23Z","title":"Zooming In on Fakes: A Novel Dataset for Localized AI-Generated Image\n  Detection with Forgery Amplification Approach","summary":"  The rise of AI-generated image editing tools has made localized forgeries\nincreasingly realistic, posing challenges for visual content integrity.\nAlthough recent efforts have explored localized AIGC detection, existing\ndatasets predominantly focus on object-level forgeries while overlooking\nbroader scene edits in regions such as sky or ground. To address these\nlimitations, we introduce \\textbf{BR-Gen}, a large-scale dataset of 150,000\nlocally forged images with diverse scene-aware annotations, which are based on\nsemantic calibration to ensure high-quality samples. BR-Gen is constructed\nthrough a fully automated Perception-Creation-Evaluation pipeline to ensure\nsemantic coherence and visual realism. In addition, we further propose\n\\textbf{NFA-ViT}, a Noise-guided Forgery Amplification Vision Transformer that\nenhances the detection of localized forgeries by amplifying forgery-related\nfeatures across the entire image. NFA-ViT mines heterogeneous regions in\nimages, \\emph{i.e.}, potential edited areas, by noise fingerprints.\nSubsequently, attention mechanism is introduced to compel the interaction\nbetween normal and abnormal features, thereby propagating the generalization\ntraces throughout the entire image, allowing subtle forgeries to influence a\nbroader context and improving overall detection robustness. Extensive\nexperiments demonstrate that BR-Gen constructs entirely new scenarios that are\nnot covered by existing methods. Take a step further, NFA-ViT outperforms\nexisting methods on BR-Gen and generalizes well across current benchmarks. All\ndata and codes are available at https://github.com/clpbc/BR-Gen.\n","authors":["Lvpan Cai","Haowei Wang","Jiayi Ji","YanShu ZhouMen","Yiwei Ma","Xiaoshuai Sun","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2504.11922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11914v1","updated":"2025-04-16T09:48:41Z","published":"2025-04-16T09:48:41Z","title":"AnomalyR1: A GRPO-based End-to-end MLLM for Industrial Anomaly Detection","summary":"  Industrial Anomaly Detection (IAD) poses a formidable challenge due to the\nscarcity of defective samples, making it imperative to deploy models capable of\nrobust generalization to detect unseen anomalies effectively. Traditional\napproaches, often constrained by hand-crafted features or domain-specific\nexpert models, struggle to address this limitation, underscoring the need for a\nparadigm shift. We introduce AnomalyR1, a pioneering framework that leverages\nVLM-R1, a Multimodal Large Language Model (MLLM) renowned for its exceptional\ngeneralization and interpretability, to revolutionize IAD. By integrating MLLM\nwith Group Relative Policy Optimization (GRPO), enhanced by our novel Reasoned\nOutcome Alignment Metric (ROAM), AnomalyR1 achieves a fully end-to-end solution\nthat autonomously processes inputs of image and domain knowledge, reasons\nthrough analysis, and generates precise anomaly localizations and masks. Based\non the latest multimodal IAD benchmark, our compact 3-billion-parameter model\noutperforms existing methods, establishing state-of-the-art results. As MLLM\ncapabilities continue to advance, this study is the first to deliver an\nend-to-end VLM-based IAD solution that demonstrates the transformative\npotential of ROAM-enhanced GRPO, positioning our framework as a forward-looking\ncornerstone for next-generation intelligent anomaly detection systems in\nindustrial applications with limited defective data.\n","authors":["Yuhao Chao","Jie Liu","Jie Tang","Gangshan Wu"],"pdf_url":"https://arxiv.org/pdf/2504.11914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09555v2","updated":"2025-04-16T09:29:01Z","published":"2025-04-13T13:03:25Z","title":"Mitigating Long-tail Distribution in Oracle Bone Inscriptions: Dataset,\n  Model, and Benchmark","summary":"  The oracle bone inscription (OBI) recognition plays a significant role in\nunderstanding the history and culture of ancient China. However, the existing\nOBI datasets suffer from a long-tail distribution problem, leading to biased\nperformance of OBI recognition models across majority and minority classes.\nWith recent advancements in generative models, OBI synthesis-based data\naugmentation has become a promising avenue to expand the sample size of\nminority classes. Unfortunately, current OBI datasets lack large-scale\nstructure-aligned image pairs for generative model training. To address these\nproblems, we first present the Oracle-P15K, a structure-aligned OBI dataset for\nOBI generation and denoising, consisting of 14,542 images infused with domain\nknowledge from OBI experts. Second, we propose a diffusion model-based pseudo\nOBI generator, called OBIDiff, to achieve realistic and controllable OBI\ngeneration. Given a clean glyph image and a target rubbing-style image, it can\neffectively transfer the noise style of the original rubbing to the glyph\nimage. Extensive experiments on OBI downstream tasks and user preference\nstudies show the effectiveness of the proposed Oracle-P15K dataset and\ndemonstrate that OBIDiff can accurately preserve inherent glyph structures\nwhile transferring authentic rubbing styles effectively.\n","authors":["Jinhao Li","Zijian Chen","Runze Jiang","Tingzhu Chen","Changbo Wang","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2504.09555v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11896v1","updated":"2025-04-16T09:23:38Z","published":"2025-04-16T09:23:38Z","title":"Learning Physics-Informed Color-Aware Transforms for Low-Light Image\n  Enhancement","summary":"  Image decomposition offers deep insights into the imaging factors of visual\ndata and significantly enhances various advanced computer vision tasks. In this\nwork, we introduce a novel approach to low-light image enhancement based on\ndecomposed physics-informed priors. Existing methods that directly map\nlow-light to normal-light images in the sRGB color space suffer from\ninconsistent color predictions and high sensitivity to spectral power\ndistribution (SPD) variations, resulting in unstable performance under diverse\nlighting conditions. To address these challenges, we introduce a\nPhysics-informed Color-aware Transform (PiCat), a learning-based framework that\nconverts low-light images from the sRGB color space into deep\nillumination-invariant descriptors via our proposed Color-aware Transform\n(CAT). This transformation enables robust handling of complex lighting and SPD\nvariations. Complementing this, we propose the Content-Noise Decomposition\nNetwork (CNDN), which refines the descriptor distributions to better align with\nwell-lit conditions by mitigating noise and other distortions, thereby\neffectively restoring content representations to low-light images. The CAT and\nthe CNDN collectively act as a physical prior, guiding the transformation\nprocess from low-light to normal-light domains. Our proposed PiCat framework\ndemonstrates superior performance compared to state-of-the-art methods across\nfive benchmark datasets.\n","authors":["Xingxing Yang","Jie Chen","Zaifeng Yang"],"pdf_url":"https://arxiv.org/pdf/2504.11896v1.pdf","comment":"Accepted by ICME 2025"},{"id":"http://arxiv.org/abs/2504.11895v1","updated":"2025-04-16T09:21:34Z","published":"2025-04-16T09:21:34Z","title":"Search is All You Need for Few-shot Anomaly Detection","summary":"  Few-shot anomaly detection (FSAD) has emerged as a crucial yet challenging\ntask in industrial inspection, where normal distribution modeling must be\naccomplished with only a few normal images. While existing approaches typically\nemploy multi-modal foundation models combining language and vision modalities\nfor prompt-guided anomaly detection, these methods often demand sophisticated\nprompt engineering and extensive manual tuning. In this paper, we demonstrate\nthat a straightforward nearest-neighbor search framework can surpass\nstate-of-the-art performance in both single-class and multi-class FSAD\nscenarios. Our proposed method, VisionAD, consists of four simple yet essential\ncomponents: (1) scalable vision foundation models that extract universal and\ndiscriminative features; (2) dual augmentation strategies - support\naugmentation to enhance feature matching adaptability and query augmentation to\naddress the oversights of single-view prediction; (3) multi-layer feature\nintegration that captures both low-frequency global context and high-frequency\nlocal details with minimal computational overhead; and (4) a class-aware visual\nmemory bank enabling efficient one-for-all multi-class detection. Extensive\nevaluations across MVTec-AD, VisA, and Real-IAD benchmarks demonstrate\nVisionAD's exceptional performance. Using only 1 normal images as support, our\nmethod achieves remarkable image-level AUROC scores of 97.4%, 94.8%, and 70.8%\nrespectively, outperforming current state-of-the-art approaches by significant\nmargins (+1.6%, +3.2%, and +1.4%). The training-free nature and superior\nfew-shot capabilities of VisionAD make it particularly appealing for real-world\napplications where samples are scarce or expensive to obtain. Code is available\nat https://github.com/Qiqigeww/VisionAD.\n","authors":["Qishan Wang","Jia Guo","Shuyong Gao","Haofen Wang","Li Xiong","Junjie Hu","Hanqi Guo","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.11895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11893v1","updated":"2025-04-16T09:20:03Z","published":"2025-04-16T09:20:03Z","title":"CAGS: Open-Vocabulary 3D Scene Understanding with Context-Aware Gaussian\n  Splatting","summary":"  Open-vocabulary 3D scene understanding is crucial for applications requiring\nnatural language-driven spatial interpretation, such as robotics and augmented\nreality. While 3D Gaussian Splatting (3DGS) offers a powerful representation\nfor scene reconstruction, integrating it with open-vocabulary frameworks\nreveals a key challenge: cross-view granularity inconsistency. This issue,\nstemming from 2D segmentation methods like SAM, results in inconsistent object\nsegmentations across views (e.g., a \"coffee set\" segmented as a single entity\nin one view but as \"cup + coffee + spoon\" in another). Existing 3DGS-based\nmethods often rely on isolated per-Gaussian feature learning, neglecting the\nspatial context needed for cohesive object reasoning, leading to fragmented\nrepresentations. We propose Context-Aware Gaussian Splatting (CAGS), a novel\nframework that addresses this challenge by incorporating spatial context into\n3DGS. CAGS constructs local graphs to propagate contextual features across\nGaussians, reducing noise from inconsistent granularity, employs mask-centric\ncontrastive learning to smooth SAM-derived features across views, and leverages\na precomputation strategy to reduce computational cost by precomputing\nneighborhood relationships, enabling efficient training in large-scale scenes.\nBy integrating spatial context, CAGS significantly improves 3D instance\nsegmentation and reduces fragmentation errors on datasets like LERF-OVS and\nScanNet, enabling robust language-guided 3D scene understanding.\n","authors":["Wei Sun","Yanzhao Zhou","Jianbin Jiao","Yuan Li"],"pdf_url":"https://arxiv.org/pdf/2504.11893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16522v2","updated":"2025-04-16T09:12:31Z","published":"2024-12-21T07:50:59Z","title":"Enhancing Contrastive Learning Inspired by the Philosophy of \"The Blind\n  Men and the Elephant\"","summary":"  Contrastive learning is a prevalent technique in self-supervised vision\nrepresentation learning, typically generating positive pairs by applying two\ndata augmentations to the same image. Designing effective data augmentation\nstrategies is crucial for the success of contrastive learning. Inspired by the\nstory of the blind men and the elephant, we introduce JointCrop and JointBlur.\nThese methods generate more challenging positive pairs by leveraging the joint\ndistribution of the two augmentation parameters, thereby enabling contrastive\nlearning to acquire more effective feature representations. To the best of our\nknowledge, this is the first effort to explicitly incorporate the joint\ndistribution of two data augmentation parameters into contrastive learning. As\na plug-and-play framework without additional computational overhead, JointCrop\nand JointBlur enhance the performance of SimCLR, BYOL, MoCo v1, MoCo v2, MoCo\nv3, SimSiam, and Dino baselines with notable improvements.\n","authors":["Yudong Zhang","Ruobing Xie","Jiansheng Chen","Xingwu Sun","Zhanhui Kang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.16522v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2504.11879v1","updated":"2025-04-16T08:59:47Z","published":"2025-04-16T08:59:47Z","title":"Learning Compatible Multi-Prize Subnetworks for Asymmetric Retrieval","summary":"  Asymmetric retrieval is a typical scenario in real-world retrieval systems,\nwhere compatible models of varying capacities are deployed on platforms with\ndifferent resource configurations. Existing methods generally train pre-defined\nnetworks or subnetworks with capacities specifically designed for\npre-determined platforms, using compatible learning. Nevertheless, these\nmethods suffer from limited flexibility for multi-platform deployment. For\nexample, when introducing a new platform into the retrieval systems, developers\nhave to train an additional model at an appropriate capacity that is compatible\nwith existing models via backward-compatible learning. In this paper, we\npropose a Prunable Network with self-compatibility, which allows developers to\ngenerate compatible subnetworks at any desired capacity through post-training\npruning. Thus it allows the creation of a sparse subnetwork matching the\nresources of the new platform without additional training. Specifically, we\noptimize both the architecture and weight of subnetworks at different\ncapacities within a dense network in compatible learning. We also design a\nconflict-aware gradient integration scheme to handle the gradient conflicts\nbetween the dense network and subnetworks during compatible learning. Extensive\nexperiments on diverse benchmarks and visual backbones demonstrate the\neffectiveness of our method. Our code and model are available at\nhttps://github.com/Bunny-Black/PrunNet.\n","authors":["Yushuai Sun","Zikun Zhou","Dongmei Jiang","Yaowei Wang","Jun Yu","Guangming Lu","Wenjie Pei"],"pdf_url":"https://arxiv.org/pdf/2504.11879v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2504.11872v1","updated":"2025-04-16T08:49:22Z","published":"2025-04-16T08:49:22Z","title":"A Category-Fragment Segmentation Framework for Pelvic Fracture\n  Segmentation in X-ray Images","summary":"  Pelvic fractures, often caused by high-impact trauma, frequently require\nsurgical intervention. Imaging techniques such as CT and 2D X-ray imaging are\nused to transfer the surgical plan to the operating room through image\nregistration, enabling quick intraoperative adjustments. Specifically,\nsegmenting pelvic fractures from 2D X-ray imaging can assist in accurately\npositioning bone fragments and guiding the placement of screws or metal plates.\nIn this study, we propose a novel deep learning-based category and fragment\nsegmentation (CFS) framework for the automatic segmentation of pelvic bone\nfragments in 2D X-ray images. The framework consists of three consecutive\nsteps: category segmentation, fragment segmentation, and post-processing. Our\nbest model achieves an IoU of 0.91 for anatomical structures and 0.78 for\nfracture segmentation. Results demonstrate that the CFS framework is effective\nand accurate.\n","authors":["Daiqi Liu","Fuxin Fan","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2504.11872v1.pdf","comment":"5 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2401.00036v3","updated":"2025-04-16T08:46:04Z","published":"2023-12-29T18:35:04Z","title":"Discrete Distribution Networks","summary":"  We introduce a novel generative model, the Discrete Distribution Networks\n(DDN), that approximates data distribution using hierarchical discrete\ndistributions. We posit that since the features within a network inherently\ncapture distributional information, enabling the network to generate multiple\nsamples simultaneously, rather than a single output, may offer an effective way\nto represent distributions. Therefore, DDN fits the target distribution,\nincluding continuous ones, by generating multiple discrete sample points. To\ncapture finer details of the target data, DDN selects the output that is\nclosest to the Ground Truth (GT) from the coarse results generated in the first\nlayer. This selected output is then fed back into the network as a condition\nfor the second layer, thereby generating new outputs more similar to the GT. As\nthe number of DDN layers increases, the representational space of the outputs\nexpands exponentially, and the generated samples become increasingly similar to\nthe GT. This hierarchical output pattern of discrete distributions endows DDN\nwith unique properties: more general zero-shot conditional generation and 1D\nlatent representation. We demonstrate the efficacy of DDN and its intriguing\nproperties through experiments on CIFAR-10 and FFHQ. The code is available at\nhttps://discrete-distribution-networks.github.io/\n","authors":["Lei Yang"],"pdf_url":"https://arxiv.org/pdf/2401.00036v3.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2401.02044v5","updated":"2025-04-16T08:43:11Z","published":"2024-01-04T03:09:39Z","title":"Multi-modal vision-language model for generalizable annotation-free\n  pathology localization and clinical diagnosis","summary":"  Defining pathologies automatically from medical images aids the understanding\nof the emergence and progression of diseases, and such an ability is crucial in\nclinical diagnostics. However, existing deep learning models heavily rely on\nexpert annotations and lack generalization capabilities in open clinical\nenvironments. In this study, we present a generalizable vision-language model\nfor Annotation-Free pathology Localization (AFLoc). The core strength of AFLoc\nlies in its extensive multi-level semantic structure-based contrastive\nlearning, which comprehensively aligns multi-granularity medical concepts from\nreports with abundant image features, to adapt to the diverse expressions of\npathologies and unseen pathologies without the reliance on image annotations\nfrom experts. We conducted primary experiments on a dataset of 220K pairs of\nimage-report chest X-ray images, and performed extensive validation across six\nexternal datasets encompassing 20 types of chest pathologies. The results\ndemonstrate that AFLoc outperforms state-of-the-art methods in both\nannotation-free localization and classification tasks. Additionally, we\nassessed the generalizability of AFLoc on other modalities, including\nhistopathology and retinal fundus images. Extensive experiments show that AFLoc\nexhibits robust generalization capabilities, even surpassing human benchmarks\nin localizing five different types of pathological images. These results\nhighlight the potential of AFLoc in reducing annotation requirements and its\napplicability in complex clinical environments.\n","authors":["Hao Yang","Hong-Yu Zhou","Jiarun Liu","Weijian Huang","Zhihuan Li","Yuanxu Gao","Cheng Li","Qiegen Liu","Yong Liang","Qi Yang","Song Wu","Tao Tan","Hairong Zheng","Kang Zhang","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02044v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13541v3","updated":"2025-04-16T08:39:39Z","published":"2024-12-18T06:40:53Z","title":"Spatio-Temporal Fuzzy-oriented Multi-Modal Meta-Learning for\n  Fine-grained Emotion Recognition","summary":"  Fine-grained emotion recognition (FER) plays a vital role in various fields,\nsuch as disease diagnosis, personalized recommendations, and multimedia mining.\nHowever, existing FER methods face three key challenges in real-world\napplications: (i) they rely on large amounts of continuously annotated data to\nensure accuracy since emotions are complex and ambiguous in reality, which is\ncostly and time-consuming; (ii) they cannot capture the temporal heterogeneity\ncaused by changing emotion patterns, because they usually assume that the\ntemporal correlation within sampling periods is the same; (iii) they do not\nconsider the spatial heterogeneity of different FER scenarios, that is, the\ndistribution of emotion information in different data may have bias or\ninterference. To address these challenges, we propose a Spatio-Temporal\nFuzzy-oriented Multi-modal Meta-learning framework (ST-F2M). Specifically,\nST-F2M first divides the multi-modal videos into multiple views, and each view\ncorresponds to one modality of one emotion. Multiple randomly selected views\nfor the same emotion form a meta-training task. Next, ST-F2M uses an integrated\nmodule with spatial and temporal convolutions to encode the data of each task,\nreflecting the spatial and temporal heterogeneity. Then it adds fuzzy semantic\ninformation to each task based on generalized fuzzy rules, which helps handle\nthe complexity and ambiguity of emotions. Finally, ST-F2M learns\nemotion-related general meta-knowledge through meta-recurrent neural networks\nto achieve fast and robust fine-grained emotion recognition. Extensive\nexperiments show that ST-F2M outperforms various state-of-the-art methods in\nterms of accuracy and model efficiency. In addition, we construct ablation\nstudies and further analysis to explore why ST-F2M performs well.\n","authors":["Jingyao Wang","Wenwen Qiang","Changwen Zheng","Fuchun Sun"],"pdf_url":"https://arxiv.org/pdf/2412.13541v3.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2504.11858v1","updated":"2025-04-16T08:29:46Z","published":"2025-04-16T08:29:46Z","title":"Synthetic Data for Blood Vessel Network Extraction","summary":"  Blood vessel networks in the brain play a crucial role in stroke research,\nwhere understanding their topology is essential for analyzing blood flow\ndynamics. However, extracting detailed topological vessel network information\nfrom microscopy data remains a significant challenge, mainly due to the\nscarcity of labeled training data and the need for high topological accuracy.\nThis work combines synthetic data generation with deep learning to\nautomatically extract vessel networks as graphs from volumetric microscopy\ndata. To combat data scarcity, we introduce a comprehensive pipeline for\ngenerating large-scale synthetic datasets that mirror the characteristics of\nreal vessel networks. Our three-stage approach progresses from abstract graph\ngeneration through vessel mask creation to realistic medical image synthesis,\nincorporating biological constraints and imaging artifacts at each stage. Using\nthis synthetic data, we develop a two-stage deep learning pipeline of 3D\nU-Net-based models for node detection and edge prediction. Fine-tuning on real\nmicroscopy data shows promising adaptation, improving edge prediction F1 scores\nfrom 0.496 to 0.626 by training on merely 5 manually labeled samples. These\nresults suggest that automated vessel network extraction is becoming\npractically feasible, opening new possibilities for large-scale vascular\nanalysis in stroke research.\n","authors":["Joël Mathys","Andreas Plesner","Jorel Elmiger","Roger Wattenhofer"],"pdf_url":"https://arxiv.org/pdf/2504.11858v1.pdf","comment":"Presented at SynthData Workshop at ICLR 2025"},{"id":"http://arxiv.org/abs/2504.11856v1","updated":"2025-04-16T08:24:42Z","published":"2025-04-16T08:24:42Z","title":"Cross-Frequency Collaborative Training Network and Dataset for\n  Semi-supervised First Molar Root Canal Segmentation","summary":"  Root canal (RC) treatment is a highly delicate and technically complex\nprocedure in clinical practice, heavily influenced by the clinicians'\nexperience and subjective judgment. Deep learning has made significant\nadvancements in the field of computer-aided diagnosis (CAD) because it can\nprovide more objective and accurate diagnostic results. However, its\napplication in RC treatment is still relatively rare, mainly due to the lack of\npublic datasets in this field. To address this issue, in this paper, we\nestablished a First Molar Root Canal segmentation dataset called FMRC-2025.\nAdditionally, to alleviate the workload of manual annotation for dentists and\nfully leverage the unlabeled data, we designed a Cross-Frequency Collaborative\ntraining semi-supervised learning (SSL) Network called CFC-Net. It consists of\ntwo components: (1) Cross-Frequency Collaborative Mean Teacher (CFC-MT), which\nintroduces two specialized students (SS) and one comprehensive teacher (CT) for\ncollaborative multi-frequency training. The CT and SS are trained on different\nfrequency components while fully integrating multi-frequency knowledge through\ncross and full frequency consistency supervisions. (2) Uncertainty-guided\nCross-Frequency Mix (UCF-Mix) mechanism enables the network to generate\nhigh-confidence pseudo-labels while learning to integrate multi-frequency\ninformation and maintaining the structural integrity of the targets. Extensive\nexperiments on FMRC-2025 and three public dental datasets demonstrate that\nCFC-MT is effective for RC segmentation and can also exhibit strong\ngeneralizability on other dental segmentation tasks, outperforming\nstate-of-the-art SSL medical image segmentation methods. Codes and dataset will\nbe released.\n","authors":["Zhenhuan Zhou","Yuchen Zhang","Along He","Peng Wang","Xueshuo Xie","Tao Li"],"pdf_url":"https://arxiv.org/pdf/2504.11856v1.pdf","comment":"12 pages, Initial submission time 25 December 2024, Now Under Review"},{"id":"http://arxiv.org/abs/2504.11850v1","updated":"2025-04-16T08:16:28Z","published":"2025-04-16T08:16:28Z","title":"ACE: Attentional Concept Erasure in Diffusion Models","summary":"  Large text-to-image diffusion models have demonstrated remarkable image\nsynthesis capabilities, but their indiscriminate training on Internet-scale\ndata has led to learned concepts that enable harmful, copyrighted, or otherwise\nundesirable content generation. We address the task of concept erasure in\ndiffusion models, i.e., removing a specified concept from a pre-trained model\nsuch that prompting the concept (or related synonyms) no longer yields its\ndepiction, while preserving the model's ability to generate other content. We\npropose a novel method, Attentional Concept Erasure (ACE), that integrates a\nclosed-form attention manipulation with lightweight fine-tuning. Theoretically,\nwe formulate concept erasure as aligning the model's conditional distribution\non the target concept with a neutral distribution. Our approach identifies and\nnullifies concept-specific latent directions in the cross-attention modules via\na gated low-rank adaptation, followed by adversarially augmented fine-tuning to\nensure thorough erasure of the concept and its synonyms. Empirically, we\ndemonstrate on multiple benchmarks, including object classes, celebrity faces,\nexplicit content, and artistic styles, that ACE achieves state-of-the-art\nconcept removal efficacy and robustness. Compared to prior methods, ACE better\nbalances generality (erasing concept and related terms) and specificity\n(preserving unrelated content), scales to dozens of concepts, and is efficient,\nrequiring only a few seconds of adaptation per concept. We will release our\ncode to facilitate safer deployment of diffusion models.\n","authors":["Finn Carter"],"pdf_url":"https://arxiv.org/pdf/2504.11850v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2503.19653v3","updated":"2025-04-16T08:07:26Z","published":"2025-03-25T13:43:16Z","title":"OpenSDI: Spotting Diffusion-Generated Images in the Open World","summary":"  This paper identifies OpenSDI, a challenge for spotting diffusion-generated\nimages in open-world settings. In response to this challenge, we define a new\nbenchmark, the OpenSDI dataset (OpenSDID), which stands out from existing\ndatasets due to its diverse use of large vision-language models that simulate\nopen-world diffusion-based manipulations. Another outstanding feature of\nOpenSDID is its inclusion of both detection and localization tasks for images\nmanipulated globally and locally by diffusion models. To address the OpenSDI\nchallenge, we propose a Synergizing Pretrained Models (SPM) scheme to build up\na mixture of foundation models. This approach exploits a collaboration\nmechanism with multiple pretrained foundation models to enhance generalization\nin the OpenSDI context, moving beyond traditional training by synergizing\nmultiple pretrained models through prompting and attending strategies. Building\non this scheme, we introduce MaskCLIP, an SPM-based model that aligns\nContrastive Language-Image Pre-Training (CLIP) with Masked Autoencoder (MAE).\nExtensive evaluations on OpenSDID show that MaskCLIP significantly outperforms\ncurrent state-of-the-art methods for the OpenSDI challenge, achieving\nremarkable relative improvements of 14.23% in IoU (14.11% in F1) and 2.05% in\naccuracy (2.38% in F1) compared to the second-best model in localization and\ndetection tasks, respectively. Our dataset and code are available at\nhttps://github.com/iamwangyabin/OpenSDI.\n","authors":["Yabin Wang","Zhiwu Huang","Xiaopeng Hong"],"pdf_url":"https://arxiv.org/pdf/2503.19653v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11845v1","updated":"2025-04-16T08:07:09Z","published":"2025-04-16T08:07:09Z","title":"Boosting Multi-View Stereo with Depth Foundation Model in the Absence of\n  Real-World Labels","summary":"  Learning-based Multi-View Stereo (MVS) methods have made remarkable progress\nin recent years. However, how to effectively train the network without using\nreal-world labels remains a challenging problem. In this paper, driven by the\nrecent advancements of vision foundation models, a novel method termed DFM-MVS,\nis proposed to leverage the depth foundation model to generate the effective\ndepth prior, so as to boost MVS in the absence of real-world labels.\nSpecifically, a depth prior-based pseudo-supervised training mechanism is\ndeveloped to simulate realistic stereo correspondences using the generated\ndepth prior, thereby constructing effective supervision for the MVS network.\nBesides, a depth prior-guided error correction strategy is presented to\nleverage the depth prior as guidance to mitigate the error propagation problem\ninherent in the widely-used coarse-to-fine network structure. Experimental\nresults on DTU and Tanks & Temples datasets demonstrate that the proposed\nDFM-MVS significantly outperforms existing MVS methods without using real-world\nlabels.\n","authors":["Jie Zhu","Bo Peng","Zhe Zhang","Bingzheng Liu","Jianjun Lei"],"pdf_url":"https://arxiv.org/pdf/2504.11845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11838v1","updated":"2025-04-16T07:53:50Z","published":"2025-04-16T07:53:50Z","title":"A Visual RAG Pipeline for Few-Shot Fine-Grained Product Classification","summary":"  Despite the rapid evolution of learning and computer vision algorithms,\nFine-Grained Classification (FGC) still poses an open problem in many\npractically relevant applications. In the retail domain, for example, the\nidentification of fast changing and visually highly similar products and their\nproperties are key to automated price-monitoring and product recommendation.\nThis paper presents a novel Visual RAG pipeline that combines the Retrieval\nAugmented Generation (RAG) approach and Vision Language Models (VLMs) for\nfew-shot FGC. This Visual RAG pipeline extracts product and promotion data in\nadvertisement leaflets from various retailers and simultaneously predicts\nfine-grained product ids along with price and discount information. Compared to\nprevious approaches, the key characteristic of the Visual RAG pipeline is that\nit allows the prediction of novel products without re-training, simply by\nadding a few class samples to the RAG database. Comparing several VLM back-ends\nlike GPT-4o [23], GPT-4o-mini [24], and Gemini 2.0 Flash [10], our approach\nachieves 86.8% accuracy on a diverse dataset.\n","authors":["Bianca Lamm","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2504.11838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07847v2","updated":"2025-04-16T07:50:15Z","published":"2024-05-13T15:36:04Z","title":"SceneFactory: A Workflow-centric and Unified Framework for Incremental\n  Scene Modeling","summary":"  We present SceneFactory, a workflow-centric and unified framework for\nincremental scene modeling, that conveniently supports a wide range of\napplications, such as (unposed and/or uncalibrated) multi-view depth\nestimation, LiDAR completion, (dense) RGB-D/RGB-L/Mono/Depth-only\nreconstruction and SLAM. The workflow-centric design uses multiple blocks as\nthe basis for constructing different production lines. The supported\napplications, i.e., productions avoid redundancy in their designs. Thus, the\nfocus is placed on each block itself for independent expansion. To support all\ninput combinations, our implementation consists of four building blocks that\nform SceneFactory: (1) tracking, (2) flexion, (3) depth estimation, and (4)\nscene reconstruction. The tracking block is based on Mono SLAM and is extended\nto support RGB-D and RGB-LiDAR (RGB-L) inputs. Flexion is used to convert the\ndepth image (untrackable) into a trackable image. For general-purpose depth\nestimation, we propose an unposed \\& uncalibrated multi-view depth estimation\nmodel (U$^2$-MVD) to estimate dense geometry. U$^2$-MVD exploits dense bundle\nadjustment to solve for poses, intrinsics, and inverse depth. A semantic-aware\nScaleCov step is then introduced to complete the multi-view depth. Relying on\nU$^2$-MVD, SceneFactory both supports user-friendly 3D creation (with just\nimages) and bridges the applications of Dense RGB-D and Dense Mono. For\nhigh-quality surface and color reconstruction, we propose Dual-purpose\nMulti-resolutional Neural Points (DM-NPs) for the first surface accessible\nSurface Color Field design, where we introduce Improved Point Rasterization\n(IPR) for point cloud based surface query. ...\n","authors":["Yijun Yuan","Michael Bleier","Andreas Nüchter"],"pdf_url":"https://arxiv.org/pdf/2405.07847v2.pdf","comment":"Accepted to IEEE Transactions on Robotics (T-RO). For project page\n  and code, please find https://jarrome.github.io/SceneFactory/"},{"id":"http://arxiv.org/abs/2504.10001v3","updated":"2025-04-16T07:43:01Z","published":"2025-04-14T09:04:01Z","title":"GaussVideoDreamer: 3D Scene Generation with Video Diffusion and\n  Inconsistency-Aware Gaussian Splatting","summary":"  Single-image 3D scene reconstruction presents significant challenges due to\nits inherently ill-posed nature and limited input constraints. Recent advances\nhave explored two promising directions: multiview generative models that train\non 3D consistent datasets but struggle with out-of-distribution generalization,\nand 3D scene inpainting and completion frameworks that suffer from cross-view\ninconsistency and suboptimal error handling, as they depend exclusively on\ndepth data or 3D smoothness, which ultimately degrades output quality and\ncomputational performance. Building upon these approaches, we present\nGaussVideoDreamer, which advances generative multimedia approaches by bridging\nthe gap between image, video, and 3D generation, integrating their strengths\nthrough two key innovations: (1) A progressive video inpainting strategy that\nharnesses temporal coherence for improved multiview consistency and faster\nconvergence. (2) A 3D Gaussian Splatting consistency mask to guide the video\ndiffusion with 3D consistent multiview evidence. Our pipeline combines three\ncore components: a geometry-aware initialization protocol, Inconsistency-Aware\nGaussian Splatting, and a progressive video inpainting strategy. Experimental\nresults demonstrate that our approach achieves 32% higher LLaVA-IQA scores and\nat least 2x speedup compared to existing methods while maintaining robust\nperformance across diverse scenes.\n","authors":["Junlin Hao","Peiheng Wang","Haoyang Wang","Xinggong Zhang","Zongming Guo"],"pdf_url":"https://arxiv.org/pdf/2504.10001v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.22328v2","updated":"2025-04-16T07:36:24Z","published":"2025-03-28T11:06:27Z","title":"VoteFlow: Enforcing Local Rigidity in Self-Supervised Scene Flow","summary":"  Scene flow estimation aims to recover per-point motion from two adjacent\nLiDAR scans. However, in real-world applications such as autonomous driving,\npoints rarely move independently of others, especially for nearby points\nbelonging to the same object, which often share the same motion. Incorporating\nthis locally rigid motion constraint has been a key challenge in\nself-supervised scene flow estimation, which is often addressed by\npost-processing or appending extra regularization. While these approaches are\nable to improve the rigidity of predicted flows, they lack an architectural\ninductive bias for local rigidity within the model structure, leading to\nsuboptimal learning efficiency and inferior performance. In contrast, we\nenforce local rigidity with a lightweight add-on module in neural network\ndesign, enabling end-to-end learning. We design a discretized voting space that\naccommodates all possible translations and then identify the one shared by\nnearby points by differentiable voting. Additionally, to ensure computational\nefficiency, we operate on pillars rather than points and learn representative\nfeatures for voting per pillar. We plug the Voting Module into popular model\ndesigns and evaluate its benefit on Argoverse 2 and Waymo datasets. We\noutperform baseline works with only marginal compute overhead. Code is\navailable at https://github.com/tudelft-iv/VoteFlow.\n","authors":["Yancong Lin","Shiming Wang","Liangliang Nan","Julian Kooij","Holger Caesar"],"pdf_url":"https://arxiv.org/pdf/2503.22328v2.pdf","comment":"CVPR 2025. Code is available at\n  https://github.com/tudelft-iv/VoteFlow. Yancong Lin and Shiming Wang have\n  equal contributions"},{"id":"http://arxiv.org/abs/2504.11825v1","updated":"2025-04-16T07:17:36Z","published":"2025-04-16T07:17:36Z","title":"TextDiffSeg: Text-guided Latent Diffusion Model for 3d Medical Images\n  Segmentation","summary":"  Diffusion Probabilistic Models (DPMs) have demonstrated significant potential\nin 3D medical image segmentation tasks. However, their high computational cost\nand inability to fully capture global 3D contextual information limit their\npractical applications. To address these challenges, we propose a novel\ntext-guided diffusion model framework, TextDiffSeg. This method leverages a\nconditional diffusion framework that integrates 3D volumetric data with natural\nlanguage descriptions, enabling cross-modal embedding and establishing a shared\nsemantic space between visual and textual modalities. By enhancing the model's\nability to recognize complex anatomical structures, TextDiffSeg incorporates\ninnovative label embedding techniques and cross-modal attention mechanisms,\neffectively reducing computational complexity while preserving global 3D\ncontextual integrity. Experimental results demonstrate that TextDiffSeg\nconsistently outperforms existing methods in segmentation tasks involving\nkidney and pancreas tumors, as well as multi-organ segmentation scenarios.\nAblation studies further validate the effectiveness of key components,\nhighlighting the synergistic interaction between text fusion, image feature\nextractor, and label encoder. TextDiffSeg provides an efficient and accurate\nsolution for 3D medical image segmentation, showcasing its broad applicability\nin clinical diagnosis and treatment planning.\n","authors":["Kangbo Ma"],"pdf_url":"https://arxiv.org/pdf/2504.11825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11820v1","updated":"2025-04-16T07:14:01Z","published":"2025-04-16T07:14:01Z","title":"Real-World Depth Recovery via Structure Uncertainty Modeling and\n  Inaccurate GT Depth Fitting","summary":"  The low-quality structure in raw depth maps is prevalent in real-world RGB-D\ndatasets, which makes real-world depth recovery a critical task in recent\nyears. However, the lack of paired raw-ground truth (raw-GT) data in the real\nworld poses challenges for generalized depth recovery. Existing methods\ninsufficiently consider the diversity of structure misalignment in raw depth\nmaps, which leads to poor generalization in real-world depth recovery. Notably,\nrandom structure misalignments are not limited to raw depth data but also\naffect GT depth in real-world datasets. In the proposed method, we tackle the\ngeneralization problem from both input and output perspectives. For input, we\nenrich the diversity of structure misalignment in raw depth maps by designing a\nnew raw depth generation pipeline, which helps the network avoid overfitting to\na specific condition. Furthermore, a structure uncertainty module is designed\nto explicitly identify the misaligned structure for input raw depth maps to\nbetter generalize in unseen scenarios. Notably the well-trained depth\nfoundation model (DFM) can help the structure uncertainty module estimate the\nstructure uncertainty better. For output, a robust feature alignment module is\ndesigned to precisely align with the accurate structure of RGB images avoiding\nthe interference of inaccurate GT depth. Extensive experiments on multiple\ndatasets demonstrate the proposed method achieves competitive accuracy and\ngeneralization capabilities across various challenging raw depth maps.\n","authors":["Delong Suzhang","Meng Yang"],"pdf_url":"https://arxiv.org/pdf/2504.11820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.04846v2","updated":"2025-04-16T07:10:09Z","published":"2022-06-10T02:41:48Z","title":"Masked Autoencoders are Robust Data Augmentors","summary":"  Deep neural networks are capable of learning powerful representations to\ntackle complex vision tasks but expose undesirable properties like the\nover-fitting issue. To this end, regularization techniques like image\naugmentation are necessary for deep neural networks to generalize well.\nNevertheless, most prevalent image augmentation recipes confine themselves to\noff-the-shelf linear transformations like scale, flip, and colorjitter. Due to\ntheir hand-crafted property, these augmentations are insufficient to generate\ntruly hard augmented examples. In this paper, we propose a novel perspective of\naugmentation to regularize the training process. Inspired by the recent success\nof applying masked image modeling to self-supervised learning, we adopt the\nself-supervised masked autoencoder to generate the distorted view of the input\nimages. We show that utilizing such model-based nonlinear transformation as\ndata augmentation can improve high-level recognition tasks. We term the\nproposed method as \\textbf{M}ask-\\textbf{R}econstruct \\textbf{A}ugmentation\n(MRA). The extensive experiments on various image classification benchmarks\nverify the effectiveness of the proposed augmentation. Specifically, MRA\nconsistently enhances the performance on supervised, semi-supervised as well as\nfew-shot classification.\n","authors":["Haohang Xu","Shuangrui Ding","Manqi Zhao","Dongsheng Jiang"],"pdf_url":"https://arxiv.org/pdf/2206.04846v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11129v2","updated":"2025-04-16T06:16:13Z","published":"2025-03-14T06:44:01Z","title":"Direction-Aware Diagonal Autoregressive Image Generation","summary":"  The raster-ordered image token sequence exhibits a significant Euclidean\ndistance between index-adjacent tokens at line breaks, making it unsuitable for\nautoregressive generation. To address this issue, this paper proposes\nDirection-Aware Diagonal Autoregressive Image Generation (DAR) method, which\ngenerates image tokens following a diagonal scanning order. The proposed\ndiagonal scanning order ensures that tokens with adjacent indices remain in\nclose proximity while enabling causal attention to gather information from a\nbroader range of directions. Additionally, two direction-aware modules: 4D-RoPE\nand direction embeddings are introduced, enhancing the model's capability to\nhandle frequent changes in generation direction. To leverage the\nrepresentational capacity of the image tokenizer, we use its codebook as the\nimage token embeddings. We propose models of varying scales, ranging from 485M\nto 2.0B. On the 256$\\times$256 ImageNet benchmark, our DAR-XL (2.0B)\noutperforms all previous autoregressive image generators, achieving a\nstate-of-the-art FID score of 1.37.\n","authors":["Yijia Xu","Jianzhong Ju","Jian Luan","Jinshi Cui"],"pdf_url":"https://arxiv.org/pdf/2503.11129v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11798v1","updated":"2025-04-16T06:13:20Z","published":"2025-04-16T06:13:20Z","title":"Neighbor-Based Feature and Index Enhancement for Person\n  Re-Identification","summary":"  Person re-identification (Re-ID) aims to match the same pedestrian in a large\ngallery with different cameras and views. Enhancing the robustness of the\nextracted feature representations is a main challenge in Re-ID. Existing\nmethods usually improve feature representation by improving model architecture,\nbut most methods ignore the potential contextual information, which limits the\neffectiveness of feature representation and retrieval performance. Neighborhood\ninformation, especially the potential information of multi-order neighborhoods,\ncan effectively enrich feature expression and improve retrieval accuracy, but\nthis has not been fully explored in existing research. Therefore, we propose a\nnovel model DMON-ARO that leverages latent neighborhood information to enhance\nboth feature representation and index performance. Our approach is built on two\ncomplementary modules: Dynamic Multi-Order Neighbor Modeling (DMON) and\nAsymmetric Relationship Optimization (ARO). The DMON module dynamically\naggregates multi-order neighbor relationships, allowing it to capture richer\ncontextual information and enhance feature representation through adaptive\nneighborhood modeling. Meanwhile, ARO refines the distance matrix by optimizing\nquery-to-gallery relationships, improving the index accuracy. Extensive\nexperiments on three benchmark datasets demonstrate that our approach achieves\nperformance improvements against baseline models, which illustrate the\neffectiveness of our model. Specifically, our model demonstrates improvements\nin Rank-1 accuracy and mAP. Moreover, this method can also be directly extended\nto other re-identification tasks.\n","authors":["Chao Yuan","Tianyi Zhang","Guanglin Niu"],"pdf_url":"https://arxiv.org/pdf/2504.11798v1.pdf","comment":"Comment: This paper has been accepted for publication in the 2025\n  IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops\n  (CVPRW)"},{"id":"http://arxiv.org/abs/2504.10070v2","updated":"2025-04-16T06:02:02Z","published":"2025-04-14T10:17:25Z","title":"DTFSal: Audio-Visual Dynamic Token Fusion for Video Saliency Prediction","summary":"  Audio-visual saliency prediction aims to mimic human visual attention by\nidentifying salient regions in videos through the integration of both visual\nand auditory information. Although visual-only approaches have significantly\nadvanced, effectively incorporating auditory cues remains challenging due to\ncomplex spatio-temporal interactions and high computational demands. To address\nthese challenges, we propose Dynamic Token Fusion Saliency (DFTSal), a novel\naudio-visual saliency prediction framework designed to balance accuracy with\ncomputational efficiency. Our approach features a multi-scale visual encoder\nequipped with two novel modules: the Learnable Token Enhancement Block (LTEB),\nwhich adaptively weights tokens to emphasize crucial saliency cues, and the\nDynamic Learnable Token Fusion Block (DLTFB), which employs a shifting\noperation to reorganize and merge features, effectively capturing long-range\ndependencies and detailed spatial information. In parallel, an audio branch\nprocesses raw audio signals to extract meaningful auditory features. Both\nvisual and audio features are integrated using our Adaptive Multimodal Fusion\nBlock (AMFB), which employs local, global, and adaptive fusion streams for\nprecise cross-modal fusion. The resulting fused features are processed by a\nhierarchical multi-decoder structure, producing accurate saliency maps.\nExtensive evaluations on six audio-visual benchmarks demonstrate that DFTSal\nachieves SOTA performance while maintaining computational efficiency.\n","authors":["Kiana Hooshanfar","Alireza Hosseini","Ahmad Kalhor","Babak Nadjar Araabi"],"pdf_url":"https://arxiv.org/pdf/2504.10070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11786v1","updated":"2025-04-16T05:39:08Z","published":"2025-04-16T05:39:08Z","title":"DART: Disease-aware Image-Text Alignment and Self-correcting\n  Re-alignment for Trustworthy Radiology Report Generation","summary":"  The automatic generation of radiology reports has emerged as a promising\nsolution to reduce a time-consuming task and accurately capture critical\ndisease-relevant findings in X-ray images. Previous approaches for radiology\nreport generation have shown impressive performance. However, there remains\nsignificant potential to improve accuracy by ensuring that retrieved reports\ncontain disease-relevant findings similar to those in the X-ray images and by\nrefining generated reports. In this study, we propose a Disease-aware\nimage-text Alignment and self-correcting Re-alignment for Trustworthy radiology\nreport generation (DART) framework. In the first stage, we generate initial\nreports based on image-to-text retrieval with disease-matching, embedding both\nimages and texts in a shared embedding space through contrastive learning. This\napproach ensures the retrieval of reports with similar disease-relevant\nfindings that closely align with the input X-ray images. In the second stage,\nwe further enhance the initial reports by introducing a self-correction module\nthat re-aligns them with the X-ray images. Our proposed framework achieves\nstate-of-the-art results on two widely used benchmarks, surpassing previous\napproaches in both report generation and clinical efficacy metrics, thereby\nenhancing the trustworthiness of radiology reports.\n","authors":["Sang-Jun Park","Keun-Soo Heo","Dong-Hee Shin","Young-Han Son","Ji-Hye Oh","Tae-Eui Kam"],"pdf_url":"https://arxiv.org/pdf/2504.11786v1.pdf","comment":"The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025"},{"id":"http://arxiv.org/abs/2412.17619v2","updated":"2025-04-16T05:35:48Z","published":"2024-12-23T14:43:51Z","title":"Kernel-Aware Graph Prompt Learning for Few-Shot Anomaly Detection","summary":"  Few-shot anomaly detection (FSAD) aims to detect unseen anomaly regions with\nthe guidance of very few normal support images from the same class. Existing\nFSAD methods usually find anomalies by directly designing complex text prompts\nto align them with visual features under the prevailing large vision-language\nmodel paradigm. However, these methods, almost always, neglect intrinsic\ncontextual information in visual features, e.g., the interaction relationships\nbetween different vision layers, which is an important clue for detecting\nanomalies comprehensively. To this end, we propose a kernel-aware graph prompt\nlearning framework, termed as KAG-prompt, by reasoning the cross-layer\nrelations among visual features for FSAD. Specifically, a kernel-aware\nhierarchical graph is built by taking the different layer features focusing on\nanomalous regions of different sizes as nodes, meanwhile, the relationships\nbetween arbitrary pairs of nodes stand for the edges of the graph. By message\npassing over this graph, KAG-prompt can capture cross-layer contextual\ninformation, thus leading to more accurate anomaly prediction. Moreover, to\nintegrate the information of multiple important anomaly signals in the\nprediction map, we propose a novel image-level scoring method based on\nmulti-level information fusion. Extensive experiments on MVTecAD and VisA\ndatasets show that KAG-prompt achieves state-of-the-art FSAD results for\nimage-level/pixel-level anomaly detection. Code is available at\nhttps://github.com/CVL-hub/KAG-prompt.git.\n","authors":["Fenfang Tao","Guo-Sen Xie","Fang Zhao","Xiangbo Shu"],"pdf_url":"https://arxiv.org/pdf/2412.17619v2.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2504.11781v1","updated":"2025-04-16T05:33:42Z","published":"2025-04-16T05:33:42Z","title":"ACMamba: Fast Unsupervised Anomaly Detection via An Asymmetrical\n  Consensus State Space Model","summary":"  Unsupervised anomaly detection in hyperspectral images (HSI), aiming to\ndetect unknown targets from backgrounds, is challenging for earth surface\nmonitoring. However, current studies are hindered by steep computational costs\ndue to the high-dimensional property of HSI and dense sampling-based training\nparadigm, constraining their rapid deployment. Our key observation is that,\nduring training, not all samples within the same homogeneous area are\nindispensable, whereas ingenious sampling can provide a powerful substitute for\nreducing costs. Motivated by this, we propose an Asymmetrical Consensus State\nSpace Model (ACMamba) to significantly reduce computational costs without\ncompromising accuracy. Specifically, we design an asymmetrical anomaly\ndetection paradigm that utilizes region-level instances as an efficient\nalternative to dense pixel-level samples. In this paradigm, a low-cost\nMamba-based module is introduced to discover global contextual attributes of\nregions that are essential for HSI reconstruction. Additionally, we develop a\nconsensus learning strategy from the optimization perspective to simultaneously\nfacilitate background reconstruction and anomaly compression, further\nalleviating the negative impact of anomaly reconstruction. Theoretical analysis\nand extensive experiments across eight benchmarks verify the superiority of\nACMamba, demonstrating a faster speed and stronger performance over the\nstate-of-the-art.\n","authors":["Guanchun Wang","Xiangrong Zhang","Yifei Zhang","Zelin Peng","Tianyang Zhang","Xu Tang","Licheng Jiao"],"pdf_url":"https://arxiv.org/pdf/2504.11781v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2504.11779v1","updated":"2025-04-16T05:32:59Z","published":"2025-04-16T05:32:59Z","title":"Multimodal Spatio-temporal Graph Learning for Alignment-free RGBT Video\n  Object Detection","summary":"  RGB-Thermal Video Object Detection (RGBT VOD) can address the limitation of\ntraditional RGB-based VOD in challenging lighting conditions, making it more\npractical and effective in many applications.\n  However, similar to most RGBT fusion tasks, it still mainly relies on\nmanually aligned multimodal image pairs.\n  In this paper, we propose a novel Multimodal Spatio-temporal Graph learning\nNetwork (MSGNet) for alignment-free RGBT VOD problem by leveraging the robust\ngraph representation learning model.\n  Specifically, we first design an Adaptive Partitioning Layer (APL) to\nestimate the corresponding regions of the Thermal image within the RGB image\n(high-resolution), achieving a preliminary inexact alignment.\n  Then, we introduce the Spatial Sparse Graph Learning Module (S-SGLM) which\nemploys a sparse information passing mechanism on the estimated inexact\nalignment to achieve reliable information interaction between different\nmodalities.\n  Moreover, to fully exploit the temporal cues for RGBT VOD problem, we\nintroduce Hybrid Structured Temporal Modeling (HSTM), which involves a Temporal\nSparse Graph Learning Module (T-SGLM) and Temporal Star Block (TSB). T-SGLM\naims to filter out some redundant information between adjacent frames by\nemploying the sparse aggregation mechanism on the temporal graph. Meanwhile,\nTSB is dedicated to achieving the complementary learning of local spatial\nrelationships.\n  Extensive comparative experiments conducted on both the aligned dataset\nVT-VOD50 and the unaligned dataset UVT-VOD2024 demonstrate the effectiveness\nand superiority of our proposed method. Our project will be made available on\nour website for free public access.\n","authors":["Qishun Wang","Zhengzheng Tu","Chenglong Li","Bo Jiang"],"pdf_url":"https://arxiv.org/pdf/2504.11779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11777v1","updated":"2025-04-16T05:31:18Z","published":"2025-04-16T05:31:18Z","title":"Bridging the Semantic Gaps: Improving Medical VQA Consistency with\n  LLM-Augmented Question Sets","summary":"  Medical Visual Question Answering (MVQA) systems can interpret medical images\nin response to natural language queries. However, linguistic variability in\nquestion phrasing often undermines the consistency of these systems. To address\nthis challenge, we propose a Semantically Equivalent Question Augmentation\n(SEQA) framework, which leverages large language models (LLMs) to generate\ndiverse yet semantically equivalent rephrasings of questions. Specifically,\nthis approach enriches linguistic diversity while preserving semantic meaning.\nWe further introduce an evaluation metric, Total Agreement Rate with\nSemantically Equivalent Input and Correct Answer (TAR-SC), which assesses a\nmodel's capability to generate consistent and correct responses to semantically\nequivalent linguistic variations. In addition, we also propose three other\ndiversity metrics - average number of QA items per image (ANQI), average number\nof questions per image with the same answer (ANQA), and average number of\nopen-ended questions per image with the same semantics (ANQS). Using the SEQA\nframework, we augmented the benchmarked MVQA public datasets of SLAKE, VQA-RAD,\nand PathVQA. As a result, all three datasets achieved significant improvements\nby incorporating more semantically equivalent questions: ANQI increased by an\naverage of 86.1, ANQA by 85.1, and ANQS by 46. Subsequent experiments evaluate\nthree MVQA models (M2I2, MUMC, and BiomedGPT) under both zero-shot and\nfine-tuning settings on the enhanced datasets. Experimental results in MVQA\ndatasets show that fine-tuned models achieve an average accuracy improvement of\n19.35%, while our proposed TAR-SC metric shows an average improvement of 11.\n61%, indicating a substantial enhancement in model consistency.\n","authors":["Yongpei Ma","Pengyu Wang","Adam Dunn","Usman Naseem","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2504.11777v1.pdf","comment":"The first two listed authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2504.11773v1","updated":"2025-04-16T05:25:04Z","published":"2025-04-16T05:25:04Z","title":"TacoDepth: Towards Efficient Radar-Camera Depth Estimation with\n  One-stage Fusion","summary":"  Radar-Camera depth estimation aims to predict dense and accurate metric depth\nby fusing input images and Radar data. Model efficiency is crucial for this\ntask in pursuit of real-time processing on autonomous vehicles and robotic\nplatforms. However, due to the sparsity of Radar returns, the prevailing\nmethods adopt multi-stage frameworks with intermediate quasi-dense depth, which\nare time-consuming and not robust. To address these challenges, we propose\nTacoDepth, an efficient and accurate Radar-Camera depth estimation model with\none-stage fusion. Specifically, the graph-based Radar structure extractor and\nthe pyramid-based Radar fusion module are designed to capture and integrate the\ngraph structures of Radar point clouds, delivering superior model efficiency\nand robustness without relying on the intermediate depth results. Moreover,\nTacoDepth can be flexible for different inference modes, providing a better\nbalance of speed and accuracy. Extensive experiments are conducted to\ndemonstrate the efficacy of our method. Compared with the previous\nstate-of-the-art approach, TacoDepth improves depth accuracy and processing\nspeed by 12.8% and 91.8%. Our work provides a new perspective on efficient\nRadar-Camera depth estimation.\n","authors":["Yiran Wang","Jiaqi Li","Chaoyi Hong","Ruibo Li","Liusheng Sun","Xiao Song","Zhe Wang","Zhiguo Cao","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2504.11773v1.pdf","comment":"Accepted by CVPR 2025 (Oral Presentation)"},{"id":"http://arxiv.org/abs/2504.10143v2","updated":"2025-04-16T05:22:32Z","published":"2025-04-14T11:54:19Z","title":"Negate or Embrace: On How Misalignment Shapes Multimodal Representation\n  Learning","summary":"  Multimodal representation learning, exemplified by multimodal contrastive\nlearning (MMCL) using image-text pairs, aims to learn powerful representations\nby aligning cues across modalities. This approach relies on the core assumption\nthat the exemplar image-text pairs constitute two representations of an\nidentical concept. However, recent research has revealed that real-world\ndatasets often exhibit misalignment. There are two distinct viewpoints on how\nto address this issue: one suggests mitigating the misalignment, and the other\nleveraging it. We seek here to reconcile these seemingly opposing perspectives,\nand to provide a practical guide for practitioners. Using latent variable\nmodels we thus formalize misalignment by introducing two specific mechanisms:\nselection bias, where some semantic variables are missing, and perturbation\nbias, where semantic variables are distorted -- both affecting latent variables\nshared across modalities. Our theoretical analysis demonstrates that, under\nmild assumptions, the representations learned by MMCL capture exactly the\ninformation related to the subset of the semantic variables invariant to\nselection and perturbation biases. This provides a unified perspective for\nunderstanding misalignment. Based on this, we further offer actionable insights\ninto how misalignment should inform the design of real-world ML systems. We\nvalidate our theoretical findings through extensive empirical studies on both\nsynthetic data and real image-text datasets, shedding light on the nuanced\nimpact of misalignment on multimodal representation learning.\n","authors":["Yichao Cai","Yuhang Liu","Erdun Gao","Tianjiao Jiang","Zhen Zhang","Anton van den Hengel","Javen Qinfeng Shi"],"pdf_url":"https://arxiv.org/pdf/2504.10143v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11218v2","updated":"2025-04-16T05:03:44Z","published":"2025-04-15T14:21:47Z","title":"3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians","summary":"  3D affordance reasoning is essential in associating human instructions with\nthe functional regions of 3D objects, facilitating precise, task-oriented\nmanipulations in embodied AI. However, current methods, which predominantly\ndepend on sparse 3D point clouds, exhibit limited generalizability and\nrobustness due to their sensitivity to coordinate variations and the inherent\nsparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers\nhigh-fidelity, real-time rendering with minimal computational overhead by\nrepresenting scenes as dense, continuous distributions. This positions 3DGS as\na highly effective approach for capturing fine-grained affordance details and\nimproving recognition accuracy. Nevertheless, its full potential remains\nlargely untapped due to the absence of large-scale, 3DGS-specific affordance\ndatasets. To overcome these limitations, we present 3DAffordSplat, the first\nlarge-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning.\nThis dataset includes 23,677 Gaussian instances, 8,354 point cloud instances,\nand 6,631 manually annotated affordance labels, encompassing 21 object\ncategories and 18 affordance types. Building upon this dataset, we introduce\nAffordSplatNet, a novel model specifically designed for affordance reasoning\nusing 3DGS representations. AffordSplatNet features an innovative cross-modal\nstructure alignment module that exploits structural consistency priors to align\n3D point cloud and 3DGS representations, resulting in enhanced affordance\nrecognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat\ndataset significantly advances affordance learning within the 3DGS domain,\nwhile AffordSplatNet consistently outperforms existing methods across both seen\nand unseen settings, highlighting its robust generalization capabilities.\n","authors":["Zeming Wei","Junyi Lin","Yang Liu","Weixing Chen","Jingzhou Luo","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2504.11218v2.pdf","comment":"The first large-scale 3D Gaussians Affordance Reasoning Benchmark"},{"id":"http://arxiv.org/abs/2504.11763v1","updated":"2025-04-16T04:56:01Z","published":"2025-04-16T04:56:01Z","title":"Extended Short- and Long-Range Mesh Learning for Fast and Generalized\n  Garment Simulation","summary":"  3D garment simulation is a critical component for producing cloth-based\ngraphics. Recent advancements in graph neural networks (GNNs) offer a promising\napproach for efficient garment simulation. However, GNNs require extensive\nmessage-passing to propagate information such as physical forces and maintain\ncontact awareness across the entire garment mesh, which becomes computationally\ninefficient at higher resolutions. To address this, we devise a novel GNN-based\nmesh learning framework with two key components to extend the message-passing\nrange with minimal overhead, namely the Laplacian-Smoothed Dual Message-Passing\n(LSDMP) and the Geodesic Self-Attention (GSA) modules. LSDMP enhances\nmessage-passing with a Laplacian features smoothing process, which efficiently\npropagates the impact of each vertex to nearby vertices. Concurrently, GSA\nintroduces geodesic distance embeddings to represent the spatial relationship\nbetween vertices and utilises attention mechanisms to capture global mesh\ninformation. The two modules operate in parallel to ensure both short- and\nlong-range mesh modelling. Extensive experiments demonstrate the\nstate-of-the-art performance of our method, requiring fewer layers and lower\ninference latency.\n","authors":["Aoran Liu","Kun Hu","Clinton Mo","Changyang Li","Zhiyong Wang"],"pdf_url":"https://arxiv.org/pdf/2504.11763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.06803v2","updated":"2025-04-16T04:46:22Z","published":"2025-04-09T11:48:37Z","title":"DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation","summary":"  Diffusion Transformer (DiT), an emerging diffusion model for visual\ngeneration, has demonstrated superior performance but suffers from substantial\ncomputational costs. Our investigations reveal that these costs primarily stem\nfrom the \\emph{static} inference paradigm, which inevitably introduces\nredundant computation in certain \\emph{diffusion timesteps} and \\emph{spatial\nregions}. To overcome this inefficiency, we propose \\textbf{Dy}namic\n\\textbf{Di}ffusion \\textbf{T}ransformer (DyDiT), an architecture that\n\\emph{dynamically} adjusts its computation along both \\emph{timestep} and\n\\emph{spatial} dimensions. Specifically, we introduce a \\emph{Timestep-wise\nDynamic Width} (TDW) approach that adapts model width conditioned on the\ngeneration timesteps. In addition, we design a \\emph{Spatial-wise Dynamic\nToken} (SDT) strategy to avoid redundant computation at unnecessary spatial\nlocations. TDW and SDT can be seamlessly integrated into DiT and significantly\naccelerates the generation process. Building on these designs, we further\nenhance DyDiT in three key aspects. First, DyDiT is integrated seamlessly with\nflow matching-based generation, enhancing its versatility. Furthermore, we\nenhance DyDiT to tackle more complex visual generation tasks, including video\ngeneration and text-to-image generation, thereby broadening its real-world\napplications. Finally, to address the high cost of full fine-tuning and\ndemocratize technology access, we investigate the feasibility of training DyDiT\nin a parameter-efficient manner and introduce timestep-based dynamic LoRA\n(TD-LoRA). Extensive experiments on diverse visual generation models, including\nDiT, SiT, Latte, and FLUX, demonstrate the effectiveness of DyDiT.\n","authors":["Wangbo Zhao","Yizeng Han","Jiasheng Tang","Kai Wang","Hao Luo","Yibing Song","Gao Huang","Fan Wang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2504.06803v2.pdf","comment":"Extended journal version for ICLR. arXiv admin note: substantial text\n  overlap with arXiv:2410.03456"},{"id":"http://arxiv.org/abs/2504.10889v2","updated":"2025-04-16T04:29:02Z","published":"2025-04-15T05:47:09Z","title":"Fine-Grained Rib Fracture Diagnosis with Hyperbolic Embeddings: A\n  Detailed Annotation Framework and Multi-Label Classification Model","summary":"  Accurate rib fracture identification and classification are essential for\ntreatment planning. However, existing datasets often lack fine-grained\nannotations, particularly regarding rib fracture characterization, type, and\nprecise anatomical location on individual ribs. To address this, we introduce a\nnovel rib fracture annotation protocol tailored for fracture classification.\nFurther, we enhance fracture classification by leveraging cross-modal\nembeddings that bridge radiological images and clinical descriptions. Our\napproach employs hyperbolic embeddings to capture the hierarchical nature of\nfracture, mapping visual features and textual descriptions into a shared\nnon-Euclidean manifold. This framework enables more nuanced similarity\ncomputations between imaging characteristics and clinical descriptions,\naccounting for the inherent hierarchical relationships in fracture taxonomy.\nExperimental results demonstrate that our approach outperforms existing methods\nacross multiple classification tasks, with average recall improvements of 6% on\nthe AirRib dataset and 17.5% on the public RibFrac dataset.\n","authors":["Shripad Pate","Aiman Farooq","Suvrankar Datta","Musadiq Aadil Sheikh","Atin Kumar","Deepak Mishra"],"pdf_url":"https://arxiv.org/pdf/2504.10889v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11347v2","updated":"2025-04-16T04:26:29Z","published":"2025-04-15T16:20:00Z","title":"DeepWheel: Generating a 3D Synthetic Wheel Dataset for Design and\n  Performance Evaluation","summary":"  Data-driven design is emerging as a powerful strategy to accelerate\nengineering innovation. However, its application to vehicle wheel design\nremains limited due to the lack of large-scale, high-quality datasets that\ninclude 3D geometry and physical performance metrics. To address this gap, this\nstudy proposes a synthetic design-performance dataset generation framework\nusing generative AI. The proposed framework first generates 2D rendered images\nusing Stable Diffusion, and then reconstructs the 3D geometry through 2.5D\ndepth estimation. Structural simulations are subsequently performed to extract\nengineering performance data. To further expand the design and performance\nspace, topology optimization is applied, enabling the generation of a more\ndiverse set of wheel designs. The final dataset, named DeepWheel, consists of\nover 6,000 photo-realistic images and 900 structurally analyzed 3D models. This\nmulti-modal dataset serves as a valuable resource for surrogate model training,\ndata-driven inverse design, and design space exploration. The proposed\nmethodology is also applicable to other complex design domains. The dataset is\nreleased under the Creative Commons Attribution-NonCommercial 4.0\nInternational(CC BY-NC 4.0) and is available on the\nhttps://www.smartdesignlab.org/datasets\n","authors":["Soyoung Yoo","Namwoo Kang"],"pdf_url":"https://arxiv.org/pdf/2504.11347v2.pdf","comment":"28 pages, 18 figures. Not yet submitted to a journal or conference"},{"id":"http://arxiv.org/abs/2504.11754v1","updated":"2025-04-16T04:13:53Z","published":"2025-04-16T04:13:53Z","title":"GrabS: Generative Embodied Agent for 3D Object Segmentation without\n  Scene Supervision","summary":"  We study the hard problem of 3D object segmentation in complex point clouds\nwithout requiring human labels of 3D scenes for supervision. By relying on the\nsimilarity of pretrained 2D features or external signals such as motion to\ngroup 3D points as objects, existing unsupervised methods are usually limited\nto identifying simple objects like cars or their segmented objects are often\ninferior due to the lack of objectness in pretrained features. In this paper,\nwe propose a new two-stage pipeline called GrabS. The core concept of our\nmethod is to learn generative and discriminative object-centric priors as a\nfoundation from object datasets in the first stage, and then design an embodied\nagent to learn to discover multiple objects by querying against the pretrained\ngenerative priors in the second stage. We extensively evaluate our method on\ntwo real-world datasets and a newly created synthetic dataset, demonstrating\nremarkable segmentation performance, clearly surpassing all existing\nunsupervised methods.\n","authors":["Zihui Zhang","Yafei Yang","Hongtao Wen","Bo Yang"],"pdf_url":"https://arxiv.org/pdf/2504.11754v1.pdf","comment":"ICLR 2025 Spotlight. Code and data are available at:\n  https://github.com/vLAR-group/GrabS"},{"id":"http://arxiv.org/abs/2504.11749v1","updated":"2025-04-16T04:01:42Z","published":"2025-04-16T04:01:42Z","title":"SkeletonX: Data-Efficient Skeleton-based Action Recognition via\n  Cross-sample Feature Aggregation","summary":"  While current skeleton action recognition models demonstrate impressive\nperformance on large-scale datasets, their adaptation to new application\nscenarios remains challenging. These challenges are particularly pronounced\nwhen facing new action categories, diverse performers, and varied skeleton\nlayouts, leading to significant performance degeneration. Additionally, the\nhigh cost and difficulty of collecting skeleton data make large-scale data\ncollection impractical. This paper studies one-shot and limited-scale learning\nsettings to enable efficient adaptation with minimal data. Existing approaches\noften overlook the rich mutual information between labeled samples, resulting\nin sub-optimal performance in low-data scenarios. To boost the utility of\nlabeled data, we identify the variability among performers and the commonality\nwithin each action as two key attributes. We present SkeletonX, a lightweight\ntraining pipeline that integrates seamlessly with existing GCN-based skeleton\naction recognizers, promoting effective training under limited labeled data.\nFirst, we propose a tailored sample pair construction strategy on two key\nattributes to form and aggregate sample pairs. Next, we develop a concise and\neffective feature aggregation module to process these pairs. Extensive\nexperiments are conducted on NTU RGB+D, NTU RGB+D 120, and PKU-MMD with various\nGCN backbones, demonstrating that the pipeline effectively improves performance\nwhen trained from scratch with limited data. Moreover, it surpasses previous\nstate-of-the-art methods in the one-shot setting, with only 1/10 of the\nparameters and much fewer FLOPs. The code and data are available at:\nhttps://github.com/zzysteve/SkeletonX\n","authors":["Zongye Zhang","Wenrui Cai","Qingjie Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2504.11749v1.pdf","comment":"Accepted by IEEE Transactions on Multimedia (TMM). 13 pages, 7\n  figures, 11 tables"},{"id":"http://arxiv.org/abs/2504.11739v1","updated":"2025-04-16T03:33:25Z","published":"2025-04-16T03:33:25Z","title":"The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for\n  Text-to-Video Generation","summary":"  The evolution of Text-to-video (T2V) generative models, trained on\nlarge-scale datasets, has been marked by significant progress. However, the\nsensitivity of T2V generative models to input prompts highlights the critical\nrole of prompt design in influencing generative outcomes. Prior research has\npredominantly relied on Large Language Models (LLMs) to align user-provided\nprompts with the distribution of training prompts, albeit without tailored\nguidance encompassing prompt vocabulary and sentence structure nuances. To this\nend, we introduce \\textbf{RAPO}, a novel \\textbf{R}etrieval-\\textbf{A}ugmented\n\\textbf{P}rompt \\textbf{O}ptimization framework. In order to address potential\ninaccuracies and ambiguous details generated by LLM-generated prompts. RAPO\nrefines the naive prompts through dual optimization branches, selecting the\nsuperior prompt for T2V generation. The first branch augments user prompts with\ndiverse modifiers extracted from a learned relational graph, refining them to\nalign with the format of training prompts via a fine-tuned LLM. Conversely, the\nsecond branch rewrites the naive prompt using a pre-trained LLM following a\nwell-defined instruction set. Extensive experiments demonstrate that RAPO can\neffectively enhance both the static and dynamic dimensions of generated videos,\ndemonstrating the significance of prompt optimization for user-provided\nprompts. Project website:\n\\href{https://whynothaha.github.io/Prompt_optimizer/RAPO.html}{GitHub}.\n","authors":["Bingjie Gao","Xinyu Gao","Xiaoxue Wu","Yujie Zhou","Yu Qiao","Li Niu","Xinyuan Chen","Yaohui Wang"],"pdf_url":"https://arxiv.org/pdf/2504.11739v1.pdf","comment":"accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2504.11101v2","updated":"2025-04-16T03:22:14Z","published":"2025-04-15T11:51:18Z","title":"Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and\n  Self-Improving OCR","summary":"  The Optical Character Recognition (OCR) task is important for evaluating\nVision-Language Models (VLMs) and providing high-quality data sources for LLM\ntraining data. While state-of-the-art VLMs show improved average OCR accuracy,\nthey still struggle with sample-level quality degradation and lack reliable\nautomatic detection of low-quality outputs. We introduce Consensus Entropy\n(CE), a training-free post-inference method that quantifies OCR uncertainty by\naggregating outputs from multiple VLMs. Our approach exploits a key insight:\ncorrect VLM OCR predictions converge in output space while errors diverge. We\ndevelop a lightweight multi-model framework that effectively identifies\nproblematic samples, selects the best outputs and combines model strengths.\nExperiments across multiple OCR benchmarks and VLMs demonstrate that CE\noutperforms VLM-as-judge approaches and single-model baselines at the same cost\nand achieves state-of-the-art results across multiple metrics. For instance,\nour solution demonstrates: achieving 15.2% higher F1 scores than VLM-as-judge\nmethods in quality verification, delivering 6.0% accuracy gains on mathematical\ncalculation tasks, and requiring rephrasing only 7.3% of inputs while\nmaintaining overall performance. Notably, the entire process requires neither\ntraining nor supervision while maintaining plug-and-play functionality\nthroughout.\n","authors":["Yulong Zhang","Tianyi Liang","Xinyue Huang","Erfei Cui","Xu Guo","Pei Chu","Chenhui Li","Ru Zhang","Wenhai Wang","Gongshen Liu"],"pdf_url":"https://arxiv.org/pdf/2504.11101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11734v1","updated":"2025-04-16T03:22:06Z","published":"2025-04-16T03:22:06Z","title":"Recent Advance in 3D Object and Scene Generation: A Survey","summary":"  In recent years, the demand for 3D content has grown exponentially with\nintelligent upgrading of interactive media, extended reality (XR), and\nMetaverse industries. In order to overcome the limitation of traditional manual\nmodeling approaches, such as labor-intensive workflows and prolonged production\ncycles, revolutionary advances have been achieved through the convergence of\nnovel 3D representation paradigms and artificial intelligence generative\ntechnologies. In this survey, we conduct a systematically review of the\ncutting-edge achievements in static 3D object and scene generation, as well as\nestablish a comprehensive technical framework through systematic\ncategorization. Specifically, we initiate our analysis with mainstream 3D\nobject representations, followed by in-depth exploration of two principal\ntechnical pathways in object generation: data-driven supervised learning\nmethods and deep generative model-based approaches. Regarding scene generation,\nwe focus on three dominant paradigms: layout-guided compositional synthesis, 2D\nprior-based scene generation, and rule-driven modeling. Finally, we critically\nexamine persistent challenges in 3D generation and propose potential research\ndirections for future investigation. This survey aims to provide readers with a\nstructured understanding of state-of-the-art 3D generation technologies while\ninspiring researchers to undertake more exploration in this domain.\n","authors":["Xiang Tang","Ruotong Li","Xiaopeng Fan"],"pdf_url":"https://arxiv.org/pdf/2504.11734v1.pdf","comment":"34 pages, 6 figures"},{"id":"http://arxiv.org/abs/2504.11733v1","updated":"2025-04-16T03:20:28Z","published":"2025-04-16T03:20:28Z","title":"DVLTA-VQA: Decoupled Vision-Language Modeling with Text-Guided\n  Adaptation for Blind Video Quality Assessment","summary":"  Inspired by the dual-stream theory of the human visual system (HVS) - where\nthe ventral stream is responsible for object recognition and detail analysis,\nwhile the dorsal stream focuses on spatial relationships and motion perception\n- an increasing number of video quality assessment (VQA) works built upon this\nframework are proposed. Recent advancements in large multi-modal models,\nnotably Contrastive Language-Image Pretraining (CLIP), have motivated\nresearchers to incorporate CLIP into dual-stream-based VQA methods. This\nintegration aims to harness the model's superior semantic understanding\ncapabilities to replicate the object recognition and detail analysis in ventral\nstream, as well as spatial relationship analysis in dorsal stream. However,\nCLIP is originally designed for images and lacks the ability to capture\ntemporal and motion information inherent in videos. %Furthermore, existing\nfeature fusion strategies in no-reference video quality assessment (NR-VQA)\noften rely on fixed weighting schemes, which fail to adaptively adjust feature\nimportance. To address the limitation, this paper propose a Decoupled\nVision-Language Modeling with Text-Guided Adaptation for Blind Video Quality\nAssessment (DVLTA-VQA), which decouples CLIP's visual and textual components,\nand integrates them into different stages of the NR-VQA pipeline.\n","authors":["Li Yu","Situo Wang","Wei Zhou","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2504.11733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16082v3","updated":"2025-04-16T03:14:30Z","published":"2023-08-30T15:14:56Z","title":"SignDiff: Diffusion Model for American Sign Language Production","summary":"  In this paper, we propose a dual-condition diffusion pre-training model named\nSignDiff that can generate human sign language speakers from a skeleton pose.\nSignDiff has a novel Frame Reinforcement Network called FR-Net, similar to\ndense human pose estimation work, which enhances the correspondence between\ntext lexical symbols and sign language dense pose frames, reduces the\noccurrence of multiple fingers in the diffusion model. In addition, we propose\na new method for American Sign Language Production (ASLP), which can generate\nASL skeletal pose videos from text input, integrating two new improved modules\nand a new loss function to improve the accuracy and quality of sign language\nskeletal posture and enhance the ability of the model to train on large-scale\ndata. We propose the first baseline for ASL production and report the scores of\n17.19 and 12.85 on BLEU-4 on the How2Sign dev/test sets. We evaluated our model\non the previous mainstream dataset PHOENIX14T, and the experiments achieved the\nSOTA results. In addition, our image quality far exceeds all previous results\nby 10 percentage points in terms of SSIM.\n","authors":["Sen Fang","Chunyu Sui","Yanghao Zhou","Xuedong Zhang","Hongbin Zhong","Yapeng Tian","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2308.16082v3.pdf","comment":"Project Page at https://signdiff.github.io"},{"id":"http://arxiv.org/abs/2504.11732v1","updated":"2025-04-16T03:12:39Z","published":"2025-04-16T03:12:39Z","title":"EgoExo-Gen: Ego-centric Video Prediction by Watching Exo-centric Videos","summary":"  Generating videos in the first-person perspective has broad application\nprospects in the field of augmented reality and embodied intelligence. In this\nwork, we explore the cross-view video prediction task, where given an\nexo-centric video, the first frame of the corresponding ego-centric video, and\ntextual instructions, the goal is to generate futur frames of the ego-centric\nvideo. Inspired by the notion that hand-object interactions (HOI) in\nego-centric videos represent the primary intentions and actions of the current\nactor, we present EgoExo-Gen that explicitly models the hand-object dynamics\nfor cross-view video prediction. EgoExo-Gen consists of two stages. First, we\ndesign a cross-view HOI mask prediction model that anticipates the HOI masks in\nfuture ego-frames by modeling the spatio-temporal ego-exo correspondence. Next,\nwe employ a video diffusion model to predict future ego-frames using the first\nego-frame and textual instructions, while incorporating the HOI masks as\nstructural guidance to enhance prediction quality. To facilitate training, we\ndevelop an automated pipeline to generate pseudo HOI masks for both ego- and\nexo-videos by exploiting vision foundation models. Extensive experiments\ndemonstrate that our proposed EgoExo-Gen achieves better prediction performance\ncompared to previous video prediction models on the Ego-Exo4D and H2O benchmark\ndatasets, with the HOI masks significantly improving the generation of hands\nand interactive objects in the ego-centric videos.\n","authors":["Jilan Xu","Yifei Huang","Baoqi Pei","Junlin Hou","Qingqiu Li","Guo Chen","Yuejie Zhang","Rui Feng","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2504.11732v1.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.12558v3","updated":"2025-04-16T03:11:44Z","published":"2025-02-18T05:50:23Z","title":"MomentSeeker: A Comprehensive Benchmark and A Strong Baseline For Moment\n  Retrieval Within Long Videos","summary":"  Retrieval augmented generation (RAG) holds great promise in addressing\nchallenges associated with long video understanding. These methods retrieve\nuseful moments from long videos for their presented tasks, thereby enabling\nmultimodal large language models (MLLMs) to generate high-quality answers in a\ncost-effective way. In this work, we present MomentSeeker, a comprehensive\nbenchmark to evaluate retrieval models' performance in handling general\nlong-video moment retrieval (LVMR) tasks. MomentSeeker offers three key\nadvantages. First, it incorporates long videos of over 500 seconds on average,\nmaking it the first benchmark specialized for long-video moment retrieval.\nSecond, it covers a wide range of task categories (including Moment Search,\nCaption Alignment, Image-conditioned Moment Search, and Video-conditioned\nMoment Search) and diverse application scenarios (e.g., sports, movies,\ncartoons, and ego), making it a comprehensive tool for assessing retrieval\nmodels' general LVMR performance. Additionally, the evaluation tasks are\ncarefully curated through human annotation, ensuring the reliability of\nassessment. We further fine-tune an MLLM-based LVMR retriever on synthetic\ndata, which demonstrates strong performance on our benchmark. We perform\nextensive experiments with various popular multimodal retrievers based on our\nbenchmark, whose results highlight the challenges of LVMR and limitations for\nexisting methods. Our created resources will be shared with community to\nadvance future research in this field.\n","authors":["Huaying Yuan","Jian Ni","Yueze Wang","Junjie Zhou","Zhengyang Liang","Zheng Liu","Zhao Cao","Zhicheng Dou","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2502.12558v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.13170v2","updated":"2025-04-16T02:58:50Z","published":"2023-02-25T21:36:39Z","title":"Partial Label Learning for Emotion Recognition from EEG","summary":"  Fully supervised learning has recently achieved promising performance in\nvarious electroencephalography (EEG) learning tasks by training on large\ndatasets with ground truth labels. However, labeling EEG data for affective\nexperiments is challenging, as it can be difficult for participants to\naccurately distinguish between similar emotions, resulting in ambiguous\nlabeling (reporting multiple emotions for one EEG instance). This notion could\ncause model performance degradation, as the ground truth is hidden within\nmultiple candidate labels. To address this issue, Partial Label Learning (PLL)\nhas been proposed to identify the ground truth from candidate labels during the\ntraining phase, and has shown good performance in the computer vision domain.\nHowever, PLL methods have not yet been adopted for EEG representation learning\nor implemented for emotion recognition tasks. In this paper, we adapt and\nre-implement six state-of-the-art PLL approaches for emotion recognition from\nEEG on two large emotion datasets (SEED-IV and SEED-V). These datasets contain\nfour and five categories of emotions, respectively. We evaluate the performance\nof all methods in classical, circumplex-based and real-world experiments. The\nresults show that PLL methods can achieve strong results in affective computing\nfrom EEG and achieve comparable performance to fully supervised learning. We\nalso investigate the effect of label disambiguation, a key step in many PLL\nmethods. The results show that in most cases, label disambiguation would\nbenefit the model when the candidate labels are generated based on their\nsimilarities to the ground truth rather than obeying a uniform distribution.\nThis finding suggests the potential of using label disambiguation-based PLL\nmethods for circumplex-based and real-world affective tasks.\n","authors":["Guangyi Zhang","Ali Etemad"],"pdf_url":"https://arxiv.org/pdf/2302.13170v2.pdf","comment":"15 pages, 13 figures"},{"id":"http://arxiv.org/abs/2405.14142v2","updated":"2025-04-16T02:53:42Z","published":"2024-05-23T03:36:31Z","title":"Imagery as Inquiry: Exploring A Multimodal Dataset for Conversational\n  Recommendation","summary":"  We introduce a multimodal dataset where users express preferences through\nimages. These images encompass a broad spectrum of visual expressions ranging\nfrom landscapes to artistic depictions. Users request recommendations for books\nor music that evoke similar feelings to those captured in the images, and\nrecommendations are endorsed by the community through upvotes. This dataset\nsupports two recommendation tasks: title generation and multiple-choice\nselection. Our experiments with large foundation models reveal their\nlimitations in these tasks. Particularly, vision-language models show no\nsignificant advantage over language-only counterparts that use descriptions,\nwhich we hypothesize is due to underutilized visual capabilities. To better\nharness these abilities, we propose the chain-of-imagery prompting, which\nresults in notable improvements. We release our code and datasets.\n","authors":["Se-eun Yoon","Hyunsik Jeon","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2405.14142v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08714v3","updated":"2025-04-16T02:43:12Z","published":"2025-03-10T08:38:25Z","title":"Versatile Multimodal Controls for Expressive Talking Human Animation","summary":"  In filmmaking, directors typically allow actors to perform freely based on\nthe script before providing specific guidance on how to present key actions.\nAI-generated content faces similar requirements, where users not only need\nautomatic generation of lip synchronization and basic gestures from audio input\nbut also desire semantically accurate and expressive body movement that can be\n``directly guided'' through text descriptions. Therefore, we present\nVersaAnimator, a versatile framework that synthesizes expressive talking human\nvideos from arbitrary portrait images. Specifically, we design a motion\ngenerator that produces basic rhythmic movements from audio input and supports\ntext-prompt control for specific actions. The generated whole-body 3D motion\ntokens can animate portraits of various scales, producing talking heads,\nhalf-body gestures and even leg movements for whole-body images. Besides, we\nintroduce a multi-modal controlled video diffusion that generates\nphotorealistic videos, where speech signals govern lip synchronization, facial\nexpressions, and head motions while body movements are guided by the 2D poses.\nFurthermore, we introduce a token2pose translator to smoothly map 3D motion\ntokens to 2D pose sequences. This design mitigates the stiffness resulting from\ndirect 3D to 2D conversion and enhances the details of the generated body\nmovements. Extensive experiments shows that VersaAnimator synthesizes\nlip-synced and identity-preserving videos while generating expressive and\nsemantically meaningful whole-body motions.\n","authors":["Zheng Qin","Ruobing Zheng","Yabing Wang","Tianqi Li","Zixin Zhu","Sanping Zhou","Ming Yang","Le Wang"],"pdf_url":"https://arxiv.org/pdf/2503.08714v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11257v2","updated":"2025-04-16T02:29:33Z","published":"2025-04-15T14:56:21Z","title":"UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction\n  Synthesis","summary":"  Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation. In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://colmon46.github.io/i2e-bench-leaderboard/ .\n","authors":["Xinyi Liu","Xiaoyi Zhang","Ziyun Zhang","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2504.11257v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11707v1","updated":"2025-04-16T02:10:42Z","published":"2025-04-16T02:10:42Z","title":"Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust\n  NSFW Defense and Million Scale Dataset","summary":"  In the past years, we have witnessed the remarkable success of Text-to-Image\n(T2I) models and their widespread use on the web. Extensive research in making\nT2I models produce hyper-realistic images has led to new concerns, such as\ngenerating Not-Safe-For-Work (NSFW) web content and polluting the web society.\nTo help prevent misuse of T2I models and create a safer web environment for\nusers features like NSFW filters and post-hoc security checks are used in these\nmodels. However, recent work unveiled how these methods can easily fail to\nprevent misuse. In particular, adversarial attacks on text and image modalities\ncan easily outplay defensive measures. %Exploiting such leads to the growing\nconcern of preventing adversarial attacks on text and image modalities.\nMoreover, there is currently no robust multimodal NSFW dataset that includes\nboth prompt and image pairs and adversarial examples. This work proposes a\nmillion-scale prompt and image dataset generated using open-source diffusion\nmodels. Second, we develop a multimodal defense to distinguish safe and NSFW\ntext and images, which is robust against adversarial attacks and directly\nalleviates current challenges. Our extensive experiments show that our model\nperforms well against existing SOTA NSFW detection methods in terms of accuracy\nand recall, drastically reducing the Attack Success Rate (ASR) in multimodal\nadversarial attack scenarios. Code:\nhttps://github.com/shahidmuneer/multimodal-nsfw-defense.\n","authors":["Muhammad Shahid Muneer","Simon S. Woo"],"pdf_url":"https://arxiv.org/pdf/2504.11707v1.pdf","comment":"Short Paper The Web Conference"},{"id":"http://arxiv.org/abs/2503.17660v2","updated":"2025-04-16T02:09:58Z","published":"2025-03-22T06:10:57Z","title":"OMR-Diffusion:Optimizing Multi-Round Enhanced Training in Diffusion\n  Models for Improved Intent Understanding","summary":"  Generative AI has significantly advanced text-driven image generation, but it\nstill faces challenges in producing outputs that consistently align with\nevolving user preferences and intents, particularly in multi-turn dialogue\nscenarios. In this research, We present a Visual Co-Adaptation (VCA) framework\nthat incorporates human-in-the-loop feedback, utilizing a well-trained reward\nmodel specifically designed to closely align with human preferences. Using a\ndiverse multi-turn dialogue dataset, the framework applies multiple reward\nfunctions (such as diversity, consistency, and preference feedback) to refine\nthe diffusion model through LoRA, effectively optimizing image generation based\non user input. We also constructed multi-round dialogue datasets with prompts\nand image pairs that well-fit user intent. Experiments show the model achieves\n508 wins in human evaluation, outperforming DALL-E 3 (463 wins) and others. It\nalso achieves 3.4 rounds in dialogue efficiency (vs. 13.7 for DALL-E 3) and\nexcels in metrics like LPIPS (0.15) and BLIP (0.59). Various experiments\ndemonstrate the effectiveness of the proposed method over state-of-the-art\nbaselines, with significant improvements in image consistency and alignment\nwith user intent.\n","authors":["Kun Li","Jianhui Wang","Miao Zhang","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2503.17660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11705v1","updated":"2025-04-16T02:05:47Z","published":"2025-04-16T02:05:47Z","title":"Learning What NOT to Count","summary":"  Few/zero-shot object counting methods reduce the need for extensive\nannotations but often struggle to distinguish between fine-grained categories,\nespecially when multiple similar objects appear in the same scene. To address\nthis limitation, we propose an annotation-free approach that enables the\nseamless integration of new fine-grained categories into existing few/zero-shot\ncounting models. By leveraging latent generative models, we synthesize\nhigh-quality, category-specific crowded scenes, providing a rich training\nsource for adapting to new categories without manual labeling. Our approach\nintroduces an attention prediction network that identifies fine-grained\ncategory boundaries trained using only synthetic pseudo-annotated data. At\ninference, these fine-grained attention estimates refine the output of existing\nfew/zero-shot counting networks. To benchmark our method, we further introduce\nthe FGTC dataset, a taxonomy-specific fine-grained object counting dataset for\nnatural images. Our method substantially enhances pre-trained state-of-the-art\nmodels on fine-grained taxon counting tasks, while using only synthetic data.\nCode and data to be released upon acceptance.\n","authors":["Adriano D'Alessandro","Ali Mahdavi-Amiri","Ghassan Hamarneh"],"pdf_url":"https://arxiv.org/pdf/2504.11705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11447v2","updated":"2025-04-16T02:02:05Z","published":"2025-04-15T17:57:13Z","title":"Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion","summary":"  The application of diffusion models in 3D LiDAR scene completion is limited\ndue to diffusion's slow sampling speed. Score distillation accelerates\ndiffusion sampling but with performance degradation, while post-training with\ndirect policy optimization (DPO) boosts performance using preference data. This\npaper proposes Distillation-DPO, a novel diffusion distillation framework for\nLiDAR scene completion with preference aligment. First, the student model\ngenerates paired completion scenes with different initial noises. Second, using\nLiDAR scene evaluation metrics as preference, we construct winning and losing\nsample pairs. Such construction is reasonable, since most LiDAR scene metrics\nare informative but non-differentiable to be optimized directly. Third,\nDistillation-DPO optimizes the student model by exploiting the difference in\nscore functions between the teacher and student models on the paired completion\nscenes. Such procedure is repeated until convergence. Extensive experiments\ndemonstrate that, compared to state-of-the-art LiDAR scene completion diffusion\nmodels, Distillation-DPO achieves higher-quality scene completion while\naccelerating the completion speed by more than 5-fold. Our method is the first\nto explore adopting preference learning in distillation to the best of our\nknowledge and provide insights into preference-aligned distillation. Our code\nis public available on https://github.com/happyw1nd/DistillationDPO.\n","authors":["An Zhao","Shengyuan Zhang","Ling Yang","Zejian Li","Jiale Wu","Haoran Xu","AnYang Wei","Perry Pengyun GU","Lingyun Sun"],"pdf_url":"https://arxiv.org/pdf/2504.11447v2.pdf","comment":"Our code is public available on\n  https://github.com/happyw1nd/DistillationDPO"},{"id":"http://arxiv.org/abs/2503.17669v2","updated":"2025-04-16T01:57:51Z","published":"2025-03-22T06:40:21Z","title":"TDRI: Two-Phase Dialogue Refinement and Co-Adaptation for Interactive\n  Image Generation","summary":"  Although text-to-image generation technologies have made significant\nadvancements, they still face challenges when dealing with ambiguous prompts\nand aligning outputs with user intent.Our proposed framework, TDRI (Two-Phase\nDialogue Refinement and Co-Adaptation), addresses these issues by enhancing\nimage generation through iterative user interaction. It consists of two phases:\nthe Initial Generation Phase, which creates base images based on user prompts,\nand the Interactive Refinement Phase, which integrates user feedback through\nthree key modules. The Dialogue-to-Prompt (D2P) module ensures that user\nfeedback is effectively transformed into actionable prompts, which improves the\nalignment between user intent and model input. By evaluating generated outputs\nagainst user expectations, the Feedback-Reflection (FR) module identifies\ndiscrepancies and facilitates improvements. In an effort to ensure consistently\nhigh-quality results, the Adaptive Optimization (AO) module fine-tunes the\ngeneration process by balancing user preferences and maintaining prompt\nfidelity. Experimental results show that TDRI outperforms existing methods by\nachieving 33.6% human preference, compared to 6.2% for GPT-4 augmentation, and\nthe highest CLIP and BLIP alignment scores (0.338 and 0.336, respectively). In\niterative feedback tasks, user satisfaction increased to 88% after 8 rounds,\nwith diminishing returns beyond 6 rounds. Furthermore, TDRI has been found to\nreduce the number of iterations and improve personalization in the creation of\nfashion products. TDRI exhibits a strong potential for a wide range of\napplications in the creative and industrial domains, as it streamlines the\ncreative process and improves alignment with user preferences\n","authors":["Yuheng Feng","Jianhui Wang","Kun Li","Sida Li","Tianyu Shi","Haoyue Han","Miao Zhang","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2503.17669v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11701v1","updated":"2025-04-16T01:54:33Z","published":"2025-04-16T01:54:33Z","title":"Non-uniform Point Cloud Upsampling via Local Manifold Distribution","summary":"  Existing learning-based point cloud upsampling methods often overlook the\nintrinsic data distribution charac?teristics of point clouds, leading to\nsuboptimal results when handling sparse and non-uniform point clouds. We\npropose a novel approach to point cloud upsampling by imposing constraints from\nthe perspective of manifold distributions. Leveraging the strong fitting\ncapability of Gaussian functions, our method employs a network to iteratively\noptimize Gaussian components and their weights, accurately representing local\nmanifolds. By utilizing the probabilistic distribution properties of Gaussian\nfunctions, we construct a unified statistical manifold to impose distribution\nconstraints on the point cloud. Experimental results on multiple datasets\ndemonstrate that our method generates higher-quality and more uniformly\ndistributed dense point clouds when processing sparse and non-uniform inputs,\noutperforming state-of-the-art point cloud upsampling techniques.\n","authors":["Yaohui Fang","Xingce Wang"],"pdf_url":"https://arxiv.org/pdf/2504.11701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04208v2","updated":"2025-04-16T01:53:31Z","published":"2024-09-06T11:57:17Z","title":"Learning to Learn Transferable Generative Attack for Person\n  Re-Identification","summary":"  Deep learning-based person re-identification (re-id) models are widely\nemployed in surveillance systems and inevitably inherit the vulnerability of\ndeep networks to adversarial attacks. Existing attacks merely consider\ncross-dataset and cross-model transferability, ignoring the cross-test\ncapability to perturb models trained in different domains. To powerfully\nexamine the robustness of real-world re-id models, the Meta Transferable\nGenerative Attack (MTGA) method is proposed, which adopts meta-learning\noptimization to promote the generative attacker producing highly transferable\nadversarial examples by learning comprehensively simulated transfer-based\ncross-model\\&dataset\\&test black-box meta attack tasks. Specifically,\ncross-model\\&dataset black-box attack tasks are first mimicked by selecting\ndifferent re-id models and datasets for meta-train and meta-test attack\nprocesses. As different models may focus on different feature regions, the\nPerturbation Random Erasing module is further devised to prevent the attacker\nfrom learning to only corrupt model-specific features. To boost the attacker\nlearning to possess cross-test transferability, the Normalization Mix strategy\nis introduced to imitate diverse feature embedding spaces by mixing\nmulti-domain statistics of target models. Extensive experiments show the\nsuperiority of MTGA, especially in cross-model\\&dataset and\ncross-model\\&dataset\\&test attacks, our MTGA outperforms the SOTA methods by\n21.5\\% and 11.3\\% on mean mAP drop rate, respectively. The code of MTGA will be\nreleased after the paper is accepted.\n","authors":["Yuan Bian","Min Liu","Xueping Wang","Yunfeng Ma","Yaonan Wang"],"pdf_url":"https://arxiv.org/pdf/2409.04208v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11698v1","updated":"2025-04-16T01:48:10Z","published":"2025-04-16T01:48:10Z","title":"An Online Adaptation Method for Robust Depth Estimation and Visual\n  Odometry in the Open World","summary":"  Recently, learning-based robotic navigation systems have gained extensive\nresearch attention and made significant progress. However, the diversity of\nopen-world scenarios poses a major challenge for the generalization of such\nsystems to practical scenarios. Specifically, learned systems for scene\nmeasurement and state estimation tend to degrade when the application scenarios\ndeviate from the training data, resulting to unreliable depth and pose\nestimation. Toward addressing this problem, this work aims to develop a visual\nodometry system that can fast adapt to diverse novel environments in an online\nmanner. To this end, we construct a self-supervised online adaptation framework\nfor monocular visual odometry aided by an online-updated depth estimation\nmodule. Firstly, we design a monocular depth estimation network with\nlightweight refiner modules, which enables efficient online adaptation. Then,\nwe construct an objective for self-supervised learning of the depth estimation\nmodule based on the output of the visual odometry system and the contextual\nsemantic information of the scene. Specifically, a sparse depth densification\nmodule and a dynamic consistency enhancement module are proposed to leverage\ncamera poses and contextual semantics to generate pseudo-depths and valid masks\nfor the online adaptation. Finally, we demonstrate the robustness and\ngeneralization capability of the proposed method in comparison with\nstate-of-the-art learning-based approaches on urban, in-house datasets and a\nrobot platform. Code is publicly available at:\nhttps://github.com/jixingwu/SOL-SLAM.\n","authors":["Xingwu Ji","Haochen Niu","Dexin Duan","Rendong Ying","Fei Wen","Peilin Liu"],"pdf_url":"https://arxiv.org/pdf/2504.11698v1.pdf","comment":"11 pages, 14 figures"},{"id":"http://arxiv.org/abs/2504.11695v1","updated":"2025-04-16T01:40:06Z","published":"2025-04-16T01:40:06Z","title":"Interpreting the Linear Structure of Vision-language Model Embedding\n  Spaces","summary":"  Vision-language models encode images and text in a joint space, minimizing\nthe distance between corresponding image and text pairs. How are language and\nimages organized in this joint space, and how do the models encode meaning and\nmodality? To investigate this, we train and release sparse autoencoders (SAEs)\non the embedding spaces of four vision-language models (CLIP, SigLIP, SigLIP2,\nand AIMv2). SAEs approximate model embeddings as sparse linear combinations of\nlearned directions, or \"concepts\". We find that, compared to other methods of\nlinear feature learning, SAEs are better at reconstructing the real embeddings,\nwhile also able to retain the most sparsity. Retraining SAEs with different\nseeds or different data diet leads to two findings: the rare, specific concepts\ncaptured by the SAEs are liable to change drastically, but we also show that\nthe key commonly-activating concepts extracted by SAEs are remarkably stable\nacross runs. Interestingly, while most concepts are strongly unimodal in\nactivation, we find they are not merely encoding modality per se. Many lie\nclose to - but not entirely within - the subspace defining modality, suggesting\nthat they encode cross-modal semantics despite their unimodal usage. To\nquantify this bridging behavior, we introduce the Bridge Score, a metric that\nidentifies concept pairs which are both co-activated across aligned image-text\ninputs and geometrically aligned in the shared space. This reveals that even\nunimodal concepts can collaborate to support cross-modal integration. We\nrelease interactive demos of the SAEs for all models, allowing researchers to\nexplore the organization of the concept spaces. Overall, our findings uncover a\nsparse linear structure within VLM embedding spaces that is shaped by modality,\nyet stitched together through latent bridges-offering new insight into how\nmultimodal meaning is constructed.\n","authors":["Isabel Papadimitriou","Huangyuan Su","Thomas Fel","Naomi Saphra","Sham Kakade","Stephanie Gil"],"pdf_url":"https://arxiv.org/pdf/2504.11695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11014v2","updated":"2025-04-16T01:38:27Z","published":"2025-04-15T09:37:54Z","title":"GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*","summary":"  The emerging trend in computer vision emphasizes developing universal models\ncapable of simultaneously addressing multiple diverse tasks. Such universality\ntypically requires joint training across multi-domain datasets to ensure\neffective generalization. However, monocular 3D object detection presents\nunique challenges in multi-domain training due to the scarcity of datasets\nannotated with accurate 3D ground-truth labels, especially beyond typical\nroad-based autonomous driving contexts. To address this challenge, we introduce\na novel weakly supervised framework leveraging pseudo-labels. Current\npretrained models often struggle to accurately detect pedestrians in non-road\nenvironments due to inherent dataset biases. Unlike generalized image-based 2D\nobject detection models, achieving similar generalization in monocular 3D\ndetection remains largely unexplored. In this paper, we propose GATE3D, a novel\nframework designed specifically for generalized monocular 3D object detection\nvia weak supervision. GATE3D effectively bridges domain gaps by employing\nconsistency losses between 2D and 3D predictions. Remarkably, our model\nachieves competitive performance on the KITTI benchmark as well as on an\nindoor-office dataset collected by us to evaluate the generalization\ncapabilities of our framework. Our results demonstrate that GATE3D\nsignificantly accelerates learning from limited annotated data through\neffective pre-training strategies, highlighting substantial potential for\nbroader impacts in robotics, augmented reality, and virtual reality\napplications. Project page: https://ies0411.github.io/GATE3D/\n","authors":["Eunsoo Im","Jung Kwon Lee","Changhyun Jee"],"pdf_url":"https://arxiv.org/pdf/2504.11014v2.pdf","comment":"9pages, 1 supple"},{"id":"http://arxiv.org/abs/2504.10786v2","updated":"2025-04-16T01:27:42Z","published":"2025-04-15T01:04:56Z","title":"Visual Language Models show widespread visual deficits on\n  neuropsychological tests","summary":"  Visual Language Models (VLMs) show remarkable performance in visual reasoning\ntasks, successfully tackling college-level challenges that require high-level\nunderstanding of images. However, some recent reports of VLMs struggling to\nreason about elemental visual concepts like orientation, position, continuity,\nand occlusion suggest a potential gulf between human and VLM vision. Here we\nuse the toolkit of neuropsychology to systematically assess the capabilities of\nthree state-of-the-art VLMs across visual domains. Using 51 tests drawn from\nsix clinical and experimental batteries, we characterise the visual abilities\nof leading VLMs relative to normative performance in healthy adults. While the\nmodels excel in straightforward object recognition tasks, we find widespread\ndeficits in low- and mid-level visual abilities that would be considered\nclinically significant in humans. These selective deficits, profiled through\nvalidated test batteries, suggest that an artificial system can achieve complex\nobject recognition without developing foundational visual concepts that in\nhumans require no explicit training.\n","authors":["Gene Tangtartharakul","Katherine R. Storrs"],"pdf_url":"https://arxiv.org/pdf/2504.10786v2.pdf","comment":"31 pages, 3 figures, 1 supplementary document with 1 figure and 51\n  sample images; corrected typo in Fig 1"}]},"2025-04-17T00:00:00Z":{"Statistical Machine Learning (Theory)":[{"id":"http://arxiv.org/abs/2504.13110v1","updated":"2025-04-17T17:24:38Z","published":"2025-04-17T17:24:38Z","title":"Propagation of Chaos in One-hidden-layer Neural Networks beyond\n  Logarithmic Time","summary":"  We study the approximation gap between the dynamics of a polynomial-width\nneural network and its infinite-width counterpart, both trained using projected\ngradient descent in the mean-field scaling regime. We demonstrate how to\ntightly bound this approximation gap through a differential equation governed\nby the mean-field dynamics. A key factor influencing the growth of this ODE is\nthe local Hessian of each particle, defined as the derivative of the particle's\nvelocity in the mean-field dynamics with respect to its position. We apply our\nresults to the canonical feature learning problem of estimating a\nwell-specified single-index model; we permit the information exponent to be\narbitrarily large, leading to convergence times that grow polynomially in the\nambient dimension $d$. We show that, due to a certain ``self-concordance''\nproperty in these problems -- where the local Hessian of a particle is bounded\nby a constant times the particle's velocity -- polynomially many neurons are\nsufficient to closely approximate the mean-field dynamics throughout training.\n","authors":["Margalit Glasgow","Denny Wu","Joan Bruna"],"pdf_url":"https://arxiv.org/pdf/2504.13110v1.pdf","comment":"70 pages"},{"id":"http://arxiv.org/abs/2504.13101v1","updated":"2025-04-17T17:10:33Z","published":"2025-04-17T17:10:33Z","title":"An Empirically Grounded Identifiability Theory Will Accelerate\n  Self-Supervised Learning Research","summary":"  Self-Supervised Learning (SSL) powers many current AI systems. As research\ninterest and investment grow, the SSL design space continues to expand. The\nPlatonic view of SSL, following the Platonic Representation Hypothesis (PRH),\nsuggests that despite different methods and engineering approaches, all\nrepresentations converge to the same Platonic ideal. However, this phenomenon\nlacks precise theoretical explanation. By synthesizing evidence from\nIdentifiability Theory (IT), we show that the PRH can emerge in SSL. However,\ncurrent IT cannot explain SSL's empirical success. To bridge the gap between\ntheory and practice, we propose expanding IT into what we term Singular\nIdentifiability Theory (SITh), a broader theoretical framework encompassing the\nentire SSL pipeline. SITh would allow deeper insights into the implicit data\nassumptions in SSL and advance the field towards learning more interpretable\nand generalizable representations. We highlight three critical directions for\nfuture research: 1) training dynamics and convergence properties of SSL; 2) the\nimpact of finite samples, batch size, and data diversity; and 3) the role of\ninductive biases in architecture, augmentations, initialization schemes, and\noptimizers.\n","authors":["Patrik Reizinger","Randall Balestriero","David Klindt","Wieland Brendel"],"pdf_url":"https://arxiv.org/pdf/2504.13101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17765v4","updated":"2025-04-17T16:40:48Z","published":"2023-03-31T01:56:13Z","title":"Learning from Similar Linear Representations: Adaptivity, Minimaxity,\n  and Robustness","summary":"  Representation multi-task learning (MTL) has achieved tremendous success in\npractice. However, the theoretical understanding of these methods is still\nlacking. Most existing theoretical works focus on cases where all tasks share\nthe same representation, and claim that MTL almost always improves performance.\nNevertheless, as the number of tasks grows, assuming all tasks share the same\nrepresentation is unrealistic. Furthermore, empirical findings often indicate\nthat a shared representation does not necessarily improve single-task learning\nperformance. In this paper, we aim to understand how to learn from tasks with\n\\textit{similar but not exactly the same} linear representations, while dealing\nwith outlier tasks. Assuming a known intrinsic dimension, we propose a\npenalized empirical risk minimization method and a spectral method that are\n\\textit{adaptive} to the similarity structure and \\textit{robust} to outlier\ntasks. Both algorithms outperform single-task learning when representations\nacross tasks are sufficiently similar and the proportion of outlier tasks is\nsmall. Moreover, they always perform at least as well as single-task learning,\neven when the representations are dissimilar. We provide information-theoretic\nlower bounds to demonstrate that both methods are nearly \\textit{minimax}\noptimal in a large regime, with the spectral method being optimal in the\nabsence of outlier tasks. Additionally, we introduce a thresholding algorithm\nto adapt to an unknown intrinsic dimension. We conduct extensive numerical\nexperiments to validate our theoretical findings.\n","authors":["Ye Tian","Yuqi Gu","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2303.17765v4.pdf","comment":"125 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2412.17152v2","updated":"2025-04-17T16:06:21Z","published":"2024-12-22T20:12:16Z","title":"Unifying Feature-Based Explanations with Functional ANOVA and\n  Cooperative Game Theory","summary":"  Feature-based explanations, using perturbations or gradients, are a prevalent\ntool to understand decisions of black box machine learning models. Yet,\ndifferences between these methods still remain mostly unknown, which limits\ntheir applicability for practitioners. In this work, we introduce a unified\nframework for local and global feature-based explanations using two\nwell-established concepts: functional ANOVA (fANOVA) from statistics, and the\nnotion of value and interaction from cooperative game theory. We introduce\nthree fANOVA decompositions that determine the influence of feature\ndistributions, and use game-theoretic measures, such as the Shapley value and\ninteractions, to specify the influence of higher-order interactions. Our\nframework combines these two dimensions to uncover similarities and differences\nbetween a wide range of explanation techniques for features and groups of\nfeatures. We then empirically showcase the usefulness of our framework on\nsynthetic and real-world datasets.\n","authors":["Fabian Fumagalli","Maximilian Muschalik","Eyke Hüllermeier","Barbara Hammer","Julia Herbinger"],"pdf_url":"https://arxiv.org/pdf/2412.17152v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07664v2","updated":"2025-04-17T16:04:48Z","published":"2024-07-10T13:44:19Z","title":"A Coding-Theoretic Analysis of Hyperspherical Prototypical Learning\n  Geometry","summary":"  Hyperspherical Prototypical Learning (HPL) is a supervised approach to\nrepresentation learning that designs class prototypes on the unit hypersphere.\nThe prototypes bias the representations to class separation in a scale\ninvariant and known geometry. Previous approaches to HPL have either of the\nfollowing shortcomings: (i) they follow an unprincipled optimisation procedure;\nor (ii) they are theoretically sound, but are constrained to only one possible\nlatent dimension. In this paper, we address both shortcomings. To address (i),\nwe present a principled optimisation procedure whose solution we show is\noptimal. To address (ii), we construct well-separated prototypes in a wide\nrange of dimensions using linear block codes. Additionally, we give a full\ncharacterisation of the optimal prototype placement in terms of achievable and\nconverse bounds, showing that our proposed methods are near-optimal.\n","authors":["Martin Lindström","Borja Rodríguez-Gálvez","Ragnar Thobaben","Mikael Skoglund"],"pdf_url":"https://arxiv.org/pdf/2407.07664v2.pdf","comment":"Changes in version 2: Minor formatting changes. Published in the\n  Proceedings of the Geometry-grounded Representation Learning and Generative\n  Modeling Workshop (GRaM), PMLR 251. Available at:\n  https://proceedings.mlr.press/v251/lindstrom24a.html 14 pages: 9 of the main\n  paper, 2 of references, and 3 of appendices.. Code is available at:\n  https://github.com/martinlindstrom/coding_theoretic_hpl"},{"id":"http://arxiv.org/abs/2504.13046v1","updated":"2025-04-17T16:02:20Z","published":"2025-04-17T16:02:20Z","title":"Variance-Reduced Fast Operator Splitting Methods for Stochastic\n  Generalized Equations","summary":"  We develop two classes of variance-reduced fast operator splitting methods to\napproximate solutions of both finite-sum and stochastic generalized equations.\nOur approach integrates recent advances in accelerated fixed-point methods,\nco-hypomonotonicity, and variance reduction. First, we introduce a class of\nvariance-reduced estimators and establish their variance-reduction bounds. This\nclass covers both unbiased and biased instances and comprises common estimators\nas special cases, including SVRG, SAGA, SARAH, and Hybrid-SGD. Next, we design\na novel accelerated variance-reduced forward-backward splitting (FBS) algorithm\nusing these estimators to solve finite-sum and stochastic generalized\nequations. Our method achieves both $\\mathcal{O}(1/k^2)$ and $o(1/k^2)$\nconvergence rates on the expected squared norm $\\mathbb{E}[ \\|\nG_{\\lambda}x^k\\|^2]$ of the FBS residual $G_{\\lambda}$, where $k$ is the\niteration counter. Additionally, we establish, for the first time, almost sure\nconvergence rates and almost sure convergence of iterates to a solution in\nstochastic accelerated methods. Unlike existing stochastic fixed-point\nalgorithms, our methods accommodate co-hypomonotone operators, which\npotentially include nonmonotone problems arising from recent applications. We\nfurther specify our method to derive an appropriate variant for each stochastic\nestimator -- SVRG, SAGA, SARAH, and Hybrid-SGD -- demonstrating that they\nachieve the best-known complexity for each without relying on enhancement\ntechniques. Alternatively, we propose an accelerated variance-reduced\nbackward-forward splitting (BFS) method, which attains similar convergence\nrates and oracle complexity as our FBS method. Finally, we validate our results\nthrough several numerical experiments and compare their performance.\n","authors":["Quoc Tran-Dinh"],"pdf_url":"https://arxiv.org/pdf/2504.13046v1.pdf","comment":"58 pages, 1 table, and 8 figures"},{"id":"http://arxiv.org/abs/2504.08937v2","updated":"2025-04-17T15:31:11Z","published":"2025-04-11T19:33:06Z","title":"Rethinking Few-Shot Image Fusion: Granular Ball Priors Enable\n  General-Purpose Deep Fusion","summary":"  In image fusion tasks, the absence of real fused images as priors presents a\nfundamental challenge. Most deep learning-based fusion methods rely on\nlarge-scale paired datasets to extract global weighting features from raw\nimages, thereby generating fused outputs that approximate real fused images. In\ncontrast to previous studies, this paper explores few-shot training of neural\nnetworks under the condition of having prior knowledge. We propose a novel\nfusion framework named GBFF, and a Granular Ball Significant Extraction\nalgorithm specifically designed for the few-shot prior setting. All pixel pairs\ninvolved in the fusion process are initially modeled as a Coarse-Grained\nGranular Ball. At the local level, Fine-Grained Granular Balls are used to\nslide through the brightness space to extract Non-Salient Pixel Pairs, and\nperform splitting operations to obtain Salient Pixel Pairs. Pixel-wise weights\nare then computed to generate a pseudo-supervised image. At the global level,\npixel pairs with significant contributions to the fusion process are\ncategorized into the Positive Region, while those whose contributions cannot be\naccurately determined are assigned to the Boundary Region. The Granular Ball\nperforms modality-aware adaptation based on the proportion of the positive\nregion, thereby adjusting the neural network's loss function and enabling it to\ncomplement the information of the boundary region. Extensive experiments\ndemonstrate the effectiveness of both the proposed algorithm and the underlying\ntheory. Compared with state-of-the-art (SOTA) methods, our approach shows\nstrong competitiveness in terms of both fusion time and image expressiveness.\nOur code is publicly available at:\n","authors":["Minjie Deng","Yan Wei","Hao Zhai","An Wu","Yuncan Ouyang","Qianyao Peng"],"pdf_url":"https://arxiv.org/pdf/2504.08937v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12988v1","updated":"2025-04-17T14:50:40Z","published":"2025-04-17T14:50:40Z","title":"Why Ask One When You Can Ask $k$? Two-Stage Learning-to-Defer to a Set\n  of Experts","summary":"  Learning-to-Defer (L2D) enables decision-making systems to improve\nreliability by selectively deferring uncertain predictions to more competent\nagents. However, most existing approaches focus exclusively on single-agent\ndeferral, which is often inadequate in high-stakes scenarios that require\ncollective expertise. We propose Top-$k$ Learning-to-Defer, a generalization of\nthe classical two-stage L2D framework that allocates each query to the $k$ most\nconfident agents instead of a single one. To further enhance flexibility and\ncost-efficiency, we introduce Top-$k(x)$ Learning-to-Defer, an adaptive\nextension that learns the optimal number of agents to consult for each query,\nbased on input complexity, agent competency distributions, and consultation\ncosts. For both settings, we derive a novel surrogate loss and prove that it is\nBayes-consistent and $(\\mathcal{R}, \\mathcal{G})$-consistent, ensuring\nconvergence to the Bayes-optimal allocation. Notably, we show that the\nwell-established model cascades paradigm arises as a restricted instance of our\nTop-$k$ and Top-$k(x)$ formulations. Extensive experiments across diverse\nbenchmarks demonstrate the effectiveness of our framework on both\nclassification and regression tasks.\n","authors":["Yannis Montreuil","Axel Carlier","Lai Xing Ng","Wei Tsang Ooi"],"pdf_url":"https://arxiv.org/pdf/2504.12988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05714v6","updated":"2025-04-17T12:18:41Z","published":"2024-06-09T10:12:08Z","title":"A conversion theorem and minimax optimality for continuum contextual\n  bandits","summary":"  We study the contextual continuum bandits problem, where the learner\nsequentially receives a side information vector and has to choose an action in\na convex set, minimizing a function associated with the context. The goal is to\nminimize all the underlying functions for the received contexts, leading to the\ncontextual notion of regret, which is stronger than the standard static regret.\nAssuming that the objective functions are $\\gamma$-H\\\"older with respect to the\ncontexts, $0<\\gamma\\le 1,$ we demonstrate that any algorithm achieving a\nsub-linear static regret can be extended to achieve a sub-linear contextual\nregret. We prove a static-to-contextual regret conversion theorem that provides\nan upper bound for the contextual regret of the output algorithm as a function\nof the static regret of the input algorithm. We further study the implications\nof this general result for three fundamental cases of dependency of the\nobjective function on the action variable: (a) Lipschitz bandits, (b) convex\nbandits, (c) strongly convex and smooth bandits. For Lipschitz bandits and\n$\\gamma=1,$ combining our results with the lower bound of Slivkins (2014), we\nprove that the minimax optimal contextual regret for the noise-free adversarial\nsetting is achieved. Then, we prove that in the presence of noise, the\ncontextual regret rate as a function of the number of queries is the same for\nconvex bandits as it is for strongly convex and smooth bandits. Lastly, we\npresent a minimax lower bound, implying two key facts. First, obtaining a\nsub-linear contextual regret may be impossible over functions that are not\ncontinuous with respect to the context. Second, for convex bandits and strongly\nconvex and smooth bandits, the algorithms that we propose achieve, up to a\nlogarithmic factor, the minimax optimal rate of contextual regret as a function\nof the number of queries.\n","authors":["Arya Akhavan","Karim Lounici","Massimiliano Pontil","Alexandre B. Tsybakov"],"pdf_url":"https://arxiv.org/pdf/2406.05714v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12860v1","updated":"2025-04-17T11:38:17Z","published":"2025-04-17T11:38:17Z","title":"When do Random Forests work?","summary":"  We study the effectiveness of randomizing split-directions in random forests.\nPrior literature has shown that, on the one hand, randomization can reduce\nvariance through decorrelation, and, on the other hand, randomization\nregularizes and works in low signal-to-noise ratio (SNR) environments. First,\nwe bring together and revisit decorrelation and regularization by presenting a\nsystematic analysis of out-of-sample mean-squared error (MSE) for different SNR\nscenarios based on commonly-used data-generating processes. We find that\nvariance reduction tends to increase with the SNR and forests outperform\nbagging when the SNR is low because, in low SNR cases, variance dominates bias\nfor both methods. Second, we show that the effectiveness of randomization is a\nquestion that goes beyond the SNR. We present a simulation study with fixed and\nmoderate SNR, in which we examine the effectiveness of randomization for other\ndata characteristics. In particular, we find that (i) randomization can\nincrease bias in the presence of fat tails in the distribution of covariates;\n(ii) in the presence of irrelevant covariates randomization is ineffective\nbecause bias dominates variance; and (iii) when covariates are mutually\ncorrelated randomization tends to be effective because variance dominates bias.\nBeyond randomization, we find that, for both bagging and random forests, bias\ncan be significantly reduced in the presence of correlated covariates. This\nlast finding goes beyond the prevailing view that averaging mostly works by\nvariance reduction. Given that in practice covariates are often correlated, our\nfindings on correlated covariates could open the way for a better understanding\nof why random forests work well in many applications.\n","authors":["C. Revelas","O. Boldea","B. J. M. Werker"],"pdf_url":"https://arxiv.org/pdf/2504.12860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01530v4","updated":"2025-04-17T11:31:49Z","published":"2023-12-03T23:08:29Z","title":"Evaluation of Active Feature Acquisition Methods for Time-varying\n  Feature Settings","summary":"  Machine learning methods often assume that input features are available at no\ncost. However, in domains like healthcare, where acquiring features could be\nexpensive or harmful, it is necessary to balance a feature's acquisition cost\nagainst its predictive value. The task of training an AI agent to decide which\nfeatures to acquire is called active feature acquisition (AFA). By deploying an\nAFA agent, we effectively alter the acquisition strategy and trigger a\ndistribution shift. To safely deploy AFA agents under this distribution shift,\nwe present the problem of active feature acquisition performance evaluation\n(AFAPE). We examine AFAPE under i) a no direct effect (NDE) assumption, stating\nthat acquisitions do not affect the underlying feature values; and ii) a no\nunobserved confounding (NUC) assumption, stating that retrospective feature\nacquisition decisions were only based on observed features. We show that one\ncan apply missing data methods under the NDE assumption and offline\nreinforcement learning under the NUC assumption. When NUC and NDE hold, we\npropose a novel semi-offline reinforcement learning framework. This framework\nrequires a weaker positivity assumption and introduces three new estimators: A\ndirect method (DM), an inverse probability weighting (IPW), and a double\nreinforcement learning (DRL) estimator.\n","authors":["Henrik von Kleist","Alireza Zamanian","Ilya Shpitser","Narges Ahmidi"],"pdf_url":"https://arxiv.org/pdf/2312.01530v4.pdf","comment":"61 pages, 4 tables, 11 Figures"},{"id":"http://arxiv.org/abs/2504.12841v1","updated":"2025-04-17T10:57:29Z","published":"2025-04-17T10:57:29Z","title":"ALT: A Python Package for Lightweight Feature Representation in Time\n  Series Classification","summary":"  We introduce ALT, an open-source Python package created for efficient and\naccurate time series classification (TSC). The package implements the adaptive\nlaw-based transformation (ALT) algorithm, which transforms raw time series data\ninto a linearly separable feature space using variable-length shifted time\nwindows. This adaptive approach enhances its predecessor, the linear law-based\ntransformation (LLT), by effectively capturing patterns of varying temporal\nscales. The software is implemented for scalability, interpretability, and ease\nof use, achieving state-of-the-art performance with minimal computational\noverhead. Extensive benchmarking on real-world datasets demonstrates the\nutility of ALT for diverse TSC tasks in physics and related domains.\n","authors":["Balázs P. Halmos","Balázs Hajós","Vince Á. Molnár","Marcell T. Kurbucz","Antal Jakovác"],"pdf_url":"https://arxiv.org/pdf/2504.12841v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.04071v2","updated":"2025-04-17T10:38:48Z","published":"2024-06-06T13:36:41Z","title":"Dynamic angular synchronization under smoothness constraints","summary":"  Given an undirected measurement graph $\\mathcal{H} = ([n], \\mathcal{E})$, the\nclassical angular synchronization problem consists of recovering unknown angles\n$\\theta_1^*,\\dots,\\theta_n^*$ from a collection of noisy pairwise measurements\nof the form $(\\theta_i^* - \\theta_j^*) \\mod 2\\pi$, for all $\\{i,j\\} \\in\n\\mathcal{E}$. This problem arises in a variety of applications, including\ncomputer vision, time synchronization of distributed networks, and ranking from\npairwise comparisons. In this paper, we consider a dynamic version of this\nproblem where the angles, and also the measurement graphs evolve over $T$ time\npoints. Assuming a smoothness condition on the evolution of the latent angles,\nwe derive three algorithms for joint estimation of the angles over all time\npoints. Moreover, for one of the algorithms, we establish non-asymptotic\nrecovery guarantees for the mean-squared error (MSE) under different\nstatistical models. In particular, we show that the MSE converges to zero as\n$T$ increases under milder conditions than in the static setting. This includes\nthe setting where the measurement graphs are highly sparse and disconnected,\nand also when the measurement noise is large and can potentially increase with\n$T$. We complement our theoretical results with experiments on synthetic data.\n","authors":["Ernesto Araya","Mihai Cucuringu","Hemant Tyagi"],"pdf_url":"https://arxiv.org/pdf/2406.04071v2.pdf","comment":"42 pages, 9 figures. Corrected typos and added clarifications, as per\n  the suggestions of reviewers. Added Remarks 4,5 and Algorithm 4 (which is\n  same as Algorithm 3 but with TRS relaced by a spectral method). Accepted in\n  JMLR"},{"id":"http://arxiv.org/abs/2410.13054v2","updated":"2025-04-17T07:48:47Z","published":"2024-10-16T21:32:31Z","title":"Systems with Switching Causal Relations: A Meta-Causal Perspective","summary":"  Most work on causality in machine learning assumes that causal relationships\nare driven by a constant underlying process. However, the flexibility of\nagents' actions or tipping points in the environmental process can change the\nqualitative dynamics of the system. As a result, new causal relationships may\nemerge, while existing ones change or disappear, resulting in an altered causal\ngraph. To analyze these qualitative changes on the causal graph, we propose the\nconcept of meta-causal states, which groups classical causal models into\nclusters based on equivalent qualitative behavior and consolidates specific\nmechanism parameterizations. We demonstrate how meta-causal states can be\ninferred from observed agent behavior, and discuss potential methods for\ndisentangling these states from unlabeled data. Finally, we direct our analysis\ntowards the application of a dynamical system, showing that meta-causal states\ncan also emerge from inherent system dynamics, and thus constitute more than a\ncontext-dependent framework in which mechanisms emerge only as a result of\nexternal factors.\n","authors":["Moritz Willig","Tim Nelson Tobiasch","Florian Peter Busch","Jonas Seng","Devendra Singh Dhami","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2410.13054v2.pdf","comment":"21 pages, 3 figures, 4 tables, ICLR 2025 Camera Ready Version"},{"id":"http://arxiv.org/abs/2502.18553v3","updated":"2025-04-17T07:48:20Z","published":"2025-02-25T18:19:06Z","title":"Applications of Statistical Field Theory in Deep Learning","summary":"  Deep learning algorithms have made incredible strides in the past decade, yet\ndue to their complexity, the science of deep learning remains in its early\nstages. Being an experimentally driven field, it is natural to seek a theory of\ndeep learning within the physics paradigm. As deep learning is largely about\nlearning functions and distributions over functions, statistical field theory,\na rich and versatile toolbox for tackling complex distributions over functions\n(fields) is an obvious choice of formalism. Research efforts carried out in the\npast few years have demonstrated the ability of field theory to provide useful\ninsights on generalization, implicit bias, and feature learning effects. Here\nwe provide a pedagogical review of this emerging line of research.\n","authors":["Zohar Ringel","Noa Rubin","Edo Mor","Moritz Helias","Inbar Seroussi"],"pdf_url":"https://arxiv.org/pdf/2502.18553v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20285v3","updated":"2025-04-17T07:47:26Z","published":"2023-10-31T08:58:16Z","title":"Accelerating Non-Conjugate Gaussian Processes By Trading Off Computation\n  For Uncertainty","summary":"  Non-conjugate Gaussian processes (NCGPs) define a flexible probabilistic\nframework to model categorical, ordinal and continuous data, and are widely\nused in practice. However, exact inference in NCGPs is prohibitively expensive\nfor large datasets, thus requiring approximations in practice. The\napproximation error adversely impacts the reliability of the model and is not\naccounted for in the uncertainty of the prediction. We introduce a family of\niterative methods that explicitly model this error. They are uniquely suited to\nparallel modern computing hardware, efficiently recycle computations, and\ncompress information to reduce both the time and memory requirements for NCGPs.\nAs we demonstrate on large-scale classification problems, our method\nsignificantly accelerates posterior inference compared to competitive baselines\nby trading off reduced computation for increased uncertainty.\n","authors":["Lukas Tatzel","Jonathan Wenger","Frank Schneider","Philipp Hennig"],"pdf_url":"https://arxiv.org/pdf/2310.20285v3.pdf","comment":"Main text: 15 pages, 7 figures; Supplements: 15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.10741v2","updated":"2025-04-17T07:12:52Z","published":"2024-12-14T08:22:49Z","title":"RegMixMatch: Optimizing Mixup Utilization in Semi-Supervised Learning","summary":"  Consistency regularization and pseudo-labeling have significantly advanced\nsemi-supervised learning (SSL). Prior works have effectively employed Mixup for\nconsistency regularization in SSL. However, our findings indicate that applying\nMixup for consistency regularization may degrade SSL performance by\ncompromising the purity of artificial labels. Moreover, most pseudo-labeling\nbased methods utilize thresholding strategy to exclude low-confidence data,\naiming to mitigate confirmation bias; however, this approach limits the utility\nof unlabeled samples. To address these challenges, we propose RegMixMatch, a\nnovel framework that optimizes the use of Mixup with both high- and\nlow-confidence samples in SSL. First, we introduce semi-supervised RegMixup,\nwhich effectively addresses reduced artificial labels purity by using both\nmixed samples and clean samples for training. Second, we develop a class-aware\nMixup technique that integrates information from the top-2 predicted classes\ninto low-confidence samples and their artificial labels, reducing the\nconfirmation bias associated with these samples and enhancing their effective\nutilization. Experimental results demonstrate that RegMixMatch achieves\nstate-of-the-art performance across various SSL benchmarks.\n","authors":["Haorong Han","Jidong Yuan","Chixuan Wei","Zhongyang Yu"],"pdf_url":"https://arxiv.org/pdf/2412.10741v2.pdf","comment":"Accepted in AAAI Conference on Artificial Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2411.17180v3","updated":"2025-04-17T07:02:56Z","published":"2024-11-26T07:41:15Z","title":"Training a neural netwok for data reduction and better generalization","summary":"  At the time of environmental concerns about artificial intelligence, in\nparticular its need for greedy storage and computation, sparsity inducing\nneural networks offer a promising path towards frugality and solution for less\nwaste.\n  Sparse learners compress the inputs (features) by selecting only the ones\nneeded for good generalization. A human scientist can then give an intelligent\ninterpretation to the few selected features. If genes are the inputs and cancer\ntype is the output, then the selected genes give the cancerologist clues on\nwhat genes have an effect on certain cancers. LASSO-type regularization leads\nto good input selection for linear associations, but few attempts have been\nmade for nonlinear associations modeled as an artificial neural network. A\nstringent but efficient way of testing whether a feature selection method works\nis to check if a phase transition occurs in the probability of retrieving the\nrelevant features, as observed and mathematically studied for linear models.\nOur method achieves just so for artificial neural networks, and, on real data,\nit has the best compromise between number of selected features and\ngeneralization performance.\n  Our method is flexible, applying to complex models ranging from shallow to\ndeep artificial neural networks and supporting various cost functions and\nsparsity-promoting penalties. It does not rely on cross-validation or on a\nvalidation set to select its single regularization parameter making it\nuser-friendly. Our approach can be seen as a form of compressed sensing for\ncomplex models, allowing to distill high-dimensional data into a compact,\ninterpretable subset of meaningful features, just the opposite of a black box.\n  A python package is available at https://github.com/VcMaxouuu/AnnHarderLasso\ncontaining all the simulations and ready-to-use models.\n","authors":["Sylvain Sardy","Maxime van Cutsem","Xiaoyu Ma"],"pdf_url":"https://arxiv.org/pdf/2411.17180v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12683v1","updated":"2025-04-17T06:17:06Z","published":"2025-04-17T06:17:06Z","title":"Cluster weighted models with multivariate skewed distributions for\n  functional data","summary":"  We propose a clustering method, funWeightClustSkew, based on mixtures of\nfunctional linear regression models and three skewed multivariate\ndistributions: the variance-gamma distribution, the skew-t distribution, and\nthe normal-inverse Gaussian distribution. Our approach follows the framework of\nthe functional high dimensional data clustering (funHDDC) method, and we extend\nto functional data the cluster weighted models based on skewed distributions\nused for finite dimensional multivariate data. We consider several parsimonious\nmodels, and to estimate the parameters we construct an expectation maximization\n(EM) algorithm. We illustrate the performance of funWeightClustSkew for\nsimulated data and for the Air Quality dataset.\n","authors":["Cristina Anton","Roy Shivam Ram Shreshtth"],"pdf_url":"https://arxiv.org/pdf/2504.12683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12625v1","updated":"2025-04-17T04:02:06Z","published":"2025-04-17T04:02:06Z","title":"Spectral Algorithms under Covariate Shift","summary":"  Spectral algorithms leverage spectral regularization techniques to analyze\nand process data, providing a flexible framework for addressing supervised\nlearning problems. To deepen our understanding of their performance in\nreal-world scenarios where the distributions of training and test data may\ndiffer, we conduct a rigorous investigation into the convergence behavior of\nspectral algorithms under distribution shifts, specifically within the\nframework of reproducing kernel Hilbert spaces. Our study focuses on the case\nof covariate shift. In this scenario, the marginal distributions of the input\ndata differ between the training and test datasets, while the conditional\ndistribution of the output given the input remains unchanged. Under this\nsetting, we analyze the generalization error of spectral algorithms and show\nthat they achieve minimax optimality when the density ratios between the\ntraining and test distributions are uniformly bounded. However, we also\nidentify a critical limitation: when the density ratios are unbounded, the\nspectral algorithms may become suboptimal. To address this limitation, we\npropose a weighted spectral algorithm that incorporates density ratio\ninformation into the learning process. Our theoretical analysis shows that this\nweighted approach achieves optimal capacity-independent convergence rates.\nFurthermore, by introducing a weight clipping technique, we demonstrate that\nthe convergence rates of the weighted spectral algorithm can approach the\noptimal capacity-dependent convergence rates arbitrarily closely. This\nimprovement resolves the suboptimality issue in unbounded density ratio\nscenarios and advances the state-of-the-art by refining existing theoretical\nresults.\n","authors":["Jun Fan","Zheng-Chu Guo","Lei Shi"],"pdf_url":"https://arxiv.org/pdf/2504.12625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12617v1","updated":"2025-04-17T03:46:03Z","published":"2025-04-17T03:46:03Z","title":"Bayesian Density-Density Regression with Application to Cell-Cell\n  Communications","summary":"  We introduce a scalable framework for regressing multivariate distributions\nonto multivariate distributions, motivated by the application of inferring\ncell-cell communication from population-scale single-cell data. The observed\ndata consist of pairs of multivariate distributions for ligands from one cell\ntype and corresponding receptors from another. For each ordered pair $e=(l,r)$\nof cell types $(l \\neq r)$ and each sample $i = 1, \\ldots, n$, we observe a\npair of distributions $(F_{ei}, G_{ei})$ of gene expressions for ligands and\nreceptors of cell types $l$ and $r$, respectively. The aim is to set up a\nregression of receptor distributions $G_{ei}$ given ligand distributions\n$F_{ei}$. A key challenge is that these distributions reside in distinct spaces\nof differing dimensions. We formulate the regression of multivariate densities\non multivariate densities using a generalized Bayes framework with the sliced\nWasserstein distance between fitted and observed distributions. Finally, we use\ninference under such regressions to define a directed graph for cell-cell\ncommunications.\n","authors":["Khai Nguyen","Yang Ni","Peter Mueller"],"pdf_url":"https://arxiv.org/pdf/2504.12617v1.pdf","comment":"42 pages, 24 figures, 1 table"},{"id":"http://arxiv.org/abs/2504.12594v1","updated":"2025-04-17T02:41:22Z","published":"2025-04-17T02:41:22Z","title":"Meta-Dependence in Conditional Independence Testing","summary":"  Constraint-based causal discovery algorithms utilize many statistical tests\nfor conditional independence to uncover networks of causal dependencies. These\napproaches to causal discovery rely on an assumed correspondence between the\ngraphical properties of a causal structure and the conditional independence\nproperties of observed variables, known as the causal Markov condition and\nfaithfulness. Finite data yields an empirical distribution that is \"close\" to\nthe actual distribution. Across these many possible empirical distributions,\nthe correspondence to the graphical properties can break down for different\nconditional independencies, and multiple violations can occur at the same time.\nWe study this \"meta-dependence\" between conditional independence properties\nusing the following geometric intuition: each conditional independence property\nconstrains the space of possible joint distributions to a manifold. The\n\"meta-dependence\" between conditional independences is informed by the position\nof these manifolds relative to the true probability distribution. We provide a\nsimple-to-compute measure of this meta-dependence using information projections\nand consolidate our findings empirically using both synthetic and real-world\ndata.\n","authors":["Bijan Mazaheri","Jiaqi Zhang","Caroline Uhler"],"pdf_url":"https://arxiv.org/pdf/2504.12594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17505v2","updated":"2025-04-17T01:52:49Z","published":"2024-09-26T03:24:59Z","title":"Sequential Kernelized Stein Discrepancy","summary":"  We present a sequential version of the kernelized Stein discrepancy\ngoodness-of-fit test, which allows for conducting goodness-of-fit tests for\nunnormalized densities that are continuously monitored and adaptively stopped.\nThat is, the sample size need not be fixed prior to data collection; the\npractitioner can choose whether to stop the test or continue to gather evidence\nat any time while controlling the false discovery rate. In stark contrast to\nrelated literature, we do not impose uniform boundedness on the Stein kernel.\nInstead, we exploit the potential boundedness of the Stein kernel at arbitrary\npoint evaluations to define test martingales, that give way to the subsequent\nnovel sequential tests. We prove the validity of the test, as well as an\nasymptotic lower bound for the logarithmic growth of the wealth process under\nthe alternative. We further illustrate the empirical performance of the test\nwith a variety of distributions, including restricted Boltzmann machines.\n","authors":["Diego Martinez-Taboada","Aaditya Ramdas"],"pdf_url":"https://arxiv.org/pdf/2409.17505v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19619v3","updated":"2025-04-17T00:28:27Z","published":"2024-06-28T03:02:25Z","title":"ScoreFusion: Fusing Score-based Generative Models via Kullback-Leibler\n  Barycenters","summary":"  We introduce ScoreFusion, a theoretically grounded method for fusing multiple\npre-trained diffusion models that are assumed to generate from auxiliary\npopulations. ScoreFusion is particularly useful for enhancing the generative\nmodeling of a target population with limited observed data. Our starting point\nconsiders the family of KL barycenters of the auxiliary populations, which is\nproven to be an optimal parametric class in the KL sense, but difficult to\nlearn. Nevertheless, by recasting the learning problem as score matching in\ndenoising diffusion, we obtain a tractable way of computing the optimal KL\nbarycenter weights. We prove a dimension-free sample complexity bound in total\nvariation distance, provided that the auxiliary models are well-fitted for\ntheir own task and the auxiliary tasks combined capture the target well. The\nsample efficiency of ScoreFusion is demonstrated by learning handwritten\ndigits. We also provide a simple adaptation of a Stable Diffusion denoising\npipeline that enables sampling from the KL barycenter of two auxiliary\ncheckpoints; on a portrait generation task, our method produces faces that\nenhance population heterogeneity relative to the auxiliary distributions.\n","authors":["Hao Liu","Junze Tony Ye","Jose Blanchet","Nian Si"],"pdf_url":"https://arxiv.org/pdf/2406.19619v3.pdf","comment":"41 pages, 21 figures. Accepted as an Oral (top 2%) paper by AISTATS\n  2025"}],"Optimization":[{"id":"http://arxiv.org/abs/2504.13170v1","updated":"2025-04-17T17:59:23Z","published":"2025-04-17T17:59:23Z","title":"A New Semidefinite Relaxation for Linear and Piecewise-Affine Optimal\n  Control with Time Scaling","summary":"  We introduce a semidefinite relaxation for optimal control of linear systems\nwith time scaling. These problems are inherently nonconvex, since the system\ndynamics involves bilinear products between the discretization time step and\nthe system state and controls. The proposed relaxation is closely related to\nthe standard second-order semidefinite relaxation for quadratic constraints,\nbut we carefully select a subset of the possible bilinear terms and apply a\nchange of variables to achieve empirically tight relaxations while keeping the\ncomputational load light. We further extend our method to handle\npiecewise-affine (PWA) systems by formulating the PWA optimal-control problem\nas a shortest-path problem in a graph of convex sets (GCS). In this GCS,\ndifferent paths represent different mode sequences for the PWA system, and the\nconvex sets model the relaxed dynamics within each mode. By combining a tight\nconvex relaxation of the GCS problem with our semidefinite relaxation with time\nscaling, we can solve PWA optimal-control problems through a single\nsemidefinite program.\n","authors":["Lujie Yang","Tobia Marcucci","Pablo A. Parrilo","Russ Tedrake"],"pdf_url":"https://arxiv.org/pdf/2504.13170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08272v3","updated":"2025-04-17T17:37:22Z","published":"2024-02-13T07:56:45Z","title":"The gradient's limit of a definable family of functions admits a\n  variational stratification","summary":"  It is well-known that the convergence of a family of smooth functions does\nnot imply the convergence of its gradients. In this work, we show that if the\nfamily is definable in an o-minimal structure (for instance semialgebraic,\nsubanalytic, or any composition of the previous with exp, log), then the\ngradient's limit admits a variational stratification and, under mild\nassumptions, is a conservative set-valued field in the sense introduced by\nBolte and Pauwels. Immediate implications of this result on convergence\nguarantees of smoothing methods are discussed. The result is established in a\ngeneral form, where the functions in the original family might be non Lipschitz\ncontinuous, be vector-valued and the gradients are replaced by their Clarke\nJacobians or an arbitrary mapping satisfying a definable variational\nstratification. In passing, we investigate various stability properties of\ndefinable variational stratifications which might be of independent interest.\n","authors":["Sholom Schechtman"],"pdf_url":"https://arxiv.org/pdf/2402.08272v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13063v1","updated":"2025-04-17T16:18:56Z","published":"2025-04-17T16:18:56Z","title":"An exact approach for the multi-depot electric vehicle scheduling\n  problem","summary":"  The \"avoid - shift - improve\" framework and the European Clean Vehicles\nDirective set the path for improving the efficiency and ultimately\ndecarbonizing the transport sector. While electric buses have already been\nadopted in several cities, regional bus lines may pose additional challenges\ndue to the potentially longer distances they have to travel. In this work, we\nmodel and solve the electric bus scheduling problem, lexicographically\nminimizing the size of the bus fleet, the number of charging stops, and the\ntotal energy consumed, to provide decision support for bus operators planning\nto replace their diesel-powered fleet with zero emission vehicles. We propose a\ngraph representation which allows partial charging without explicitly relying\non time variables and derive 3-index and 2-index mixed-integer linear\nprogramming formulations for the multi-depot electric vehicle scheduling\nproblem. While the 3-index model can be solved by an off-the-shelf solver\ndirectly, the 2-index model relies on an exponential number of constraints to\nensure the correct depot pairing. These are separated in a cutting plane\nfashion. We propose a set of instances with up to 80 service trips to compare\nthe two approaches, showing that, with a small number of depots, the compact\n3-index model performs very well. However, as the number of depots increases\nthe developed branch-and-cut algorithm proves to be of value. These findings\nnot only offer algorithmic insights but the developed approaches also provide\nactionable guidance for transit agencies and operators, allowing to quantify\ntrade-offs between fleet size, energy efficiency, and infrastructure needs\nunder realistic operational conditions.\n","authors":["Xenia Haslinger","Elisabeth Gaar","Sophie N. Parragh"],"pdf_url":"https://arxiv.org/pdf/2504.13063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13046v1","updated":"2025-04-17T16:02:20Z","published":"2025-04-17T16:02:20Z","title":"Variance-Reduced Fast Operator Splitting Methods for Stochastic\n  Generalized Equations","summary":"  We develop two classes of variance-reduced fast operator splitting methods to\napproximate solutions of both finite-sum and stochastic generalized equations.\nOur approach integrates recent advances in accelerated fixed-point methods,\nco-hypomonotonicity, and variance reduction. First, we introduce a class of\nvariance-reduced estimators and establish their variance-reduction bounds. This\nclass covers both unbiased and biased instances and comprises common estimators\nas special cases, including SVRG, SAGA, SARAH, and Hybrid-SGD. Next, we design\na novel accelerated variance-reduced forward-backward splitting (FBS) algorithm\nusing these estimators to solve finite-sum and stochastic generalized\nequations. Our method achieves both $\\mathcal{O}(1/k^2)$ and $o(1/k^2)$\nconvergence rates on the expected squared norm $\\mathbb{E}[ \\|\nG_{\\lambda}x^k\\|^2]$ of the FBS residual $G_{\\lambda}$, where $k$ is the\niteration counter. Additionally, we establish, for the first time, almost sure\nconvergence rates and almost sure convergence of iterates to a solution in\nstochastic accelerated methods. Unlike existing stochastic fixed-point\nalgorithms, our methods accommodate co-hypomonotone operators, which\npotentially include nonmonotone problems arising from recent applications. We\nfurther specify our method to derive an appropriate variant for each stochastic\nestimator -- SVRG, SAGA, SARAH, and Hybrid-SGD -- demonstrating that they\nachieve the best-known complexity for each without relying on enhancement\ntechniques. Alternatively, we propose an accelerated variance-reduced\nbackward-forward splitting (BFS) method, which attains similar convergence\nrates and oracle complexity as our FBS method. Finally, we validate our results\nthrough several numerical experiments and compare their performance.\n","authors":["Quoc Tran-Dinh"],"pdf_url":"https://arxiv.org/pdf/2504.13046v1.pdf","comment":"58 pages, 1 table, and 8 figures"},{"id":"http://arxiv.org/abs/2504.13041v1","updated":"2025-04-17T15:55:37Z","published":"2025-04-17T15:55:37Z","title":"QI-MPC: A Hybrid Quantum-Inspired Model Predictive Control for Learning\n  Optimal Policies","summary":"  In this paper, we present Quantum-Inspired Model Predictive Control (QIMPC),\nan approach that uses Variational Quantum Circuits (VQCs) to learn control\npolices in MPC problems. The viability of the approach is tested in five\nexperiments: A target-tracking control strategy, energy-efficient building\nclimate control, autonomous vehicular dynamics, the simple pendulum, and the\ncompound pendulum. Three safety guarantees were established for the approach,\nand the experiments gave the motivation for two important theoretical results\nthat, in essence, identify systems for which the approach works best.\n","authors":["Muhammad Al-Zafar Khan","Jamal Al-Karaki"],"pdf_url":"https://arxiv.org/pdf/2504.13041v1.pdf","comment":"41 pages, 21 figures"},{"id":"http://arxiv.org/abs/2410.03931v2","updated":"2025-04-17T15:25:44Z","published":"2024-10-04T21:16:11Z","title":"Insights into Weighted Sum Sampling Approaches for Multi-Criteria\n  Decision Making Problems","summary":"  In this paper we explore several approaches for sampling weight vectors in\nthe context of weighted sum scalarisation approaches for solving multi-criteria\ndecision making (MCDM) problems. This established method converts a\nmulti-objective problem into a (single) scalar optimisation problem. It does so\nby assigning weights to each objective. We outline various methods to select\nthese weights, with a focus on ensuring computational efficiency and avoiding\nredundancy. The challenges and computational complexity of these approaches are\nexplored and numerical examples are provided. The theoretical results\ndemonstrate the trade-offs between systematic and randomised weight generation\ntechniques, highlighting their performance for different problem settings.\nThese sampling approaches will be tested and compared computationally in an\nupcoming paper.\n","authors":["Aled Williams","Yilun Cai"],"pdf_url":"https://arxiv.org/pdf/2410.03931v2.pdf","comment":"32 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.13006v1","updated":"2025-04-17T15:15:34Z","published":"2025-04-17T15:15:34Z","title":"Mathematical programs with complementarity constraints and application\n  to hyperparameter tuning for nonlinear support vector machines","summary":"  We consider the Mathematical Program with Complementarity Constraints (MPCC).\nOne of the main challenges in solving this problem is the systematic failure of\nstandard Constraint Qualifications (CQs). Carefully accounting for the\ncombinatorial nature of the complementarity constraints, tractable versions of\nthe Mangasarian Fromovitz Constraint Qualification (MFCQ) have been designed\nand widely studied in the literature. This paper looks closely at two such\nMPCC-MFCQs and their influence on MPCC algorithms. As a key contribution, we\nprove the convergence of the sequential penalisation and Scholtes relaxation\nalgorithms under a relaxed MPCC-MFCQ that is much weaker than the CQs currently\nused in the literature. We then form the problem of tuning hyperparameters of a\nnonlinear Support Vector Machine (SVM), a fundamental machine learning problem\nfor classification, as a MPCC. For this application, we establish that the\naforementioned relaxed MPCC-MFCQ holds under a very mild assumption. Moreover,\nwe program robust implementations and comprehensive numerical experimentation\non real-world data sets, where we show that the sequential penalisation method\napplied to the MPCC formulation for tuning SVM hyperparameters can outperform\nboth the Scholtes relaxation technique and the state-of-the-art derivative-free\nmethods from the machine learning literature.\n","authors":["Samuel Ward","Alain Zemkoho","Selin Ahipasaoglu"],"pdf_url":"https://arxiv.org/pdf/2504.13006v1.pdf","comment":"50 pages, 15 figures"},{"id":"http://arxiv.org/abs/2312.13807v2","updated":"2025-04-17T14:28:16Z","published":"2023-12-21T12:56:40Z","title":"Cluster-based classification with neural ODEs via control","summary":"  We address binary classification using neural ordinary differential equations\nfrom the perspective of simultaneous control of $N$ data points. We consider a\nsingle-neuron architecture with parameters fixed as piecewise constant\nfunctions of time. In this setting, the model complexity can be quantified by\nthe number of control switches. Previous work has shown that classification can\nbe achieved using a point-by-point strategy that requires $O(N)$ switches. We\npropose a new control method that classifies any arbitrary dataset by\nsequentially steering clusters of $d$ points, thereby reducing the complexity\nto $O(N/d)$ switches. The optimality of this result, particularly in high\ndimensions, is supported by some numerical experiments. Our complexity bound is\nsufficient but often conservative because same-class points tend to appear in\nlarger clusters, simplifying classification. This motivates studying the\nprobability distribution of the number of switches required. We introduce a\nsimple control method that imposes a collinearity constraint on the parameters,\nand analyze a worst-case scenario where both classes have the same size and all\npoints are i.i.d. Our results highlight the benefits of high-dimensional\nspaces, showing that classification using constant controls becomes more\nprobable as $d$ increases.\n","authors":["Antonio Álvarez-López","Rafael Orive-Illera","Enrique Zuazua"],"pdf_url":"https://arxiv.org/pdf/2312.13807v2.pdf","comment":"28 pages, 27 figures"},{"id":"http://arxiv.org/abs/2504.12958v1","updated":"2025-04-17T14:04:50Z","published":"2025-04-17T14:04:50Z","title":"An ILP formulation to optimize flood evacuation paths by minimizing\n  pedestrian speed, length and effort","summary":"  This document presents an Integer Linear Programming (ILP) approach to\noptimize pedestrian evacuation in flood-prone historic urban areas. The model\naims to minimize total evacuation cost by integrating pedestrian speed, route\nlength, and effort, while also selecting the optimal number and position of\nshelters. A modified minimum cost flow formulation is used to capture complex\nhydrodynamic and behavioral conditions within a directed street network. The\nevacuation problem is modeled through an extended graph representing the urban\nstreet network, where nodes and links simulate paths and shelters, including\nincomplete evacuations (deadly nodes), enabling accurate representation of\nreal-world constraints and network dynamics.\n","authors":["Fabrizio Marinelli","Andrea Pizzuti","Guido Romano","Gabriele Bernardini","Enrico Quagliarini"],"pdf_url":"https://arxiv.org/pdf/2504.12958v1.pdf","comment":"5 pages, 2 tables, 1 figure"},{"id":"http://arxiv.org/abs/2504.12944v1","updated":"2025-04-17T13:45:14Z","published":"2025-04-17T13:45:14Z","title":"A Bi-Objective MDP Design approach to redundancy allocation with dynamic\n  maintenance for a parallel system","summary":"  The reliability of a system can be improved by the addition of redundant\nelements, giving rise to the well-known redundancy allocation problem (RAP),\nwhich can be seen as a design problem. We propose a novel extension to the RAP\ncalled the Bi-Objective Integrated Design and Dynamic Maintenance Problem\n(BO-IDDMP) which allows for future dynamic maintenance decisions to be\nincorporated. This leads to a problem with first-stage redundancy design\ndecisions and a second-stage sequential decision problem. To the best of our\nknowledge, this is the first use of a continuous-time Markov Decision Process\nDesign framework to formulate a problem with non-trivial dynamics, as well as\nits first use alongside bi-objective optimization. A general heuristic\noptimization methodology for two-stage bi-objective programmes is developed,\nwhich is then applied to the BO-IDDMP. The efficiency and accuracy of our\nmethodology are demonstrated against an exact optimization formulation. The\nheuristic is shown to be orders of magnitude faster, and in only 2 out of 42\ncases fails to find one of the Pareto-optimal solutions found by the exact\nmethod. The inclusion of dynamic maintenance policies is shown to yield\nstronger and better-populated Pareto fronts, allowing more flexibility for the\ndecision-maker. The impacts of varying parameters unique to our problem are\nalso investigated.\n","authors":["Luke Fairley","Rob Shone","Peter Jacko","Jefferson Huang"],"pdf_url":"https://arxiv.org/pdf/2504.12944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16371v3","updated":"2025-04-17T13:26:56Z","published":"2025-01-22T21:19:42Z","title":"Which Optimizer Works Best for Physics-Informed Neural Networks and\n  Kolmogorov-Arnold Networks?","summary":"  Physics-Informed Neural Networks (PINNs) have revolutionized the computation\nof PDE solutions by integrating partial differential equations (PDEs) into the\nneural network's training process as soft constraints, becoming an important\ncomponent of the scientific machine learning (SciML) ecosystem. More recently,\nphysics-informed Kolmogorv-Arnold networks (PIKANs) have also shown to be\neffective and comparable in accuracy with PINNs. In their current\nimplementation, both PINNs and PIKANs are mainly optimized using first-order\nmethods like Adam, as well as quasi-Newton methods such as BFGS and its\nlow-memory variant, L-BFGS. However, these optimizers often struggle with\nhighly non-linear and non-convex loss landscapes, leading to challenges such as\nslow convergence, local minima entrapment, and (non)degenerate saddle points.\nIn this study, we investigate the performance of Self-Scaled BFGS (SSBFGS),\nSelf-Scaled Broyden (SSBroyden) methods and other advanced quasi-Newton\nschemes, including BFGS and L-BFGS with different line search strategies\napproaches. These methods dynamically rescale updates based on historical\ngradient information, thus enhancing training efficiency and accuracy. We\nsystematically compare these optimizers -- using both PINNs and PIKANs -- on\nkey challenging linear, stiff, multi-scale and non-linear PDEs, including the\nBurgers, Allen-Cahn, Kuramoto-Sivashinsky, and Ginzburg-Landau equations. Our\nfindings provide state-of-the-art results with orders-of-magnitude accuracy\nimprovements without the use of adaptive weights or any other enhancements\ntypically employed in PINNs. More broadly, our results reveal insights into the\neffectiveness of second-order optimization strategies in significantly\nimproving the convergence and accurate generalization of PINNs and PIKANs.\n","authors":["Elham Kiyani","Khemraj Shukla","Jorge F. Urbán","Jérôme Darbon","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2501.16371v3.pdf","comment":"36 pages, 27 figures"},{"id":"http://arxiv.org/abs/2504.12924v1","updated":"2025-04-17T13:15:26Z","published":"2025-04-17T13:15:26Z","title":"The Monge--Kantorovich problem, the Schur--Horn theorem, and the\n  diffeomorphism group of the annulus","summary":"  First, we analyze the discrete Monge--Kantorovich problem, linking it with\nthe minimization problem of linear functionals over adjoint orbits. Second, we\nconsider its generalization to the setting of area preserving diffeomorphisms\nof the annulus. In both cases, we show how the problem can be linked to\npermutohedra, majorization, and to gradient flows with respect to a suitable\nmetric.\n","authors":["Anthony M. Bloch","Tudor S. Ratiu"],"pdf_url":"https://arxiv.org/pdf/2504.12924v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2504.12922v1","updated":"2025-04-17T13:11:26Z","published":"2025-04-17T13:11:26Z","title":"On the asymptotic behaviour of stochastic processes, with applications\n  to supermartingale convergence, Dvoretzky's approximation theorem, and\n  stochastic quasi-Fejér monotonicity","summary":"  We prove a novel and general result on the asymptotic behavior of stochastic\nprocesses which conform to a certain relaxed supermartingale condition. Our\nresult provides quantitative information in the form of an explicit and\neffective construction of a rate of convergence for this process, both in mean\nand almost surely, that is moreover highly uniform in the sense that it only\ndepends on very few data of the surrounding objects involved in the iteration.\nWe then apply this result to derive new quantitative versions of well-known\nconcepts and theorems from stochastic approximation, in particular providing\neffective rates for a variant of the Robbins-Siegmund theorem, Dvoretzky's\nconvergence theorem, as well as the convergence of stochastic quasi-Fej\\'er\nmonotone sequences, the latter of which formulated in a novel and highly\ngeneral metric context. We utilize the classic and widely studied Robbins-Monro\nprocedure as a template to evaluate our quantitative results and their\napplicability in greater detail. We conclude by illustrating the breadth of\npotential further applications with a brief discussion on a variety of other\nwell-known iterative procedures from stochastic approximation, covering a range\nof different applied scenarios to which our methods can be immediately applied.\nThroughout, we isolate and discuss special cases of our results which even\nallow for the construction of fast, and in particular linear, rates.\n","authors":["Morenikeji Neri","Nicholas Pischke","Thomas Powell"],"pdf_url":"https://arxiv.org/pdf/2504.12922v1.pdf","comment":"41 pages"},{"id":"http://arxiv.org/abs/2504.12901v1","updated":"2025-04-17T12:46:00Z","published":"2025-04-17T12:46:00Z","title":"Control of blow-up profiles for the mass-critical focusing nonlinear\n  Schrödinger equation on bounded domains","summary":"  In this paper, we consider the mass-critical focusing nonlinear Schr\\\"odinger\non bounded two-dimensional domains with Dirichlet boundary conditions. In the\nabsence of control, it is well-known that free solutions starting from initial\ndata sufficiently large can blow-up. More precisely, given a finite number of\npoints, there exists particular profiles blowing up exactly at these points at\nthe blow-up time. For pertubations of these profiles, we show that, with the\nhelp of an appropriate nonlinear feedback law located in an open set containing\nthe blow-up points, the blow-up can be prevented from happening. More\nspecifically, we construct a small-time control acting just before the blow-up\ntime. The solution may then be extended globally in time. This is the first\nresult of control for blow-up profiles for nonlinear Schr\\\"odinger type\nequations. Assuming further a geometrical control condition on the support of\nthe control, we are able to prove a null-controllability result for such\nblow-up profiles. Finally, we discuss possible extensions to three-dimensional\ndomains.\n","authors":["Ludovick Gagnon","Kévin Le Balc'h"],"pdf_url":"https://arxiv.org/pdf/2504.12901v1.pdf","comment":"Comments welcome"},{"id":"http://arxiv.org/abs/2412.00986v2","updated":"2025-04-17T12:08:48Z","published":"2024-12-01T22:27:00Z","title":"A model of strategic sustainable investment","summary":"  We study a problem of optimal irreversible investment and emission reduction\nformulated as a nonzero-sum dynamic game between an investor with environmental\npreferences and a firm. The game is set in continuous time on an infinite-time\nhorizon. The firm generates profits with a stochastic dynamics and may spend\npart of its revenues towards emission reduction (e.g., renovating the\ninfrastructure). The firm's objective is to maximize the discounted expectation\nof a function of its profits. The investor participates in the profits, may\ndecide to invest to support the firm's production capacity and uses a profit\nfunction which accounts for both financial and environmental factors. Nash\nequilibria of the game are obtained via a system of variational inequalities.\nWe formulate a general verification theorem for this system in a diffusive\nsetup and construct an explicit solution in the zero-noise limit. Our explicit\nresults and numerical approximations show that both the investor's and the\nfirm's optimal actions are triggered by moving boundaries that increase with\nthe total amount of emission abatement.\n","authors":["Tiziano De Angelis","Caio César Graciani Rodrigues","Peter Tankov"],"pdf_url":"https://arxiv.org/pdf/2412.00986v2.pdf","comment":"44 pages; 9 figures; improved exposition and expanded numerical\n  analysis"},{"id":"http://arxiv.org/abs/2504.12835v1","updated":"2025-04-17T10:50:24Z","published":"2025-04-17T10:50:24Z","title":"Kinetic simulated annealing optimization with entropy-based cooling rate","summary":"  We present a modified simulated annealing method with a dynamical choice of\nthe cooling temperature. The latter is determined via a closed-loop control and\nis proven to yield exponential decay of the entropy of the particle system. The\nanalysis is carried out through kinetic equations for interacting particle\nsystems describing the simulated annealing method in an extended phase space.\nDecay estimates are derived under the quasi-invariant scaling of the resulting\nsystem of Boltzmann-type equations to assess the consistency with their\nmean-field limit. Numerical results are provided to illustrate and support the\ntheoretical findings.\n","authors":["Michael Herty","Mattia Zanella"],"pdf_url":"https://arxiv.org/pdf/2504.12835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12819v1","updated":"2025-04-17T10:24:33Z","published":"2025-04-17T10:24:33Z","title":"A scalable mixed-integer conic optimization approach to\n  cardinality-constrained Poisson regression with safe screening","summary":"  This paper introduces a novel approach for cardinality-constrained Poisson\nregression to address feature selection challenges in high-dimensional count\ndata. We formulate the problem as a mixed-integer conic optimization, enabling\nthe use of modern solvers for optimal solutions. To enhance computational\nefficiency, we develop a safe screening based on Fenchel conjugates, thereby\neffectively removing irrelevant features before optimization. Experiments on\nsynthetic datasets demonstrate that our safe screening significantly reduces\nthe problem size, leading to substantial improvements in computational time.\nOur approach can solve Poisson regression problems with tens of thousands of\nfeatures, exceeding the scale of previous studies. This work provides a\nvaluable tool for interpretable feature selection in high-dimensional Poisson\nregression.\n","authors":["Kota Kurihara","Yoichi Izunaga"],"pdf_url":"https://arxiv.org/pdf/2504.12819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12814v1","updated":"2025-04-17T10:17:32Z","published":"2025-04-17T10:17:32Z","title":"Integral control of the proximal gradient method for unbiased sparse\n  optimization","summary":"  Proximal gradient methods are popular in sparse optimization as they are\nstraightforward to implement. Nevertheless, they achieve biased solutions,\nrequiring many iterations to converge. This work addresses these issues through\na suitable feedback control of the algorithm's hyperparameter. Specifically, by\ndesigning an integral control that does not substantially impact the\ncomputational complexity, we can reach an unbiased solution in a reasonable\nnumber of iterations. In the paper, we develop and analyze the convergence of\nthe proposed approach for strongly-convex problems. Moreover, numerical\nsimulations validate and extend the theoretical results to the non-strongly\nconvex framework.\n","authors":["V. Cerone","S. M. Fosson","A. Re","D. Regruto"],"pdf_url":"https://arxiv.org/pdf/2504.12814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12792v1","updated":"2025-04-17T09:46:06Z","published":"2025-04-17T09:46:06Z","title":"Open Loop Layout Optimization: Feasible Path Planning and Exact\n  Door-to-Door Distance Calculation","summary":"  The Open Loop Layout Problem (OLLP) seeks to position rectangular cells of\nvarying dimensions on a plane without overlap, minimizing transportation costs\ncomputed as the flow-weighted sum of pairwise distances between cells. A key\nchallenge in OLLP is to compute accurate inter-cell distances along feasible\npaths that avoid rectangle intersections. Existing approaches approximate\ninter-cell distances using centroids, a simplification that can ignore physical\nconstraints, resulting in infeasible layouts or underestimated distances. This\nstudy proposes the first mathematical model that incorporates exact\ndoor-to-door distances and feasible paths under the Euclidean metric, with cell\ndoors acting as pickup and delivery points. Feasible paths between doors must\neither follow rectangle edges as corridors or take direct, unobstructed routes.\nTo address the NP-hardness of the problem, we present a metaheuristic framework\nwith a novel encoding scheme that embeds exact path calculations. Experiments\non standard benchmark instances confirm that our approach consistently\noutperforms existing methods, delivering superior solution quality and\npractical applicability.\n","authors":["Seyed Mahdi Shavarani","Bela Vizvari","Kovacs Gergely"],"pdf_url":"https://arxiv.org/pdf/2504.12792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12775v1","updated":"2025-04-17T09:17:45Z","published":"2025-04-17T09:17:45Z","title":"Linear ordinary differential equations constrained Gaussian Processes\n  for solving optimal control problems","summary":"  This paper presents an intrinsic approach for addressing control problems\nwith systems governed by linear ordinary differential equations (ODEs). We use\ncomputer algebra to constrain a Gaussian Process on solutions of ODEs. We\nobtain control functions via conditioning on datapoints. Our approach thereby\nconnects Algebra, Functional Analysis, Machine Learning and Control theory. We\ndiscuss the optimality of the control functions generated by the posterior mean\nof the Gaussian Process. We present numerical examples which underline the\npracticability of our approach.\n","authors":["Andreas Besginow","Markus Lange-Hegermann","Jörn Tebbe"],"pdf_url":"https://arxiv.org/pdf/2504.12775v1.pdf","comment":"Accepted at 9th IFAC Symposium on System Structure and Control (SSSC\n  2025)"},{"id":"http://arxiv.org/abs/2504.12759v1","updated":"2025-04-17T08:55:42Z","published":"2025-04-17T08:55:42Z","title":"Perturbed Proximal Gradient ADMM for Nonconvex Composite Optimization","summary":"  This paper proposes a Perturbed Proximal Gradient ADMM (PPG-ADMM) framework\nfor solving general nonconvex composite optimization problems, where the\nobjective function consists of a smooth nonconvex term and a nonsmooth weakly\nconvex term for both primal variables.\n  Unlike existing ADMM-based methods which necessitate the function associated\nwith the last updated primal variable to be smooth, the proposed PPG-ADMM\nremoves this restriction by introducing a perturbation mechanism, which also\nhelps reduce oscillations in the primal-dual updates, thereby improving\nconvergence stability.\n  By employing a linearization technique for the smooth term and the proximal\noperator for the nonsmooth and weakly convex term, the subproblems have\nclosed-form solutions, significantly reducing computational complexity. The\nconvergence is established through a technically constructed Lyapunov function,\nwhich guarantees sufficient descent and has a well-defined lower bound.\n  With properly chosen parameters, PPG-ADMM converges to an\n$\\epsilon$-approximate stationary point at a sublinear convergence rate of\n$\\mathcal{O}(1/\\sqrt{K})$.\n  Furthermore, by appropriately tuning the perturbation parameter $\\beta$, it\nachieves an $\\epsilon$-stationary point, providing stronger optimality\nguarantees. We further apply PPG-ADMM to two practical distributed nonconvex\ncomposite optimization problems, i.e., the distributed partial consensus\nproblem and the resource allocation problem. The algorithm operates in a fully\ndecentralized manner without a central coordinating node. Finally, numerical\nexperiments validate the effectiveness of PPG-ADMM, demonstrating its improved\nconvergence performance.\n","authors":["Yuan Zhou","Xinli Shi","Luyao Guo","Jinde Cao","Mahmoud Abdel-Aty"],"pdf_url":"https://arxiv.org/pdf/2504.12759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12742v1","updated":"2025-04-17T08:32:25Z","published":"2025-04-17T08:32:25Z","title":"Decentralized Nonconvex Composite Federated Learning with Gradient\n  Tracking and Momentum","summary":"  Decentralized Federated Learning (DFL) eliminates the reliance on the\nserver-client architecture inherent in traditional federated learning,\nattracting significant research interest in recent years. Simultaneously, the\nobjective functions in machine learning tasks are often nonconvex and\nfrequently incorporate additional, potentially nonsmooth regularization terms\nto satisfy practical requirements, thereby forming nonconvex composite\noptimization problems. Employing DFL methods to solve such general optimization\nproblems leads to the formulation of Decentralized Nonconvex Composite\nFederated Learning (DNCFL), a topic that remains largely underexplored. In this\npaper, we propose a novel DNCFL algorithm, termed \\bf{DEPOSITUM}. Built upon\nproximal stochastic gradient tracking, DEPOSITUM mitigates the impact of data\nheterogeneity by enabling clients to approximate the global gradient. The\nintroduction of momentums in the proximal gradient descent step, replacing\ntracking variables, reduces the variance introduced by stochastic gradients.\nAdditionally, DEPOSITUM supports local updates of client variables,\nsignificantly reducing communication costs. Theoretical analysis demonstrates\nthat DEPOSITUM achieves an expected $\\epsilon$-stationary point with an\niteration complexity of $\\mathcal{O}(1/\\epsilon^2)$. The proximal gradient,\nconsensus errors, and gradient estimation errors decrease at a sublinear rate\nof $\\mathcal{O}(1/T)$. With appropriate parameter selection, the algorithm\nachieves network-independent linear speedup without requiring mega-batch\nsampling. Finally, we apply DEPOSITUM to the training of neural networks on\nreal-world datasets, systematically examining the influence of various\nhyperparameters on its performance. Comparisons with other federated composite\noptimization algorithms validate the effectiveness of the proposed method.\n","authors":["Yuan Zhou","Xinli Shi","Xuelong Li","Jiachen Zhong","Guanghui Wen","Jinde Cao"],"pdf_url":"https://arxiv.org/pdf/2504.12742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12728v1","updated":"2025-04-17T08:09:23Z","published":"2025-04-17T08:09:23Z","title":"Seierstad Sufficient Conditions for Stochastic Optimal Control Problems\n  with Infinite Horizon","summary":"  In this note we consider a problem of stochastic optimal control with the\ninfinite-time horizon. We present analogues of the Seierstad sufficient\nconditions of overtaking optimality based on the dual variables stochastic\ndescribed by BSDEs appeared in the Bismut-Pontryagin maximum principle.\n","authors":["Anton O. Belyakov","Yuri M. Kabanov","Ivan A. Terekhov","Maxim M. Savinov"],"pdf_url":"https://arxiv.org/pdf/2504.12728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12713v1","updated":"2025-04-17T07:37:08Z","published":"2025-04-17T07:37:08Z","title":"Efficient Primal-dual Forward-backward Splitting Method for\n  Wasserstein-like Gradient Flows with General Nonlinear Mobilities","summary":"  We construct an efficient primal-dual forward-backward (PDFB) splitting\nmethod for computing a class of minimizing movement schemes with nonlinear\nmobility transport distances, and apply it to computing Wasserstein-like\ngradient flows. This approach introduces a novel saddle point formulation for\nthe minimizing movement schemes, leveraging a support function form from the\nBenamou-Brenier dynamical formulation of optimal transport. The resulting\nframework allows for flexible computation of Wasserstein-like gradient flows by\nsolving the corresponding saddle point problem at the fully discrete level, and\ncan be easily extended to handle general nonlinear mobilities. We also provide\na detailed convergence analysis of the PDFB splitting method, along with\npractical remarks on its implementation and application. The effectiveness of\nthe method is demonstrated through several challenging numerical examples.\n","authors":["Yunhong Deng","Li Wang","Chaozhen Wei"],"pdf_url":"https://arxiv.org/pdf/2504.12713v1.pdf","comment":"47pages, 12 figures"},{"id":"http://arxiv.org/abs/2504.12712v1","updated":"2025-04-17T07:35:48Z","published":"2025-04-17T07:35:48Z","title":"Convergence and Implicit Bias of Gradient Descent on Continual Linear\n  Classification","summary":"  We study continual learning on multiple linear classification tasks by\nsequentially running gradient descent (GD) for a fixed budget of iterations per\ntask. When all tasks are jointly linearly separable and are presented in a\ncyclic/random order, we show the directional convergence of the trained linear\nclassifier to the joint (offline) max-margin solution. This is surprising\nbecause GD training on a single task is implicitly biased towards the\nindividual max-margin solution for the task, and the direction of the joint\nmax-margin solution can be largely different from these individual solutions.\nAdditionally, when tasks are given in a cyclic order, we present a\nnon-asymptotic analysis on cycle-averaged forgetting, revealing that (1)\nalignment between tasks is indeed closely tied to catastrophic forgetting and\nbackward knowledge transfer and (2) the amount of forgetting vanishes to zero\nas the cycle repeats. Lastly, we analyze the case where the tasks are no longer\njointly separable and show that the model trained in a cyclic order converges\nto the unique minimum of the joint loss function.\n","authors":["Hyunji Jung","Hanseul Cho","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2504.12712v1.pdf","comment":"67 pages, 11 figures, accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2504.06932v2","updated":"2025-04-17T07:04:13Z","published":"2025-04-09T14:38:09Z","title":"Maximizing Battery Storage Profits via High-Frequency Intraday Trading","summary":"  Maximizing revenue for grid-scale battery energy storage systems in\ncontinuous intraday electricity markets requires strategies that are able to\nseize trading opportunities as soon as new information arrives. This paper\nintroduces and evaluates an automated high-frequency trading strategy for\nbattery energy storage systems trading on the intraday market for power while\nexplicitly considering the dynamics of the limit order book, market rules, and\ntechnical parameters. The standard rolling intrinsic strategy is adapted for\ncontinuous intraday electricity markets and solved using a dynamic programming\napproximation that is two to three orders of magnitude faster than an exact\nmixed-integer linear programming solution. A detailed backtest over a full year\nof German order book data demonstrates that the proposed dynamic programming\nformulation does not reduce trading profits and enables the policy to react to\nevery relevant order book update, enabling realistic rapid backtesting. Our\nresults show the significant revenue potential of high-frequency trading: our\npolicy earns 58% more than when re-optimizing only once every hour and 14% more\nthan when re-optimizing once per minute, highlighting that profits critically\ndepend on trading speed. Furthermore, we leverage the speed of our algorithm to\ntrain a parametric extension of the rolling intrinsic, increasing yearly\nrevenue by 8.4% out of sample.\n","authors":["David Schaurecker","David Wozabal","Nils Löhndorf","Thorsten Staake"],"pdf_url":"https://arxiv.org/pdf/2504.06932v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12635v1","updated":"2025-04-17T04:34:14Z","published":"2025-04-17T04:34:14Z","title":"On Equivalence Between Decentralized Policy-Profile Mixtures and\n  Behavioral Coordination Policies in Multi-Agent Systems","summary":"  Constrained decentralized team problem formulations are good models for many\ncooperative multi-agent systems. Constraints necessitate randomization when\nsolving for optimal solutions -- our past results show that joint randomization\namongst the team is necessary for (strong) Lagrangian duality to hold -- , but\na better understanding of randomization still remains. For a partially observed\nmulti-agent system with Borel hidden state and finite observations and actions,\nwe prove the equivalence between joint mixtures of decentralized\npolicy-profiles (both pure and behavioral) and common-information based\nbehavioral coordination policies (also mixtures of them). This generalizes past\nwork that shows equivalence between pure decentralized policy-profiles and pure\ncoordination policies. The equivalence can be exploited to develop results on\nstrong duality and number of randomizations.\n","authors":["Nouman Khan","Vijay G. Subramanian"],"pdf_url":"https://arxiv.org/pdf/2504.12635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04649v2","updated":"2025-04-17T03:47:25Z","published":"2025-03-06T17:35:37Z","title":"Transferable Foundation Models for Geometric Tasks on Point Cloud\n  Representations: Geometric Neural Operators","summary":"  We introduce methods for obtaining pretrained Geometric Neural Operators\n(GNPs) that can serve as basal foundation models for use in obtaining geometric\nfeatures. These can be used within data processing pipelines for machine\nlearning tasks and numerical methods. We show how our GNPs can be trained to\nlearn robust latent representations for the differential geometry of\npoint-clouds to provide estimates of metric, curvature, and other shape-related\nfeatures. We demonstrate how our pre-trained GNPs can be used (i) to estimate\nthe geometric properties of surfaces of arbitrary shape and topologies with\nrobustness in the presence of noise, (ii) to approximate solutions of geometric\npartial differential equations (PDEs) on manifolds, and (iii) to solve\nequations for shape deformations such as curvature driven flows. We release\ncodes and weights for using GNPs in the package geo_neural_op. This allows for\nincorporating our pre-trained GNPs as components for reuse within existing and\nnew data processing pipelines. The GNPs also can be used as part of numerical\nsolvers involving geometry or as part of methods for performing inference and\nother geometric tasks.\n","authors":["Blaine Quackenbush","Paul J. Atzberger"],"pdf_url":"https://arxiv.org/pdf/2503.04649v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12601v1","updated":"2025-04-17T02:56:20Z","published":"2025-04-17T02:56:20Z","title":"Stochastic Gradient Descent in Non-Convex Problems: Asymptotic\n  Convergence with Relaxed Step-Size via Stopping Time Methods","summary":"  Stochastic Gradient Descent (SGD) is widely used in machine learning\nresearch. Previous convergence analyses of SGD under the vanishing step-size\nsetting typically require Robbins-Monro conditions. However, in practice, a\nwider variety of step-size schemes are frequently employed, yet existing\nconvergence results remain limited and often rely on strong assumptions. This\npaper bridges this gap by introducing a novel analytical framework based on a\nstopping-time method, enabling asymptotic convergence analysis of SGD under\nmore relaxed step-size conditions and weaker assumptions. In the non-convex\nsetting, we prove the almost sure convergence of SGD iterates for step-sizes $\n\\{ \\epsilon_t \\}_{t \\geq 1} $ satisfying $\\sum_{t=1}^{+\\infty} \\epsilon_t =\n+\\infty$ and $\\sum_{t=1}^{+\\infty} \\epsilon_t^p < +\\infty$ for some $p > 2$.\nCompared with previous studies, our analysis eliminates the global Lipschitz\ncontinuity assumption on the loss function and relaxes the boundedness\nrequirements for higher-order moments of stochastic gradients. Building upon\nthe almost sure convergence results, we further establish $L_2$ convergence.\nThese significantly relaxed assumptions make our theoretical results more\ngeneral, thereby enhancing their applicability in practical scenarios.\n","authors":["Ruinan Jin","Difei Cheng","Hong Qiao","Xin Shi","Shaodong Liu","Bo Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.12601v1.pdf","comment":"42 pages"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2504.13180v1","updated":"2025-04-17T17:59:56Z","published":"2025-04-17T17:59:56Z","title":"PerceptionLM: Open-Access Data and Models for Detailed Visual\n  Understanding","summary":"  Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models.\n","authors":["Jang Hyun Cho","Andrea Madotto","Effrosyni Mavroudi","Triantafyllos Afouras","Tushar Nagarajan","Muhammad Maaz","Yale Song","Tengyu Ma","Shuming Hu","Suyog Jain","Miguel Martin","Huiyu Wang","Hanoona Rasheed","Peize Sun","Po-Yao Huang","Daniel Bolya","Nikhila Ravi","Shashank Jain","Tammy Stark","Shane Moon","Babak Damavandi","Vivian Lee","Andrew Westbury","Salman Khan","Philipp Krähenbühl","Piotr Dollár","Lorenzo Torresani","Kristen Grauman","Christoph Feichtenhofer"],"pdf_url":"https://arxiv.org/pdf/2504.13180v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2504.13178v1","updated":"2025-04-17T17:59:54Z","published":"2025-04-17T17:59:54Z","title":"Aligning Constraint Generation with Design Intent in Parametric CAD","summary":"  We adapt alignment techniques from reasoning LLMs to the task of generating\nengineering sketch constraints found in computer-aided design (CAD) models.\nEngineering sketches consist of geometric primitives (e.g. points, lines)\nconnected by constraints (e.g. perpendicular, tangent) that define the\nrelationships between them. For a design to be easily editable, the constraints\nmust effectively capture design intent, ensuring the geometry updates\npredictably when parameters change. Although current approaches can generate\nCAD designs, an open challenge remains to align model outputs with design\nintent, we label this problem `design alignment'. A critical first step towards\naligning generative CAD models is to generate constraints which fully-constrain\nall geometric primitives, without over-constraining or distorting sketch\ngeometry. Using alignment techniques to train an existing constraint generation\nmodel with feedback from a constraint solver, we are able to fully-constrain\n93% of sketches compared to 34% when using a na\\\"ive supervised fine-tuning\n(SFT) baseline and only 8.9% without alignment. Our approach can be applied to\nany existing constraint generation model and sets the stage for further\nresearch bridging alignment strategies between the language and design domains.\n","authors":["Evan Casey","Tianyu Zhang","Shu Ishida","John Roger Thompson","Amir Khasahmadi","Joseph George Lambourne","Pradeep Kumar Jayaraman","Karl D. D. Willis"],"pdf_url":"https://arxiv.org/pdf/2504.13178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13173v1","updated":"2025-04-17T17:59:33Z","published":"2025-04-17T17:59:33Z","title":"It's All Connected: A Journey Through Test-Time Memorization,\n  Attentional Bias, Retention, and Online Optimization","summary":"  Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.\n","authors":["Ali Behrouz","Meisam Razaviyayn","Peilin Zhong","Vahab Mirrokni"],"pdf_url":"https://arxiv.org/pdf/2504.13173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13151v1","updated":"2025-04-17T17:55:45Z","published":"2025-04-17T17:55:45Z","title":"MIB: A Mechanistic Interpretability Benchmark","summary":"  How can we know whether new mechanistic interpretability methods achieve real\nimprovements? In pursuit of meaningful and lasting evaluation standards, we\npropose MIB, a benchmark with two tracks spanning four tasks and five models.\nMIB favors methods that precisely and concisely recover relevant causal\npathways or specific causal variables in neural language models. The circuit\nlocalization track compares methods that locate the model components - and\nconnections between them - most important for performing a task (e.g.,\nattribution patching or information flow routes). The causal variable\nlocalization track compares methods that featurize a hidden vector, e.g.,\nsparse autoencoders (SAEs) or distributed alignment search (DAS), and locate\nmodel features for a causal variable relevant to the task. Using MIB, we find\nthat attribution and mask optimization methods perform best on circuit\nlocalization. For causal variable localization, we find that the supervised DAS\nmethod performs best, while SAE features are not better than neurons, i.e.,\nstandard dimensions of hidden vectors. These findings illustrate that MIB\nenables meaningful comparisons of methods, and increases our confidence that\nthere has been real progress in the field.\n","authors":["Aaron Mueller","Atticus Geiger","Sarah Wiegreffe","Dana Arad","Iván Arcuschin","Adam Belfki","Yik Siu Chan","Jaden Fiotto-Kaufman","Tal Haklay","Michael Hanna","Jing Huang","Rohan Gupta","Yaniv Nikankin","Hadas Orgad","Nikhil Prakash","Anja Reusch","Aruna Sankaranarayanan","Shun Shao","Alessandro Stolfo","Martin Tutek","Amir Zur","David Bau","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2504.13151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15610v2","updated":"2025-04-17T17:52:57Z","published":"2025-02-21T17:31:22Z","title":"A general language model for peptide identification","summary":"  Advances in peptide identification are revolutionizing our ability to\ndecipher protein functions and accelerate therapeutic discovery. We present\nPDeepPP, a deep learning framework that integrates pretrained protein language\nmodels with parallel transformer-CNN architectures, achieving state-of-the-art\nperformance in peptide characterization tasks. The model's hybrid architecture\ndemonstrates unique capabilities in capturing both local sequence motifs and\nglobal structural features, as evidenced by 29% improved cluster separation in\nUMAP visualizations compared to conventional approaches. Evaluated across 33\nbiological recognition tasks - including post-translational modification site\nprediction and bioactive peptide identification - PDeepPP outperformed existing\nmethods in 25 tasks with average AUC improvements of 4.2%. Notably, it achieved\n0.9726 accuracy with PR AUC 0.9977 in antimicrobial peptide detection while\nreducing false negatives by 37.5% in antimalarial recognition scenarios. This\nframework enables accurate large-scale peptide analysis, achieving 218*\nacceleration over sequence-alignment-based methods while maintaining 99.5%\nspecificity in critical glycosylation site detection.PDeepPP establishes a new\nparadigm for computational peptide analysis through its synergistic\narchitecture design, enabling rapid yet precise functional annotation that\nbridges molecular pattern recognition with translational biomedical\napplications.We have made our implementation, including code, data, and\npretrained models, publicly available via GitHub\n(https://github.com/fondress/PDeepPP) and Hugging Face\n(https://huggingface.co/fondress/PDeppPP).\n","authors":["Jixiu Zhai","Tianchi Lu","Haitian Zhong","Ziyang Xu","Yuhuan Liu","Shengrui Xu","Jingwan Wang","Dan Huang"],"pdf_url":"https://arxiv.org/pdf/2502.15610v2.pdf","comment":"21 pages, 9 figures, 4 tables, submitted to arXiv"},{"id":"http://arxiv.org/abs/2504.13142v1","updated":"2025-04-17T17:51:38Z","published":"2025-04-17T17:51:38Z","title":"Transfer Learning via Auxiliary Labels with Application to\n  Cold-Hardiness Prediction","summary":"  Cold temperatures can cause significant frost damage to fruit crops depending\non their resilience, or cold hardiness, which changes throughout the dormancy\nseason. This has led to the development of predictive cold-hardiness models,\nwhich help farmers decide when to deploy expensive frost-mitigation measures.\nUnfortunately, cold-hardiness data for model training is only available for\nsome fruit cultivars due to the need for specialized equipment and expertise.\nRather, farmers often do have years of phenological data (e.g. date of\nbudbreak) that they regularly collect for their crops. In this work, we\nintroduce a new transfer-learning framework, Transfer via Auxiliary Labels\n(TAL), that allows farmers to leverage the phenological data to produce more\naccurate cold-hardiness predictions, even when no cold-hardiness data is\navailable for their specific crop. The framework assumes a set of source tasks\n(cultivars) where each has associated primary labels (cold hardiness) and\nauxiliary labels (phenology). However, the target task (new cultivar) is\nassumed to only have the auxiliary labels. The goal of TAL is to predict\nprimary labels for the target task via transfer from the source tasks.\nSurprisingly, despite the vast literature on transfer learning, to our\nknowledge, the TAL formulation has not been previously addressed. Thus, we\npropose several new TAL approaches based on model selection and averaging that\ncan leverage recent deep multi-task models for cold-hardiness prediction. Our\nresults on real-world cold-hardiness and phenological data for multiple grape\ncultivars demonstrate that TAL can leverage the phenological data to improve\ncold-hardiness predictions in the absence of cold-hardiness data.\n","authors":["Kristen Goebel","Paola Pesantez-Cabrera","Markus Keller","Alan Fern"],"pdf_url":"https://arxiv.org/pdf/2504.13142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13213v4","updated":"2025-04-17T17:51:35Z","published":"2024-09-20T04:50:49Z","title":"MalMixer: Few-Shot Malware Classification with Retrieval-Augmented\n  Semi-Supervised Learning","summary":"  Recent growth and proliferation of malware have tested practitioners ability\nto promptly classify new samples according to malware families. In contrast to\nlabor-intensive reverse engineering efforts, machine learning approaches have\ndemonstrated increased speed and accuracy. However, most existing deep-learning\nmalware family classifiers must be calibrated using a large number of samples\nthat are painstakingly manually analyzed before training. Furthermore, as novel\nmalware samples arise that are beyond the scope of the training set, additional\nreverse engineering effort must be employed to update the training set. The\nsheer volume of new samples found in the wild creates substantial pressure on\npractitioners ability to reverse engineer enough malware to adequately train\nmodern classifiers. In this paper, we present MalMixer, a malware family\nclassifier using semi-supervised learning that achieves high accuracy with\nsparse training data. We present a domain-knowledge-aware data augmentation\ntechnique for malware feature representations, enhancing few-shot performance\nof semi-supervised malware family classification. We show that MalMixer\nachieves state-of-the-art performance in few-shot malware family classification\nsettings. Our research confirms the feasibility and effectiveness of\nlightweight, domain-knowledge-aware data augmentation methods for malware\nfeatures and shows the capabilities of similar semi-supervised classifiers in\naddressing malware classification issues.\n","authors":["Jiliang Li","Yifan Zhang","Yu Huang","Kevin Leach"],"pdf_url":"https://arxiv.org/pdf/2409.13213v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13139v1","updated":"2025-04-17T17:49:40Z","published":"2025-04-17T17:49:40Z","title":"Syntactic and Semantic Control of Large Language Models via Sequential\n  Monte Carlo","summary":"  A wide range of LM applications require generating text that conforms to\nsyntactic or semantic constraints. Imposing such constraints can be naturally\nframed as probabilistic conditioning, but exact generation from the resulting\ndistribution -- which can differ substantially from the LM's base distribution\n-- is generally intractable. In this work, we develop an architecture for\ncontrolled LM generation based on sequential Monte Carlo (SMC). Our SMC\nframework allows us to flexibly incorporate domain- and problem-specific\nconstraints at inference time, and efficiently reallocate computational\nresources in light of new information during the course of generation. By\ncomparing to a number of alternatives and ablations on four challenging domains\n-- Python code generation for data science, text-to-SQL, goal inference, and\nmolecule synthesis -- we demonstrate that, with little overhead, our approach\nallows small open-source language models to outperform models over 8x larger,\nas well as closed-source, fine-tuned ones. In support of the probabilistic\nperspective, we show that these performance improvements are driven by better\napproximation to the posterior distribution. Our system builds on the framework\nof Lew et al. (2023) and integrates with its language model probabilistic\nprogramming language, giving users a simple, programmable way to apply SMC to a\nbroad variety of controlled generation problems.\n","authors":["João Loula","Benjamin LeBrun","Li Du","Ben Lipkin","Clemente Pasti","Gabriel Grand","Tianyu Liu","Yahya Emara","Marjorie Freedman","Jason Eisner","Ryan Cotterel","Vikash Mansinghka","Alexander K. Lew","Tim Vieira","Timothy J. O'Donnell"],"pdf_url":"https://arxiv.org/pdf/2504.13139v1.pdf","comment":"34 pages, 4 figures"},{"id":"http://arxiv.org/abs/2501.17070v3","updated":"2025-04-17T17:48:53Z","published":"2025-01-28T16:55:39Z","title":"Contextual Agent Security: A Policy for Every Purpose","summary":"  Judging an action's safety requires knowledge of the context in which the\naction takes place. To human agents who act in various contexts, this may seem\nobvious: performing an action such as email deletion may or may not be\nappropriate depending on the email's content, the goal (e.g., to erase\nsensitive emails or to clean up trash), and the type of email address (e.g.,\nwork or personal). Unlike people, computational systems have often had only\nlimited agency in limited contexts. Thus, manually crafted policies and user\nconfirmation (e.g., smartphone app permissions or network access control\nlists), while imperfect, have sufficed to restrict harmful actions. However,\nwith the upcoming deployment of generalist agents that support a multitude of\ntasks (e.g., an automated personal assistant), we argue that we must rethink\nsecurity designs to adapt to the scale of contexts and capabilities of these\nsystems. As a first step, this paper explores contextual security in the domain\nof agents and proposes contextual agent security (Conseca), a framework to\ngenerate just-in-time, contextual, and human-verifiable security policies.\n","authors":["Lillian Tsai","Eugene Bagdasarian"],"pdf_url":"https://arxiv.org/pdf/2501.17070v3.pdf","comment":"Workshop in Hot Topics in Operating Systems (HotOS) 2025"},{"id":"http://arxiv.org/abs/2504.13134v1","updated":"2025-04-17T17:47:15Z","published":"2025-04-17T17:47:15Z","title":"Energy-Based Reward Models for Robust Language Model Alignment","summary":"  Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM.\n","authors":["Anamika Lochab","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.13134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13129v1","updated":"2025-04-17T17:44:19Z","published":"2025-04-17T17:44:19Z","title":"Science-T2I: Addressing Scientific Illusions in Image Synthesis","summary":"  We present a novel approach to integrating scientific knowledge into\ngenerative models, enhancing their realism and consistency in image synthesis.\nFirst, we introduce Science-T2I, an expert-annotated adversarial dataset\ncomprising adversarial 20k image pairs with 9k prompts, covering wide distinct\nscientific knowledge categories. Leveraging Science-T2I, we present SciScore,\nan end-to-end reward model that refines the assessment of generated images\nbased on scientific knowledge, which is achieved by augmenting both the\nscientific comprehension and visual capabilities of pre-trained CLIP model.\nAdditionally, based on SciScore, we propose a two-stage training framework,\ncomprising a supervised fine-tuning phase and a masked online fine-tuning\nphase, to incorporate scientific knowledge into existing generative models.\nThrough comprehensive experiments, we demonstrate the effectiveness of our\nframework in establishing new standards for evaluating the scientific realism\nof generated content. Specifically, SciScore attains performance comparable to\nhuman-level, demonstrating a 5% improvement similar to evaluations conducted by\nexperienced human evaluators. Furthermore, by applying our proposed fine-tuning\nmethod to FLUX, we achieve a performance enhancement exceeding 50% on SciScore.\n","authors":["Jialuo Li","Wenhao Chai","Xingyu Fu","Haiyang Xu","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2504.13129v1.pdf","comment":"Accepted to CVPR 2025. Code, docs, weight, benchmark and training\n  data are all avaliable at https://jialuo-li.github.io/Science-T2I-Web"},{"id":"http://arxiv.org/abs/2504.13125v1","updated":"2025-04-17T17:42:02Z","published":"2025-04-17T17:42:02Z","title":"LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM\n  Leaderboard","summary":"  This paper investigates the application of large language models (LLMs) to\nfinancial tasks. We fine-tuned foundation models using the Open FinLLM\nLeaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed\ntechniques including supervised fine-tuning (SFT), direct preference\noptimization (DPO), and reinforcement learning (RL) to enhance their financial\ncapabilities. The fine-tuned models demonstrated substantial performance gains\nacross a wide range of financial tasks. Moreover, we measured the data scaling\nlaw in the financial domain. Our work demonstrates the potential of large\nlanguage models (LLMs) in financial applications.\n","authors":["Varun Rao","Youran Sun","Mahendra Kumar","Tejas Mutneja","Agastya Mukherjee","Haizhao Yang"],"pdf_url":"https://arxiv.org/pdf/2504.13125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13122v1","updated":"2025-04-17T17:39:41Z","published":"2025-04-17T17:39:41Z","title":"VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models","summary":"  Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO.\n","authors":["Haojian Huang","Haodong Chen","Shengqiong Wu","Meng Luo","Jinlan Fu","Xinya Du","Hanwang Zhang","Hao Fei"],"pdf_url":"https://arxiv.org/pdf/2504.13122v1.pdf","comment":"Code and Data: https://github.com/HaroldChen19/VistaDPO"},{"id":"http://arxiv.org/abs/2504.13116v1","updated":"2025-04-17T17:33:15Z","published":"2025-04-17T17:33:15Z","title":"Predicting BVD Re-emergence in Irish Cattle From Highly Imbalanced\n  Herd-Level Data Using Machine Learning Algorithms","summary":"  Bovine Viral Diarrhoea (BVD) has been the focus of a successful eradication\nprogramme in Ireland, with the herd-level prevalence declining from 11.3% in\n2013 to just 0.2% in 2023. As the country moves toward BVD freedom, the\ndevelopment of predictive models for targeted surveillance becomes increasingly\nimportant to mitigate the risk of disease re-emergence. In this study, we\nevaluate the performance of a range of machine learning algorithms, including\nbinary classification and anomaly detection techniques, for predicting\nBVD-positive herds using highly imbalanced herd-level data. We conduct an\nextensive simulation study to assess model performance across varying sample\nsizes and class imbalance ratios, incorporating resampling, class weighting,\nand appropriate evaluation metrics (sensitivity, positive predictive value,\nF1-score and AUC values). Random forests and XGBoost models consistently\noutperformed other methods, with the random forest model achieving the highest\nsensitivity and AUC across scenarios, including real-world prediction of 2023\nherd status, correctly identifying 219 of 250 positive herds while halving the\nnumber of herds that require compared to a blanket-testing strategy.\n","authors":["Niamh Mimnagh","Andrew Parnell","Conor McAloon","Jaden Carlson","Maria Guelbenzu","Jonas Brock","Damien Barrett","Guy McGrath","Jamie Tratalos","Rafael Moral"],"pdf_url":"https://arxiv.org/pdf/2504.13116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13113v1","updated":"2025-04-17T17:27:39Z","published":"2025-04-17T17:27:39Z","title":"Quorum: Zero-Training Unsupervised Anomaly Detection using Quantum\n  Autoencoders","summary":"  Detecting mission-critical anomalous events and data is a crucial challenge\nacross various industries, including finance, healthcare, and energy. Quantum\ncomputing has recently emerged as a powerful tool for tackling several machine\nlearning tasks, but training quantum machine learning models remains\nchallenging, particularly due to the difficulty of gradient calculation. The\nchallenge is even greater for anomaly detection, where unsupervised learning\nmethods are essential to ensure practical applicability. To address these\nissues, we propose Quorum, the first quantum anomaly detection framework\ndesigned for unsupervised learning that operates without requiring any\ntraining.\n","authors":["Jason Zev Ludmir","Sophia Rebello","Jacob Ruiz","Tirthak Patel"],"pdf_url":"https://arxiv.org/pdf/2504.13113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13112v1","updated":"2025-04-17T17:26:29Z","published":"2025-04-17T17:26:29Z","title":"Hadamard product in deep learning: Introduction, Advances and Challenges","summary":"  While convolution and self-attention mechanisms have dominated architectural\ndesign in deep learning, this survey examines a fundamental yet understudied\nprimitive: the Hadamard product. Despite its widespread implementation across\nvarious applications, the Hadamard product has not been systematically analyzed\nas a core architectural primitive. We present the first comprehensive taxonomy\nof its applications in deep learning, identifying four principal domains:\nhigher-order correlation, multimodal data fusion, dynamic representation\nmodulation, and efficient pairwise operations. The Hadamard product's ability\nto model nonlinear interactions with linear computational complexity makes it\nparticularly valuable for resource-constrained deployments and edge computing\nscenarios. We demonstrate its natural applicability in multimodal fusion tasks,\nsuch as visual question answering, and its effectiveness in representation\nmasking for applications including image inpainting and pruning. This\nsystematic review not only consolidates existing knowledge about the Hadamard\nproduct's role in deep learning architectures but also establishes a foundation\nfor future architectural innovations. Our analysis reveals the Hadamard product\nas a versatile primitive that offers compelling trade-offs between\ncomputational efficiency and representational power, positioning it as a\ncrucial component in the deep learning toolkit.\n","authors":["Grigorios G Chrysos","Yongtao Wu","Razvan Pascanu","Philip Torr","Volkan Cevher"],"pdf_url":"https://arxiv.org/pdf/2504.13112v1.pdf","comment":"Accepted in IEEE T-PAMI"},{"id":"http://arxiv.org/abs/2504.13111v1","updated":"2025-04-17T17:24:50Z","published":"2025-04-17T17:24:50Z","title":"Uncertainty-Aware Trajectory Prediction via Rule-Regularized\n  Heteroscedastic Deep Classification","summary":"  Deep learning-based trajectory prediction models have demonstrated promising\ncapabilities in capturing complex interactions. However, their\nout-of-distribution generalization remains a significant challenge,\nparticularly due to unbalanced data and a lack of enough data and diversity to\nensure robustness and calibration. To address this, we propose SHIFT (Spectral\nHeteroscedastic Informed Forecasting for Trajectories), a novel framework that\nuniquely combines well-calibrated uncertainty modeling with informative priors\nderived through automated rule extraction. SHIFT reformulates trajectory\nprediction as a classification task and employs heteroscedastic\nspectral-normalized Gaussian processes to effectively disentangle epistemic and\naleatoric uncertainties. We learn informative priors from training labels,\nwhich are automatically generated from natural language driving rules, such as\nstop rules and drivability constraints, using a retrieval-augmented generation\nframework powered by a large language model. Extensive evaluations over the\nnuScenes dataset, including challenging low-data and cross-location scenarios,\ndemonstrate that SHIFT outperforms state-of-the-art methods, achieving\nsubstantial gains in uncertainty calibration and displacement metrics. In\nparticular, our model excels in complex scenarios, such as intersections, where\nuncertainty is inherently higher. Project page:\nhttps://kumarmanas.github.io/SHIFT/.\n","authors":["Kumar Manas","Christian Schlauch","Adrian Paschke","Christian Wirth","Nadja Klein"],"pdf_url":"https://arxiv.org/pdf/2504.13111v1.pdf","comment":"17 Pages, 9 figures. Accepted to Robotics: Science and Systems(RSS),\n  2025"},{"id":"http://arxiv.org/abs/2504.13110v1","updated":"2025-04-17T17:24:38Z","published":"2025-04-17T17:24:38Z","title":"Propagation of Chaos in One-hidden-layer Neural Networks beyond\n  Logarithmic Time","summary":"  We study the approximation gap between the dynamics of a polynomial-width\nneural network and its infinite-width counterpart, both trained using projected\ngradient descent in the mean-field scaling regime. We demonstrate how to\ntightly bound this approximation gap through a differential equation governed\nby the mean-field dynamics. A key factor influencing the growth of this ODE is\nthe local Hessian of each particle, defined as the derivative of the particle's\nvelocity in the mean-field dynamics with respect to its position. We apply our\nresults to the canonical feature learning problem of estimating a\nwell-specified single-index model; we permit the information exponent to be\narbitrarily large, leading to convergence times that grow polynomially in the\nambient dimension $d$. We show that, due to a certain ``self-concordance''\nproperty in these problems -- where the local Hessian of a particle is bounded\nby a constant times the particle's velocity -- polynomially many neurons are\nsufficient to closely approximate the mean-field dynamics throughout training.\n","authors":["Margalit Glasgow","Denny Wu","Joan Bruna"],"pdf_url":"https://arxiv.org/pdf/2504.13110v1.pdf","comment":"70 pages"},{"id":"http://arxiv.org/abs/2411.04228v2","updated":"2025-04-17T17:23:08Z","published":"2024-11-06T19:50:00Z","title":"dsld: A Socially Relevant Tool for Teaching Statistics","summary":"  The growing power of data science can play a crucial role in addressing\nsocial discrimination, necessitating nuanced understanding and effective\nmitigation strategies for biases. \"Data Science Looks At Discrimination\" (DSLD)\nis an R and Python package designed to provide users with a comprehensive\ntoolkit of statistical and graphical methods for assessing possible\ndiscrimination related to protected groups such as race, gender, and age. The\npackage addresses critical issues by identifying and mitigating confounders and\nreducing bias against protected groups in prediction algorithms.\n  In educational settings, DSLD offers instructors powerful tools to teach\nstatistical principles through motivating real world examples of discrimination\nanalysis. The inclusion of an 80 page Quarto book further supports users from\nstatistics educators to legal professionals in effectively applying these\nanalytical tools to real world scenarios.\n","authors":["Taha Abdullah","Arjun Ashok","Brandon Zarate","Shubhada Martha","Billy Ouattara","Norman Matloff","Aditya Mittal"],"pdf_url":"https://arxiv.org/pdf/2411.04228v2.pdf","comment":"To be submitted to journal"},{"id":"http://arxiv.org/abs/2504.13101v1","updated":"2025-04-17T17:10:33Z","published":"2025-04-17T17:10:33Z","title":"An Empirically Grounded Identifiability Theory Will Accelerate\n  Self-Supervised Learning Research","summary":"  Self-Supervised Learning (SSL) powers many current AI systems. As research\ninterest and investment grow, the SSL design space continues to expand. The\nPlatonic view of SSL, following the Platonic Representation Hypothesis (PRH),\nsuggests that despite different methods and engineering approaches, all\nrepresentations converge to the same Platonic ideal. However, this phenomenon\nlacks precise theoretical explanation. By synthesizing evidence from\nIdentifiability Theory (IT), we show that the PRH can emerge in SSL. However,\ncurrent IT cannot explain SSL's empirical success. To bridge the gap between\ntheory and practice, we propose expanding IT into what we term Singular\nIdentifiability Theory (SITh), a broader theoretical framework encompassing the\nentire SSL pipeline. SITh would allow deeper insights into the implicit data\nassumptions in SSL and advance the field towards learning more interpretable\nand generalizable representations. We highlight three critical directions for\nfuture research: 1) training dynamics and convergence properties of SSL; 2) the\nimpact of finite samples, batch size, and data diversity; and 3) the role of\ninductive biases in architecture, augmentations, initialization schemes, and\noptimizers.\n","authors":["Patrik Reizinger","Randall Balestriero","David Klindt","Wieland Brendel"],"pdf_url":"https://arxiv.org/pdf/2504.13101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09597v2","updated":"2025-04-17T16:53:17Z","published":"2025-04-13T14:31:52Z","title":"Understanding LLM Behaviors via Compression: Data Generation, Knowledge\n  Acquisition and Scaling Laws","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions.\n","authors":["Zhixuan Pan","Shaowen Wang","Jian Li"],"pdf_url":"https://arxiv.org/pdf/2504.09597v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17765v4","updated":"2025-04-17T16:40:48Z","published":"2023-03-31T01:56:13Z","title":"Learning from Similar Linear Representations: Adaptivity, Minimaxity,\n  and Robustness","summary":"  Representation multi-task learning (MTL) has achieved tremendous success in\npractice. However, the theoretical understanding of these methods is still\nlacking. Most existing theoretical works focus on cases where all tasks share\nthe same representation, and claim that MTL almost always improves performance.\nNevertheless, as the number of tasks grows, assuming all tasks share the same\nrepresentation is unrealistic. Furthermore, empirical findings often indicate\nthat a shared representation does not necessarily improve single-task learning\nperformance. In this paper, we aim to understand how to learn from tasks with\n\\textit{similar but not exactly the same} linear representations, while dealing\nwith outlier tasks. Assuming a known intrinsic dimension, we propose a\npenalized empirical risk minimization method and a spectral method that are\n\\textit{adaptive} to the similarity structure and \\textit{robust} to outlier\ntasks. Both algorithms outperform single-task learning when representations\nacross tasks are sufficiently similar and the proportion of outlier tasks is\nsmall. Moreover, they always perform at least as well as single-task learning,\neven when the representations are dissimilar. We provide information-theoretic\nlower bounds to demonstrate that both methods are nearly \\textit{minimax}\noptimal in a large regime, with the spectral method being optimal in the\nabsence of outlier tasks. Additionally, we introduce a thresholding algorithm\nto adapt to an unknown intrinsic dimension. We conduct extensive numerical\nexperiments to validate our theoretical findings.\n","authors":["Ye Tian","Yuqi Gu","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2303.17765v4.pdf","comment":"125 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2504.13075v1","updated":"2025-04-17T16:37:41Z","published":"2025-04-17T16:37:41Z","title":"An All-Atom Generative Model for Designing Protein Complexes","summary":"  Proteins typically exist in complexes, interacting with other proteins or\nbiomolecules to perform their specific biological roles. Research on\nsingle-chain protein modeling has been extensively and deeply explored, with\nadvancements seen in models like the series of ESM and AlphaFold. Despite these\ndevelopments, the study and modeling of multi-chain proteins remain largely\nuncharted, though they are vital for understanding biological functions.\nRecognizing the importance of these interactions, we introduce APM (All-Atom\nProtein Generative Model), a model specifically designed for modeling\nmulti-chain proteins. By integrating atom-level information and leveraging data\non multi-chain proteins, APM is capable of precisely modeling inter-chain\ninteractions and designing protein complexes with binding capabilities from\nscratch. It also performs folding and inverse-folding tasks for multi-chain\nproteins. Moreover, APM demonstrates versatility in downstream applications: it\nachieves enhanced performance through supervised fine-tuning (SFT) while also\nsupporting zero-shot sampling in certain tasks, achieving state-of-the-art\nresults. Code will be released at https://github.com/bytedance/apm.\n","authors":["Ruizhe Chen","Dongyu Xue","Xiangxin Zhou","Zaixiang Zheng","Xiangxiang Zeng","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2504.13075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15528v2","updated":"2025-04-17T16:22:17Z","published":"2024-09-23T20:26:51Z","title":"Learning Diverse Robot Striking Motions with Diffusion Models and\n  Kinematically Constrained Gradient Guidance","summary":"  Advances in robot learning have enabled robots to generate skills for a\nvariety of tasks. Yet, robot learning is typically sample inefficient,\nstruggles to learn from data sources exhibiting varied behaviors, and does not\nnaturally incorporate constraints. These properties are critical for fast,\nagile tasks such as playing table tennis. Modern techniques for learning from\ndemonstration improve sample efficiency and scale to diverse data, but are\nrarely evaluated on agile tasks. In the case of reinforcement learning,\nachieving good performance requires training on high-fidelity simulators. To\novercome these limitations, we develop a novel diffusion modeling approach that\nis offline, constraint-guided, and expressive of diverse agile behaviors. The\nkey to our approach is a kinematic constraint gradient guidance (KCGG)\ntechnique that computes gradients through both the forward kinematics of the\nrobot arm and the diffusion model to direct the sampling process. KCGG\nminimizes the cost of violating constraints while simultaneously keeping the\nsampled trajectory in-distribution of the training data. We demonstrate the\neffectiveness of our approach for time-critical robotic tasks by evaluating\nKCGG in two challenging domains: simulated air hockey and real table tennis. In\nsimulated air hockey, we achieved a 25.4% increase in block rate, while in\ntable tennis, we saw a 17.3% increase in success rate compared to imitation\nlearning baselines.\n","authors":["Kin Man Lee","Sean Ye","Qingyu Xiao","Zixuan Wu","Zulfiqar Zaidi","David B. D'Ambrosio","Pannag R. Sanketi","Matthew Gombolay"],"pdf_url":"https://arxiv.org/pdf/2409.15528v2.pdf","comment":"ICRA 2025"},{"id":"http://arxiv.org/abs/2504.13061v1","updated":"2025-04-17T16:15:38Z","published":"2025-04-17T16:15:38Z","title":"ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation\n  Models","summary":"  Text-to-image models based on diffusion processes, such as DALL-E, Stable\nDiffusion, and Midjourney, are capable of transforming texts into detailed\nimages and have widespread applications in art and design. As such, amateur\nusers can easily imitate professional-level paintings by collecting an artist's\nwork and fine-tuning the model, leading to concerns about artworks' copyright\ninfringement. To tackle these issues, previous studies either add visually\nimperceptible perturbation to the artwork to change its underlying styles\n(perturbation-based methods) or embed post-training detectable watermarks in\nthe artwork (watermark-based methods). However, when the artwork or the model\nhas been published online, i.e., modification to the original artwork or model\nretraining is not feasible, these strategies might not be viable.\n  To this end, we propose a novel method for data-use auditing in the\ntext-to-image generation model. The general idea of ArtistAuditor is to\nidentify if a suspicious model has been finetuned using the artworks of\nspecific artists by analyzing the features related to the style. Concretely,\nArtistAuditor employs a style extractor to obtain the multi-granularity style\nrepresentations and treats artworks as samplings of an artist's style. Then,\nArtistAuditor queries a trained discriminator to gain the auditing decisions.\nThe experimental results on six combinations of models and datasets show that\nArtistAuditor can achieve high AUC values (> 0.937). By studying\nArtistAuditor's transferability and core modules, we provide valuable insights\ninto the practical implementation. Finally, we demonstrate the effectiveness of\nArtistAuditor in real-world cases by an online platform Scenario. ArtistAuditor\nis open-sourced at https://github.com/Jozenn/ArtistAuditor.\n","authors":["Linkang Du","Zheng Zhu","Min Chen","Zhou Su","Shouling Ji","Peng Cheng","Jiming Chen","Zhikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.13061v1.pdf","comment":"To appear in the ACM Web Conference 2025, Sydney, Australia"},{"id":"http://arxiv.org/abs/2412.07468v2","updated":"2025-04-17T16:13:40Z","published":"2024-12-10T12:35:37Z","title":"AHSG: Adversarial Attack on High-level Semantics in Graph Neural\n  Networks","summary":"  Adversarial attacks on Graph Neural Networks aim to perturb the performance\nof the learner by carefully modifying the graph topology and node attributes.\nExisting methods achieve attack stealthiness by constraining the modification\nbudget and differences in graph properties. However, these methods typically\ndisrupt task-relevant primary semantics directly, which results in low\ndefensibility and detectability of the attack. In this paper, we propose an\nAdversarial Attack on High-level Semantics for Graph Neural Networks (AHSG),\nwhich is a graph structure attack model that ensures the retention of primary\nsemantics. By combining latent representations with shared primary semantics,\nour model retains detectable attributes and relational patterns of the original\ngraph while leveraging more subtle changes to carry out the attack. Then we use\nthe Projected Gradient Descent algorithm to map the latent representations with\nattack effects to the adversarial graph. Through experiments on robust graph\ndeep learning models equipped with defense strategies, we demonstrate that AHSG\noutperforms other state-of-the-art methods in attack effectiveness.\nAdditionally, using Contextual Stochastic Block Models to detect the attacked\ngraph further validates that our method preserves the primary semantics of the\ngraph.\n","authors":["Kai Yuan","Jiahao Zhang","Yidi Wang","Xiaobing Pei"],"pdf_url":"https://arxiv.org/pdf/2412.07468v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17152v2","updated":"2025-04-17T16:06:21Z","published":"2024-12-22T20:12:16Z","title":"Unifying Feature-Based Explanations with Functional ANOVA and\n  Cooperative Game Theory","summary":"  Feature-based explanations, using perturbations or gradients, are a prevalent\ntool to understand decisions of black box machine learning models. Yet,\ndifferences between these methods still remain mostly unknown, which limits\ntheir applicability for practitioners. In this work, we introduce a unified\nframework for local and global feature-based explanations using two\nwell-established concepts: functional ANOVA (fANOVA) from statistics, and the\nnotion of value and interaction from cooperative game theory. We introduce\nthree fANOVA decompositions that determine the influence of feature\ndistributions, and use game-theoretic measures, such as the Shapley value and\ninteractions, to specify the influence of higher-order interactions. Our\nframework combines these two dimensions to uncover similarities and differences\nbetween a wide range of explanation techniques for features and groups of\nfeatures. We then empirically showcase the usefulness of our framework on\nsynthetic and real-world datasets.\n","authors":["Fabian Fumagalli","Maximilian Muschalik","Eyke Hüllermeier","Barbara Hammer","Julia Herbinger"],"pdf_url":"https://arxiv.org/pdf/2412.17152v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10445v4","updated":"2025-04-17T16:05:20Z","published":"2024-04-16T10:31:06Z","title":"SparseDM: Toward Sparse Efficient Diffusion Models","summary":"  Diffusion models represent a powerful family of generative models widely used\nfor image and video generation. However, the time-consuming deployment, long\ninference time, and requirements on large memory hinder their applications on\nresource constrained devices. In this paper, we propose a method based on the\nimproved Straight-Through Estimator to improve the deployment efficiency of\ndiffusion models. Specifically, we add sparse masks to the Convolution and\nLinear layers in a pre-trained diffusion model, then transfer learn the sparse\nmodel during the fine-tuning stage and turn on the sparse masks during\ninference. Experimental results on a Transformer and UNet-based diffusion\nmodels demonstrate that our method reduces MACs by 50% while maintaining FID.\nSparse models are accelerated by approximately 1.2x on the GPU. Under other\nMACs conditions, the FID is also lower than 1 compared to other methods.\n","authors":["Kafeng Wang","Jianfei Chen","He Li","Zhenpeng Mi","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.10445v4.pdf","comment":"This paper has been accepted by ICME 2025"},{"id":"http://arxiv.org/abs/2411.07863v3","updated":"2025-04-17T16:05:10Z","published":"2024-11-12T15:22:14Z","title":"CDXLSTM: Boosting Remote Sensing Change Detection with Extended Long\n  Short-Term Memory","summary":"  In complex scenes and varied conditions, effectively integrating\nspatial-temporal context is crucial for accurately identifying changes.\nHowever, current RS-CD methods lack a balanced consideration of performance and\nefficiency. CNNs lack global context, Transformers are computationally\nexpensive, and Mambas face CUDA dependence and local correlation loss. In this\npaper, we propose CDXLSTM, with a core component that is a powerful XLSTM-based\nfeature enhancement layer, integrating the advantages of linear computational\ncomplexity, global context perception, and strong interpret-ability.\nSpecifically, we introduce a scale-specific Feature Enhancer layer,\nincorporating a Cross-Temporal Global Perceptron customized for\nsemantic-accurate deep features, and a Cross-Temporal Spatial Refiner\ncustomized for detail-rich shallow features. Additionally, we propose a\nCross-Scale Interactive Fusion module to progressively interact global change\nrepresentations with spatial responses. Extensive experimental results\ndemonstrate that CDXLSTM achieves state-of-the-art performance across three\nbenchmark datasets, offering a compelling balance between efficiency and\naccuracy. Code is available at https://github.com/xwmaxwma/rschange.\n","authors":["Zhenkai Wu","Xiaowen Ma","Rongrong Lian","Kai Zheng","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.07863v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07664v2","updated":"2025-04-17T16:04:48Z","published":"2024-07-10T13:44:19Z","title":"A Coding-Theoretic Analysis of Hyperspherical Prototypical Learning\n  Geometry","summary":"  Hyperspherical Prototypical Learning (HPL) is a supervised approach to\nrepresentation learning that designs class prototypes on the unit hypersphere.\nThe prototypes bias the representations to class separation in a scale\ninvariant and known geometry. Previous approaches to HPL have either of the\nfollowing shortcomings: (i) they follow an unprincipled optimisation procedure;\nor (ii) they are theoretically sound, but are constrained to only one possible\nlatent dimension. In this paper, we address both shortcomings. To address (i),\nwe present a principled optimisation procedure whose solution we show is\noptimal. To address (ii), we construct well-separated prototypes in a wide\nrange of dimensions using linear block codes. Additionally, we give a full\ncharacterisation of the optimal prototype placement in terms of achievable and\nconverse bounds, showing that our proposed methods are near-optimal.\n","authors":["Martin Lindström","Borja Rodríguez-Gálvez","Ragnar Thobaben","Mikael Skoglund"],"pdf_url":"https://arxiv.org/pdf/2407.07664v2.pdf","comment":"Changes in version 2: Minor formatting changes. Published in the\n  Proceedings of the Geometry-grounded Representation Learning and Generative\n  Modeling Workshop (GRaM), PMLR 251. Available at:\n  https://proceedings.mlr.press/v251/lindstrom24a.html 14 pages: 9 of the main\n  paper, 2 of references, and 3 of appendices.. Code is available at:\n  https://github.com/martinlindstrom/coding_theoretic_hpl"},{"id":"http://arxiv.org/abs/2504.00638v2","updated":"2025-04-17T16:01:23Z","published":"2025-04-01T10:48:00Z","title":"Impact of Data Duplication on Deep Neural Network-Based Image\n  Classifiers: Robust vs. Standard Models","summary":"  The accuracy and robustness of machine learning models against adversarial\nattacks are significantly influenced by factors such as training data quality,\nmodel architecture, the training process, and the deployment environment. In\nrecent years, duplicated data in training sets, especially in language models,\nhas attracted considerable attention. It has been shown that deduplication\nenhances both training performance and model accuracy in language models. While\nthe importance of data quality in training image classifier Deep Neural\nNetworks (DNNs) is widely recognized, the impact of duplicated images in the\ntraining set on model generalization and performance has received little\nattention.\n  In this paper, we address this gap and provide a comprehensive study on the\neffect of duplicates in image classification. Our analysis indicates that the\npresence of duplicated images in the training set not only negatively affects\nthe efficiency of model training but also may result in lower accuracy of the\nimage classifier. This negative impact of duplication on accuracy is\nparticularly evident when duplicated data is non-uniform across classes or when\nduplication, whether uniform or non-uniform, occurs in the training set of an\nadversarially trained model. Even when duplicated samples are selected in a\nuniform way, increasing the amount of duplication does not lead to a\nsignificant improvement in accuracy.\n","authors":["Alireza Aghabagherloo","Aydin Abadi","Sumanta Sarkar","Vishnu Asutosh Dasu","Bart Preneel"],"pdf_url":"https://arxiv.org/pdf/2504.00638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13044v1","updated":"2025-04-17T15:59:15Z","published":"2025-04-17T15:59:15Z","title":"The Dissipation Theory of Aging: A Quantitative Analysis Using a\n  Cellular Aging Map","summary":"  We propose a new theory for aging based on dynamical systems and provide a\ndata-driven computational method to quantify the changes at the cellular level.\nWe use ergodic theory to decompose the dynamics of changes during aging and\nshow that aging is fundamentally a dissipative process within biological\nsystems, akin to dynamical systems where dissipation occurs due to\nnon-conservative forces. To quantify the dissipation dynamics, we employ a\ntransformer-based machine learning algorithm to analyze gene expression data,\nincorporating age as a token to assess how age-related dissipation is reflected\nin the embedding space. By evaluating the dynamics of gene and age embeddings,\nwe provide a cellular aging map (CAM) and identify patterns indicative of\ndivergence in gene embedding space, nonlinear transitions, and entropy\nvariations during aging for various tissues and cell types. Our results provide\na novel perspective on aging as a dissipative process and introduce a\ncomputational framework that enables measuring age-related changes with\nmolecular resolution.\n","authors":["Farhan Khodaee","Rohola Zandie","Yufan Xia","Elazer R. Edelman"],"pdf_url":"https://arxiv.org/pdf/2504.13044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.02546v2","updated":"2025-04-17T15:53:07Z","published":"2025-04-03T12:53:41Z","title":"GPG: A Simple and Strong Reinforcement Learning Baseline for Model\n  Reasoning","summary":"  Reinforcement Learning (RL) can directly enhance the reasoning capabilities\nof large language models without extensive reliance on Supervised Fine-Tuning\n(SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism\nand propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike\nconventional methods, GPG directly optimize the original RL objective, thus\nobviating the need for surrogate loss functions. By eliminating the critic and\nreference models, avoiding KL divergence constraints, and addressing the\nadvantage and gradient estimation bias, our approach significantly simplifies\nthe training process compared to Group Relative Policy Optimization (GRPO). Our\napproach achieves superior performance without relying on auxiliary techniques\nor adjustments. As illustrated in Figure 1, extensive experiments demonstrate\nthat our method not only reduces computational costs but also consistently\noutperforms GRPO across various unimodal and multimodal tasks. Our code is\navailable at https://github.com/AMAP-ML/GPG.\n","authors":["Xiangxiang Chu","Hailang Huang","Xiao Zhang","Fei Wei","Yong Wang"],"pdf_url":"https://arxiv.org/pdf/2504.02546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13034v1","updated":"2025-04-17T15:42:13Z","published":"2025-04-17T15:42:13Z","title":"Inference-friendly Graph Compression for Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) have demonstrated promising performance in graph\nanalysis. Nevertheless, the inference process of GNNs remains costly, hindering\ntheir applications for large graphs. This paper proposes inference-friendly\ngraph compression (IFGC), a graph compression scheme to accelerate GNNs\ninference. Given a graph $G$ and a GNN $M$, an IFGC computes a small compressed\ngraph $G_c$, to best preserve the inference results of $M$ over $G$, such that\nthe result can be directly inferred by accessing $G_c$ with no or little\ndecompression cost. (1) We characterize IFGC with a class of inference\nequivalence relation. The relation captures the node pairs in $G$ that are not\ndistinguishable for GNN inference. (2) We introduce three practical\nspecifications of IFGC for representative GNNs: structural preserving\ncompression (SPGC), which computes $G_c$ that can be directly processed by GNN\ninference without decompression; ($\\alpha$, $r$)-compression, that allows for a\nconfigurable trade-off between compression ratio and inference quality, and\nanchored compression that preserves inference results for specific nodes of\ninterest. For each scheme, we introduce compression and inference algorithms\nwith guarantees of efficiency and quality of the inferred results. We conduct\nextensive experiments on diverse sets of large-scale graphs, which verifies the\neffectiveness and efficiency of our graph compression approaches.\n","authors":["Yangxin Fan","Haolai Che","Yinghui Wu"],"pdf_url":"https://arxiv.org/pdf/2504.13034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08937v2","updated":"2025-04-17T15:31:11Z","published":"2025-04-11T19:33:06Z","title":"Rethinking Few-Shot Image Fusion: Granular Ball Priors Enable\n  General-Purpose Deep Fusion","summary":"  In image fusion tasks, the absence of real fused images as priors presents a\nfundamental challenge. Most deep learning-based fusion methods rely on\nlarge-scale paired datasets to extract global weighting features from raw\nimages, thereby generating fused outputs that approximate real fused images. In\ncontrast to previous studies, this paper explores few-shot training of neural\nnetworks under the condition of having prior knowledge. We propose a novel\nfusion framework named GBFF, and a Granular Ball Significant Extraction\nalgorithm specifically designed for the few-shot prior setting. All pixel pairs\ninvolved in the fusion process are initially modeled as a Coarse-Grained\nGranular Ball. At the local level, Fine-Grained Granular Balls are used to\nslide through the brightness space to extract Non-Salient Pixel Pairs, and\nperform splitting operations to obtain Salient Pixel Pairs. Pixel-wise weights\nare then computed to generate a pseudo-supervised image. At the global level,\npixel pairs with significant contributions to the fusion process are\ncategorized into the Positive Region, while those whose contributions cannot be\naccurately determined are assigned to the Boundary Region. The Granular Ball\nperforms modality-aware adaptation based on the proportion of the positive\nregion, thereby adjusting the neural network's loss function and enabling it to\ncomplement the information of the boundary region. Extensive experiments\ndemonstrate the effectiveness of both the proposed algorithm and the underlying\ntheory. Compared with state-of-the-art (SOTA) methods, our approach shows\nstrong competitiveness in terms of both fusion time and image expressiveness.\nOur code is publicly available at:\n","authors":["Minjie Deng","Yan Wei","Hao Zhai","An Wu","Yuncan Ouyang","Qianyao Peng"],"pdf_url":"https://arxiv.org/pdf/2504.08937v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12991v1","updated":"2025-04-17T14:59:29Z","published":"2025-04-17T14:59:29Z","title":"Chain-of-Thought Prompting for Out-of-Distribution Samples: A\n  Latent-Variable Study","summary":"  Chain-of-Thought (CoT) prompting has emerged as a powerful technique to\nimprove in-context learning (ICL) in large language models (LLMs) by breaking\ncomplex reasoning into intermediate steps. However, the ability of CoT to\ngeneralize under distribution shift remains poorly understood. In this work, we\nextend a latent-variable framework for CoT prompting and study its behavior on\ntwo prototypical out-of-distribution (OOD) scenarios: (i) the latent variables\nfor CoT steps are permuted into novel combinations, and (ii) the latent\nvariables uniformly scaled by a factor. Our experiments demonstrate that CoT\ninference generalizes effectively to OOD samples whose latent variables closely\nresemble those seen during training, but its performance degrades as this\nsimilarity decreases. These findings provide foundational insights into the\nstrengths and limitations of CoT prompting under OOD conditions and suggest\ndirections for developing more resilient reasoning strategies in future LLMs.\n","authors":["Yu Wang","Fu-Chieh Chang","Pei-Yuan Wu"],"pdf_url":"https://arxiv.org/pdf/2504.12991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15499v3","updated":"2025-04-17T14:56:49Z","published":"2024-12-20T02:25:31Z","title":"A Robust Prototype-Based Network with Interpretable RBF Classifier\n  Foundations","summary":"  Prototype-based classification learning methods are known to be inherently\ninterpretable. However, this paradigm suffers from major limitations compared\nto deep models, such as lower performance. This led to the development of the\nso-called deep Prototype-Based Networks (PBNs), also known as prototypical\nparts models. In this work, we analyze these models with respect to different\nproperties, including interpretability. In particular, we focus on the\nClassification-by-Components (CBC) approach, which uses a probabilistic model\nto ensure interpretability and can be used as a shallow or deep architecture.\nWe show that this model has several shortcomings, like creating contradicting\nexplanations. Based on these findings, we propose an extension of CBC that\nsolves these issues. Moreover, we prove that this extension has robustness\nguarantees and derive a loss that optimizes robustness. Additionally, our\nanalysis shows that most (deep) PBNs are related to (deep) RBF classifiers,\nwhich implies that our robustness guarantees generalize to shallow RBF\nclassifiers. The empirical evaluation demonstrates that our deep PBN yields\nstate-of-the-art classification accuracy on different benchmarks while\nresolving the interpretability shortcomings of other approaches. Further, our\nshallow PBN variant outperforms other shallow PBNs while being inherently\ninterpretable and exhibiting provable robustness guarantees.\n","authors":["Sascha Saralajew","Ashish Rana","Thomas Villmann","Ammar Shaker"],"pdf_url":"https://arxiv.org/pdf/2412.15499v3.pdf","comment":"To appear at AAAI 2025. Includes the Appendix of the AAAI submission.\n  In v2, the font size has been increased in some figures. In v3, an incorrect\n  hyperparameter specification (Table 6; $\\lambda$) has been corrected"},{"id":"http://arxiv.org/abs/2504.12989v1","updated":"2025-04-17T14:54:00Z","published":"2025-04-17T14:54:00Z","title":"Query Complexity of Classical and Quantum Channel Discrimination","summary":"  Quantum channel discrimination has been studied from an information-theoretic\nperspective, wherein one is interested in the optimal decay rate of error\nprobabilities as a function of the number of unknown channel accesses. In this\npaper, we study the query complexity of quantum channel discrimination, wherein\nthe goal is to determine the minimum number of channel uses needed to reach a\ndesired error probability. To this end, we show that the query complexity of\nbinary channel discrimination depends logarithmically on the inverse error\nprobability and inversely on the negative logarithm of the (geometric and\nHolevo) channel fidelity. As a special case of these findings, we precisely\ncharacterize the query complexity of discriminating between two classical\nchannels. We also provide lower and upper bounds on the query complexity of\nbinary asymmetric channel discrimination and multiple quantum channel\ndiscrimination. For the former, the query complexity depends on the geometric\nR\\'enyi and Petz R\\'enyi channel divergences, while for the latter, it depends\non the negative logarithm of (geometric and Uhlmann) channel fidelity. For\nmultiple channel discrimination, the upper bound scales as the logarithm of the\nnumber of channels.\n","authors":["Theshani Nuradha","Mark M. Wilde"],"pdf_url":"https://arxiv.org/pdf/2504.12989v1.pdf","comment":"22 pages; see also the independent work \"Sampling complexity of\n  quantum channel discrimination\" DOI 10.1088/1572-9494/adcb9e"},{"id":"http://arxiv.org/abs/2407.11061v2","updated":"2025-04-17T14:53:45Z","published":"2024-07-10T16:05:43Z","title":"Exploring the Boundaries of On-Device Inference: When Tiny Falls Short,\n  Go Hierarchical","summary":"  On-device inference holds great potential for increased energy efficiency,\nresponsiveness, and privacy in edge ML systems. However, due to less capable ML\nmodels that can be embedded in resource-limited devices, use cases are limited\nto simple inference tasks such as visual keyword spotting, gesture recognition,\nand predictive analytics. In this context, the Hierarchical Inference (HI)\nsystem has emerged as a promising solution that augments the capabilities of\nthe local ML by offloading selected samples to an edge server or cloud for\nremote ML inference. Existing works demonstrate through simulation that HI\nimproves accuracy. However, they do not account for the latency and energy\nconsumption on the device, nor do they consider three key heterogeneous\ndimensions that characterize ML systems: hardware, network connectivity, and\nmodels. In contrast, this paper systematically compares the performance of HI\nwith on-device inference based on measurements of accuracy, latency, and energy\nfor running embedded ML models on five devices with different capabilities and\nthree image classification datasets. For a given accuracy requirement, the HI\nsystems we designed achieved up to 73% lower latency and up to 77% lower device\nenergy consumption than an on-device inference system. The key to building an\nefficient HI system is the availability of small-size, reasonably accurate\non-device models whose outputs can be effectively differentiated for samples\nthat require remote inference. Despite the performance gains, HI requires\non-device inference for all samples, which adds a fixed overhead to its latency\nand energy consumption. Therefore, we design a hybrid system, Early Exit with\nHI (EE-HI), and demonstrate that compared to HI, EE-HI reduces the latency by\nup to 59.7% and lowers the device's energy consumption by up to 60.4%.\n","authors":["Adarsh Prasad Behera","Paulius Daubaris","Iñaki Bravo","José Gallego","Roberto Morabito","Joerg Widmer","Jaya Prakash Varma Champati"],"pdf_url":"https://arxiv.org/pdf/2407.11061v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02825v2","updated":"2025-04-17T14:52:17Z","published":"2024-08-05T20:47:14Z","title":"The Impact of Environment Configurations on the Stability of AI-Enabled\n  Systems","summary":"  Nowadays, software systems tend to include Artificial Intelligence (AI)\ncomponents. Changes in the operational environment have been known to\nnegatively impact the stability of AI-enabled software systems by causing\nunintended changes in behavior. However, how an environment configuration\nimpacts the behavior of such systems has yet to be explored. Understanding and\nquantifying the degree of instability caused by different environment settings\ncan help practitioners decide the best environment configuration for the most\nstable AI systems. To achieve this goal, we performed experiments with eight\ndifferent combinations of three key environment variables (operating system,\nPython version, and CPU architecture) on $30$ open-source AI-enabled systems\nusing the Travis CI platform. We determine the existence and the degree of\ninstability introduced by each configuration using three metrics: the output of\nan AI component of the system (model performance), the time required to build\nand run the system (processing time), and the cost associated with building and\nrunning the system (expense). Our results indicate that changes in environment\nconfigurations lead to instability across all three metrics; however, it is\nobserved more frequently with respect to processing time and expense rather\nthan model performance. For example, between Linux and MacOS, instability is\nobserved in 23\\%, 96.67\\%, and 100\\% of the studied projects in model\nperformance, processing time, and expense, respectively. Our findings\nunderscore the importance of identifying the optimal combination of\nconfiguration settings to mitigate drops in model performance and reduce the\nprocessing time and expense before deploying an AI-enabled system.\n","authors":["Musfiqur Rahman","SayedHassan Khatoonabadi","Ahmad Abdellatif","Haya Samaana","Emad Shihab"],"pdf_url":"https://arxiv.org/pdf/2408.02825v2.pdf","comment":"Accepted for publication at the International Conference on\n  Evaluation and Assessment in Software Engineering (EASE 2025)"},{"id":"http://arxiv.org/abs/2504.12988v1","updated":"2025-04-17T14:50:40Z","published":"2025-04-17T14:50:40Z","title":"Why Ask One When You Can Ask $k$? Two-Stage Learning-to-Defer to a Set\n  of Experts","summary":"  Learning-to-Defer (L2D) enables decision-making systems to improve\nreliability by selectively deferring uncertain predictions to more competent\nagents. However, most existing approaches focus exclusively on single-agent\ndeferral, which is often inadequate in high-stakes scenarios that require\ncollective expertise. We propose Top-$k$ Learning-to-Defer, a generalization of\nthe classical two-stage L2D framework that allocates each query to the $k$ most\nconfident agents instead of a single one. To further enhance flexibility and\ncost-efficiency, we introduce Top-$k(x)$ Learning-to-Defer, an adaptive\nextension that learns the optimal number of agents to consult for each query,\nbased on input complexity, agent competency distributions, and consultation\ncosts. For both settings, we derive a novel surrogate loss and prove that it is\nBayes-consistent and $(\\mathcal{R}, \\mathcal{G})$-consistent, ensuring\nconvergence to the Bayes-optimal allocation. Notably, we show that the\nwell-established model cascades paradigm arises as a restricted instance of our\nTop-$k$ and Top-$k(x)$ formulations. Extensive experiments across diverse\nbenchmarks demonstrate the effectiveness of our framework on both\nclassification and regression tasks.\n","authors":["Yannis Montreuil","Axel Carlier","Lai Xing Ng","Wei Tsang Ooi"],"pdf_url":"https://arxiv.org/pdf/2504.12988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12984v1","updated":"2025-04-17T14:45:03Z","published":"2025-04-17T14:45:03Z","title":"A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM\n  Serving","summary":"  Serving Large Language Models (LLMs) is critical for AI-powered applications\nbut demands substantial computational resources, particularly in memory\nbandwidth and computational throughput. Low-precision computation has emerged\nas a key technique to improve efficiency while reducing resource consumption.\nExisting approaches for generating low-precision kernels are limited to weight\nbit widths that are powers of two and suffer from suboptimal performance due to\nhigh-level GPU programming abstractions. These abstractions restrict critical\noptimizations, such as fine-grained register management and optimized memory\naccess patterns, which are essential for efficient low-precision computations.\nIn this paper, we introduce a virtual machine (VM) designed for General-Purpose\nGPU (GPGPU) computing, enabling support for low-precision data types with\narbitrary bit widths while maintaining GPU programmability. The proposed VM\nfeatures a thread-block-level programming model, a hierarchical memory space, a\nnovel algebraic layout system, and extensive support for diverse low-precision\ndata types. VM programs are compiled into highly efficient GPU programs with\nautomatic vectorization and instruction selection. Extensive experiments\ndemonstrate that our VM efficiently supports a full spectrum of low-precision\ndata types, and outperforms state-of-the-art low-precision kernels on their\nsupported types. Compared to existing compilers like Triton and Ladder, as well\nas hand-optimized kernels such as QuantLLM and Marlin, our VM achieves\nperformance improvements of 1.75x, 2.61x, 1.29x and 1.03x, respectively.\n","authors":["Yaoyao Ding","Bohan Hou","Xiao Zhang","Allan Lin","Tianqi Chen","Cody Yu Hao","Yida Wang","Gennady Pekhimenko"],"pdf_url":"https://arxiv.org/pdf/2504.12984v1.pdf","comment":"18 pages, 15 figures"},{"id":"http://arxiv.org/abs/2404.05424v2","updated":"2025-04-17T14:33:17Z","published":"2024-04-08T11:47:46Z","title":"What Are the Odds? Improving the foundations of Statistical Model\n  Checking","summary":"  Markov decision processes (MDPs) are a fundamental model for decision making\nunder uncertainty. They exhibit non-deterministic choice as well as\nprobabilistic uncertainty. Traditionally, verification algorithms assume exact\nknowledge of the probabilities that govern the behaviour of an MDP. As this\nassumption is often unrealistic in practice, statistical model checking (SMC)\nwas developed in the past two decades. It allows to analyse MDPs with unknown\ntransition probabilities and provide probably approximately correct (PAC)\nguarantees on the result. Model-based SMC algorithms sample the MDP and build a\nmodel of it by estimating all transition probabilities, essentially for every\ntransition answering the question: ``What are the odds?'' However, so far the\nstatistical methods employed by the state of the art SMC algorithms are quite\nnaive. Our contribution are several fundamental improvements to those methods:\nOn the one hand, we survey statistics literature for better concentration\ninequalities; on the other hand, we propose specialised approaches that exploit\nour knowledge of the MDP. Our improvements are generally applicable to many\nkinds of problem statements because they are largely independent of the\nsetting. Moreover, our experimental evaluation shows that they lead to\nsignificant gains, reducing the number of samples that the SMC algorithm has to\ncollect by up to two orders of magnitude.\n","authors":["Tobias Meggendorfer","Maximilian Weininger","Patrick Wienhöft"],"pdf_url":"https://arxiv.org/pdf/2404.05424v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13807v2","updated":"2025-04-17T14:28:16Z","published":"2023-12-21T12:56:40Z","title":"Cluster-based classification with neural ODEs via control","summary":"  We address binary classification using neural ordinary differential equations\nfrom the perspective of simultaneous control of $N$ data points. We consider a\nsingle-neuron architecture with parameters fixed as piecewise constant\nfunctions of time. In this setting, the model complexity can be quantified by\nthe number of control switches. Previous work has shown that classification can\nbe achieved using a point-by-point strategy that requires $O(N)$ switches. We\npropose a new control method that classifies any arbitrary dataset by\nsequentially steering clusters of $d$ points, thereby reducing the complexity\nto $O(N/d)$ switches. The optimality of this result, particularly in high\ndimensions, is supported by some numerical experiments. Our complexity bound is\nsufficient but often conservative because same-class points tend to appear in\nlarger clusters, simplifying classification. This motivates studying the\nprobability distribution of the number of switches required. We introduce a\nsimple control method that imposes a collinearity constraint on the parameters,\nand analyze a worst-case scenario where both classes have the same size and all\npoints are i.i.d. Our results highlight the benefits of high-dimensional\nspaces, showing that classification using constant controls becomes more\nprobable as $d$ increases.\n","authors":["Antonio Álvarez-López","Rafael Orive-Illera","Enrique Zuazua"],"pdf_url":"https://arxiv.org/pdf/2312.13807v2.pdf","comment":"28 pages, 27 figures"},{"id":"http://arxiv.org/abs/2504.12971v1","updated":"2025-04-17T14:22:28Z","published":"2025-04-17T14:22:28Z","title":"Transferrable Surrogates in Expressive Neural Architecture Search Spaces","summary":"  Neural architecture search (NAS) faces a challenge in balancing the\nexploration of expressive, broad search spaces that enable architectural\ninnovation with the need for efficient evaluation of architectures to\neffectively search such spaces. We investigate surrogate model training for\nimproving search in highly expressive NAS search spaces based on context-free\ngrammars. We show that i) surrogate models trained either using zero-cost-proxy\nmetrics and neural graph features (GRAF) or by fine-tuning an off-the-shelf LM\nhave high predictive power for the performance of architectures both within and\nacross datasets, ii) these surrogates can be used to filter out bad\narchitectures when searching on novel datasets, thereby significantly speeding\nup search and achieving better final performances, and iii) the surrogates can\nbe further used directly as the search objective for huge speed-ups.\n","authors":["Shiwen Qin","Gabriela Kadlecová","Martin Pilát","Shay B. Cohen","Roman Neruda","Elliot J. Crowley","Jovita Lukasik","Linus Ericsson"],"pdf_url":"https://arxiv.org/pdf/2504.12971v1.pdf","comment":"Project page at: https://shiwenqin.github.io/TransferrableSurrogate/"},{"id":"http://arxiv.org/abs/2504.12966v1","updated":"2025-04-17T14:19:09Z","published":"2025-04-17T14:19:09Z","title":"Vision and Language Integration for Domain Generalization","summary":"  Domain generalization aims at training on source domains to uncover a\ndomain-invariant feature space, allowing the model to perform robust\ngeneralization ability on unknown target domains. However, due to domain gaps,\nit is hard to find reliable common image feature space, and the reason for that\nis the lack of suitable basic units for images. Different from image in vision\nspace, language has comprehensive expression elements that can effectively\nconvey semantics. Inspired by the semantic completeness of language and\nintuitiveness of image, we propose VLCA, which combine language space and\nvision space, and connect the multiple image domains by using semantic space as\nthe bridge domain. Specifically, in language space, by taking advantage of the\ncompleteness of language basic units, we tend to capture the semantic\nrepresentation of the relations between categories through word vector\ndistance. Then, in vision space, by taking advantage of the intuitiveness of\nimage features, the common pattern of sample features with the same class is\nexplored through low-rank approximation. In the end, the language\nrepresentation is aligned with the vision representation through the multimodal\nspace of text and image. Experiments demonstrate the effectiveness of the\nproposed method.\n","authors":["Yanmei Wang","Xiyao Liu","Fupeng Chu","Zhi Han"],"pdf_url":"https://arxiv.org/pdf/2504.12966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12951v1","updated":"2025-04-17T13:52:48Z","published":"2025-04-17T13:52:48Z","title":"Are Retrials All You Need? Enhancing Large Language Model Reasoning\n  Without Verbalized Feedback","summary":"  Recent advancements in large language models (LLMs) have catalyzed the\ndevelopment of general-purpose autonomous agents, demonstrating remarkable\nperformance in complex reasoning tasks across various domains. This surge has\nspurred the evolution of a plethora of prompt-based reasoning frameworks. A\nrecent focus has been on iterative reasoning strategies that refine outputs\nthrough self-evaluation and verbalized feedback. However, these strategies\nrequire additional computational complexity to enable models to recognize and\ncorrect their mistakes, leading to a significant increase in their cost. In\nthis work, we introduce the concept of ``retrials without feedback'', an\nembarrassingly simple yet powerful mechanism for enhancing reasoning frameworks\nby allowing LLMs to retry problem-solving attempts upon identifying incorrect\nanswers. Unlike conventional iterative refinement methods, our method does not\nrequire explicit self-reflection or verbalized feedback, simplifying the\nrefinement process. Our findings indicate that simpler retrial-based approaches\noften outperform more sophisticated reasoning frameworks, suggesting that the\nbenefits of complex methods may not always justify their computational costs.\nBy challenging the prevailing assumption that more intricate reasoning\nstrategies inherently lead to better performance, our work offers new insights\ninto how simpler, more efficient approaches can achieve optimal results. So,\nare retrials all you need?\n","authors":["Nearchos Potamitis","Akhil Arora"],"pdf_url":"https://arxiv.org/pdf/2504.12951v1.pdf","comment":"8 pages, 16 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2405.06691"},{"id":"http://arxiv.org/abs/2504.12949v1","updated":"2025-04-17T13:50:55Z","published":"2025-04-17T13:50:55Z","title":"RL-PINNs: Reinforcement Learning-Driven Adaptive Sampling for Efficient\n  Training of PINNs","summary":"  Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework\nfor solving partial differential equations (PDEs). However, their performance\nheavily relies on the strategy used to select training points. Conventional\nadaptive sampling methods, such as residual-based refinement, often require\nmulti-round sampling and repeated retraining of PINNs, leading to computational\ninefficiency due to redundant points and costly gradient\ncomputations-particularly in high-dimensional or high-order derivative\nscenarios. To address these limitations, we propose RL-PINNs, a reinforcement\nlearning(RL)-driven adaptive sampling framework that enables efficient training\nwith only a single round of sampling. Our approach formulates adaptive sampling\nas a Markov decision process, where an RL agent dynamically selects optimal\ntraining points by maximizing a long-term utility metric. Critically, we\nreplace gradient-dependent residual metrics with a computationally efficient\nfunction variation as the reward signal, eliminating the overhead of derivative\ncalculations. Furthermore, we employ a delayed reward mechanism to prioritize\nlong-term training stability over short-term gains. Extensive experiments\nacross diverse PDE benchmarks, including low-regular, nonlinear,\nhigh-dimensional, and high-order problems, demonstrate that RL-PINNs\nsignificantly outperforms existing residual-driven adaptive methods in\naccuracy. Notably, RL-PINNs achieve this with negligible sampling overhead,\nmaking them scalable to high-dimensional and high-order problems.\n","authors":["Zhenao Song"],"pdf_url":"https://arxiv.org/pdf/2504.12949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11467v2","updated":"2025-04-17T13:43:47Z","published":"2024-11-18T11:03:15Z","title":"Integrating Physics and Topology in Neural Networks for Learning Rigid\n  Body Dynamics","summary":"  Rigid body interactions are fundamental to numerous scientific disciplines,\nbut remain challenging to simulate due to their abrupt nonlinear nature and\nsensitivity to complex, often unknown environmental factors. These challenges\ncall for adaptable learning-based methods capable of capturing complex\ninteractions beyond explicit physical models and simulations. While graph\nneural networks can handle simple scenarios, they struggle with complex scenes\nand long-term predictions. We introduce a novel framework for modeling rigid\nbody dynamics and learning collision interactions, addressing key limitations\nof existing graph-based methods. Our approach extends the traditional\nrepresentation of meshes by incorporating higher-order topology complexes,\noffering a physically consistent representation. Additionally, we propose a\nphysics-informed message-passing neural architecture, embedding physical laws\ndirectly in the model. Our method demonstrates superior accuracy, even during\nlong rollouts, and exhibits strong generalization to unseen scenarios.\nImportantly, this work addresses the challenge of multi-entity dynamic\ninteractions, with applications spanning diverse scientific and engineering\ndomains.\n","authors":["Amaury Wei","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2411.11467v2.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2504.12939v1","updated":"2025-04-17T13:37:47Z","published":"2025-04-17T13:37:47Z","title":"Disentangling Polysemantic Channels in Convolutional Neural Networks","summary":"  Mechanistic interpretability is concerned with analyzing individual\ncomponents in a (convolutional) neural network (CNN) and how they form larger\ncircuits representing decision mechanisms. These investigations are challenging\nsince CNNs frequently learn polysemantic channels that encode distinct\nconcepts, making them hard to interpret. To address this, we propose an\nalgorithm to disentangle a specific kind of polysemantic channel into multiple\nchannels, each responding to a single concept. Our approach restructures\nweights in a CNN, utilizing that different concepts within the same channel\nexhibit distinct activation patterns in the previous layer. By disentangling\nthese polysemantic features, we enhance the interpretability of CNNs,\nultimately improving explanatory techniques such as feature visualizations.\n","authors":["Robin Hesse","Jonas Fischer","Simone Schaub-Meyer","Stefan Roth"],"pdf_url":"https://arxiv.org/pdf/2504.12939v1.pdf","comment":"Accepted at CVPR 2025 Workshop on Mechanistic Interpretability for\n  Vision (MIV). Code: https://github.com/visinf/disentangle-channels"},{"id":"http://arxiv.org/abs/2501.16371v3","updated":"2025-04-17T13:26:56Z","published":"2025-01-22T21:19:42Z","title":"Which Optimizer Works Best for Physics-Informed Neural Networks and\n  Kolmogorov-Arnold Networks?","summary":"  Physics-Informed Neural Networks (PINNs) have revolutionized the computation\nof PDE solutions by integrating partial differential equations (PDEs) into the\nneural network's training process as soft constraints, becoming an important\ncomponent of the scientific machine learning (SciML) ecosystem. More recently,\nphysics-informed Kolmogorv-Arnold networks (PIKANs) have also shown to be\neffective and comparable in accuracy with PINNs. In their current\nimplementation, both PINNs and PIKANs are mainly optimized using first-order\nmethods like Adam, as well as quasi-Newton methods such as BFGS and its\nlow-memory variant, L-BFGS. However, these optimizers often struggle with\nhighly non-linear and non-convex loss landscapes, leading to challenges such as\nslow convergence, local minima entrapment, and (non)degenerate saddle points.\nIn this study, we investigate the performance of Self-Scaled BFGS (SSBFGS),\nSelf-Scaled Broyden (SSBroyden) methods and other advanced quasi-Newton\nschemes, including BFGS and L-BFGS with different line search strategies\napproaches. These methods dynamically rescale updates based on historical\ngradient information, thus enhancing training efficiency and accuracy. We\nsystematically compare these optimizers -- using both PINNs and PIKANs -- on\nkey challenging linear, stiff, multi-scale and non-linear PDEs, including the\nBurgers, Allen-Cahn, Kuramoto-Sivashinsky, and Ginzburg-Landau equations. Our\nfindings provide state-of-the-art results with orders-of-magnitude accuracy\nimprovements without the use of adaptive weights or any other enhancements\ntypically employed in PINNs. More broadly, our results reveal insights into the\neffectiveness of second-order optimization strategies in significantly\nimproving the convergence and accurate generalization of PINNs and PIKANs.\n","authors":["Elham Kiyani","Khemraj Shukla","Jorge F. Urbán","Jérôme Darbon","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2501.16371v3.pdf","comment":"36 pages, 27 figures"},{"id":"http://arxiv.org/abs/2411.00826v2","updated":"2025-04-17T13:24:31Z","published":"2024-10-29T04:29:44Z","title":"Uncertainty Quantification via Hölder Divergence for Multi-View\n  Representation Learning","summary":"  Evidence-based deep learning represents a burgeoning paradigm for uncertainty\nestimation, offering reliable predictions with negligible extra computational\noverheads. Existing methods usually adopt Kullback-Leibler divergence to\nestimate the uncertainty of network predictions, ignoring domain gaps among\nvarious modalities. To tackle this issue, this paper introduces a novel\nalgorithm based on H\\\"older Divergence (HD) to enhance the reliability of\nmulti-view learning by addressing inherent uncertainty challenges from\nincomplete or noisy data. Generally, our method extracts the representations of\nmultiple modalities through parallel network branches, and then employs HD to\nestimate the prediction uncertainties. Through the Dempster-Shafer theory,\nintegration of uncertainty from different modalities, thereby generating a\ncomprehensive result that considers all available representations.\nMathematically, HD proves to better measure the ``distance'' between real data\ndistribution and predictive distribution of the model and improve the\nperformances of multi-class recognition tasks.\n  Specifically, our method surpass the existing state-of-the-art counterparts\non all evaluating benchmarks.\n  We further conduct extensive experiments on different backbones to verify our\nsuperior robustness. It is demonstrated that our method successfully pushes the\ncorresponding performance boundaries. Finally, we perform experiments on more\nchallenging scenarios, \\textit{i.e.}, learning with incomplete or noisy data,\nrevealing that our method exhibits a high tolerance to such corrupted data.\n","authors":["Yan Zhang","Ming Li","Chun Li","Zhaoxia Liu","Ye Zhang","Fei Richard Yu"],"pdf_url":"https://arxiv.org/pdf/2411.00826v2.pdf","comment":"NA"},{"id":"http://arxiv.org/abs/2504.12922v1","updated":"2025-04-17T13:11:26Z","published":"2025-04-17T13:11:26Z","title":"On the asymptotic behaviour of stochastic processes, with applications\n  to supermartingale convergence, Dvoretzky's approximation theorem, and\n  stochastic quasi-Fejér monotonicity","summary":"  We prove a novel and general result on the asymptotic behavior of stochastic\nprocesses which conform to a certain relaxed supermartingale condition. Our\nresult provides quantitative information in the form of an explicit and\neffective construction of a rate of convergence for this process, both in mean\nand almost surely, that is moreover highly uniform in the sense that it only\ndepends on very few data of the surrounding objects involved in the iteration.\nWe then apply this result to derive new quantitative versions of well-known\nconcepts and theorems from stochastic approximation, in particular providing\neffective rates for a variant of the Robbins-Siegmund theorem, Dvoretzky's\nconvergence theorem, as well as the convergence of stochastic quasi-Fej\\'er\nmonotone sequences, the latter of which formulated in a novel and highly\ngeneral metric context. We utilize the classic and widely studied Robbins-Monro\nprocedure as a template to evaluate our quantitative results and their\napplicability in greater detail. We conclude by illustrating the breadth of\npotential further applications with a brief discussion on a variety of other\nwell-known iterative procedures from stochastic approximation, covering a range\nof different applied scenarios to which our methods can be immediately applied.\nThroughout, we isolate and discuss special cases of our results which even\nallow for the construction of fast, and in particular linear, rates.\n","authors":["Morenikeji Neri","Nicholas Pischke","Thomas Powell"],"pdf_url":"https://arxiv.org/pdf/2504.12922v1.pdf","comment":"41 pages"},{"id":"http://arxiv.org/abs/2504.12921v1","updated":"2025-04-17T13:11:13Z","published":"2025-04-17T13:11:13Z","title":"IdentiARAT: Toward Automated Identification of Individual ARAT Items\n  from Wearable Sensors","summary":"  This study explores the potential of using wrist-worn inertial sensors to\nautomate the labeling of ARAT (Action Research Arm Test) items. While the ARAT\nis commonly used to assess upper limb motor function, its limitations include\nsubjectivity and time consumption of clinical staff. By using IMU (Inertial\nMeasurement Unit) sensors and MiniROCKET as a time series classification\ntechnique, this investigation aims to classify ARAT items based on sensor\nrecordings. We test common preprocessing strategies to efficiently leverage\nincluded information in the data. Afterward, we use the best preprocessing to\nimprove the classification. The dataset includes recordings of 45 participants\nperforming various ARAT items. Results show that MiniROCKET offers a fast and\nreliable approach for classifying ARAT domains, although challenges remain in\ndistinguishing between individual resembling items. Future work may involve\nimproving classification through more advanced machine-learning models and data\nenhancements.\n","authors":["Daniel Homm","Patrick Carqueville","Christian Eichhorn","Thomas Weikert","Thomas Menard","David A. Plecher","Chris Awai Easthope"],"pdf_url":"https://arxiv.org/pdf/2504.12921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12918v1","updated":"2025-04-17T13:07:26Z","published":"2025-04-17T13:07:26Z","title":"Sliced-Wasserstein Distance-based Data Selection","summary":"  We propose a new unsupervised anomaly detection method based on the\nsliced-Wasserstein distance for training data selection in machine learning\napproaches. Our filtering technique is interesting for decision-making\npipelines deploying machine learning models in critical sectors, e.g., power\nsystems, as it offers a conservative data selection and an optimal transport\ninterpretation. To ensure the scalability of our method, we provide two\nefficient approximations. The first approximation processes reduced-cardinality\nrepresentations of the datasets concurrently. The second makes use of a\ncomputationally light Euclidian distance approximation. Additionally, we open\nthe first dataset showcasing localized critical peak rebate demand response in\na northern climate. We present the filtering patterns of our method on\nsynthetic datasets and numerically benchmark our method for training data\nselection. Finally, we employ our method as part of a first forecasting\nbenchmark for our open-source dataset.\n","authors":["Julien Pallage","Antoine Lesage-Landry"],"pdf_url":"https://arxiv.org/pdf/2504.12918v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2410.21712"},{"id":"http://arxiv.org/abs/2504.12916v1","updated":"2025-04-17T13:05:33Z","published":"2025-04-17T13:05:33Z","title":"Exact Learning Dynamics of In-Context Learning in Linear Transformers\n  and Its Application to Non-Linear Transformers","summary":"  Transformer models exhibit remarkable in-context learning (ICL), adapting to\nnovel tasks from examples within their context, yet the underlying mechanisms\nremain largely mysterious. Here, we provide an exact analytical\ncharacterization of ICL emergence by deriving the closed-form stochastic\ngradient descent (SGD) dynamics for a simplified linear transformer performing\nregression tasks. Our analysis reveals key properties: (1) a natural separation\nof timescales directly governed by the input data's covariance structure,\nleading to staged learning; (2) an exact description of how ICL develops,\nincluding fixed points corresponding to learned algorithms and conservation\nlaws constraining the dynamics; and (3) surprisingly nonlinear learning\nbehavior despite the model's linearity. We hypothesize this phenomenology\nextends to non-linear models. To test this, we introduce theory-inspired\nmacroscopic measures (spectral rank dynamics, subspace stability) and use them\nto provide mechanistic explanations for (1) the sudden emergence of ICL in\nattention-only networks and (2) delayed generalization (grokking) in modular\narithmetic models. Our work offers an exact dynamical model for ICL and\ntheoretically grounded tools for analyzing complex transformer training.\n","authors":["Nischal Mainali","Lucas Teixeira"],"pdf_url":"https://arxiv.org/pdf/2504.12916v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.10735v2","updated":"2025-04-17T12:53:23Z","published":"2025-04-14T22:06:24Z","title":"Frozen Layers: Memory-efficient Many-fidelity Hyperparameter\n  Optimization","summary":"  As model sizes grow, finding efficient and cost-effective hyperparameter\noptimization (HPO) methods becomes increasingly crucial for deep learning\npipelines. While multi-fidelity HPO (MF-HPO) trades off computational resources\nrequired for DL training with lower fidelity estimations, existing fidelity\nsources often fail under lower compute and memory constraints. We propose a\nnovel fidelity source: the number of layers that are trained or frozen during\ntraining. For deep networks, this approach offers significant compute and\nmemory savings while preserving rank correlations between hyperparameters at\nlow fidelities compared to full model training. We demonstrate this in our\nempirical evaluation across ResNets and Transformers and additionally analyze\nthe utility of frozen layers as a fidelity in using GPU resources as a fidelity\nin HPO, and for a combined MF-HPO with other fidelity sources. This\ncontribution opens new applications for MF-HPO with hardware resources as a\nfidelity and creates opportunities for improved algorithms navigating joint\nfidelity spaces.\n","authors":["Timur Carstensen","Neeratyoy Mallik","Frank Hutter","Martin Rapp"],"pdf_url":"https://arxiv.org/pdf/2504.10735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11513v2","updated":"2025-04-17T12:49:13Z","published":"2025-04-15T13:40:16Z","title":"Multi-output Classification Framework and Frequency Layer Normalization\n  for Compound Fault Diagnosis in Motor","summary":"  This work introduces a multi-output classification (MOC) framework designed\nfor domain adaptation in fault diagnosis, particularly under partially labeled\n(PL) target domain scenarios and compound fault conditions in rotating\nmachinery. Unlike traditional multi-class classification (MCC) methods that\ntreat each fault combination as a distinct class, the proposed approach\nindependently estimates the severity of each fault type, improving both\ninterpretability and diagnostic accuracy. The model incorporates multi-kernel\nmaximum mean discrepancy (MK-MMD) and entropy minimization (EM) losses to\nfacilitate feature transfer from the source to the target domain. In addition,\nfrequency layer normalization (FLN) is applied to preserve structural\nproperties in the frequency domain, which are strongly influenced by system\ndynamics and are often stationary with respect to changes in rpm. Evaluations\nacross six domain adaptation cases with PL data demonstrate that MOC\noutperforms baseline models in macro F1 score. Moreover, MOC consistently\nachieves better classification performance for individual fault types, and FLN\nshows superior adaptability compared to other normalization techniques.\n","authors":["Wonjun Yi","Yong-Hwa Park"],"pdf_url":"https://arxiv.org/pdf/2504.11513v2.pdf","comment":"Extended version of \"Multi-output Classification for Compound Fault\n  Diagnosis in Motor under Partially Labeled Target Domain\" Will not be\n  published in any conferences or journels"},{"id":"http://arxiv.org/abs/2410.03802v3","updated":"2025-04-17T12:45:16Z","published":"2024-10-04T09:39:15Z","title":"Mesh-Informed Reduced Order Models for Aneurysm Rupture Risk Prediction","summary":"  The complexity of the cardiovascular system needs to be accurately reproduced\nin order to promptly acknowledge health conditions; to this aim, advanced\nmultifidelity and multiphysics numerical models are crucial. On one side, Full\nOrder Models (FOMs) deliver accurate hemodynamic assessments, but their high\ncomputational demands hinder their real-time clinical application. In contrast,\nReduced Order Models (ROMs) provide more efficient yet accurate solutions,\nessential for personalized healthcare and timely clinical decision-making. In\nthis work, we explore the application of computational fluid dynamics (CFD) in\ncardiovascular medicine by integrating FOMs with ROMs for predicting the risk\nof aortic aneurysm growth and rupture. Wall Shear Stress (WSS) and the\nOscillatory Shear Index (OSI), sampled at different growth stages of the\nthoracic aortic aneurysm, are predicted by means of Graph Neural Networks\n(GNNs). GNNs exploit the natural graph structure of the mesh obtained by the\nFinite Volume (FV) discretization, taking into account the spatial local\ninformation, regardless of the dimension of the input graph. Our experimental\nvalidation framework yields promising results, confirming our method as a valid\nalternative that overcomes the curse of dimensionality.\n","authors":["Giuseppe Alessio D'Inverno","Saeid Moradizadeh","Sajad Salavatidezfouli","Pasquale Claudio Africa","Gianluigi Rozza"],"pdf_url":"https://arxiv.org/pdf/2410.03802v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04833v2","updated":"2025-04-17T12:34:20Z","published":"2024-12-06T07:56:25Z","title":"Wavelet Diffusion Neural Operator","summary":"  Simulating and controlling physical systems described by partial differential\nequations (PDEs) are crucial tasks across science and engineering. Recently,\ndiffusion generative models have emerged as a competitive class of methods for\nthese tasks due to their ability to capture long-term dependencies and model\nhigh-dimensional states. However, diffusion models typically struggle with\nhandling system states with abrupt changes and generalizing to higher\nresolutions. In this work, we propose Wavelet Diffusion Neural Operator (WDNO),\na novel PDE simulation and control framework that enhances the handling of\nthese complexities. WDNO comprises two key innovations. Firstly, WDNO performs\ndiffusion-based generative modeling in the wavelet domain for the entire\ntrajectory to handle abrupt changes and long-term dependencies effectively.\nSecondly, to address the issue of poor generalization across different\nresolutions, which is one of the fundamental tasks in modeling physical\nsystems, we introduce multi-resolution training. We validate WDNO on five\nphysical systems, including 1D advection equation, three challenging physical\nsystems with abrupt changes (1D Burgers' equation, 1D compressible\nNavier-Stokes equation and 2D incompressible fluid), and a real-world dataset\nERA5, which demonstrates superior performance on both simulation and control\ntasks over state-of-the-art methods, with significant improvements in long-term\nand detail prediction accuracy. Remarkably, in the challenging context of the\n2D high-dimensional and indirect control task aimed at reducing smoke leakage,\nWDNO reduces the leakage by 33.2% compared to the second-best baseline. The\ncode can be found at https://github.com/AI4Science-WestlakeU/wdno.git.\n","authors":["Peiyan Hu","Rui Wang","Xiang Zheng","Tao Zhang","Haodong Feng","Ruiqi Feng","Long Wei","Yue Wang","Zhi-Ming Ma","Tailin Wu"],"pdf_url":"https://arxiv.org/pdf/2412.04833v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05803v3","updated":"2025-04-17T12:23:46Z","published":"2025-01-10T09:10:30Z","title":"Test-time Alignment of Diffusion Models without Reward Over-optimization","summary":"  Diffusion models excel in generative tasks, but aligning them with specific\nobjectives while maintaining their versatility remains challenging. Existing\nfine-tuning methods often suffer from reward over-optimization, while\napproximate guidance approaches fail to optimize target rewards effectively.\nAddressing these limitations, we propose a training-free, test-time method\nbased on Sequential Monte Carlo (SMC) to sample from the reward-aligned target\ndistribution. Our approach, tailored for diffusion sampling and incorporating\ntempering techniques, achieves comparable or superior target rewards to\nfine-tuning methods while preserving diversity and cross-reward generalization.\nWe demonstrate its effectiveness in single-reward optimization, multi-objective\nscenarios, and online black-box optimization. This work offers a robust\nsolution for aligning diffusion models with diverse downstream objectives\nwithout compromising their general capabilities. Code is available at\nhttps://github.com/krafton-ai/DAS.\n","authors":["Sunwoo Kim","Minkyu Kim","Dongmin Park"],"pdf_url":"https://arxiv.org/pdf/2501.05803v3.pdf","comment":"ICLR 2025 (Spotlight). The Thirteenth International Conference on\n  Learning Representations. 2025"},{"id":"http://arxiv.org/abs/2408.11969v2","updated":"2025-04-17T12:18:48Z","published":"2024-08-21T19:47:41Z","title":"DrivAerML: High-Fidelity Computational Fluid Dynamics Dataset for\n  Road-Car External Aerodynamics","summary":"  Machine Learning (ML) has the potential to revolutionise the field of\nautomotive aerodynamics, enabling split-second flow predictions early in the\ndesign process. However, the lack of open-source training data for realistic\nroad cars, using high-fidelity CFD methods, represents a barrier to their\ndevelopment. To address this, a high-fidelity open-source (CC-BY-SA) public\ndataset for automotive aerodynamics has been generated, based on 500\nparametrically morphed variants of the widely-used DrivAer notchback generic\nvehicle. Mesh generation and scale-resolving CFD was executed using consistent\nand validated automatic workflows representative of the industrial\nstate-of-the-art. Geometries and rich aerodynamic data are published in\nopen-source formats. To our knowledge, this is the first large, public-domain\ndataset for complex automotive configurations generated using high-fidelity\nCFD.\n","authors":["Neil Ashton","Charles Mockett","Marian Fuchs","Louis Fliessbach","Hendrik Hetmann","Thilo Knacke","Norbert Schonwald","Vangelis Skaperdas","Grigoris Fotiadis","Astrid Walle","Burkhard Hupertz","Danielle Maddix"],"pdf_url":"https://arxiv.org/pdf/2408.11969v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05714v6","updated":"2025-04-17T12:18:41Z","published":"2024-06-09T10:12:08Z","title":"A conversion theorem and minimax optimality for continuum contextual\n  bandits","summary":"  We study the contextual continuum bandits problem, where the learner\nsequentially receives a side information vector and has to choose an action in\na convex set, minimizing a function associated with the context. The goal is to\nminimize all the underlying functions for the received contexts, leading to the\ncontextual notion of regret, which is stronger than the standard static regret.\nAssuming that the objective functions are $\\gamma$-H\\\"older with respect to the\ncontexts, $0<\\gamma\\le 1,$ we demonstrate that any algorithm achieving a\nsub-linear static regret can be extended to achieve a sub-linear contextual\nregret. We prove a static-to-contextual regret conversion theorem that provides\nan upper bound for the contextual regret of the output algorithm as a function\nof the static regret of the input algorithm. We further study the implications\nof this general result for three fundamental cases of dependency of the\nobjective function on the action variable: (a) Lipschitz bandits, (b) convex\nbandits, (c) strongly convex and smooth bandits. For Lipschitz bandits and\n$\\gamma=1,$ combining our results with the lower bound of Slivkins (2014), we\nprove that the minimax optimal contextual regret for the noise-free adversarial\nsetting is achieved. Then, we prove that in the presence of noise, the\ncontextual regret rate as a function of the number of queries is the same for\nconvex bandits as it is for strongly convex and smooth bandits. Lastly, we\npresent a minimax lower bound, implying two key facts. First, obtaining a\nsub-linear contextual regret may be impossible over functions that are not\ncontinuous with respect to the context. Second, for convex bandits and strongly\nconvex and smooth bandits, the algorithms that we propose achieve, up to a\nlogarithmic factor, the minimax optimal rate of contextual regret as a function\nof the number of queries.\n","authors":["Arya Akhavan","Karim Lounici","Massimiliano Pontil","Alexandre B. Tsybakov"],"pdf_url":"https://arxiv.org/pdf/2406.05714v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12883v1","updated":"2025-04-17T12:17:51Z","published":"2025-04-17T12:17:51Z","title":"Mirror, Mirror of the Flow: How Does Regularization Shape Implicit Bias?","summary":"  Implicit bias plays an important role in explaining how overparameterized\nmodels generalize well. Explicit regularization like weight decay is often\nemployed in addition to prevent overfitting. While both concepts have been\nstudied separately, in practice, they often act in tandem. Understanding their\ninterplay is key to controlling the shape and strength of implicit bias, as it\ncan be modified by explicit regularization. To this end, we incorporate\nexplicit regularization into the mirror flow framework and analyze its lasting\neffects on the geometry of the training dynamics, covering three distinct\neffects: positional bias, type of bias, and range shrinking. Our analytical\napproach encompasses a broad class of problems, including sparse coding, matrix\nsensing, single-layer attention, and LoRA, for which we demonstrate the utility\nof our insights. To exploit the lasting effect of regularization and highlight\nthe potential benefit of dynamic weight decay schedules, we propose to switch\noff weight decay during training, which can improve generalization, as we\ndemonstrate in experiments.\n","authors":["Tom Jacobs","Chao Zhou","Rebekka Burkholz"],"pdf_url":"https://arxiv.org/pdf/2504.12883v1.pdf","comment":"26 pages, 16 figures"},{"id":"http://arxiv.org/abs/2402.01744v4","updated":"2025-04-17T12:16:35Z","published":"2024-01-29T17:23:25Z","title":"Unveiling Molecular Moieties through Hierarchical Grad-CAM Graph\n  Explainability","summary":"  Background: Virtual Screening (VS) has become an essential tool in drug\ndiscovery, enabling the rapid and cost-effective identification of potential\nbioactive molecules. Among recent advancements, Graph Neural Networks (GNNs)\nhave gained prominence for their ability to model complex molecular structures\nusing graph-based representations. However, the integration of explainable\nmethods to elucidate the specific contributions of molecular substructures to\nbiological activity remains a significant challenge. This limitation hampers\nboth the interpretability of predictive models and the rational design of novel\ntherapeutics.\\\\ Results: We trained 20 GNN models on a dataset of small\nmolecules with the goal of predicting their activity on 20 distinct protein\ntargets from the Kinase family. These classifiers achieved state-of-the-art\nperformance in virtual screening tasks, demonstrating high accuracy and\nrobustness on different targets. Building upon these models, we implemented the\nHierarchical Grad-CAM graph Explainer (HGE) framework, enabling an in-depth\nanalysis of the molecular moieties driving protein-ligand binding\nstabilization. HGE exploits Grad-CAM explanations at the atom, ring, and\nwhole-molecule levels, leveraging the message-passing mechanism to highlight\nthe most relevant chemical moieties. Validation against experimental data from\nthe literature confirmed the ability of the explainer to recognize a molecular\npattern of drugs and correctly annotate them to the known target. Conclusion:\nOur approach may represent a valid support to shorten both the screening and\nthe hit discovery process. Detailed knowledge of the molecular substructures\nthat play a role in the binding process can help the computational chemist to\ngain insights into the structure optimization, as well as in drug repurposing\ntasks.\n","authors":["Salvatore Contino","Paolo Sortino","Maria Rita Gulotta","Ugo Perricone","Roberto Pirrone"],"pdf_url":"https://arxiv.org/pdf/2402.01744v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12880v1","updated":"2025-04-17T12:13:25Z","published":"2025-04-17T12:13:25Z","title":"Can Masked Autoencoders Also Listen to Birds?","summary":"  Masked Autoencoders (MAEs) pretrained on AudioSet fail to capture the\nfine-grained acoustic characteristics of specialized domains such as\nbioacoustic monitoring. Bird sound classification is critical for assessing\nenvironmental health, yet general-purpose models inadequately address its\nunique acoustic challenges. To address this, we introduce Bird-MAE, a\ndomain-specialized MAE pretrained on the large-scale BirdSet dataset. We\nexplore adjustments to pretraining, fine-tuning and utilizing frozen\nrepresentations. Bird-MAE achieves state-of-the-art results across all BirdSet\ndownstream tasks, substantially improving multi-label classification\nperformance compared to the general-purpose Audio-MAE baseline. Additionally,\nwe propose prototypical probing, a parameter-efficient method for leveraging\nMAEs' frozen representations. Bird-MAE's prototypical probes outperform linear\nprobing by up to 37\\% in MAP and narrow the gap to fine-tuning to approximately\n3\\% on average on BirdSet.\n","authors":["Lukas Rauch","Ilyass Moummad","René Heinrich","Alexis Joly","Bernhard Sick","Christoph Scholz"],"pdf_url":"https://arxiv.org/pdf/2504.12880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06235v2","updated":"2025-04-17T12:12:38Z","published":"2024-12-09T06:21:11Z","title":"VariFace: Fair and Diverse Synthetic Dataset Generation for Face\n  Recognition","summary":"  The use of large-scale, web-scraped datasets to train face recognition models\nhas raised significant privacy and bias concerns. Synthetic methods mitigate\nthese concerns and provide scalable and controllable face generation to enable\nfair and accurate face recognition. However, existing synthetic datasets\ndisplay limited intraclass and interclass diversity and do not match the face\nrecognition performance obtained using real datasets. Here, we propose\nVariFace, a two-stage diffusion-based pipeline to create fair and diverse\nsynthetic face datasets to train face recognition models. Specifically, we\nintroduce three methods: Face Recognition Consistency to refine demographic\nlabels, Face Vendi Score Guidance to improve interclass diversity, and\nDivergence Score Conditioning to balance the identity preservation-intraclass\ndiversity trade-off. When constrained to the same dataset size, VariFace\nconsiderably outperforms previous synthetic datasets (0.9200 $\\rightarrow$\n0.9405) and achieves comparable performance to face recognition models trained\nwith real data (Real Gap = -0.0065). In an unconstrained setting, VariFace not\nonly consistently achieves better performance compared to previous synthetic\nmethods across dataset sizes but also, for the first time, outperforms the real\ndataset (CASIA-WebFace) across six evaluation datasets. This sets a new\nstate-of-the-art performance with an average face verification accuracy of\n0.9567 (Real Gap = +0.0097) across LFW, CFP-FP, CPLFW, AgeDB, and CALFW\ndatasets and 0.9366 (Real Gap = +0.0380) on the RFW dataset.\n","authors":["Michael Yeung","Toya Teramoto","Songtao Wu","Tatsuo Fujiwara","Kenji Suzuki","Tamaki Kojima"],"pdf_url":"https://arxiv.org/pdf/2412.06235v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12875v1","updated":"2025-04-17T12:03:02Z","published":"2025-04-17T12:03:02Z","title":"A Client-level Assessment of Collaborative Backdoor Poisoning in Non-IID\n  Federated Learning","summary":"  Federated learning (FL) enables collaborative model training using\ndecentralized private data from multiple clients. While FL has shown robustness\nagainst poisoning attacks with basic defenses, our research reveals new\nvulnerabilities stemming from non-independent and identically distributed\n(non-IID) data among clients. These vulnerabilities pose a substantial risk of\nmodel poisoning in real-world FL scenarios.\n  To demonstrate such vulnerabilities, we develop a novel collaborative\nbackdoor poisoning attack called CollaPois. In this attack, we distribute a\nsingle pre-trained model infected with a Trojan to a group of compromised\nclients. These clients then work together to produce malicious gradients,\ncausing the FL model to consistently converge towards a low-loss region\ncentered around the Trojan-infected model. Consequently, the impact of the\nTrojan is amplified, especially when the benign clients have diverse local data\ndistributions and scattered local gradients. CollaPois stands out by achieving\nits goals while involving only a limited number of compromised clients, setting\nit apart from existing attacks. Also, CollaPois effectively avoids noticeable\nshifts or degradation in the FL model's performance on legitimate data samples,\nallowing it to operate stealthily and evade detection by advanced robust FL\nalgorithms.\n  Thorough theoretical analysis and experiments conducted on various benchmark\ndatasets demonstrate the superiority of CollaPois compared to state-of-the-art\nbackdoor attacks. Notably, CollaPois bypasses existing backdoor defenses,\nespecially in scenarios where clients possess diverse data distributions.\nMoreover, the results show that CollaPois remains effective even when involving\na small number of compromised clients. Notably, clients whose local data is\nclosely aligned with compromised clients experience higher risks of backdoor\ninfections.\n","authors":["Phung Lai","Guanxiong Liu","Hai Phan","Issa Khalil","Abdallah Khreishah","Xintao Wu"],"pdf_url":"https://arxiv.org/pdf/2504.12875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08534v2","updated":"2025-04-17T11:38:29Z","published":"2024-12-11T16:48:18Z","title":"Protecting Confidentiality, Privacy and Integrity in Collaborative\n  Learning","summary":"  A collaboration between dataset owners and model owners is needed to\nfacilitate effective machine learning (ML) training. During this collaboration,\nhowever, dataset owners and model owners want to protect the confidentiality of\ntheir respective assets (i.e., datasets, models and training code), with the\ndataset owners also caring about the privacy of individual users whose data is\nin their datasets. Existing solutions either provide limited confidentiality\nfor models and training code, or suffer from privacy issues due to collusion.\n  We present Citadel++, a collaborative ML training system designed to\nsimultaneously protect the confidentiality of datasets, models and training\ncode as well as the privacy of individual users. Citadel++ enhances\ndifferential privacy mechanisms to safeguard the privacy of individual user\ndata while maintaining model utility. By employing Virtual Machine-level\nTrusted Execution Environments (TEEs) as well as the improved sandboxing and\nintegrity mechanisms through OS-level techniques, Citadel++ effectively\npreserves the confidentiality of datasets, models and training code, and\nenforces our privacy mechanisms even when the models and training code have\nbeen maliciously designed. Our experiments show that Citadel++ provides model\nutility and performance while adhering to the confidentiality and privacy\nrequirements of dataset owners and model owners, outperforming the\nstate-of-the-art privacy-preserving training systems by up to 543x on CPU and\n113x on GPU TEEs.\n","authors":["Dong Chen","Alice Dethise","Istemi Ekin Akkus","Ivica Rimac","Klaus Satzke","Antti Koskela","Marco Canini","Wei Wang","Ruichuan Chen"],"pdf_url":"https://arxiv.org/pdf/2412.08534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12860v1","updated":"2025-04-17T11:38:17Z","published":"2025-04-17T11:38:17Z","title":"When do Random Forests work?","summary":"  We study the effectiveness of randomizing split-directions in random forests.\nPrior literature has shown that, on the one hand, randomization can reduce\nvariance through decorrelation, and, on the other hand, randomization\nregularizes and works in low signal-to-noise ratio (SNR) environments. First,\nwe bring together and revisit decorrelation and regularization by presenting a\nsystematic analysis of out-of-sample mean-squared error (MSE) for different SNR\nscenarios based on commonly-used data-generating processes. We find that\nvariance reduction tends to increase with the SNR and forests outperform\nbagging when the SNR is low because, in low SNR cases, variance dominates bias\nfor both methods. Second, we show that the effectiveness of randomization is a\nquestion that goes beyond the SNR. We present a simulation study with fixed and\nmoderate SNR, in which we examine the effectiveness of randomization for other\ndata characteristics. In particular, we find that (i) randomization can\nincrease bias in the presence of fat tails in the distribution of covariates;\n(ii) in the presence of irrelevant covariates randomization is ineffective\nbecause bias dominates variance; and (iii) when covariates are mutually\ncorrelated randomization tends to be effective because variance dominates bias.\nBeyond randomization, we find that, for both bagging and random forests, bias\ncan be significantly reduced in the presence of correlated covariates. This\nlast finding goes beyond the prevailing view that averaging mostly works by\nvariance reduction. Given that in practice covariates are often correlated, our\nfindings on correlated covariates could open the way for a better understanding\nof why random forests work well in many applications.\n","authors":["C. Revelas","O. Boldea","B. J. M. Werker"],"pdf_url":"https://arxiv.org/pdf/2504.12860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20268v2","updated":"2025-04-17T11:32:53Z","published":"2025-02-27T16:55:18Z","title":"Large Language Models as Attribution Regularizers for Efficient Model\n  Training","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains. However, effectively leveraging their vast knowledge for\ntraining smaller downstream models remains an open challenge, especially in\ndomains like tabular data learning, where simpler models are often preferred\ndue to interpretability and efficiency.\n  In this paper, we introduce a novel yet straightforward method for\nincorporating LLM-generated global task feature attributions into the training\nprocess of smaller networks. Specifically, we propose an attribution-matching\nregularization term that aligns the training dynamics of the smaller model with\nthe insights provided by the LLM. By doing so, our approach yields superior\nperformance in few-shot learning scenarios. Notably, our method requires only\nblack-box API access to the LLM, making it easy to integrate into existing\ntraining pipelines with minimal computational overhead.\n  Furthermore, we demonstrate how this method can be used to address common\nissues in real-world datasets, such as skewness and bias. By integrating\nhigh-level knowledge from LLMs, our approach improves generalization, even when\ntraining data is limited or imbalanced. We validate its effectiveness through\nextensive experiments across multiple tasks, demonstrating improved learning\nefficiency and model robustness.\n","authors":["Davor Vukadin","Marin Šilić","Goran Delač"],"pdf_url":"https://arxiv.org/pdf/2502.20268v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01530v4","updated":"2025-04-17T11:31:49Z","published":"2023-12-03T23:08:29Z","title":"Evaluation of Active Feature Acquisition Methods for Time-varying\n  Feature Settings","summary":"  Machine learning methods often assume that input features are available at no\ncost. However, in domains like healthcare, where acquiring features could be\nexpensive or harmful, it is necessary to balance a feature's acquisition cost\nagainst its predictive value. The task of training an AI agent to decide which\nfeatures to acquire is called active feature acquisition (AFA). By deploying an\nAFA agent, we effectively alter the acquisition strategy and trigger a\ndistribution shift. To safely deploy AFA agents under this distribution shift,\nwe present the problem of active feature acquisition performance evaluation\n(AFAPE). We examine AFAPE under i) a no direct effect (NDE) assumption, stating\nthat acquisitions do not affect the underlying feature values; and ii) a no\nunobserved confounding (NUC) assumption, stating that retrospective feature\nacquisition decisions were only based on observed features. We show that one\ncan apply missing data methods under the NDE assumption and offline\nreinforcement learning under the NUC assumption. When NUC and NDE hold, we\npropose a novel semi-offline reinforcement learning framework. This framework\nrequires a weaker positivity assumption and introduces three new estimators: A\ndirect method (DM), an inverse probability weighting (IPW), and a double\nreinforcement learning (DRL) estimator.\n","authors":["Henrik von Kleist","Alireza Zamanian","Ilya Shpitser","Narges Ahmidi"],"pdf_url":"https://arxiv.org/pdf/2312.01530v4.pdf","comment":"61 pages, 4 tables, 11 Figures"},{"id":"http://arxiv.org/abs/2504.12856v1","updated":"2025-04-17T11:23:17Z","published":"2025-04-17T11:23:17Z","title":"3D-PNAS: 3D Industrial Surface Anomaly Synthesis with Perlin Noise","summary":"  Large pretrained vision foundation models have shown significant potential in\nvarious vision tasks. However, for industrial anomaly detection, the scarcity\nof real defect samples poses a critical challenge in leveraging these models.\nWhile 2D anomaly generation has significantly advanced with established\ngenerative models, the adoption of 3D sensors in industrial manufacturing has\nmade leveraging 3D data for surface quality inspection an emerging trend. In\ncontrast to 2D techniques, 3D anomaly generation remains largely unexplored,\nlimiting the potential of 3D data in industrial quality inspection. To address\nthis gap, we propose a novel yet simple 3D anomaly generation method, 3D-PNAS,\nbased on Perlin noise and surface parameterization. Our method generates\nrealistic 3D surface anomalies by projecting the point cloud onto a 2D plane,\nsampling multi-scale noise values from a Perlin noise field, and perturbing the\npoint cloud along its normal direction. Through comprehensive visualization\nexperiments, we demonstrate how key parameters - including noise scale,\nperturbation strength, and octaves, provide fine-grained control over the\ngenerated anomalies, enabling the creation of diverse defect patterns from\npronounced deformations to subtle surface variations. Additionally, our\ncross-category experiments show that the method produces consistent yet\ngeometrically plausible anomalies across different object types, adapting to\ntheir specific surface characteristics. We also provide a comprehensive\ncodebase and visualization toolkit to facilitate future research.\n","authors":["Yifeng Cheng","Juan Du"],"pdf_url":"https://arxiv.org/pdf/2504.12856v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12850v1","updated":"2025-04-17T11:17:53Z","published":"2025-04-17T11:17:53Z","title":"iHHO-SMOTe: A Cleansed Approach for Handling Outliers and Reducing Noise\n  to Improve Imbalanced Data Classification","summary":"  Classifying imbalanced datasets remains a significant challenge in machine\nlearning, particularly with big data where instances are unevenly distributed\namong classes, leading to class imbalance issues that impact classifier\nperformance. While Synthetic Minority Over-sampling Technique (SMOTE) addresses\nthis challenge by generating new instances for the under-represented minority\nclass, it faces obstacles in the form of noise and outliers during the creation\nof new samples. In this paper, a proposed approach, iHHO-SMOTe, which addresses\nthe limitations of SMOTE by first cleansing the data from noise points. This\nprocess involves employing feature selection using a random forest to identify\nthe most valuable features, followed by applying the Density-Based Spatial\nClustering of Applications with Noise (DBSCAN) algorithm to detect outliers\nbased on the selected features. The identified outliers from the minority\nclasses are then removed, creating a refined dataset for subsequent\noversampling using the hybrid approach called iHHO-SMOTe. The comprehensive\nexperiments across diverse datasets demonstrate the exceptional performance of\nthe proposed model, with an AUC score exceeding 0.99, a high G-means score of\n0.99 highlighting its robustness, and an outstanding F1-score consistently\nexceeding 0.967. These findings collectively establish Cleansed iHHO-SMOTe as a\nformidable contender in addressing imbalanced datasets, focusing on noise\nreduction and outlier handling for improved classification models.\n","authors":["Khaled SH. Raslan","Almohammady S. Alsharkawy","K. R. Raslan"],"pdf_url":"https://arxiv.org/pdf/2504.12850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.14000v4","updated":"2025-04-17T11:11:51Z","published":"2022-07-28T10:44:46Z","title":"Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study\n  on Out-of-Distribution Generalisation","summary":"  Combining deep learning with symbolic logic reasoning aims to capitalize on\nthe success of both fields and is drawing increasing attention. Inspired by\nDeepLogic, an end-to-end model trained to perform inference on logic programs,\nwe introduce IMA-GloVe-GA, an iterative neural inference network for multi-step\nreasoning expressed in natural language. In our model, reasoning is performed\nusing an iterative memory neural network based on RNN with a gated attention\nmechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES\nV1 and CONCEPTRULES V2. Experimental results show DeepLogic with gated\nattention can achieve higher test accuracy than DeepLogic and other RNN\nbaseline models. Our model achieves better out-of-distribution generalisation\nthan RoBERTa-Large when the rules have been shuffled. Furthermore, to address\nthe issue of unbalanced distribution of reasoning depths in the current\nmulti-step reasoning datasets, we develop PARARULE-Plus, a large dataset with\nmore examples that require deeper reasoning steps. Experimental results show\nthat the addition of PARARULE-Plus can increase the model's performance on\nexamples requiring deeper reasoning depths. The source code and data are\navailable at\nhttps://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.\n","authors":["Qiming Bao","Alex Yuxuan Peng","Tim Hartill","Neset Tan","Zhenyun Deng","Michael Witbrock","Jiamou Liu"],"pdf_url":"https://arxiv.org/pdf/2207.14000v4.pdf","comment":"10 pages, 3 figures, The 2nd International Joint Conference on\n  Learning & Reasoning and 16th International Workshop on Neural-Symbolic\n  Learning and Reasoning (IJCLR-NeSy 2022)"},{"id":"http://arxiv.org/abs/2504.12849v1","updated":"2025-04-17T11:08:51Z","published":"2025-04-17T11:08:51Z","title":"FedX: Adaptive Model Decomposition and Quantization for IoT Federated\n  Learning","summary":"  Federated Learning (FL) allows collaborative training among multiple devices\nwithout data sharing, thus enabling privacy-sensitive applications on mobile or\nInternet of Things (IoT) devices, such as mobile health and asset tracking.\nHowever, designing an FL system with good model utility that works with low\ncomputation/communication overhead on heterogeneous, resource-constrained\nmobile/IoT devices is challenging. To address this problem, this paper proposes\nFedX, a novel adaptive model decomposition and quantization FL system for IoT.\nTo balance utility with resource constraints on IoT devices, FedX decomposes a\nglobal FL model into different sub-networks with adaptive numbers of quantized\nbits for different devices. The key idea is that a device with fewer resources\nreceives a smaller sub-network for lower overhead but utilizes a larger number\nof quantized bits for higher model utility, and vice versa. The quantization\noperations in FedX are done at the server to reduce the computational load on\ndevices. FedX iteratively minimizes the losses in the devices' local data and\nin the server's public data using quantized sub-networks under a regularization\nterm, and thus it maximizes the benefits of combining FL with model\nquantization through knowledge sharing among the server and devices in a\ncost-effective training process. Extensive experiments show that FedX\nsignificantly improves quantization times by up to 8.43X, on-device computation\ntime by 1.5X, and total end-to-end training time by 1.36X, compared with\nbaseline FL systems. We guarantee the global model convergence theoretically\nand validate local model convergence empirically, highlighting FedX's\noptimization efficiency.\n","authors":["Phung Lai","Xiaopeng Jiang","Hai Phan","Cristian Borcea","Khang Tran","An Chen","Vijaya Datta Mayyuri","Ruoming Jin"],"pdf_url":"https://arxiv.org/pdf/2504.12849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12841v1","updated":"2025-04-17T10:57:29Z","published":"2025-04-17T10:57:29Z","title":"ALT: A Python Package for Lightweight Feature Representation in Time\n  Series Classification","summary":"  We introduce ALT, an open-source Python package created for efficient and\naccurate time series classification (TSC). The package implements the adaptive\nlaw-based transformation (ALT) algorithm, which transforms raw time series data\ninto a linearly separable feature space using variable-length shifted time\nwindows. This adaptive approach enhances its predecessor, the linear law-based\ntransformation (LLT), by effectively capturing patterns of varying temporal\nscales. The software is implemented for scalability, interpretability, and ease\nof use, achieving state-of-the-art performance with minimal computational\noverhead. Extensive benchmarking on real-world datasets demonstrates the\nutility of ALT for diverse TSC tasks in physics and related domains.\n","authors":["Balázs P. Halmos","Balázs Hajós","Vince Á. Molnár","Marcell T. Kurbucz","Antal Jakovác"],"pdf_url":"https://arxiv.org/pdf/2504.12841v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.12833v1","updated":"2025-04-17T10:46:39Z","published":"2025-04-17T10:46:39Z","title":"Image-Editing Specialists: An RLAIF Approach for Diffusion Models","summary":"  We present a novel approach to training specialized instruction-based\nimage-editing diffusion models, addressing key challenges in structural\npreservation with input images and semantic alignment with user prompts. We\nintroduce an online reinforcement learning framework that aligns the diffusion\nmodel with human preferences without relying on extensive human annotations or\ncurating a large dataset. Our method significantly improves the realism and\nalignment with instructions in two ways. First, the proposed models achieve\nprecise and structurally coherent modifications in complex scenes while\nmaintaining high fidelity in instruction-irrelevant areas. Second, they capture\nfine nuances in the desired edit by leveraging a visual prompt, enabling\ndetailed control over visual edits without lengthy textual prompts. This\napproach simplifies users' efforts to achieve highly specific edits, requiring\nonly 5 reference images depicting a certain concept for training. Experimental\nresults demonstrate that our models can perform intricate edits in complex\nscenes, after just 10 training steps. Finally, we showcase the versatility of\nour method by applying it to robotics, where enhancing the visual realism of\nsimulated environments through targeted sim-to-real image edits improves their\nutility as proxies for real-world settings.\n","authors":["Elior Benarous","Yilun Du","Heng Yang"],"pdf_url":"https://arxiv.org/pdf/2504.12833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08141v2","updated":"2025-04-17T10:44:22Z","published":"2025-04-10T22:03:14Z","title":"Variational quantum and neural quantum states algorithms for the linear\n  complementarity problem","summary":"  Variational quantum algorithms (VQAs) are promising hybrid quantum-classical\nmethods designed to leverage the computational advantages of quantum computing\nwhile mitigating the limitations of current noisy intermediate-scale quantum\n(NISQ) hardware. Although VQAs have been demonstrated as proofs of concept,\ntheir practical utility in solving real-world problems -- and whether\nquantum-inspired classical algorithms can match their performance -- remains an\nopen question. We present a novel application of the variational quantum linear\nsolver (VQLS) and its classical neural quantum states-based counterpart, the\nvariational neural linear solver (VNLS), as key components within a minimum map\nNewton solver for a complementarity-based rigid body contact model. We\ndemonstrate using the VNLS that our solver accurately simulates the dynamics of\nrigid spherical bodies during collision events. These results suggest that\nquantum and quantum-inspired linear algebra algorithms can serve as viable\nalternatives to standard linear algebra solvers for modeling certain physical\nsystems.\n","authors":["Saibal De","Oliver Knitter","Rohan Kodati","Paramsothy Jayakumar","James Stokes","Shravan Veerapaneni"],"pdf_url":"https://arxiv.org/pdf/2504.08141v2.pdf","comment":"13 pages, 5 figures, to appear in Philosophical Transactions of the\n  Royal Society A"},{"id":"http://arxiv.org/abs/2504.12828v1","updated":"2025-04-17T10:42:38Z","published":"2025-04-17T10:42:38Z","title":"Predicting Stock Prices using Permutation Decision Trees and Strategic\n  Trailing","summary":"  In this paper, we explore the application of Permutation Decision Trees (PDT)\nand strategic trailing for predicting stock market movements and executing\nprofitable trades in the Indian stock market. We focus on high-frequency data\nusing 5-minute candlesticks for the top 50 stocks listed in the NIFTY 50 index.\nWe implement a trading strategy that aims to buy stocks at lower prices and\nsell them at higher prices, capitalizing on short-term market fluctuations. Due\nto regulatory constraints in India, short selling is not considered in our\nstrategy. The model incorporates various technical indicators and employs\nhyperparameters such as the trailing stop-loss value and support thresholds to\nmanage risk effectively. Our results indicate that the proposed trading bot has\nthe potential to outperform the market average and yield returns higher than\nthe risk-free rate offered by 10-year Indian government bonds. We trained and\ntested data on a 60 day dataset provided by Yahoo Finance. Specifically, 12\ndays for testing and 48 days for training. Our bot based on permutation\ndecision tree achieved a profit of 1.3468 % over a 12-day testing period, where\nas a bot based on LSTM gave a return of 0.1238 % over a 12-day testing period\nand a bot based on RNN gave a return of 0.3096 % over a 12-day testing period.\nAll of the bots outperform the buy-and-hold strategy, which resulted in a loss\nof 2.2508 %.\n","authors":["Vishrut Ramraj","Nithin Nagaraj","Harikrishnan N B"],"pdf_url":"https://arxiv.org/pdf/2504.12828v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.04071v2","updated":"2025-04-17T10:38:48Z","published":"2024-06-06T13:36:41Z","title":"Dynamic angular synchronization under smoothness constraints","summary":"  Given an undirected measurement graph $\\mathcal{H} = ([n], \\mathcal{E})$, the\nclassical angular synchronization problem consists of recovering unknown angles\n$\\theta_1^*,\\dots,\\theta_n^*$ from a collection of noisy pairwise measurements\nof the form $(\\theta_i^* - \\theta_j^*) \\mod 2\\pi$, for all $\\{i,j\\} \\in\n\\mathcal{E}$. This problem arises in a variety of applications, including\ncomputer vision, time synchronization of distributed networks, and ranking from\npairwise comparisons. In this paper, we consider a dynamic version of this\nproblem where the angles, and also the measurement graphs evolve over $T$ time\npoints. Assuming a smoothness condition on the evolution of the latent angles,\nwe derive three algorithms for joint estimation of the angles over all time\npoints. Moreover, for one of the algorithms, we establish non-asymptotic\nrecovery guarantees for the mean-squared error (MSE) under different\nstatistical models. In particular, we show that the MSE converges to zero as\n$T$ increases under milder conditions than in the static setting. This includes\nthe setting where the measurement graphs are highly sparse and disconnected,\nand also when the measurement noise is large and can potentially increase with\n$T$. We complement our theoretical results with experiments on synthetic data.\n","authors":["Ernesto Araya","Mihai Cucuringu","Hemant Tyagi"],"pdf_url":"https://arxiv.org/pdf/2406.04071v2.pdf","comment":"42 pages, 9 figures. Corrected typos and added clarifications, as per\n  the suggestions of reviewers. Added Remarks 4,5 and Algorithm 4 (which is\n  same as Algorithm 3 but with TRS relaced by a spectral method). Accepted in\n  JMLR"},{"id":"http://arxiv.org/abs/2411.04011v2","updated":"2025-04-17T10:30:26Z","published":"2024-11-06T15:49:28Z","title":"Predicting and Publishing Accurate Imbalance Prices Using Monte Carlo\n  Tree Search","summary":"  The growing reliance on renewable energy sources, particularly solar and\nwind, has introduced challenges due to their uncontrollable production. This\ncomplicates maintaining the electrical grid balance, prompting some\ntransmission system operators in Western Europe to implement imbalance tariffs\nthat penalize unsustainable power deviations. These tariffs create an implicit\ndemand response framework to mitigate grid instability. Yet, several challenges\nlimit active participation. In Belgium, for example, imbalance prices are only\ncalculated at the end of each 15-minute settlement period, creating high risk\ndue to price uncertainty. This risk is further amplified by the inherent\nvolatility of imbalance prices, discouraging participation. Although\ntransmission system operators provide minute-based price predictions, the\nsystem imbalance volatility makes accurate price predictions challenging to\nobtain and requires sophisticated techniques. Moreover, publishing price\nestimates can prompt participants to adjust their schedules, potentially\naffecting the system balance and the final price, adding further complexity. To\naddress these challenges, we propose a Monte Carlo Tree Search method that\npublishes accurate imbalance prices while accounting for potential response\nactions. Our approach models the system dynamics using a neural network\nforecaster and a cluster of virtual batteries controlled by reinforcement\nlearning agents. Compared to Belgium's current publication method, our\ntechnique improves price accuracy by 20.4% under ideal conditions and by 12.8%\nin more realistic scenarios. This research addresses an unexplored, yet crucial\nproblem, positioning this paper as a pioneering work in analyzing the potential\nof more advanced imbalance price publishing techniques.\n","authors":["Fabio Pavirani","Jonas Van Gompel","Seyed Soroush Karimi Madahi","Bert Claessens","Chris Develder"],"pdf_url":"https://arxiv.org/pdf/2411.04011v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12806v1","updated":"2025-04-17T10:12:38Z","published":"2025-04-17T10:12:38Z","title":"A Numerical Gradient Inversion Attack in Variational Quantum\n  Neural-Networks","summary":"  The loss landscape of Variational Quantum Neural Networks (VQNNs) is\ncharacterized by local minima that grow exponentially with increasing qubits.\nBecause of this, it is more challenging to recover information from model\ngradients during training compared to classical Neural Networks (NNs). In this\npaper we present a numerical scheme that successfully reconstructs input\ntraining, real-world, practical data from trainable VQNNs' gradients. Our\nscheme is based on gradient inversion that works by combining gradients\nestimation with the finite difference method and adaptive low-pass filtering.\nThe scheme is further optimized with Kalman filter to obtain efficient\nconvergence. Our experiments show that our algorithm can invert even\nbatch-trained data, given the VQNN model is sufficiently over-parameterized.\n","authors":["Georgios Papadopoulos","Shaltiel Eloul","Yash Satsangi","Jamie Heredge","Niraj Kumar","Chun-Fu Chen","Marco Pistoia"],"pdf_url":"https://arxiv.org/pdf/2504.12806v1.pdf","comment":"9 pages, 17 figures"},{"id":"http://arxiv.org/abs/2501.00584v2","updated":"2025-04-17T10:10:16Z","published":"2024-12-31T18:17:05Z","title":"Online Video Understanding: OVBench and VideoChat-Online","summary":"  Multimodal Large Language Models (MLLMs) have significantly progressed in\noffline video understanding. However, applying these models to real-world\nscenarios, such as autonomous driving and human-computer interaction, presents\nunique challenges due to the need for real-time processing of continuous online\nvideo streams. To this end, this paper presents systematic efforts from three\nperspectives: evaluation benchmark, model architecture, and training strategy.\nFirst, we introduce OVBench, a comprehensive question-answering benchmark\ndesigned to evaluate models' ability to perceive, memorize, and reason within\nonline video contexts. It features 6 core task types across three temporal\ncontexts-past, current, and future-forming 16 subtasks from diverse datasets.\nSecond, we propose a new Pyramid Memory Bank (PMB) that effectively retains key\nspatiotemporal information in video streams. Third, we proposed an\noffline-to-online learning paradigm, designing an interleaved dialogue format\nfor online video data and constructing an instruction-tuning dataset tailored\nfor online video training. This framework led to the development of\nVideoChat-Online, a robust and efficient model for online video understanding.\nDespite the lower computational cost and higher efficiency, VideoChat-Online\noutperforms existing state-of-the-art offline and online models across popular\noffline video benchmarks and OVBench, demonstrating the effectiveness of our\nmodel architecture and training strategy. % Our approach surpasses existing\nstate-of-the-art offline models Qwen2-VL 7B and online models Flash-VStream, by\n4.19% and 23.7% on OVBench, respectively.\n","authors":["Zhenpeng Huang","Xinhao Li","Jiaqi Li","Jing Wang","Xiangyu Zeng","Cheng Liang","Tao Wu","Xi Chen","Liang Li","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2501.00584v2.pdf","comment":"CVPR 2025 Camera Ready Version. Project Page:\n  https://videochat-online.github.io"},{"id":"http://arxiv.org/abs/2504.12803v1","updated":"2025-04-17T10:05:10Z","published":"2025-04-17T10:05:10Z","title":"Enhancing Explainability and Reliable Decision-Making in Particle Swarm\n  Optimization through Communication Topologies","summary":"  Swarm intelligence effectively optimizes complex systems across fields like\nengineering and healthcare, yet algorithm solutions often suffer from low\nreliability due to unclear configurations and hyperparameters. This study\nanalyzes Particle Swarm Optimization (PSO), focusing on how different\ncommunication topologies Ring, Star, and Von Neumann affect convergence and\nsearch behaviors. Using an adapted IOHxplainer , an explainable benchmarking\ntool, we investigate how these topologies influence information flow,\ndiversity, and convergence speed, clarifying the balance between exploration\nand exploitation. Through visualization and statistical analysis, the research\nenhances interpretability of PSO's decisions and provides practical guidelines\nfor choosing suitable topologies for specific optimization tasks. Ultimately,\nthis contributes to making swarm based optimization more transparent, robust,\nand trustworthy.\n","authors":["Nitin Gupta","Indu Bala","Bapi Dutta","Luis Martínez","Anupam Yadav"],"pdf_url":"https://arxiv.org/pdf/2504.12803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12801v1","updated":"2025-04-17T10:01:59Z","published":"2025-04-17T10:01:59Z","title":"Sign-In to the Lottery: Reparameterizing Sparse Training From Scratch","summary":"  The performance gap between training sparse neural networks from scratch\n(PaI) and dense-to-sparse training presents a major roadblock for efficient\ndeep learning. According to the Lottery Ticket Hypothesis, PaI hinges on\nfinding a problem specific parameter initialization. As we show, to this end,\ndetermining correct parameter signs is sufficient. Yet, they remain elusive to\nPaI. To address this issue, we propose Sign-In, which employs a dynamic\nreparameterization that provably induces sign flips. Such sign flips are\ncomplementary to the ones that dense-to-sparse training can accomplish,\nrendering Sign-In as an orthogonal method. While our experiments and theory\nsuggest performance improvements of PaI, they also carve out the main open\nchallenge to close the gap between PaI and dense-to-sparse training.\n","authors":["Advait Gadhikar","Tom Jacobs","Chao Zhou","Rebekka Burkholz"],"pdf_url":"https://arxiv.org/pdf/2504.12801v1.pdf","comment":"21 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.23749v6","updated":"2025-04-17T09:49:31Z","published":"2024-10-31T09:09:39Z","title":"LSEAttention is All You Need for Time Series Forecasting","summary":"  Transformer-based architectures have achieved remarkable success in natural\nlanguage processing and computer vision. However, their performance in\nmultivariate long-term forecasting often falls short compared to simpler linear\nbaselines. Previous research has identified the traditional attention mechanism\nas a key factor limiting their effectiveness in this domain. To bridge this\ngap, we introduce LATST, a novel approach designed to mitigate entropy collapse\nand training instability common challenges in Transformer-based time series\nforecasting. We rigorously evaluate LATST across multiple real-world\nmultivariate time series datasets, demonstrating its ability to outperform\nexisting state-of-the-art Transformer models. Notably, LATST manages to achieve\ncompetitive performance with fewer parameters than some linear models on\ncertain datasets, highlighting its efficiency and effectiveness.\n","authors":["Dizhen Liang"],"pdf_url":"https://arxiv.org/pdf/2410.23749v6.pdf","comment":"8 pages with referencing, 1 figure, 5 tables"},{"id":"http://arxiv.org/abs/2504.12782v1","updated":"2025-04-17T09:29:30Z","published":"2025-04-17T09:29:30Z","title":"Set You Straight: Auto-Steering Denoising Trajectories to Sidestep\n  Unwanted Concepts","summary":"  Ensuring the ethical deployment of text-to-image models requires effective\ntechniques to prevent the generation of harmful or inappropriate content. While\nconcept erasure methods offer a promising solution, existing finetuning-based\napproaches suffer from notable limitations. Anchor-free methods risk disrupting\nsampling trajectories, leading to visual artifacts, while anchor-based methods\nrely on the heuristic selection of anchor concepts. To overcome these\nshortcomings, we introduce a finetuning framework, dubbed ANT, which\nAutomatically guides deNoising Trajectories to avoid unwanted concepts. ANT is\nbuilt on a key insight: reversing the condition direction of classifier-free\nguidance during mid-to-late denoising stages enables precise content\nmodification without sacrificing early-stage structural integrity. This\ninspires a trajectory-aware objective that preserves the integrity of the\nearly-stage score function field, which steers samples toward the natural image\nmanifold, without relying on heuristic anchor concept selection. For\nsingle-concept erasure, we propose an augmentation-enhanced weight saliency map\nto precisely identify the critical parameters that most significantly\ncontribute to the unwanted concept, enabling more thorough and efficient\nerasure. For multi-concept erasure, our objective function offers a versatile\nplug-and-play solution that significantly boosts performance. Extensive\nexperiments demonstrate that ANT achieves state-of-the-art results in both\nsingle and multi-concept erasure, delivering high-quality, safe outputs without\ncompromising the generative fidelity. Code is available at\nhttps://github.com/lileyang1210/ANT\n","authors":["Leyang Li","Shilin Lu","Yan Ren","Adams Wai-Kin Kong"],"pdf_url":"https://arxiv.org/pdf/2504.12782v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.09708v2","updated":"2025-04-17T09:27:02Z","published":"2024-10-13T03:34:19Z","title":"Control the GNN: Utilizing Neural Controller with Lyapunov Stability for\n  Test-Time Feature Reconstruction","summary":"  The performance of graph neural networks (GNNs) is susceptible to\ndiscrepancies between training and testing sample distributions. Prior studies\nhave attempted to mitigating the impact of distribution shift by reconstructing\nnode features during the testing phase without modifying the model parameters.\nHowever, these approaches lack theoretical analysis of the proximity between\npredictions and ground truth at test time. In this paper, we propose a novel\nnode feature reconstruction method grounded in Lyapunov stability theory.\nSpecifically, we model the GNN as a control system during the testing phase,\nconsidering node features as control variables. A neural controller that\nadheres to the Lyapunov stability criterion is then employed to reconstruct\nthese node features, ensuring that the predictions progressively approach the\nground truth at test time. We validate the effectiveness of our approach\nthrough extensive experiments across multiple datasets, demonstrating\nsignificant performance improvements.\n","authors":["Jielong Yang","Rui Ding","Feng Ji","Hongbin Wang","Linbo Xie"],"pdf_url":"https://arxiv.org/pdf/2410.09708v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2504.08401v2","updated":"2025-04-17T09:01:57Z","published":"2025-04-11T10:08:38Z","title":"Graph Reduction with Unsupervised Learning in Column Generation: A\n  Routing Application","summary":"  Column Generation (CG) is a popular method dedicated to enhancing\ncomputational efficiency in large scale Combinatorial Optimization (CO)\nproblems. It reduces the number of decision variables in a problem by solving a\npricing problem. For many CO problems, the pricing problem is an Elementary\nShortest Path Problem with Resource Constraints (ESPPRC). Large ESPPRC\ninstances are difficult to solve to near-optimality. Consequently, we use a\nGraph neural Network (GNN) to reduces the size of the ESPPRC such that it\nbecomes computationally tractable with standard solving techniques. Our GNN is\ntrained by Unsupervised Learning and outputs a distribution for the arcs to be\nretained in the reduced PP. The reduced PP is solved by a local search that\nfinds columns with large reduced costs and speeds up convergence. We apply our\nmethod on a set of Capacitated Vehicle Routing Problems with Time Windows and\nshow significant improvements in convergence compared to simple reduction\ntechniques from the literature. For a fixed computational budget, we improve\nthe objective values by over 9\\% for larger instances. We also analyze the\nperformance of our CG algorithm and test the generalization of our method to\ndifferent classes of instances than the training data.\n","authors":["Abdo Abouelrous","Laurens Bliek","Adriana F. Gabor","Yaoxin Wu","Yingqian Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.08401v2.pdf","comment":"22 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2504.12764v1","updated":"2025-04-17T09:01:16Z","published":"2025-04-17T09:01:16Z","title":"GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large\n  Language Models on Graph-theoretic Tasks","summary":"  In this paper, we presented GraphOmni, a comprehensive benchmark framework\nfor systematically evaluating the graph reasoning capabilities of LLMs. By\nanalyzing critical dimensions, including graph types, serialization formats,\nand prompt schemes, we provided extensive insights into the strengths and\nlimitations of current LLMs. Our empirical findings emphasize that no single\nserialization or prompting strategy consistently outperforms others. Motivated\nby these insights, we propose a reinforcement learning-based approach that\ndynamically selects the best serialization-prompt pairings, resulting in\nsignificant accuracy improvements. GraphOmni's modular and extensible design\nestablishes a robust foundation for future research, facilitating advancements\ntoward general-purpose graph reasoning models.\n","authors":["Hao Xu","Xiangru Jian","Xinjian Zhao","Wei Pang","Chao Zhang","Suyuchen Wang","Qixin Zhang","Joao Monteiro","Qiuzhuang Sun","Tianshu Yu"],"pdf_url":"https://arxiv.org/pdf/2504.12764v1.pdf","comment":"82 pages"},{"id":"http://arxiv.org/abs/2504.12758v1","updated":"2025-04-17T08:53:30Z","published":"2025-04-17T08:53:30Z","title":"Universal Approximation with XL MIMO Systems: OTA Classification via\n  Trainable Analog Combining","summary":"  In this paper, we demonstrate that an eXtremely Large (XL) Multiple-Input\nMultiple-Output (MIMO) wireless system with appropriate analog combining\ncomponents exhibits the properties of a universal function approximator,\nsimilar to a feedforward neural network. By treating the XL MIMO channel\ncoefficients as the random nodes of a hidden layer, and the receiver's analog\ncombiner as a trainable output layer, we cast the end-to-end system to the\nExtreme Learning Machine (ELM) framework, leading to a novel formulation for\nOver-The-Air (OTA) edge inference without requiring traditional digital\nprocessing nor pre-processing at the transmitter. Through theoretical analysis\nand numerical evaluation, we showcase that XL-MIMO-ELM enables\nnear-instantaneous training and efficient classification, suggesting the\nparadigm shift of beyond massive MIMO systems as neural networks alongside\ntheir profound communications role. Compared to deep learning approaches and\nconventional ELMs, the proposed framework achieves on par performance with\norders of magnitude lower complexity, making it highly attractive for ultra low\npower wireless devices.\n","authors":["Kyriakos Stylianopoulos","George C. Alexandropoulos"],"pdf_url":"https://arxiv.org/pdf/2504.12758v1.pdf","comment":"Submitted to IEEE SPAWC 2025"},{"id":"http://arxiv.org/abs/2504.06235v2","updated":"2025-04-17T08:52:03Z","published":"2025-04-08T17:32:56Z","title":"Decentralized Federated Domain Generalization with Style Sharing: A\n  Formal Modeling and Convergence Analysis","summary":"  Much of the federated learning (FL) literature focuses on settings where\nlocal dataset statistics remain the same between training and testing time.\nRecent advances in domain generalization (DG) aim to use data from source\n(training) domains to train a model that generalizes well to data from unseen\ntarget (testing) domains. In this paper, we are motivated by two major gaps in\nexisting work on FL and DG: (1) the lack of formal mathematical analysis of DG\nobjectives and training processes; and (2) DG research in FL being limited to\nthe conventional star-topology architecture. Addressing the second gap, we\ndevelop $\\textit{Decentralized Federated Domain Generalization with Style\nSharing}$ ($\\texttt{StyleDDG}$), a fully decentralized DG algorithm designed to\nallow devices in a peer-to-peer network to achieve DG based on sharing style\ninformation inferred from their datasets. Additionally, we fill the first gap\nby providing the first systematic approach to mathematically analyzing\nstyle-based DG training optimization. We cast existing centralized DG\nalgorithms within our framework, and employ their formalisms to model\n$\\texttt{StyleDDG}$. Based on this, we obtain analytical conditions under which\na sub-linear convergence rate of $\\texttt{StyleDDG}$ can be obtained. Through\nexperiments on two popular DG datasets, we demonstrate that $\\texttt{StyleDDG}$\ncan obtain significant improvements in accuracy across target domains with\nminimal added communication overhead compared to decentralized gradient methods\nthat do not employ style sharing.\n","authors":["Shahryar Zehtabi","Dong-Jun Han","Seyyedali Hosseinalipour","Christopher G. Brinton"],"pdf_url":"https://arxiv.org/pdf/2504.06235v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12742v1","updated":"2025-04-17T08:32:25Z","published":"2025-04-17T08:32:25Z","title":"Decentralized Nonconvex Composite Federated Learning with Gradient\n  Tracking and Momentum","summary":"  Decentralized Federated Learning (DFL) eliminates the reliance on the\nserver-client architecture inherent in traditional federated learning,\nattracting significant research interest in recent years. Simultaneously, the\nobjective functions in machine learning tasks are often nonconvex and\nfrequently incorporate additional, potentially nonsmooth regularization terms\nto satisfy practical requirements, thereby forming nonconvex composite\noptimization problems. Employing DFL methods to solve such general optimization\nproblems leads to the formulation of Decentralized Nonconvex Composite\nFederated Learning (DNCFL), a topic that remains largely underexplored. In this\npaper, we propose a novel DNCFL algorithm, termed \\bf{DEPOSITUM}. Built upon\nproximal stochastic gradient tracking, DEPOSITUM mitigates the impact of data\nheterogeneity by enabling clients to approximate the global gradient. The\nintroduction of momentums in the proximal gradient descent step, replacing\ntracking variables, reduces the variance introduced by stochastic gradients.\nAdditionally, DEPOSITUM supports local updates of client variables,\nsignificantly reducing communication costs. Theoretical analysis demonstrates\nthat DEPOSITUM achieves an expected $\\epsilon$-stationary point with an\niteration complexity of $\\mathcal{O}(1/\\epsilon^2)$. The proximal gradient,\nconsensus errors, and gradient estimation errors decrease at a sublinear rate\nof $\\mathcal{O}(1/T)$. With appropriate parameter selection, the algorithm\nachieves network-independent linear speedup without requiring mega-batch\nsampling. Finally, we apply DEPOSITUM to the training of neural networks on\nreal-world datasets, systematically examining the influence of various\nhyperparameters on its performance. Comparisons with other federated composite\noptimization algorithms validate the effectiveness of the proposed method.\n","authors":["Yuan Zhou","Xinli Shi","Xuelong Li","Jiachen Zhong","Guanghui Wen","Jinde Cao"],"pdf_url":"https://arxiv.org/pdf/2504.12742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10291v4","updated":"2025-04-17T08:31:14Z","published":"2024-10-14T08:45:35Z","title":"Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal\n  Perspective","summary":"  Accurate interpretation and visualization of human instructions are crucial\nfor text-to-image (T2I) synthesis. However, current models struggle to capture\nsemantic variations from word order changes, and existing evaluations, relying\non indirect metrics like text-image similarity, fail to reliably assess these\nchallenges. This often obscures poor performance on complex or uncommon\nlinguistic patterns by the focus on frequent word combinations. To address\nthese deficiencies, we propose a novel metric called SemVarEffect and a\nbenchmark named SemVarBench, designed to evaluate the causality between\nsemantic variations in inputs and outputs in T2I synthesis. Semantic variations\nare achieved through two types of linguistic permutations, while avoiding\neasily predictable literal variations. Experiments reveal that the\nCogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.\nSemantic variations in object relations are less understood than attributes,\nscoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in\nUNet or Transformers plays a crucial role in handling semantic variations, a\nfactor previously overlooked by a focus on textual encoders. Our work\nestablishes an effective evaluation framework that advances the T2I synthesis\ncommunity's exploration of human instruction understanding. Our benchmark and\ncode are available at https://github.com/zhuxiangru/SemVarBench .\n","authors":["Xiangru Zhu","Penglei Sun","Yaoxian Song","Yanghua Xiao","Zhixu Li","Chengyu Wang","Jun Huang","Bei Yang","Xiaoxiao Xu"],"pdf_url":"https://arxiv.org/pdf/2410.10291v4.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2504.12740v1","updated":"2025-04-17T08:29:14Z","published":"2025-04-17T08:29:14Z","title":"GPMFS: Global Foundation and Personalized Optimization for Multi-Label\n  Feature Selection","summary":"  As artificial intelligence methods are increasingly applied to complex task\nscenarios, high dimensional multi-label learning has emerged as a prominent\nresearch focus. At present, the curse of dimensionality remains one of the\nmajor bottlenecks in high-dimensional multi-label learning, which can be\neffectively addressed through multi-label feature selection methods. However,\nexisting multi-label feature selection methods mostly focus on identifying\nglobal features shared across all labels, which overlooks personalized\ncharacteristics and specific requirements of individual labels. This\nglobal-only perspective may limit the ability to capture label-specific\ndiscriminative information, thereby affecting overall performance. In this\npaper, we propose a novel method called GPMFS (Global Foundation and\nPersonalized Optimization for Multi-Label Feature Selection). GPMFS firstly\nidentifies global features by exploiting label correlations, then adaptively\nsupplements each label with a personalized subset of discriminative features\nusing a threshold-controlled strategy. Experiments on multiple real-world\ndatasets demonstrate that GPMFS achieves superior performance while maintaining\nstrong interpretability and robustness. Furthermore, GPMFS provides insights\ninto the label-specific strength across different multi-label datasets, thereby\ndemonstrating the necessity and potential applicability of personalized feature\nselection approaches.\n","authors":["Yifan Cao","Zhilong Mi","Ziqiao Yin","Binghui Guo","Jin Dong"],"pdf_url":"https://arxiv.org/pdf/2504.12740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19422v2","updated":"2025-04-17T08:28:21Z","published":"2024-12-27T03:16:56Z","title":"De Novo Generation of Hit-like Molecules from Gene Expression Profiles\n  via Deep Learning","summary":"  De novo generation of hit-like molecules is a challenging task in the drug\ndiscovery process. Most methods in previous studies learn the semantics and\nsyntax of molecular structures by analyzing molecular graphs or simplified\nmolecular input line entry system (SMILES) strings; however, they do not take\ninto account the drug responses of the biological systems consisting of genes\nand proteins. In this study we propose a hybrid neural network, HNN2Mol, which\nutilizes gene expression profiles to generate molecular structures with\ndesirable phenotypes for arbitrary target proteins. In the algorithm, a\nvariational autoencoder is employed as a feature extractor to learn the latent\nfeature distribution of the gene expression profiles. Then, a long short-term\nmemory is leveraged as the chemical generator to produce syntactically valid\nSMILES strings that satisfy the feature conditions of the gene expression\nprofile extracted by the feature extractor. Experimental results and case\nstudies demonstrate that the proposed HNN2Mol model can produce new molecules\nwith potential bioactivities and drug-like properties.\n","authors":["Chen Li","Yoshihiro Yamanishi"],"pdf_url":"https://arxiv.org/pdf/2412.19422v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10273v2","updated":"2025-04-17T08:18:15Z","published":"2024-12-13T16:46:46Z","title":"unPIC: A Geometric Multiview Prior for Image to 3D Synthesis","summary":"  We introduce a hierarchical probabilistic approach to go from a 2D image to\nmultiview 3D: a diffusion \"prior\" predicts the unseen 3D geometry, which then\nconditions a diffusion \"decoder\" to generate novel views of the subject. We use\na pointmap-based geometric representation to coordinate the generation of\nmultiple target views simultaneously. We construct a predictable distribution\nof geometric features per target view to enable learnability across examples,\nand generalization to arbitrary inputs images. Our modular, geometry-driven\napproach to novel-view synthesis (called \"unPIC\") beats competing baselines\nsuch as CAT3D, EscherNet, Free3D, and One-2-3-45 on held-out objects from\nObjaverseXL, as well as unseen real-world objects from Google Scanned Objects,\nAmazon Berkeley Objects, and the Digital Twin Catalog.\n","authors":["Rishabh Kabra","Drew A. Hudson","Sjoerd van Steenkiste","Joao Carreira","Niloy J. Mitra"],"pdf_url":"https://arxiv.org/pdf/2412.10273v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15787v2","updated":"2025-04-17T08:18:02Z","published":"2024-10-21T08:49:51Z","title":"Arithmetic Transformers Can Length-Generalize in Both Operand Length and\n  Count","summary":"  Transformers often struggle with length generalization, meaning they fail to\ngeneralize to sequences longer than those encountered during training. While\narithmetic tasks are commonly used to study length generalization, certain\ntasks are considered notoriously difficult, e.g., multi-operand addition\n(requiring generalization over both the number of operands and their lengths)\nand multiplication (requiring generalization over both operand lengths). In\nthis work, we achieve approximately 2-3x length generalization on both tasks,\nwhich is the first such achievement in arithmetic Transformers. We design\ntask-specific scratchpads enabling the model to focus on a fixed number of\ntokens per each next-token prediction step, and apply multi-level versions of\n\\Position Coupling (Cho et al., 2024; McLeish et al., 2024) to let Transformers\nknow the right position to attend to. On the theory side, we prove that a\n1-layer Transformer using our method can solve multi-operand addition, up to\noperand length and operand count that are exponential in embedding dimension.\n","authors":["Hanseul Cho","Jaeyoung Cha","Srinadh Bhojanapalli","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2410.15787v2.pdf","comment":"44 pages, 20 figures, 26 tables, accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2406.15459v2","updated":"2025-04-17T08:05:19Z","published":"2024-06-11T03:36:00Z","title":"Large-Scale Contextual Market Equilibrium Computation through Deep\n  Learning","summary":"  Market equilibrium is one of the most fundamental solution concepts in\neconomics and social optimization analysis. Existing works on market\nequilibrium computation primarily focus on settings with relatively few buyers.\nMotivated by this, our paper investigates the computation of market equilibrium\nin scenarios with a large-scale buyer population, where buyers and goods are\nrepresented by their contexts. Building on this realistic and generalized\ncontextual market model, we introduce MarketFCNet, a deep learning-based method\nfor approximating market equilibrium. We start by parameterizing the allocation\nof each good to each buyer using a neural network, which depends solely on the\ncontext of the buyer and the good. Next, we propose an efficient method to\nunbiasedly estimate the loss function of the training algorithm, enabling us to\noptimize the network parameters through gradient. To evaluate the approximated\nsolution, we propose a metric called Nash Gap, which quantifies the deviation\nof the given allocation and price pair from the market equilibrium.\nExperimental results indicate that MarketFCNet delivers competitive performance\nand significantly lower running times compared to existing methods as the\nmarket scale expands, demonstrating the potential of deep learning-based\nmethods to accelerate the approximation of large-scale contextual market\nequilibrium.\n","authors":["Yunxuan Ma","Yide Bian","Hao Xu","Weitao Yang","Jingshu Zhao","Zhijian Duan","Feng Wang","Xiaotie Deng"],"pdf_url":"https://arxiv.org/pdf/2406.15459v2.pdf","comment":"25 pages, 4 figures, recieved at IJTCS2025 conference"},{"id":"http://arxiv.org/abs/2504.12721v1","updated":"2025-04-17T07:54:26Z","published":"2025-04-17T07:54:26Z","title":"TimeCapsule: Solving the Jigsaw Puzzle of Long-Term Time Series\n  Forecasting with Compressed Predictive Representations","summary":"  Recent deep learning models for Long-term Time Series Forecasting (LTSF)\noften emphasize complex, handcrafted designs, while simpler architectures like\nlinear models or MLPs have often outperformed these intricate solutions. In\nthis paper, we revisit and organize the core ideas behind several key\ntechniques, such as redundancy reduction and multi-scale modeling, which are\nfrequently employed in advanced LTSF models. Our goal is to streamline these\nideas for more efficient deep learning utilization. To this end, we introduce\nTimeCapsule, a model built around the principle of high-dimensional information\ncompression that unifies these techniques in a generalized yet simplified\nframework. Specifically, we model time series as a 3D tensor, incorporating\ntemporal, variate, and level dimensions, and leverage mode production to\ncapture multi-mode dependencies while achieving dimensionality compression. We\npropose an internal forecast within the compressed representation domain,\nsupported by the Joint-Embedding Predictive Architecture (JEPA), to monitor the\nlearning of predictive representations. Extensive experiments on challenging\nbenchmarks demonstrate the versatility of our method, showing that TimeCapsule\ncan achieve state-of-the-art performance.\n","authors":["Yihang Lu","Yangyang Xu","Qitao Qing","Xianwei Meng"],"pdf_url":"https://arxiv.org/pdf/2504.12721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13054v2","updated":"2025-04-17T07:48:47Z","published":"2024-10-16T21:32:31Z","title":"Systems with Switching Causal Relations: A Meta-Causal Perspective","summary":"  Most work on causality in machine learning assumes that causal relationships\nare driven by a constant underlying process. However, the flexibility of\nagents' actions or tipping points in the environmental process can change the\nqualitative dynamics of the system. As a result, new causal relationships may\nemerge, while existing ones change or disappear, resulting in an altered causal\ngraph. To analyze these qualitative changes on the causal graph, we propose the\nconcept of meta-causal states, which groups classical causal models into\nclusters based on equivalent qualitative behavior and consolidates specific\nmechanism parameterizations. We demonstrate how meta-causal states can be\ninferred from observed agent behavior, and discuss potential methods for\ndisentangling these states from unlabeled data. Finally, we direct our analysis\ntowards the application of a dynamical system, showing that meta-causal states\ncan also emerge from inherent system dynamics, and thus constitute more than a\ncontext-dependent framework in which mechanisms emerge only as a result of\nexternal factors.\n","authors":["Moritz Willig","Tim Nelson Tobiasch","Florian Peter Busch","Jonas Seng","Devendra Singh Dhami","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2410.13054v2.pdf","comment":"21 pages, 3 figures, 4 tables, ICLR 2025 Camera Ready Version"},{"id":"http://arxiv.org/abs/2502.18553v3","updated":"2025-04-17T07:48:20Z","published":"2025-02-25T18:19:06Z","title":"Applications of Statistical Field Theory in Deep Learning","summary":"  Deep learning algorithms have made incredible strides in the past decade, yet\ndue to their complexity, the science of deep learning remains in its early\nstages. Being an experimentally driven field, it is natural to seek a theory of\ndeep learning within the physics paradigm. As deep learning is largely about\nlearning functions and distributions over functions, statistical field theory,\na rich and versatile toolbox for tackling complex distributions over functions\n(fields) is an obvious choice of formalism. Research efforts carried out in the\npast few years have demonstrated the ability of field theory to provide useful\ninsights on generalization, implicit bias, and feature learning effects. Here\nwe provide a pedagogical review of this emerging line of research.\n","authors":["Zohar Ringel","Noa Rubin","Edo Mor","Moritz Helias","Inbar Seroussi"],"pdf_url":"https://arxiv.org/pdf/2502.18553v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20285v3","updated":"2025-04-17T07:47:26Z","published":"2023-10-31T08:58:16Z","title":"Accelerating Non-Conjugate Gaussian Processes By Trading Off Computation\n  For Uncertainty","summary":"  Non-conjugate Gaussian processes (NCGPs) define a flexible probabilistic\nframework to model categorical, ordinal and continuous data, and are widely\nused in practice. However, exact inference in NCGPs is prohibitively expensive\nfor large datasets, thus requiring approximations in practice. The\napproximation error adversely impacts the reliability of the model and is not\naccounted for in the uncertainty of the prediction. We introduce a family of\niterative methods that explicitly model this error. They are uniquely suited to\nparallel modern computing hardware, efficiently recycle computations, and\ncompress information to reduce both the time and memory requirements for NCGPs.\nAs we demonstrate on large-scale classification problems, our method\nsignificantly accelerates posterior inference compared to competitive baselines\nby trading off reduced computation for increased uncertainty.\n","authors":["Lukas Tatzel","Jonathan Wenger","Frank Schneider","Philipp Hennig"],"pdf_url":"https://arxiv.org/pdf/2310.20285v3.pdf","comment":"Main text: 15 pages, 7 figures; Supplements: 15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2504.12717v1","updated":"2025-04-17T07:46:19Z","published":"2025-04-17T07:46:19Z","title":"Post-pre-training for Modality Alignment in Vision-Language Foundation\n  Models","summary":"  Contrastive language image pre-training (CLIP) is an essential component of\nbuilding modern vision-language foundation models. While CLIP demonstrates\nremarkable zero-shot performance on downstream tasks, the multi-modal feature\nspaces still suffer from a modality gap, which is a gap between image and text\nfeature clusters and limits downstream task performance. Although existing\nworks attempt to address the modality gap by modifying pre-training or\nfine-tuning, they struggle with heavy training costs with large datasets or\ndegradations of zero-shot performance. This paper presents CLIP-Refine, a\npost-pre-training method for CLIP models at a phase between pre-training and\nfine-tuning. CLIP-Refine aims to align the feature space with 1 epoch training\non small image-text datasets without zero-shot performance degradations. To\nthis end, we introduce two techniques: random feature alignment (RaFA) and\nhybrid contrastive-distillation (HyCD). RaFA aligns the image and text features\nto follow a shared prior distribution by minimizing the distance to random\nreference vectors sampled from the prior. HyCD updates the model with hybrid\nsoft labels generated by combining ground-truth image-text pair labels and\noutputs from the pre-trained CLIP model. This contributes to achieving both\nmaintaining the past knowledge and learning new knowledge to align features.\nOur extensive experiments with multiple classification and retrieval tasks show\nthat CLIP-Refine succeeds in mitigating the modality gap and improving the\nzero-shot performance.\n","authors":["Shin'ya Yamaguchi","Dewei Feng","Sekitoshi Kanai","Kazuki Adachi","Daiki Chijiwa"],"pdf_url":"https://arxiv.org/pdf/2504.12717v1.pdf","comment":"Accepted to CVPR 2025; Code: https://github.com/yshinya6/clip-refine"},{"id":"http://arxiv.org/abs/2504.12715v1","updated":"2025-04-17T07:43:52Z","published":"2025-04-17T07:43:52Z","title":"Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based\n  Code Selection","summary":"  Graph self-supervised learning has gained significant attention recently.\nHowever, many existing approaches heavily depend on perturbations, and\ninappropriate perturbations may corrupt the graph's inherent information. The\nVector Quantized Variational Autoencoder (VQ-VAE) is a powerful autoencoder\nextensively used in fields such as computer vision; however, its application to\ngraph data remains underexplored. In this paper, we provide an empirical\nanalysis of vector quantization in the context of graph autoencoders,\ndemonstrating its significant enhancement of the model's capacity to capture\ngraph topology. Furthermore, we identify two key challenges associated with\nvector quantization when applying in graph data: codebook underutilization and\ncodebook space sparsity. For the first challenge, we propose an annealing-based\nencoding strategy that promotes broad code utilization in the early stages of\ntraining, gradually shifting focus toward the most effective codes as training\nprogresses. For the second challenge, we introduce a hierarchical two-layer\ncodebook that captures relationships between embeddings through clustering. The\nsecond layer codebook links similar codes, encouraging the model to learn\ncloser embeddings for nodes with similar features and structural topology in\nthe graph. Our proposed model outperforms 16 representative baseline methods in\nself-supervised link prediction and node classification tasks across multiple\ndatasets.\n","authors":["Long Zeng","Jianxiang Yu","Jiapeng Zhu","Qingsong Zhong","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2504.12715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12714v1","updated":"2025-04-17T07:41:25Z","published":"2025-04-17T07:41:25Z","title":"Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination","summary":"  Zero-shot coordination (ZSC), the ability to adapt to a new partner in a\ncooperative task, is a critical component of human-compatible AI. While prior\nwork has focused on training agents to cooperate on a single task, these\nspecialized models do not generalize to new tasks, even if they are highly\nsimilar. Here, we study how reinforcement learning on a distribution of\nenvironments with a single partner enables learning general cooperative skills\nthat support ZSC with many new partners on many new problems. We introduce two\nJax-based, procedural generators that create billions of solvable coordination\nchallenges. We develop a new paradigm called Cross-Environment Cooperation\n(CEC), and show that it outperforms competitive baselines quantitatively and\nqualitatively when collaborating with real people. Our findings suggest that\nlearning to collaborate across many unique scenarios encourages agents to\ndevelop general norms, which prove effective for collaboration with different\npartners. Together, our results suggest a new route toward designing generalist\ncooperative agents capable of interacting with humans without requiring human\ndata.\n","authors":["Kunal Jha","Wilka Carvalho","Yancheng Liang","Simon S. Du","Max Kleiman-Weiner","Natasha Jaques"],"pdf_url":"https://arxiv.org/pdf/2504.12714v1.pdf","comment":"Accepted to CogSci 2025, In-review for ICML 2025"},{"id":"http://arxiv.org/abs/2504.12712v1","updated":"2025-04-17T07:35:48Z","published":"2025-04-17T07:35:48Z","title":"Convergence and Implicit Bias of Gradient Descent on Continual Linear\n  Classification","summary":"  We study continual learning on multiple linear classification tasks by\nsequentially running gradient descent (GD) for a fixed budget of iterations per\ntask. When all tasks are jointly linearly separable and are presented in a\ncyclic/random order, we show the directional convergence of the trained linear\nclassifier to the joint (offline) max-margin solution. This is surprising\nbecause GD training on a single task is implicitly biased towards the\nindividual max-margin solution for the task, and the direction of the joint\nmax-margin solution can be largely different from these individual solutions.\nAdditionally, when tasks are given in a cyclic order, we present a\nnon-asymptotic analysis on cycle-averaged forgetting, revealing that (1)\nalignment between tasks is indeed closely tied to catastrophic forgetting and\nbackward knowledge transfer and (2) the amount of forgetting vanishes to zero\nas the cycle repeats. Lastly, we analyze the case where the tasks are no longer\njointly separable and show that the model trained in a cyclic order converges\nto the unique minimum of the joint loss function.\n","authors":["Hyunji Jung","Hanseul Cho","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2504.12712v1.pdf","comment":"67 pages, 11 figures, accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2412.10741v2","updated":"2025-04-17T07:12:52Z","published":"2024-12-14T08:22:49Z","title":"RegMixMatch: Optimizing Mixup Utilization in Semi-Supervised Learning","summary":"  Consistency regularization and pseudo-labeling have significantly advanced\nsemi-supervised learning (SSL). Prior works have effectively employed Mixup for\nconsistency regularization in SSL. However, our findings indicate that applying\nMixup for consistency regularization may degrade SSL performance by\ncompromising the purity of artificial labels. Moreover, most pseudo-labeling\nbased methods utilize thresholding strategy to exclude low-confidence data,\naiming to mitigate confirmation bias; however, this approach limits the utility\nof unlabeled samples. To address these challenges, we propose RegMixMatch, a\nnovel framework that optimizes the use of Mixup with both high- and\nlow-confidence samples in SSL. First, we introduce semi-supervised RegMixup,\nwhich effectively addresses reduced artificial labels purity by using both\nmixed samples and clean samples for training. Second, we develop a class-aware\nMixup technique that integrates information from the top-2 predicted classes\ninto low-confidence samples and their artificial labels, reducing the\nconfirmation bias associated with these samples and enhancing their effective\nutilization. Experimental results demonstrate that RegMixMatch achieves\nstate-of-the-art performance across various SSL benchmarks.\n","authors":["Haorong Han","Jidong Yuan","Chixuan Wei","Zhongyang Yu"],"pdf_url":"https://arxiv.org/pdf/2412.10741v2.pdf","comment":"Accepted in AAAI Conference on Artificial Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2411.17180v3","updated":"2025-04-17T07:02:56Z","published":"2024-11-26T07:41:15Z","title":"Training a neural netwok for data reduction and better generalization","summary":"  At the time of environmental concerns about artificial intelligence, in\nparticular its need for greedy storage and computation, sparsity inducing\nneural networks offer a promising path towards frugality and solution for less\nwaste.\n  Sparse learners compress the inputs (features) by selecting only the ones\nneeded for good generalization. A human scientist can then give an intelligent\ninterpretation to the few selected features. If genes are the inputs and cancer\ntype is the output, then the selected genes give the cancerologist clues on\nwhat genes have an effect on certain cancers. LASSO-type regularization leads\nto good input selection for linear associations, but few attempts have been\nmade for nonlinear associations modeled as an artificial neural network. A\nstringent but efficient way of testing whether a feature selection method works\nis to check if a phase transition occurs in the probability of retrieving the\nrelevant features, as observed and mathematically studied for linear models.\nOur method achieves just so for artificial neural networks, and, on real data,\nit has the best compromise between number of selected features and\ngeneralization performance.\n  Our method is flexible, applying to complex models ranging from shallow to\ndeep artificial neural networks and supporting various cost functions and\nsparsity-promoting penalties. It does not rely on cross-validation or on a\nvalidation set to select its single regularization parameter making it\nuser-friendly. Our approach can be seen as a form of compressed sensing for\ncomplex models, allowing to distill high-dimensional data into a compact,\ninterpretable subset of meaningful features, just the opposite of a black box.\n  A python package is available at https://github.com/VcMaxouuu/AnnHarderLasso\ncontaining all the simulations and ready-to-use models.\n","authors":["Sylvain Sardy","Maxime van Cutsem","Xiaoyu Ma"],"pdf_url":"https://arxiv.org/pdf/2411.17180v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13964v4","updated":"2025-04-17T07:01:17Z","published":"2024-05-22T20:00:19Z","title":"Design Editing for Offline Model-based Optimization","summary":"  Offline model-based optimization (MBO) aims to maximize a black-box objective\nfunction using only an offline dataset of designs and scores. These tasks span\nvarious domains, such as robotics, material design, and protein and molecular\nengineering. A common approach involves training a surrogate model using\nexisting designs and their corresponding scores, and then generating new\ndesigns through gradient-based updates with respect to the surrogate model.\nThis method suffers from the out-of-distribution issue, where the surrogate\nmodel may erroneously predict high scores for unseen designs. To address this\nchallenge, we introduce a novel method, Design Editing for Offline Model-based\nOptimization (DEMO), which leverages a diffusion prior to calibrate overly\noptimized designs. DEMO first generates pseudo design candidates by performing\ngradient ascent with respect to a surrogate model. While these pseudo design\ncandidates contain information beyond the offline dataset, they might be\ninvalid or have erroneously high predicted scores. Therefore, to address this\nchallenge while utilizing the information provided by pseudo design candidates,\nwe propose an editing process to refine these pseudo design candidates. We\nintroduce noise to the pseudo design candidates and subsequently denoise them\nwith a diffusion prior trained on the offline dataset, ensuring they align with\nthe distribution of valid designs. Empirical evaluations on seven offline MBO\ntasks show that, with properly tuned hyperparameters, DEMOs score is\ncompetitive with the best previously reported scores in the literature.\n","authors":["Ye Yuan","Youyuan Zhang","Can Chen","Haolun Wu","Zixuan Li","Jianmo Li","James J. Clark","Xue Liu"],"pdf_url":"https://arxiv.org/pdf/2405.13964v4.pdf","comment":"Accepted by Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2504.12700v1","updated":"2025-04-17T06:57:37Z","published":"2025-04-17T06:57:37Z","title":"A Two-Phase Perspective on Deep Learning Dynamics","summary":"  We propose that learning in deep neural networks proceeds in two phases: a\nrapid curve fitting phase followed by a slower compression or coarse graining\nphase. This view is supported by the shared temporal structure of three\nphenomena: grokking, double descent and the information bottleneck, all of\nwhich exhibit a delayed onset of generalization well after training error\nreaches zero. We empirically show that the associated timescales align in two\nrather different settings. Mutual information between hidden layers and input\ndata emerges as a natural progress measure, complementing circuit-based metrics\nsuch as local complexity and the linear mapping number. We argue that the\nsecond phase is not actively optimized by standard training algorithms and may\nbe unnecessarily prolonged. Drawing on an analogy with the renormalization\ngroup, we suggest that this compression phase reflects a principled form of\nforgetting, critical for generalization.\n","authors":["Robert de Mello Koch","Animik Ghosh"],"pdf_url":"https://arxiv.org/pdf/2504.12700v1.pdf","comment":"17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2405.11008v3","updated":"2025-04-17T06:54:25Z","published":"2024-05-17T11:09:33Z","title":"A Systematic Review on Sleep Stage Classification and Sleep Disorder\n  Detection Using Artificial Intelligence","summary":"  Sleep is vital for people's physical and mental health, and sound sleep can\nhelp them focus on daily activities. Therefore, a sleep study that includes\nsleep patterns and sleep disorders is crucial to enhancing our knowledge about\nindividuals' health status. This study aims to provide a comprehensive,\nsystematic review of the recent literature to analyze the different approaches\nand their outcomes in sleep studies, which includes works on \"sleep stages\nclassification\" and \"sleep disorder detection\" using AI. In this review, 183\narticles were initially selected from different journals, among which 80\nrecords were enlisted for explicit review, ranging from 2016 to 2023. Brain\nwaves were the most commonly employed body parameters for sleep staging and\ndisorder studies (almost 29% of the research used brain activity signals\nexclusively, and 77% combined with the other signals). The convolutional neural\nnetwork (CNN), the most widely used of the 34 distinct artificial intelligence\nmodels, comprised 27%. The other models included the long short-term memory\n(LSTM), support vector machine (SVM), random forest (RF), and recurrent neural\nnetwork (RNN), which consisted of 11%, 6%, 6%, and 5% sequentially. For\nperformance metrics, accuracy was widely used for a maximum of 83.75% of the\ncases, the F1 score of 45%, Kappa of 36.25%, Sensitivity of 31.25%, and\nSpecificity of 30% of cases, along with the other metrics. This article would\nhelp physicians and researchers get the gist of AI's contribution to sleep\nstudies and the feasibility of their intended work.\n","authors":["Tayab Uddin Wara","Ababil Hossain Fahad","Adri Shankar Das","Md. Mehedi Hasan Shawon"],"pdf_url":"https://arxiv.org/pdf/2405.11008v3.pdf","comment":"39 pages, 11 Figures, 8 Tables"},{"id":"http://arxiv.org/abs/2504.12695v1","updated":"2025-04-17T06:48:51Z","published":"2025-04-17T06:48:51Z","title":"Attractor-merging Crises and Intermittency in Reservoir Computing","summary":"  Reservoir computing can embed attractors into random neural networks (RNNs),\ngenerating a ``mirror'' of a target attractor because of its inherent\nsymmetrical constraints. In these RNNs, we report that an attractor-merging\ncrisis accompanied by intermittency emerges simply by adjusting the global\nparameter. We further reveal its underlying mechanism through a detailed\nanalysis of the phase-space structure and demonstrate that this bifurcation\nscenario is intrinsic to a general class of RNNs, independent of training data.\n","authors":["Tempei Kabayama","Motomasa Komuro","Yasuo Kuniyoshi","Kazuyuki Aihara","Kohei Nakajima"],"pdf_url":"https://arxiv.org/pdf/2504.12695v1.pdf","comment":"20 pages, 15 figures"},{"id":"http://arxiv.org/abs/2504.12683v1","updated":"2025-04-17T06:17:06Z","published":"2025-04-17T06:17:06Z","title":"Cluster weighted models with multivariate skewed distributions for\n  functional data","summary":"  We propose a clustering method, funWeightClustSkew, based on mixtures of\nfunctional linear regression models and three skewed multivariate\ndistributions: the variance-gamma distribution, the skew-t distribution, and\nthe normal-inverse Gaussian distribution. Our approach follows the framework of\nthe functional high dimensional data clustering (funHDDC) method, and we extend\nto functional data the cluster weighted models based on skewed distributions\nused for finite dimensional multivariate data. We consider several parsimonious\nmodels, and to estimate the parameters we construct an expectation maximization\n(EM) algorithm. We illustrate the performance of funWeightClustSkew for\nsimulated data and for the Air Quality dataset.\n","authors":["Cristina Anton","Roy Shivam Ram Shreshtth"],"pdf_url":"https://arxiv.org/pdf/2504.12683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12675v1","updated":"2025-04-17T06:06:53Z","published":"2025-04-17T06:06:53Z","title":"Physics Informed Constrained Learning of Dynamics from Static Data","summary":"  A physics-informed neural network (PINN) models the dynamics of a system by\nintegrating the governing physical laws into the architecture of a neural\nnetwork. By enforcing physical laws as constraints, PINN overcomes challenges\nwith data scarsity and potentially high dimensionality. Existing PINN\nframeworks rely on fully observed time-course data, the acquisition of which\ncould be prohibitive for many systems. In this study, we developed a new PINN\nlearning paradigm, namely Constrained Learning, that enables the approximation\nof first-order derivatives or motions using non-time course or partially\nobserved data. Computational principles and a general mathematical formulation\nof Constrained Learning were developed. We further introduced MPOCtrL (Message\nPassing Optimization-based Constrained Learning) an optimization approach\ntailored for the Constrained Learning framework that strives to balance the\nfitting of physical models and observed data. Its code is available at github\nlink: https://github.com/ptdang1001/MPOCtrL Experiments on synthetic and\nreal-world data demonstrated that MPOCtrL can effectively detect the nonlinear\ndependency between observed data and the underlying physical properties of the\nsystem. In particular, on the task of metabolic flux analysis, MPOCtrL\noutperforms all existing data-driven flux estimators.\n","authors":["Pengtao Dang","Tingbo Guo","Sha Cao","Chi Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.12675v1.pdf","comment":"39 pages, 10 figures"},{"id":"http://arxiv.org/abs/2504.12672v1","updated":"2025-04-17T06:05:10Z","published":"2025-04-17T06:05:10Z","title":"Post-processing improves accuracy of Artificial Intelligence weather\n  forecasts","summary":"  Artificial Intelligence (AI) weather models are now reaching\noperational-grade performance for some variables, but like traditional\nNumerical Weather Prediction (NWP) models, they exhibit systematic biases and\nreliability issues. We test the application of the Bureau of Meteorology's\nexisting statistical post-processing system, IMPROVER, to ECMWF's deterministic\nArtificial Intelligence Forecasting System (AIFS), and compare results against\npost-processed outputs from the ECMWF HRES and ENS models. Without any\nmodification to configuration or processing workflows, post-processing yields\ncomparable accuracy improvements for AIFS as for traditional NWP forecasts, in\nboth expected value and probabilistic outputs. We show that blending AIFS with\nNWP models improves overall forecast skill, even when AIFS alone is not the\nmost accurate component. These findings show that statistical post-processing\nmethods developed for NWP are directly applicable to AI models, enabling\nnational meteorological centres to incorporate AI forecasts into existing\nworkflows in a low-risk, incremental fashion.\n","authors":["Belinda Trotta","Robert Johnson","Catherine de Burgh-Day","Debra Hudson","Esteban Abellan","James Canvin","Andrew Kelly","Daniel Mentiplay","Benjamin Owen","Jennifer Whelan"],"pdf_url":"https://arxiv.org/pdf/2504.12672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09676v2","updated":"2025-04-17T06:02:28Z","published":"2024-02-15T03:05:45Z","title":"HyperMagNet: A Magnetic Laplacian based Hypergraph Neural Network","summary":"  In data science, hypergraphs are natural models for data exhibiting multi-way\nrelations, whereas graphs only capture pairwise. Nonetheless, many proposed\nhypergraph neural networks effectively reduce hypergraphs to undirected graphs\nvia symmetrized matrix representations, potentially losing important\ninformation. We propose an alternative approach to hypergraph neural networks\nin which the hypergraph is represented as a non-reversible Markov chain. We use\nthis Markov chain to construct a complex Hermitian Laplacian matrix - the\nmagnetic Laplacian - which serves as the input to our proposed hypergraph\nneural network. We study HyperMagNet for the task of node classification, and\ndemonstrate its effectiveness over graph-reduction based hypergraph neural\nnetworks.\n","authors":["Tatyana Benko","Martin Buck","Ilya Amburg","Stephen J. Young","Sinan G. Aksoy"],"pdf_url":"https://arxiv.org/pdf/2402.09676v2.pdf","comment":"13 pages, 2 figures"},{"id":"http://arxiv.org/abs/2411.06447v2","updated":"2025-04-17T05:54:02Z","published":"2024-11-10T12:40:33Z","title":"Multi-Parameter Molecular MRI Quantification using Physics-Informed\n  Self-Supervised Learning","summary":"  Biophysical model fitting plays a key role in obtaining quantitative\nparameters from physiological signals and images. However, the model complexity\nfor molecular magnetic resonance imaging (MRI) often translates into excessive\ncomputation time, which makes clinical use impractical. Here, we present a\ngeneric computational approach for solving the parameter extraction inverse\nproblem posed by ordinary differential equation (ODE) modeling coupled with\nexperimental measurement of the system dynamics. This is achieved by\nformulating a numerical ODE solver to function as a step-wise analytical one,\nthereby making it compatible with automatic differentiation-based optimization.\nThis enables efficient gradient-based model fitting, and provides a new\napproach to parameter quantification based on self-supervised learning from a\nsingle data observation. The neural-network-based train-by-fit pipeline was\nused to quantify semisolid magnetization transfer (MT) and chemical exchange\nsaturation transfer (CEST) amide proton exchange parameters in the human brain,\nin an in-vivo molecular MRI study (n = 4). The entire pipeline of the first\nwhole brain quantification was completed in 18.3 $\\pm$ 8.3 minutes. Reusing the\nsingle-subject-trained network for inference in new subjects took 1.0 $\\pm$ 0.2\ns, to provide results in agreement with literature values and scan-specific fit\nresults.\n","authors":["Alex Finkelstein","Nikita Vladimirov","Moritz Zaiss","Or Perlman"],"pdf_url":"https://arxiv.org/pdf/2411.06447v2.pdf","comment":"This project was funded by the European Union (ERC, BabyMagnet,\n  project no. 101115639), the Ministry of Innovation, Science and Technology,\n  Israel, and a grant from the Tel Aviv University Center for AI and Data\n  Science (TAD, The Blavatnik AI and Data Science Fund). None of above can be\n  held responsible for views and opinions expressed, which are those of the\n  authors alone"},{"id":"http://arxiv.org/abs/2504.12665v1","updated":"2025-04-17T05:50:33Z","published":"2025-04-17T05:50:33Z","title":"Predicting Driver's Perceived Risk: a Model Based on Semi-Supervised\n  Learning Strategy","summary":"  Drivers' perception of risk determines their acceptance, trust, and use of\nthe Automated Driving Systems (ADSs). However, perceived risk is subjective and\ndifficult to evaluate using existing methods. To address this issue, a driver's\nsubjective perceived risk (DSPR) model is proposed, regarding perceived risk as\na dynamically triggered mechanism with anisotropy and attenuation. 20\nparticipants are recruited for a driver-in-the-loop experiment to report their\nreal-time subjective risk ratings (SRRs) when experiencing various automatic\ndriving scenarios. A convolutional neural network and bidirectional long\nshort-term memory network with temporal pattern attention (CNN-Bi-LSTM-TPA) is\nembedded into a semi-supervised learning strategy to predict SRRs, aiming to\nreduce data noise caused by subjective randomness of participants. The results\nillustrate that DSPR achieves the highest prediction accuracy of 87.91% in\npredicting SRRs, compared to three state-of-the-art risk models. The\nsemi-supervised strategy improves accuracy by 20.12%. Besides, CNN-Bi-LSTM-TPA\nnetwork presents the highest accuracy among four different LSTM structures.\nThis study offers an effective method for assessing driver's perceived risk,\nproviding support for the safety enhancement of ADS and driver's trust\nimprovement.\n","authors":["Siwei Huang","Chenhao Yang","Chuan Hu"],"pdf_url":"https://arxiv.org/pdf/2504.12665v1.pdf","comment":"6pages, 8figures, 5tables. Accepted to be presented at the 2025 36th\n  IEEE Intelligent Vehicles Symposium (IV) (IV 2025)"},{"id":"http://arxiv.org/abs/2504.12661v1","updated":"2025-04-17T05:46:41Z","published":"2025-04-17T05:46:41Z","title":"VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven\n  Prompt Optimization","summary":"  Aligning Vision-Language Models (VLMs) with safety standards is essential to\nmitigate risks arising from their multimodal complexity, where integrating\nvision and language unveils subtle threats beyond the reach of conventional\nsafeguards. Inspired by the insight that reasoning across modalities is key to\npreempting intricate vulnerabilities, we propose a novel direction for VLM\nsafety: multimodal reasoning-driven prompt rewriting. To this end, we introduce\nVLMGuard-R1, a proactive framework that refines user inputs through a\nreasoning-guided rewriter, dynamically interpreting text-image interactions to\ndeliver refined prompts that bolster safety across diverse VLM architectures\nwithout altering their core parameters. To achieve this, we devise a\nthree-stage reasoning pipeline to synthesize a dataset that trains the rewriter\nto infer subtle threats, enabling tailored, actionable responses over generic\nrefusals. Extensive experiments across three benchmarks with five VLMs reveal\nthat VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1\nachieves a remarkable 43.59\\% increase in average safety across five models on\nthe SIUO benchmark.\n","authors":["Menglan Chen","Xianghe Pang","Jingjing Dong","WenHao Wang","Yaxin Du","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2504.12661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12651v1","updated":"2025-04-17T05:22:17Z","published":"2025-04-17T05:22:17Z","title":"Feature selection based on cluster assumption in PU learning","summary":"  Feature selection is essential for efficient data mining and sometimes\nencounters the positive-unlabeled (PU) learning scenario, where only a few\npositive labels are available, while most data remains unlabeled. In certain\nreal-world PU learning tasks, data subjected to adequate feature selection\noften form clusters with concentrated positive labels. Conventional feature\nselection methods that treat unlabeled data as negative may fail to capture the\nstatistical characteristics of positive data in such scenarios, leading to\nsuboptimal performance. To address this, we propose a novel feature selection\nmethod based on the cluster assumption in PU learning, called FSCPU. FSCPU\nformulates the feature selection problem as a binary optimization task, with an\nobjective function explicitly designed to incorporate the cluster assumption in\nthe PU learning setting. Experiments on synthetic datasets demonstrate the\neffectiveness of FSCPU across various data conditions. Moreover, comparisons\nwith 10 conventional algorithms on three open datasets show that FSCPU achieves\ncompetitive performance in downstream classification tasks, even when the\ncluster assumption does not strictly hold.\n","authors":["Motonobu Uchikoshi","Youhei Akimoto"],"pdf_url":"https://arxiv.org/pdf/2504.12651v1.pdf","comment":"Accepted at GECCO 2025"},{"id":"http://arxiv.org/abs/2504.12644v1","updated":"2025-04-17T05:08:08Z","published":"2025-04-17T05:08:08Z","title":"Quantum Computing Supported Adversarial Attack-Resilient Autonomous\n  Vehicle Perception Module for Traffic Sign Classification","summary":"  Deep learning (DL)-based image classification models are essential for\nautonomous vehicle (AV) perception modules since incorrect categorization might\nhave severe repercussions. Adversarial attacks are widely studied cyberattacks\nthat can lead DL models to predict inaccurate output, such as incorrectly\nclassified traffic signs by the perception module of an autonomous vehicle. In\nthis study, we create and compare hybrid classical-quantum deep learning\n(HCQ-DL) models with classical deep learning (C-DL) models to demonstrate\nrobustness against adversarial attacks for perception modules. Before feeding\nthem into the quantum system, we used transfer learning models, alexnet and\nvgg-16, as feature extractors. We tested over 1000 quantum circuits in our\nHCQ-DL models for projected gradient descent (PGD), fast gradient sign attack\n(FGSA), and gradient attack (GA), which are three well-known untargeted\nadversarial approaches. We evaluated the performance of all models during\nadversarial attacks and no-attack scenarios. Our HCQ-DL models maintain\naccuracy above 95\\% during a no-attack scenario and above 91\\% for GA and FGSA\nattacks, which is higher than C-DL models. During the PGD attack, our\nalexnet-based HCQ-DL model maintained an accuracy of 85\\% compared to C-DL\nmodels that achieved accuracies below 21\\%. Our results highlight that the\nHCQ-DL models provide improved accuracy for traffic sign classification under\nadversarial settings compared to their classical counterparts.\n","authors":["Reek Majumder","Mashrur Chowdhury","Sakib Mahmud Khan","Zadid Khan","Fahim Ahmad","Frank Ngeni","Gurcan Comert","Judith Mwakalonge","Dimitra Michalaka"],"pdf_url":"https://arxiv.org/pdf/2504.12644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10925v2","updated":"2025-04-17T04:59:47Z","published":"2025-04-15T07:12:00Z","title":"Transfer Learning for Temporal Link Prediction","summary":"  Link prediction on graphs has applications spanning from recommender systems\nto drug discovery. Temporal link prediction (TLP) refers to predicting future\nlinks in a temporally evolving graph and adds additional complexity related to\nthe dynamic nature of graphs. State-of-the-art TLP models incorporate memory\nmodules alongside graph neural networks to learn both the temporal mechanisms\nof incoming nodes and the evolving graph topology. However, memory modules only\nstore information about nodes seen at train time, and hence such models cannot\nbe directly transferred to entirely new graphs at test time and deployment. In\nthis work, we study a new transfer learning task for temporal link prediction,\nand develop transfer-effective methods for memory-laden models. Specifically,\nmotivated by work showing the informativeness of structural signals for the TLP\ntask, we augment a structural mapping module to the existing TLP model\narchitectures, which learns a mapping from graph structural (topological)\nfeatures to memory embeddings. Our work paves the way for a memory-free\nfoundation model for TLP.\n","authors":["Ayan Chatterjee","Barbara Ikica","Babak Ravandi","John Palowitch"],"pdf_url":"https://arxiv.org/pdf/2504.10925v2.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.11490v2","updated":"2025-04-17T04:52:15Z","published":"2024-06-17T12:55:56Z","title":"Interventional Imbalanced Multi-Modal Representation Learning via\n  $β$-Generalization Front-Door Criterion","summary":"  Multi-modal methods establish comprehensive superiority over uni-modal\nmethods. However, the imbalanced contributions of different modalities to\ntask-dependent predictions constantly degrade the discriminative performance of\ncanonical multi-modal methods. Based on the contribution to task-dependent\npredictions, modalities can be identified as predominant and auxiliary\nmodalities. Benchmark methods raise a tractable solution: augmenting the\nauxiliary modality with a minor contribution during training. However, our\nempirical explorations challenge the fundamental idea behind such behavior, and\nwe further conclude that benchmark approaches suffer from certain defects:\ninsufficient theoretical interpretability and limited exploration capability of\ndiscriminative knowledge. To this end, we revisit multi-modal representation\nlearning from a causal perspective and build the Structural Causal Model.\nFollowing the empirical explorations, we determine to capture the true\ncausality between the discriminative knowledge of predominant modality and\npredictive label while considering the auxiliary modality. Thus, we introduce\nthe $\\beta$-generalization front-door criterion. Furthermore, we propose a\nnovel network for sufficiently exploring multi-modal discriminative knowledge.\nRigorous theoretical analyses and various empirical evaluations are provided to\nsupport the effectiveness of the innate mechanism behind our proposed method.\n","authors":["Yi Li","Fei Song","Changwen Zheng","Jiangmeng Li","Fuchun Sun","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2406.11490v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.03160v4","updated":"2025-04-17T04:46:08Z","published":"2025-04-04T04:41:28Z","title":"DeepResearcher: Scaling Deep Research via Reinforcement Learning in\n  Real-world Environments","summary":"  Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.\n","authors":["Yuxiang Zheng","Dayuan Fu","Xiangkun Hu","Xiaojie Cai","Lyumanshan Ye","Pengrui Lu","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2504.03160v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12627v1","updated":"2025-04-17T04:02:53Z","published":"2025-04-17T04:02:53Z","title":"Uncertainty Quantification in Graph Neural Networks with Shallow\n  Ensembles","summary":"  Machine-learned potentials (MLPs) have revolutionized materials discovery by\nproviding accurate and efficient predictions of molecular and material\nproperties. Graph Neural Networks (GNNs) have emerged as a state-of-the-art\napproach due to their ability to capture complex atomic interactions. However,\nGNNs often produce unreliable predictions when encountering out-of-domain data\nand it is difficult to identify when that happens. To address this challenge,\nwe explore Uncertainty Quantification (UQ) techniques, focusing on Direct\nPropagation of Shallow Ensembles (DPOSE) as a computationally efficient\nalternative to deep ensembles. By integrating DPOSE into the SchNet model, we\nassess its ability to provide reliable uncertainty estimates across diverse\nDensity Functional Theory datasets, including QM9, OC20, and Gold Molecular\nDynamics. Our findings often demonstrate that DPOSE successfully distinguishes\nbetween in-domain and out-of-domain samples, exhibiting higher uncertainty for\nunobserved molecule and material classes. This work highlights the potential of\nlightweight UQ methods in improving the robustness of GNN-based materials\nmodeling and lays the foundation for future integration with active learning\nstrategies.\n","authors":["Tirtha Vinchurkar","Kareem Abdelmaqsoud","John R. Kitchin"],"pdf_url":"https://arxiv.org/pdf/2504.12627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12625v1","updated":"2025-04-17T04:02:06Z","published":"2025-04-17T04:02:06Z","title":"Spectral Algorithms under Covariate Shift","summary":"  Spectral algorithms leverage spectral regularization techniques to analyze\nand process data, providing a flexible framework for addressing supervised\nlearning problems. To deepen our understanding of their performance in\nreal-world scenarios where the distributions of training and test data may\ndiffer, we conduct a rigorous investigation into the convergence behavior of\nspectral algorithms under distribution shifts, specifically within the\nframework of reproducing kernel Hilbert spaces. Our study focuses on the case\nof covariate shift. In this scenario, the marginal distributions of the input\ndata differ between the training and test datasets, while the conditional\ndistribution of the output given the input remains unchanged. Under this\nsetting, we analyze the generalization error of spectral algorithms and show\nthat they achieve minimax optimality when the density ratios between the\ntraining and test distributions are uniformly bounded. However, we also\nidentify a critical limitation: when the density ratios are unbounded, the\nspectral algorithms may become suboptimal. To address this limitation, we\npropose a weighted spectral algorithm that incorporates density ratio\ninformation into the learning process. Our theoretical analysis shows that this\nweighted approach achieves optimal capacity-independent convergence rates.\nFurthermore, by introducing a weight clipping technique, we demonstrate that\nthe convergence rates of the weighted spectral algorithm can approach the\noptimal capacity-dependent convergence rates arbitrarily closely. This\nimprovement resolves the suboptimality issue in unbounded density ratio\nscenarios and advances the state-of-the-art by refining existing theoretical\nresults.\n","authors":["Jun Fan","Zheng-Chu Guo","Lei Shi"],"pdf_url":"https://arxiv.org/pdf/2504.12625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12876v3","updated":"2025-04-17T03:51:06Z","published":"2024-10-15T05:01:19Z","title":"In-context KV-Cache Eviction for LLMs via Attention-Gate","summary":"  The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.\n","authors":["Zihao Zeng","Bokai Lin","Tianqi Hou","Hao Zhang","Zhijie Deng"],"pdf_url":"https://arxiv.org/pdf/2410.12876v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09593v2","updated":"2025-04-17T03:50:19Z","published":"2025-04-13T14:18:35Z","title":"ControlNET: A Firewall for RAG-based LLM System","summary":"  Retrieval-Augmented Generation (RAG) has significantly enhanced the factual\naccuracy and domain adaptability of Large Language Models (LLMs). This\nadvancement has enabled their widespread deployment across sensitive domains\nsuch as healthcare, finance, and enterprise applications. RAG mitigates\nhallucinations by integrating external knowledge, yet introduces privacy risk\nand security risk, notably data breaching risk and data poisoning risk. While\nrecent studies have explored prompt injection and poisoning attacks, there\nremains a significant gap in comprehensive research on controlling inbound and\noutbound query flows to mitigate these threats. In this paper, we propose an AI\nfirewall, ControlNET, designed to safeguard RAG-based LLM systems from these\nvulnerabilities. ControlNET controls query flows by leveraging activation shift\nphenomena to detect adversarial queries and mitigate their impact through\nsemantic divergence. We conduct comprehensive experiments on four different\nbenchmark datasets including Msmarco, HotpotQA, FinQA, and MedicalSys using\nstate-of-the-art open source LLMs (Llama3, Vicuna, and Mistral). Our results\ndemonstrate that ControlNET achieves over 0.909 AUROC in detecting and\nmitigating security threats while preserving system harmlessness. Overall,\nControlNET offers an effective, robust, harmless defense mechanism, marking a\nsignificant advancement toward the secure deployment of RAG-based LLM systems.\n","authors":["Hongwei Yao","Haoran Shi","Yidou Chen","Yixin Jiang","Cong Wang","Zhan Qin"],"pdf_url":"https://arxiv.org/pdf/2504.09593v2.pdf","comment":"Project Page: https://ai.zjuicsr.cn/firewall"},{"id":"http://arxiv.org/abs/2503.04649v2","updated":"2025-04-17T03:47:25Z","published":"2025-03-06T17:35:37Z","title":"Transferable Foundation Models for Geometric Tasks on Point Cloud\n  Representations: Geometric Neural Operators","summary":"  We introduce methods for obtaining pretrained Geometric Neural Operators\n(GNPs) that can serve as basal foundation models for use in obtaining geometric\nfeatures. These can be used within data processing pipelines for machine\nlearning tasks and numerical methods. We show how our GNPs can be trained to\nlearn robust latent representations for the differential geometry of\npoint-clouds to provide estimates of metric, curvature, and other shape-related\nfeatures. We demonstrate how our pre-trained GNPs can be used (i) to estimate\nthe geometric properties of surfaces of arbitrary shape and topologies with\nrobustness in the presence of noise, (ii) to approximate solutions of geometric\npartial differential equations (PDEs) on manifolds, and (iii) to solve\nequations for shape deformations such as curvature driven flows. We release\ncodes and weights for using GNPs in the package geo_neural_op. This allows for\nincorporating our pre-trained GNPs as components for reuse within existing and\nnew data processing pipelines. The GNPs also can be used as part of numerical\nsolvers involving geometry or as part of methods for performing inference and\nother geometric tasks.\n","authors":["Blaine Quackenbush","Paul J. Atzberger"],"pdf_url":"https://arxiv.org/pdf/2503.04649v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13145v2","updated":"2025-04-17T03:28:45Z","published":"2025-03-17T13:16:25Z","title":"High-entropy Advantage in Neural Networks' Generalizability","summary":"  One of the central challenges in modern machine learning is understanding how\nneural networks generalize knowledge learned from training data to unseen test\ndata. While numerous empirical techniques have been proposed to improve\ngeneralization, a theoretical understanding of the mechanism of generalization\nremains elusive. Here we introduce the concept of Boltzmann entropy into neural\nnetworks by re-conceptualizing such networks as hypothetical molecular systems\nwhere weights and biases are atomic coordinates, and the loss function is the\npotential energy. By employing molecular simulation algorithms, we compute\nentropy landscapes as functions of both training loss and test accuracy (or\ntest loss), on networks with up to 1 million parameters, across four distinct\nmachine learning tasks: arithmetic question, real-world tabular data, image\nrecognition, and language modeling. Our results reveal the existence of\nhigh-entropy advantage, wherein high-entropy network states generally\noutperform those reached via conventional training techniques like stochastic\ngradient descent. This entropy advantage provides a thermodynamic explanation\nfor neural network generalizability: the generalizable states occupy a larger\npart of the parameter space than its non-generalizable analog at low train\nloss. Furthermore, we find this advantage more pronounced in narrower neural\nnetworks, indicating a need for different training optimizers tailored to\ndifferent sizes of networks.\n","authors":["Entao Yang","Xiaotian Zhang","Yue Shang","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.13145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12610v1","updated":"2025-04-17T03:19:49Z","published":"2025-04-17T03:19:49Z","title":"Machine Learning Methods for Gene Regulatory Network Inference","summary":"  Gene Regulatory Networks (GRNs) are intricate biological systems that control\ngene expression and regulation in response to environmental and developmental\ncues. Advances in computational biology, coupled with high throughput\nsequencing technologies, have significantly improved the accuracy of GRN\ninference and modeling. Modern approaches increasingly leverage artificial\nintelligence (AI), particularly machine learning techniques including\nsupervised, unsupervised, semi-supervised, and contrastive learning to analyze\nlarge scale omics data and uncover regulatory gene interactions. To support\nboth the application of GRN inference in studying gene regulation and the\ndevelopment of novel machine learning methods, we present a comprehensive\nreview of machine learning based GRN inference methodologies, along with the\ndatasets and evaluation metrics commonly used. Special emphasis is placed on\nthe emerging role of cutting edge deep learning techniques in enhancing\ninference performance. The potential future directions for improving GRN\ninference are also discussed.\n","authors":["Akshata Hegde","Tom Nguyen","Jianlin Cheng"],"pdf_url":"https://arxiv.org/pdf/2504.12610v1.pdf","comment":"40 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2504.02269v2","updated":"2025-04-17T03:14:31Z","published":"2025-04-03T04:30:10Z","title":"Engineering Artificial Intelligence: Framework, Challenges, and Future\n  Direction","summary":"  Over the past ten years, the application of artificial intelligence (AI) and\nmachine learning (ML) in engineering domains has gained significant popularity,\nshowcasing their potential in data-driven contexts. However, the complexity and\ndiversity of engineering problems often require the development of\ndomain-specific AI approaches, which are frequently hindered by a lack of\nsystematic methodologies, scalability, and robustness during the development\nprocess. To address this gap, this paper introduces the \"ABCDE\" as the key\nelements of Engineering AI and proposes a unified, systematic engineering AI\necosystem framework, including eight essential layers, along with attributes,\ngoals, and applications, to guide the development and deployment of AI\nsolutions for specific engineering needs. Additionally, key challenges are\nexamined, and eight future research directions are highlighted. By providing a\ncomprehensive perspective, this paper aims to advance the strategic\nimplementation of AI, fostering the development of next-generation engineering\nAI solutions.\n","authors":["Jay Lee","Hanqi Su","Dai-Yan Ji","Takanobu Minami"],"pdf_url":"https://arxiv.org/pdf/2504.02269v2.pdf","comment":"The paper submitted to the Journal Machine Learning: Engineering has\n  been accepted"},{"id":"http://arxiv.org/abs/2501.13810v2","updated":"2025-04-17T03:05:03Z","published":"2025-01-23T16:32:01Z","title":"Learning to Help in Multi-Class Settings","summary":"  Deploying complex machine learning models on resource-constrained devices is\nchallenging due to limited computational power, memory, and model\nretrainability. To address these limitations, a hybrid system can be\nestablished by augmenting the local model with a server-side model, where\nsamples are selectively deferred by a rejector and then sent to the server for\nprocessing. The hybrid system enables efficient use of computational resources\nwhile minimizing the overhead associated with server usage. The recently\nproposed Learning to Help (L2H) model trains a server model given a fixed local\n(client) model, differing from the Learning to Defer (L2D) framework, which\ntrains the client for a fixed (expert) server. In both L2D and L2H, the\ntraining includes learning a rejector at the client to determine when to query\nthe server. In this work, we extend the L2H model from binary to multi-class\nclassification problems and demonstrate its applicability in a number of\ndifferent scenarios of practical interest in which access to the server may be\nlimited by cost, availability, or policy. We derive a stage-switching surrogate\nloss function that is differentiable, convex, and consistent with the Bayes\nrule corresponding to the 0-1 loss for the L2H model. Experiments show that our\nproposed methods offer an efficient and practical solution for multi-class\nclassification in resource-constrained environments.\n","authors":["Yu Wu","Yansong Li","Zeyu Dong","Nitya Sathyavageeswaran","Anand D. Sarwate"],"pdf_url":"https://arxiv.org/pdf/2501.13810v2.pdf","comment":"30 pages, 7 figures, conference, ICLR 2025"},{"id":"http://arxiv.org/abs/2408.07246v5","updated":"2025-04-17T03:01:41Z","published":"2024-08-14T01:16:40Z","title":"ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area","summary":"  Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B.\n","authors":["Junxian Li","Di Zhang","Xunzhi Wang","Zeying Hao","Jingdi Lei","Qian Tan","Cai Zhou","Wei Liu","Yaotian Yang","Xinrui Xiong","Weiyun Wang","Zhe Chen","Wenhai Wang","Wei Li","Shufei Zhang","Mao Su","Wanli Ouyang","Yuqiang Li","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.07246v5.pdf","comment":"11 pages, updated version"},{"id":"http://arxiv.org/abs/2504.12601v1","updated":"2025-04-17T02:56:20Z","published":"2025-04-17T02:56:20Z","title":"Stochastic Gradient Descent in Non-Convex Problems: Asymptotic\n  Convergence with Relaxed Step-Size via Stopping Time Methods","summary":"  Stochastic Gradient Descent (SGD) is widely used in machine learning\nresearch. Previous convergence analyses of SGD under the vanishing step-size\nsetting typically require Robbins-Monro conditions. However, in practice, a\nwider variety of step-size schemes are frequently employed, yet existing\nconvergence results remain limited and often rely on strong assumptions. This\npaper bridges this gap by introducing a novel analytical framework based on a\nstopping-time method, enabling asymptotic convergence analysis of SGD under\nmore relaxed step-size conditions and weaker assumptions. In the non-convex\nsetting, we prove the almost sure convergence of SGD iterates for step-sizes $\n\\{ \\epsilon_t \\}_{t \\geq 1} $ satisfying $\\sum_{t=1}^{+\\infty} \\epsilon_t =\n+\\infty$ and $\\sum_{t=1}^{+\\infty} \\epsilon_t^p < +\\infty$ for some $p > 2$.\nCompared with previous studies, our analysis eliminates the global Lipschitz\ncontinuity assumption on the loss function and relaxes the boundedness\nrequirements for higher-order moments of stochastic gradients. Building upon\nthe almost sure convergence results, we further establish $L_2$ convergence.\nThese significantly relaxed assumptions make our theoretical results more\ngeneral, thereby enhancing their applicability in practical scenarios.\n","authors":["Ruinan Jin","Difei Cheng","Hong Qiao","Xin Shi","Shaodong Liu","Bo Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.12601v1.pdf","comment":"42 pages"},{"id":"http://arxiv.org/abs/2504.01797v2","updated":"2025-04-17T02:54:57Z","published":"2025-04-02T15:05:32Z","title":"Rethinking industrial artificial intelligence: a unified foundation\n  framework","summary":"  Recent advancements in industrial artificial intelligence (AI) are reshaping\nthe industry by driving smarter manufacturing, predictive maintenance, and\nintelligent decision-making. However, existing approaches often focus primarily\non algorithms and models while overlooking the importance of systematically\nintegrating domain knowledge, data, and models to develop more comprehensive\nand effective AI solutions. Therefore, the effective development and deployment\nof industrial AI require a more comprehensive and systematic approach. To\naddress this gap, this paper reviews previous research, rethinks the role of\nindustrial AI, and proposes a unified industrial AI foundation framework\ncomprising three core modules: the knowledge module, data module, and model\nmodule. These modules help to extend and enhance the industrial AI methodology\nplatform, supporting various industrial applications. In addition, a case study\non rotating machinery diagnosis is presented to demonstrate the effectiveness\nof the proposed framework, and several future directions are highlighted for\nthe development of the industrial AI foundation framework.\n","authors":["Jay Lee","Hanqi Su"],"pdf_url":"https://arxiv.org/pdf/2504.01797v2.pdf","comment":"The paper submitted to IJAMD, the International Journal of AI for\n  Materials and Design, has been accepted"},{"id":"http://arxiv.org/abs/2411.01639v3","updated":"2025-04-17T02:45:08Z","published":"2024-11-03T17:32:00Z","title":"Know Where You're Uncertain When Planning with Multimodal Foundation\n  Models: A Formal Framework","summary":"  Multimodal foundation models offer a promising framework for robotic\nperception and planning by processing sensory inputs to generate actionable\nplans. However, addressing uncertainty in both perception (sensory\ninterpretation) and decision-making (plan generation) remains a critical\nchallenge for ensuring task reliability. We present a comprehensive framework\nto disentangle, quantify, and mitigate these two forms of uncertainty. We first\nintroduce a framework for uncertainty disentanglement, isolating perception\nuncertainty arising from limitations in visual understanding and decision\nuncertainty relating to the robustness of generated plans.\n  To quantify each type of uncertainty, we propose methods tailored to the\nunique properties of perception and decision-making: we use conformal\nprediction to calibrate perception uncertainty and introduce\nFormal-Methods-Driven Prediction (FMDP) to quantify decision uncertainty,\nleveraging formal verification techniques for theoretical guarantees. Building\non this quantification, we implement two targeted intervention mechanisms: an\nactive sensing process that dynamically re-observes high-uncertainty scenes to\nenhance visual input quality and an automated refinement procedure that\nfine-tunes the model on high-certainty data, improving its capability to meet\ntask specifications. Empirical validation in real-world and simulated robotic\ntasks demonstrates that our uncertainty disentanglement framework reduces\nvariability by up to 40% and enhances task success rates by 5% compared to\nbaselines. These improvements are attributed to the combined effect of both\ninterventions and highlight the importance of uncertainty disentanglement,\nwhich facilitates targeted interventions that enhance the robustness and\nreliability of autonomous systems. Fine-tuned models, code, and datasets are\navailable at https://uncertainty-in-planning.github.io/.\n","authors":["Neel P. Bhatt","Yunhao Yang","Rohan Siva","Daniel Milan","Ufuk Topcu","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01639v3.pdf","comment":"Fine-tuned models, code, and datasets are available at\n  https://uncertainty-in-planning.github.io/"},{"id":"http://arxiv.org/abs/2502.03638v2","updated":"2025-04-17T02:41:44Z","published":"2025-02-05T21:48:48Z","title":"SymmCD: Symmetry-Preserving Crystal Generation with Diffusion Models","summary":"  Generating novel crystalline materials has the potential to lead to\nadvancements in fields such as electronics, energy storage, and catalysis. The\ndefining characteristic of crystals is their symmetry, which plays a central\nrole in determining their physical properties. However, existing crystal\ngeneration methods either fail to generate materials that display the\nsymmetries of real-world crystals, or simply replicate the symmetry information\nfrom examples in a database. To address this limitation, we propose SymmCD, a\nnovel diffusion-based generative model that explicitly incorporates\ncrystallographic symmetry into the generative process. We decompose crystals\ninto two components and learn their joint distribution through diffusion: 1)\nthe asymmetric unit, the smallest subset of the crystal which can generate the\nwhole crystal through symmetry transformations, and; 2) the symmetry\ntransformations needed to be applied to each atom in the asymmetric unit. We\nalso use a novel and interpretable representation for these transformations,\nenabling generalization across different crystallographic symmetry groups. We\nshowcase the competitive performance of SymmCD on a subset of the Materials\nProject, obtaining diverse and valid crystals with realistic symmetries and\npredicted properties.\n","authors":["Daniel Levy","Siba Smarak Panigrahi","Sékou-Oumar Kaba","Qiang Zhu","Kin Long Kelvin Lee","Mikhail Galkin","Santiago Miret","Siamak Ravanbakhsh"],"pdf_url":"https://arxiv.org/pdf/2502.03638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12594v1","updated":"2025-04-17T02:41:22Z","published":"2025-04-17T02:41:22Z","title":"Meta-Dependence in Conditional Independence Testing","summary":"  Constraint-based causal discovery algorithms utilize many statistical tests\nfor conditional independence to uncover networks of causal dependencies. These\napproaches to causal discovery rely on an assumed correspondence between the\ngraphical properties of a causal structure and the conditional independence\nproperties of observed variables, known as the causal Markov condition and\nfaithfulness. Finite data yields an empirical distribution that is \"close\" to\nthe actual distribution. Across these many possible empirical distributions,\nthe correspondence to the graphical properties can break down for different\nconditional independencies, and multiple violations can occur at the same time.\nWe study this \"meta-dependence\" between conditional independence properties\nusing the following geometric intuition: each conditional independence property\nconstrains the space of possible joint distributions to a manifold. The\n\"meta-dependence\" between conditional independences is informed by the position\nof these manifolds relative to the true probability distribution. We provide a\nsimple-to-compute measure of this meta-dependence using information projections\nand consolidate our findings empirically using both synthetic and real-world\ndata.\n","authors":["Bijan Mazaheri","Jiaqi Zhang","Caroline Uhler"],"pdf_url":"https://arxiv.org/pdf/2504.12594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12589v1","updated":"2025-04-17T02:08:51Z","published":"2025-04-17T02:08:51Z","title":"Efficient MAP Estimation of LLM Judgment Performance with Prior Transfer","summary":"  LLM ensembles are widely used for LLM judges. However, how to estimate their\naccuracy, especially in an efficient way, is unknown. In this paper, we present\na principled maximum a posteriori (MAP) framework for an economical and precise\nestimation of the performance of LLM ensemble judgment. We first propose a\nmixture of Beta-Binomial distributions to model the judgment distribution,\nrevising from the vanilla Binomial distribution. Next, we introduce a conformal\nprediction-driven approach that enables adaptive stopping during iterative\nsampling to balance accuracy with efficiency. Furthermore, we design a prior\ntransfer mechanism that utilizes learned distributions on open-source datasets\nto improve estimation on a target dataset when only scarce annotations are\navailable. Finally, we present BetaConform, a framework that integrates our\ndistribution assumption, adaptive stopping, and the prior transfer mechanism to\ndeliver a theoretically guaranteed distribution estimation of LLM ensemble\njudgment with minimum labeled samples. BetaConform is also validated\nempirically. For instance, with only 10 samples from the TruthfulQA dataset,\nfor a Llama ensembled judge, BetaConform gauges its performance with error\nmargin as small as 3.37%.\n","authors":["Huaizhi Qu","Inyoung Choi","Zhen Tan","Song Wang","Sukwon Yun","Qi Long","Faizan Siddiqui","Kwonjoon Lee","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2504.12589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12588v1","updated":"2025-04-17T02:06:50Z","published":"2025-04-17T02:06:50Z","title":"Simplifying Graph Transformers","summary":"  Transformers have attained outstanding performance across various modalities,\nemploying scaled-dot-product (SDP) attention mechanisms. Researchers have\nattempted to migrate Transformers to graph learning, but most advanced Graph\nTransformers are designed with major architectural differences, either\nintegrating message-passing or incorporating sophisticated attention\nmechanisms. These complexities prevent the easy adoption of Transformer\ntraining advances. We propose three simple modifications to the plain\nTransformer to render it applicable to graphs without introducing major\narchitectural distortions. Specifically, we advocate for the use of (1)\nsimplified $L_2$ attention to measure the magnitude closeness of tokens; (2)\nadaptive root-mean-square normalization to preserve token magnitude\ninformation; and (3) a relative positional encoding bias with a shared encoder.\nSignificant performance gains across a variety of graph datasets justify the\neffectiveness of our proposed modifications. Furthermore, empirical evaluation\non the expressiveness benchmark reveals noteworthy realized expressiveness in\nthe graph isomorphism.\n","authors":["Liheng Ma","Soumyasundar Pal","Yingxue Zhang","Philip H. S. Torr","Mark Coates"],"pdf_url":"https://arxiv.org/pdf/2504.12588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12587v1","updated":"2025-04-17T02:06:05Z","published":"2025-04-17T02:06:05Z","title":"Software Engineering Principles for Fairer Systems: Experiments with\n  GroupCART","summary":"  Discrimination-aware classification aims to make accurate predictions while\nsatisfying fairness constraints. Traditional decision tree learners typically\noptimize for information gain in the target attribute alone, which can result\nin models that unfairly discriminate against protected social groups (e.g.,\ngender, ethnicity). Motivated by these shortcomings, we propose GroupCART, a\ntree-based ensemble optimizer that avoids bias during model construction by\noptimizing not only for decreased entropy in the target attribute but also for\nincreased entropy in protected attributes. Our experiments show that GroupCART\nachieves fairer models without data transformation and with minimal performance\ndegradation. Furthermore, the method supports customizable weighting, offering\na smooth and flexible trade-off between predictive performance and fairness\nbased on user requirements. These results demonstrate that algorithmic bias in\ndecision tree models can be mitigated through multi-task, fairness-aware\nlearning. All code and datasets used in this study are available at:\nhttps://github.com/anonymous12138/groupCART.\n","authors":["Kewen Peng","Hao Zhuo","Yicheng Yang","Tim Menzies"],"pdf_url":"https://arxiv.org/pdf/2504.12587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06691v2","updated":"2025-04-17T02:02:57Z","published":"2024-05-07T09:36:23Z","title":"Fleet of Agents: Coordinated Problem Solving with Large Language Models","summary":"  While numerous frameworks have been developed to enhance the reasoning\nabilities of large language models (LLMs), there is a scarcity of methods that\neffectively balance the trade-off between cost and quality. In this paper, we\nintroduce Fleet of Agents (FoA), a novel and intuitive yet principled framework\nutilizing LLMs as agents to navigate through dynamic tree searches, employing a\ngenetic-type particle filtering approach. FoA spawns a multitude of agents,\neach exploring the search space autonomously, followed by a selection phase\nwhere resampling based on a heuristic value function optimizes the balance\nbetween exploration and exploitation. This mechanism enables dynamic branching,\nadapting the exploration strategy based on discovered solutions. We conduct\nextensive experiments on three benchmark tasks, ``Game of 24'',\n``Mini-Crosswords'', and ``WebShop'', utilizing four different LLMs,\n``GPT-3.5'', ``GPT-4'', ``LLaMA3.2-11B'', and ``LLaMA3.2-90B''. On average\nacross all tasks and LLMs, FoA obtains a quality improvement of ~5% while\nrequiring only ~40% of the cost of previous SOTA methods. Notably, our analyses\nreveal that (1) FoA achieves the best cost-quality trade-off among all\nbenchmarked methods and (2) FoA + LLaMA3.2-11B surpasses the Llama3.2-90B\nmodel. FoA is publicly available at https://github.com/au-clan/FoA.\n","authors":["Nearchos Potamitis","Lars Klein","Roland Aydin","Robert West","Caglar Gulcehre","Akhil Arora"],"pdf_url":"https://arxiv.org/pdf/2405.06691v2.pdf","comment":"28 pages, 68 figures, 8 tables"},{"id":"http://arxiv.org/abs/2504.12585v1","updated":"2025-04-17T02:00:53Z","published":"2025-04-17T02:00:53Z","title":"Identifying and Mitigating the Influence of the Prior Distribution in\n  Large Language Models","summary":"  Large language models (LLMs) sometimes fail to respond appropriately to\ndeterministic tasks -- such as counting or forming acronyms -- because the\nimplicit prior distribution they have learned over sequences of tokens\ninfluences their responses. In this work, we show that, in at least some cases,\nLLMs actually compute the information needed to perform these tasks correctly,\nand we identify some interventions that can allow them to access this\ninformation to improve their performance. First, we show that simply prompting\nthe language model to not rely on its prior knowledge leads to dramatic\nimprovements in prior-dominated tasks. We then use mechanistic interpretability\ntechniques to localize the prior within the LLM and manipulate the extent to\nwhich that prior influences its responses. Specifically, we show that it is\npossible to identify layers of the underlying neural network that correlate\nwith the prior probability of a response and that lightweight finetuning of\nthese layers with basic prompts on prior-dominated tasks achieves high\nperformance on held-out answers. These results suggest that the information\nrequired to produce a correct response is contained within the representations\nof the problems formed by the models. Furthermore, we show that this finetuning\nis significantly more effective for prior-dominated tasks, and that the error\nafter finetuning is no longer correlated with the prior. Our results suggest\nthat it may be possible to define effective methods for manipulating the extent\nto which LLMs rely upon their priors in solving problems, potentially\nincreasing their performance in settings where LLMs hallucinate for reasons\nrelated to the prior probability of token sequences.\n","authors":["Liyi Zhang","Veniamin Veselovsky","R. Thomas McCoy","Thomas L. Griffiths"],"pdf_url":"https://arxiv.org/pdf/2504.12585v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.12580v1","updated":"2025-04-17T01:53:28Z","published":"2025-04-17T01:53:28Z","title":"ChemKANs for Combustion Chemistry Modeling and Acceleration","summary":"  Efficient chemical kinetic model inference and application for combustion\nproblems is challenging due to large ODE systems and wideley separated time\nscales. Machine learning techniques have been proposed to streamline these\nmodels, though strong nonlinearity and numerical stiffness combined with noisy\ndata sources makes their application challenging. The recently developed\nKolmogorov-Arnold Networks (KANs) and KAN ordinary differential equations\n(KAN-ODEs) have been demonstrated as powerful tools for scientific applications\nthanks to their rapid neural scaling, improved interpretability, and smooth\nactivation functions. Here, we develop ChemKANs by augmenting the KAN-ODE\nframework with physical knowledge of the flow of information through the\nrelevant kinetic and thermodynamic laws, as well as an elemental conservation\nloss term. This novel framework encodes strong inductive bias that enables\nstreamlined training and higher accuracy predictions, while facilitating\nparameter sparsity through full sharing of information across all inputs and\noutputs. In a model inference investigation, we find that ChemKANs exhibit no\noverfitting or model degradation when tasked with extracting predictive models\nfrom data that is both sparse and noisy, a task that a standard DeepONet\nstruggles to accomplish. Next, we find that a remarkably parameter-lean ChemKAN\n(only 344 parameters) can accurately represent hydrogen combustion chemistry,\nproviding a 2x acceleration over the detailed chemistry in a solver that is\ngeneralizable to larger-scale turbulent flow simulations. These demonstrations\nindicate potential for ChemKANs in combustion physics and chemical kinetics,\nand demonstrate the scalability of generic KAN-ODEs in significantly larger and\nmore numerically challenging problems than previously studied.\n","authors":["Benjamin C. Koenig","Suyong Kim","Sili Deng"],"pdf_url":"https://arxiv.org/pdf/2504.12580v1.pdf","comment":"B.C.K. and S.K. contributed equally to this work. 23 pages, 8\n  figures, and 1 table"},{"id":"http://arxiv.org/abs/2409.17505v2","updated":"2025-04-17T01:52:49Z","published":"2024-09-26T03:24:59Z","title":"Sequential Kernelized Stein Discrepancy","summary":"  We present a sequential version of the kernelized Stein discrepancy\ngoodness-of-fit test, which allows for conducting goodness-of-fit tests for\nunnormalized densities that are continuously monitored and adaptively stopped.\nThat is, the sample size need not be fixed prior to data collection; the\npractitioner can choose whether to stop the test or continue to gather evidence\nat any time while controlling the false discovery rate. In stark contrast to\nrelated literature, we do not impose uniform boundedness on the Stein kernel.\nInstead, we exploit the potential boundedness of the Stein kernel at arbitrary\npoint evaluations to define test martingales, that give way to the subsequent\nnovel sequential tests. We prove the validity of the test, as well as an\nasymptotic lower bound for the logarithmic growth of the wealth process under\nthe alternative. We further illustrate the empirical performance of the test\nwith a variety of distributions, including restricted Boltzmann machines.\n","authors":["Diego Martinez-Taboada","Aaditya Ramdas"],"pdf_url":"https://arxiv.org/pdf/2409.17505v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12577v1","updated":"2025-04-17T01:50:24Z","published":"2025-04-17T01:50:24Z","title":"Local Data Quantity-Aware Weighted Averaging for Federated Learning with\n  Dishonest Clients","summary":"  Federated learning (FL) enables collaborative training of deep learning\nmodels without requiring data to leave local clients, thereby preserving client\nprivacy. The aggregation process on the server plays a critical role in the\nperformance of the resulting FL model. The most commonly used aggregation\nmethod is weighted averaging based on the amount of data from each client,\nwhich is thought to reflect each client's contribution. However, this method is\nprone to model bias, as dishonest clients might report inaccurate training data\nvolumes to the server, which is hard to verify. To address this issue, we\npropose a novel secure \\underline{Fed}erated \\underline{D}ata\nq\\underline{u}antity-\\underline{a}ware weighted averaging method (FedDua). It\nenables FL servers to accurately predict the amount of training data from each\nclient based on their local model gradients uploaded. Furthermore, it can be\nseamlessly integrated into any FL algorithms that involve server-side model\naggregation. Extensive experiments on three benchmarking datasets demonstrate\nthat FedDua improves the global model performance by an average of 3.17%\ncompared to four popular FL aggregation methods in the presence of inaccurate\nclient data volume declarations.\n","authors":["Leming Wu","Yaochu Jin","Kuangrong Hao","Han Yu"],"pdf_url":"https://arxiv.org/pdf/2504.12577v1.pdf","comment":"The paper has been accepted by ICME 2025"},{"id":"http://arxiv.org/abs/2504.12575v1","updated":"2025-04-17T01:49:02Z","published":"2025-04-17T01:49:02Z","title":"Featuremetric benchmarking: Quantum computer benchmarks based on circuit\n  features","summary":"  Benchmarks that concisely summarize the performance of many-qubit quantum\ncomputers are essential for measuring progress towards the goal of useful\nquantum computation. In this work, we present a benchmarking framework that is\nbased on quantifying how a quantum computer's performance on quantum circuits\nvaries as a function of features of those circuits, such as circuit depth,\nwidth, two-qubit gate density, problem input size, or algorithmic depth. Our\nfeaturemetric benchmarking framework generalizes volumetric benchmarking -- a\nwidely-used methodology that quantifies performance versus circuit width and\ndepth -- and we show that it enables richer and more faithful models of quantum\ncomputer performance. We demonstrate featuremetric benchmarking with example\nbenchmarks run on IBM Q and IonQ systems of up to 27 qubits, and we show how to\nproduce performance summaries from the data using Gaussian process regression.\nOur data analysis methods are also of interest in the special case of\nvolumetric benchmarking, as they enable the creation of intuitive\ntwo-dimensional capability regions using data from few circuits.\n","authors":["Timothy Proctor","Anh Tran","Xingxin Liu","Aditya Dhumuntarao","Stefan Seritan","Alaina Green","Norbert M Linke"],"pdf_url":"https://arxiv.org/pdf/2504.12575v1.pdf","comment":null}],"Computation and Language":[{"id":"http://arxiv.org/abs/2410.17385v2","updated":"2025-04-17T17:59:27Z","published":"2024-10-22T19:39:15Z","title":"Do Vision-Language Models Represent Space and How? Evaluating Spatial\n  Frame of Reference Under Ambiguities","summary":"  Spatial expressions in situated communication can be ambiguous, as their\nmeanings vary depending on the frames of reference (FoR) adopted by speakers\nand listeners. While spatial language understanding and reasoning by\nvision-language models (VLMs) have gained increasing attention, potential\nambiguities in these models are still under-explored. To address this issue, we\npresent the COnsistent Multilingual Frame Of Reference Test (COMFORT), an\nevaluation protocol to systematically assess the spatial reasoning capabilities\nof VLMs. We evaluate nine state-of-the-art VLMs using COMFORT. Despite showing\nsome alignment with English conventions in resolving ambiguities, our\nexperiments reveal significant shortcomings of VLMs: notably, the models (1)\nexhibit poor robustness and consistency, (2) lack the flexibility to\naccommodate multiple FoRs, and (3) fail to adhere to language-specific or\nculture-specific conventions in cross-lingual tests, as English tends to\ndominate other languages. With a growing effort to align vision-language models\nwith human cognitive intuitions, we call for more attention to the ambiguous\nnature and cross-cultural diversity of spatial reasoning.\n","authors":["Zheyuan Zhang","Fengyuan Hu","Jayjun Lee","Freda Shi","Parisa Kordjamshidi","Joyce Chai","Ziqiao Ma"],"pdf_url":"https://arxiv.org/pdf/2410.17385v2.pdf","comment":"Accepted to ICLR 2025 (Oral) | Project page:\n  https://spatial-comfort.github.io/"},{"id":"http://arxiv.org/abs/2504.13172v1","updated":"2025-04-17T17:59:27Z","published":"2025-04-17T17:59:27Z","title":"SemCORE: A Semantic-Enhanced Generative Cross-Modal Retrieval Framework\n  with MLLMs","summary":"  Cross-modal retrieval (CMR) is a fundamental task in multimedia research,\nfocused on retrieving semantically relevant targets across different\nmodalities. While traditional CMR methods match text and image via\nembedding-based similarity calculations, recent advancements in pre-trained\ngenerative models have established generative retrieval as a promising\nalternative. This paradigm assigns each target a unique identifier and\nleverages a generative model to directly predict identifiers corresponding to\ninput queries without explicit indexing. Despite its great potential, current\ngenerative CMR approaches still face semantic information insufficiency in both\nidentifier construction and generation processes. To address these limitations,\nwe propose a novel unified Semantic-enhanced generative Cross-mOdal REtrieval\nframework (SemCORE), designed to unleash the semantic understanding\ncapabilities in generative cross-modal retrieval task. Specifically, we first\nconstruct a Structured natural language IDentifier (SID) that effectively\naligns target identifiers with generative models optimized for natural language\ncomprehension and generation. Furthermore, we introduce a Generative Semantic\nVerification (GSV) strategy enabling fine-grained target discrimination.\nAdditionally, to the best of our knowledge, SemCORE is the first framework to\nsimultaneously consider both text-to-image and image-to-text retrieval tasks\nwithin generative cross-modal retrieval. Extensive experiments demonstrate that\nour framework outperforms state-of-the-art generative cross-modal retrieval\nmethods. Notably, SemCORE achieves substantial improvements across benchmark\ndatasets, with an average increase of 8.65 points in Recall@1 for text-to-image\nretrieval.\n","authors":["Haoxuan Li","Yi Bin","Yunshan Ma","Guoqing Wang","Yang Yang","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2504.13172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13171v1","updated":"2025-04-17T17:59:25Z","published":"2025-04-17T17:59:25Z","title":"Sleep-time Compute: Beyond Inference Scaling at Test-time","summary":"  Scaling test-time compute has emerged as a key ingredient for enabling large\nlanguage models (LLMs) to solve difficult problems, but comes with high latency\nand inference cost. We introduce sleep-time compute, which allows models to\n\"think\" offline about contexts before queries are presented: by anticipating\nwhat queries users might ask and pre-computing useful quantities, we can\nsignificantly reduce the compute requirements at test-time. To demonstrate the\nefficacy of our method, we create modified versions of two reasoning tasks -\nStateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can\nreduce the amount of test-time compute needed to achieve the same accuracy by ~\n5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time\ncompute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic\nand 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,\nwhich extends GSM-Symbolic by including multiple related queries per context.\nBy amortizing sleep-time compute across related queries about the same context\nusing Multi-Query GSM-Symbolic, we can decrease the average cost per query by\n2.5x. We then conduct additional analysis to understand when sleep-time compute\nis most effective, finding the predictability of the user query to be well\ncorrelated with the efficacy of sleep-time compute. Finally, we conduct a\ncase-study of applying sleep-time compute to a realistic agentic SWE task.\n","authors":["Kevin Lin","Charlie Snell","Yu Wang","Charles Packer","Sarah Wooders","Ion Stoica","Joseph E. Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2504.13171v1.pdf","comment":"Code and data released at:\n  https://github.com/letta-ai/sleep-time-compute"},{"id":"http://arxiv.org/abs/2504.13161v1","updated":"2025-04-17T17:58:13Z","published":"2025-04-17T17:58:13Z","title":"CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for\n  Language Model Pre-training","summary":"  Pre-training datasets are typically collected from web content and lack\ninherent domain divisions. For instance, widely used datasets like Common Crawl\ndo not include explicit domain labels, while manually curating labeled datasets\nsuch as The Pile is labor-intensive. Consequently, identifying an optimal\npre-training data mixture remains a challenging problem, despite its\nsignificant benefits for pre-training performance. To address these challenges,\nwe propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an\nautomated framework that discovers, evaluates, and refines data mixtures in a\npre-training setting. Specifically, CLIMB embeds and clusters large-scale\ndatasets in a semantic space and then iteratively searches for optimal mixtures\nusing a smaller proxy model and a predictor. When continuously trained on 400B\ntokens with this mixture, our 1B model exceeds the state-of-the-art\nLlama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific\ndomain (e.g., Social Sciences) yields a 5% improvement over random sampling.\nFinally, we introduce ClimbLab, a filtered 1.2-trillion-token corpus with 20\nclusters as a research playground, and ClimbMix, a compact yet powerful\n400-billion-token dataset designed for efficient pre-training that delivers\nsuperior performance under an equal token budget. We analyze the final data\nmixture, elucidating the characteristics of an optimal data mixture. Our data\nis available at: https://research.nvidia.com/labs/lpr/climb/\n","authors":["Shizhe Diao","Yu Yang","Yonggan Fu","Xin Dong","Dan Su","Markus Kliegl","Zijia Chen","Peter Belcak","Yoshi Suhara","Hongxu Yin","Mostofa Patwary"," Yingyan"," Lin","Jan Kautz","Pavlo Molchanov"],"pdf_url":"https://arxiv.org/pdf/2504.13161v1.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2504.13151v1","updated":"2025-04-17T17:55:45Z","published":"2025-04-17T17:55:45Z","title":"MIB: A Mechanistic Interpretability Benchmark","summary":"  How can we know whether new mechanistic interpretability methods achieve real\nimprovements? In pursuit of meaningful and lasting evaluation standards, we\npropose MIB, a benchmark with two tracks spanning four tasks and five models.\nMIB favors methods that precisely and concisely recover relevant causal\npathways or specific causal variables in neural language models. The circuit\nlocalization track compares methods that locate the model components - and\nconnections between them - most important for performing a task (e.g.,\nattribution patching or information flow routes). The causal variable\nlocalization track compares methods that featurize a hidden vector, e.g.,\nsparse autoencoders (SAEs) or distributed alignment search (DAS), and locate\nmodel features for a causal variable relevant to the task. Using MIB, we find\nthat attribution and mask optimization methods perform best on circuit\nlocalization. For causal variable localization, we find that the supervised DAS\nmethod performs best, while SAE features are not better than neurons, i.e.,\nstandard dimensions of hidden vectors. These findings illustrate that MIB\nenables meaningful comparisons of methods, and increases our confidence that\nthere has been real progress in the field.\n","authors":["Aaron Mueller","Atticus Geiger","Sarah Wiegreffe","Dana Arad","Iván Arcuschin","Adam Belfki","Yik Siu Chan","Jaden Fiotto-Kaufman","Tal Haklay","Michael Hanna","Jing Huang","Rohan Gupta","Yaniv Nikankin","Hadas Orgad","Nikhil Prakash","Anja Reusch","Aruna Sankaranarayanan","Shun Shao","Alessandro Stolfo","Martin Tutek","Amir Zur","David Bau","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2504.13151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13146v1","updated":"2025-04-17T17:54:14Z","published":"2025-04-17T17:54:14Z","title":"Antidistillation Sampling","summary":"  Frontier models that generate extended reasoning traces inadvertently produce\nrich token sequences that can facilitate model distillation. Recognizing this\nvulnerability, model owners may seek sampling strategies that limit the\neffectiveness of distillation without compromising model performance.\n\\emph{Antidistillation sampling} provides exactly this capability. By\nstrategically modifying a model's next-token probability distribution,\nantidistillation sampling poisons reasoning traces, rendering them\nsignificantly less effective for distillation while preserving the model's\npractical utility. For further details, see https://antidistillation.com.\n","authors":["Yash Savani","Asher Trockman","Zhili Feng","Avi Schwarzschild","Alexander Robey","Marc Finzi","J. Zico Kolter"],"pdf_url":"https://arxiv.org/pdf/2504.13146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13139v1","updated":"2025-04-17T17:49:40Z","published":"2025-04-17T17:49:40Z","title":"Syntactic and Semantic Control of Large Language Models via Sequential\n  Monte Carlo","summary":"  A wide range of LM applications require generating text that conforms to\nsyntactic or semantic constraints. Imposing such constraints can be naturally\nframed as probabilistic conditioning, but exact generation from the resulting\ndistribution -- which can differ substantially from the LM's base distribution\n-- is generally intractable. In this work, we develop an architecture for\ncontrolled LM generation based on sequential Monte Carlo (SMC). Our SMC\nframework allows us to flexibly incorporate domain- and problem-specific\nconstraints at inference time, and efficiently reallocate computational\nresources in light of new information during the course of generation. By\ncomparing to a number of alternatives and ablations on four challenging domains\n-- Python code generation for data science, text-to-SQL, goal inference, and\nmolecule synthesis -- we demonstrate that, with little overhead, our approach\nallows small open-source language models to outperform models over 8x larger,\nas well as closed-source, fine-tuned ones. In support of the probabilistic\nperspective, we show that these performance improvements are driven by better\napproximation to the posterior distribution. Our system builds on the framework\nof Lew et al. (2023) and integrates with its language model probabilistic\nprogramming language, giving users a simple, programmable way to apply SMC to a\nbroad variety of controlled generation problems.\n","authors":["João Loula","Benjamin LeBrun","Li Du","Ben Lipkin","Clemente Pasti","Gabriel Grand","Tianyu Liu","Yahya Emara","Marjorie Freedman","Jason Eisner","Ryan Cotterel","Vikash Mansinghka","Alexander K. Lew","Tim Vieira","Timothy J. O'Donnell"],"pdf_url":"https://arxiv.org/pdf/2504.13139v1.pdf","comment":"34 pages, 4 figures"},{"id":"http://arxiv.org/abs/2501.17070v3","updated":"2025-04-17T17:48:53Z","published":"2025-01-28T16:55:39Z","title":"Contextual Agent Security: A Policy for Every Purpose","summary":"  Judging an action's safety requires knowledge of the context in which the\naction takes place. To human agents who act in various contexts, this may seem\nobvious: performing an action such as email deletion may or may not be\nappropriate depending on the email's content, the goal (e.g., to erase\nsensitive emails or to clean up trash), and the type of email address (e.g.,\nwork or personal). Unlike people, computational systems have often had only\nlimited agency in limited contexts. Thus, manually crafted policies and user\nconfirmation (e.g., smartphone app permissions or network access control\nlists), while imperfect, have sufficed to restrict harmful actions. However,\nwith the upcoming deployment of generalist agents that support a multitude of\ntasks (e.g., an automated personal assistant), we argue that we must rethink\nsecurity designs to adapt to the scale of contexts and capabilities of these\nsystems. As a first step, this paper explores contextual security in the domain\nof agents and proposes contextual agent security (Conseca), a framework to\ngenerate just-in-time, contextual, and human-verifiable security policies.\n","authors":["Lillian Tsai","Eugene Bagdasarian"],"pdf_url":"https://arxiv.org/pdf/2501.17070v3.pdf","comment":"Workshop in Hot Topics in Operating Systems (HotOS) 2025"},{"id":"http://arxiv.org/abs/2504.13134v1","updated":"2025-04-17T17:47:15Z","published":"2025-04-17T17:47:15Z","title":"Energy-Based Reward Models for Robust Language Model Alignment","summary":"  Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM.\n","authors":["Anamika Lochab","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.13134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13128v1","updated":"2025-04-17T17:44:06Z","published":"2025-04-17T17:44:06Z","title":"FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on\n  Technical Documents","summary":"  We introduce FreshStack, a reusable framework for automatically building\ninformation retrieval (IR) evaluation benchmarks from community-asked questions\nand answers. FreshStack conducts the following steps: (1) automatic corpus\ncollection from code and technical documentation, (2) nugget generation from\ncommunity-asked questions and answers, and (3) nugget-level support, retrieving\ndocuments using a fusion of retrieval techniques and hybrid architectures. We\nuse FreshStack to build five datasets on fast-growing, recent, and niche topics\nto ensure the tasks are sufficiently challenging. On FreshStack, existing\nretrieval models, when applied out-of-the-box, significantly underperform\noracle approaches on all five topics, denoting plenty of headroom to improve IR\nquality. In addition, we identify cases where rerankers do not clearly improve\nfirst-stage retrieval accuracy (two out of five topics). We hope that\nFreshStack will facilitate future work toward constructing realistic, scalable,\nand uncontaminated IR and RAG evaluation benchmarks. FreshStack datasets are\navailable at: https://fresh-stack.github.io.\n","authors":["Nandan Thakur","Jimmy Lin","Sam Havens","Michael Carbin","Omar Khattab","Andrew Drozdov"],"pdf_url":"https://arxiv.org/pdf/2504.13128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13125v1","updated":"2025-04-17T17:42:02Z","published":"2025-04-17T17:42:02Z","title":"LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM\n  Leaderboard","summary":"  This paper investigates the application of large language models (LLMs) to\nfinancial tasks. We fine-tuned foundation models using the Open FinLLM\nLeaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed\ntechniques including supervised fine-tuning (SFT), direct preference\noptimization (DPO), and reinforcement learning (RL) to enhance their financial\ncapabilities. The fine-tuned models demonstrated substantial performance gains\nacross a wide range of financial tasks. Moreover, we measured the data scaling\nlaw in the financial domain. Our work demonstrates the potential of large\nlanguage models (LLMs) in financial applications.\n","authors":["Varun Rao","Youran Sun","Mahendra Kumar","Tejas Mutneja","Agastya Mukherjee","Haizhao Yang"],"pdf_url":"https://arxiv.org/pdf/2504.13125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13120v1","updated":"2025-04-17T17:38:18Z","published":"2025-04-17T17:38:18Z","title":"Probing and Inducing Combinational Creativity in Vision-Language Models","summary":"  The ability to combine existing concepts into novel ideas stands as a\nfundamental hallmark of human intelligence. Recent advances in Vision-Language\nModels (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their\noutputs reflect combinational creativity--defined by M. A. Boden (1998) as\nsynthesizing novel ideas through combining existing concepts--or sophisticated\npattern matching of training data. Drawing inspiration from cognitive science,\nwe investigate the combinational creativity of VLMs from the lens of concept\nblending. We propose the Identification-Explanation-Implication (IEI)\nframework, which decomposes creative processes into three levels: identifying\ninput spaces, extracting shared attributes, and deriving novel semantic\nimplications. To validate this framework, we curate CreativeMashup, a\nhigh-quality dataset of 666 artist-generated visual mashups annotated according\nto the IEI framework. Through extensive experiments, we demonstrate that in\ncomprehension tasks, best VLMs have surpassed average human performance while\nfalling short of expert-level understanding; in generation tasks, incorporating\nour IEI framework into the generation pipeline significantly enhances the\ncreative quality of VLMs outputs. Our findings establish both a theoretical\nfoundation for evaluating artificial creativity and practical guidelines for\nimproving creative generation in VLMs.\n","authors":["Yongqian Peng","Yuxi Ma","Mengmeng Wang","Yuxuan Wang","Yizhou Wang","Chi Zhang","Yixin Zhu","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2504.13120v1.pdf","comment":"Project page: https://ppyyqq.github.io/aicc/ The first two authors\n  contribute equally"},{"id":"http://arxiv.org/abs/2504.09081v2","updated":"2025-04-17T17:34:21Z","published":"2025-04-12T04:45:48Z","title":"SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction\n  Fine-Tuning","summary":"  We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset\ndesigned for instruction fine-tuning and pre-training of speech-text large\nlanguage models (LLMs). SIFT-50M is built from publicly available speech\ncorpora, which collectively contain 14K hours of speech, and leverages LLMs\nalong with off-the-shelf expert models. The dataset spans five languages,\nencompassing a diverse range of speech understanding as well as controllable\nspeech generation instructions. Using SIFT-50M, we train SIFT-LLM, which\noutperforms existing speech-text LLMs on instruction-following benchmarks while\nachieving competitive performance on foundational speech tasks. To support\nfurther research, we also introduce EvalSIFT, a benchmark dataset specifically\ndesigned to evaluate the instruction-following capabilities of speech-text\nLLMs.\n","authors":["Prabhat Pandey","Rupak Vignesh Swaminathan","K V Vijay Girish","Arunasish Sen","Jian Xie","Grant P. Strimel","Andreas Schwarz"],"pdf_url":"https://arxiv.org/pdf/2504.09081v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16063v4","updated":"2025-04-17T17:28:29Z","published":"2024-02-25T11:24:41Z","title":"Citation-Enhanced Generation for LLM-based Chatbots","summary":"  Large language models (LLMs) exhibit powerful general intelligence across\ndiverse scenarios, including their integration into chatbots. However, a vital\nchallenge of LLM-based chatbots is that they may produce hallucinated content\nin responses, which significantly limits their applicability. Various efforts\nhave been made to alleviate hallucination, such as retrieval augmented\ngeneration and reinforcement learning with human feedback, but most of them\nrequire additional training and data annotation. In this paper, we propose a\nnovel post-hoc Citation-Enhanced Generation (CEG) approach combined with\nretrieval argumentation. Unlike previous studies that focus on preventing\nhallucinations during generation, our method addresses this issue in a post-hoc\nway. It incorporates a retrieval module to search for supporting documents\nrelevant to the generated content, and employs a natural language\ninference-based citation generation module. Once the statements in the\ngenerated content lack of reference, our model can regenerate responses until\nall statements are supported by citations. Note that our method is a\ntraining-free plug-and-play plugin that is capable of various LLMs. Experiments\non various hallucination-related datasets show our framework outperforms\nstate-of-the-art methods in both hallucination detection and response\nregeneration on three benchmarks. Our codes and dataset will be publicly\navailable.\n","authors":["Weitao Li","Junkai Li","Weizhi Ma","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2402.16063v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09012v2","updated":"2025-04-17T17:14:09Z","published":"2025-01-15T18:56:22Z","title":"Multimodal LLMs Can Reason about Aesthetics in Zero-Shot","summary":"  The rapid progress of generative art has democratized the creation of\nvisually pleasing imagery. However, achieving genuine artistic impact - the\nkind that resonates with viewers on a deeper, more meaningful level - requires\na sophisticated aesthetic sensibility. This sensibility involves a\nmulti-faceted reasoning process extending beyond mere visual appeal, which is\noften overlooked by current computational models. This paper pioneers an\napproach to capture this complex process by investigating how the reasoning\ncapabilities of Multimodal LLMs (MLLMs) can be effectively elicited for\naesthetic judgment. Our analysis reveals a critical challenge: MLLMs exhibit a\ntendency towards hallucinations during aesthetic reasoning, characterized by\nsubjective opinions and unsubstantiated artistic interpretations. We further\ndemonstrate that these limitations can be overcome by employing an\nevidence-based, objective reasoning process, as substantiated by our proposed\nbaseline, ArtCoT. MLLMs prompted by this principle produce multi-faceted and\nin-depth aesthetic reasoning that aligns significantly better with human\njudgment. These findings have direct applications in areas such as AI art\ntutoring and as reward models for generative art. Ultimately, our work paves\nthe way for AI systems that can truly understand, appreciate, and generate\nartworks that align with the sensible human aesthetic standard.\n","authors":["Ruixiang Jiang","Changwen Chen"],"pdf_url":"https://arxiv.org/pdf/2501.09012v2.pdf","comment":"WIP, Homepage https://github.com/songrise/MLLM4Art"},{"id":"http://arxiv.org/abs/2504.13085v1","updated":"2025-04-17T16:53:14Z","published":"2025-04-17T16:53:14Z","title":"Tackling Social Bias against the Poor: A Dataset and Taxonomy on\n  Aporophobia","summary":"  Eradicating poverty is the first goal in the United Nations Sustainable\nDevelopment Goals. However, aporophobia -- the societal bias against people\nliving in poverty -- constitutes a major obstacle to designing, approving and\nimplementing poverty-mitigation policies. This work presents an initial step\ntowards operationalizing the concept of aporophobia to identify and track\nharmful beliefs and discriminative actions against poor people on social media.\nIn close collaboration with non-profits and governmental organizations, we\nconduct data collection and exploration. Then we manually annotate a corpus of\nEnglish tweets from five world regions for the presence of (1) direct\nexpressions of aporophobia, and (2) statements referring to or criticizing\naporophobic views or actions of others, to comprehensively characterize the\nsocial media discourse related to bias and discrimination against the poor.\nBased on the annotated data, we devise a taxonomy of categories of aporophobic\nattitudes and actions expressed through speech on social media. Finally, we\ntrain several classifiers and identify the main challenges for automatic\ndetection of aporophobia in social networks. This work paves the way towards\nidentifying, tracking, and mitigating aporophobic views on social media at\nscale.\n","authors":["Georgina Curto","Svetlana Kiritchenko","Muhammad Hammad Fahim Siddiqui","Isar Nejadgholi","Kathleen C. Fraser"],"pdf_url":"https://arxiv.org/pdf/2504.13085v1.pdf","comment":"In Findings of the Association for Computational Linguistics: NAACL\n  2025"},{"id":"http://arxiv.org/abs/2504.13079v1","updated":"2025-04-17T16:46:11Z","published":"2025-04-17T16:46:11Z","title":"Retrieval-Augmented Generation with Conflicting Evidence","summary":"  Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation.\n","authors":["Han Wang","Archiki Prasad","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2504.13079v1.pdf","comment":"Our data and code is available at:\n  https://github.com/HanNight/RAMDocs"},{"id":"http://arxiv.org/abs/2504.11536v2","updated":"2025-04-17T16:46:07Z","published":"2025-04-15T18:10:22Z","title":"ReTool: Reinforcement Learning for Strategic Tool Use in LLMs","summary":"  While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems.\n","authors":["Jiazhan Feng","Shijue Huang","Xingwei Qu","Ge Zhang","Yujia Qin","Baoquan Zhong","Chengquan Jiang","Jinxin Chi","Wanjun Zhong"],"pdf_url":"https://arxiv.org/pdf/2504.11536v2.pdf","comment":"fix typos"},{"id":"http://arxiv.org/abs/2504.13068v1","updated":"2025-04-17T16:29:08Z","published":"2025-04-17T16:29:08Z","title":"Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative\n  Classification Models","summary":"  This study explores the relationship between deep learning (DL) model\naccuracy and expert agreement in the classification of crash narratives. We\nevaluate five DL models -- including BERT variants, the Universal Sentence\nEncoder (USE), and a zero-shot classifier -- against expert-labeled data and\nnarrative text. The analysis is further extended to four large language models\n(LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive\ntrend: models with higher technical accuracy often exhibit lower agreement with\ndomain experts, whereas LLMs demonstrate greater expert alignment despite\nrelatively lower accuracy scores. To quantify and interpret model-expert\nagreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and\nSHAP-based explainability techniques. Findings indicate that expert-aligned\nmodels tend to rely more on contextual and temporal language cues, rather than\nlocation-specific keywords. These results underscore that accuracy alone is\ninsufficient for evaluating models in safety-critical NLP applications. We\nadvocate for incorporating expert agreement as a complementary metric in model\nevaluation frameworks and highlight the promise of LLMs as interpretable,\nscalable tools for crash analysis pipelines.\n","authors":["Sudesh Ramesh Bhagat","Ibne Farabi Shihab","Anuj Sharma"],"pdf_url":"https://arxiv.org/pdf/2504.13068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13059v1","updated":"2025-04-17T16:14:24Z","published":"2025-04-17T16:14:24Z","title":"RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins","summary":"  In the rapidly advancing field of robotics, dual-arm coordination and complex\nobject manipulation are essential capabilities for developing advanced\nautonomous systems. However, the scarcity of diverse, high-quality\ndemonstration data and real-world-aligned evaluation benchmarks severely limits\nsuch development. To address this, we introduce RoboTwin, a generative digital\ntwin framework that uses 3D generative foundation models and large language\nmodels to produce diverse expert datasets and provide a real-world-aligned\nevaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates\nvaried digital twins of objects from single 2D images, generating realistic and\ninteractive scenarios. It also introduces a spatial relation-aware code\ngeneration framework that combines object annotations with large language\nmodels to break down tasks, determine spatial constraints, and generate precise\nrobotic movement code. Our framework offers a comprehensive benchmark with both\nsimulated and real-world data, enabling standardized evaluation and better\nalignment between simulated training and real-world performance. We validated\nour approach using the open-source COBOT Magic Robot platform. Policies\npre-trained on RoboTwin-generated data and fine-tuned with limited real-world\nsamples demonstrate significant potential for enhancing dual-arm robotic\nmanipulation systems by improving success rates by over 70% for single-arm\ntasks and over 40% for dual-arm tasks compared to models trained solely on\nreal-world data.\n","authors":["Yao Mu","Tianxing Chen","Zanxin Chen","Shijia Peng","Zhiqian Lan","Zeyu Gao","Zhixuan Liang","Qiaojun Yu","Yude Zou","Mingkun Xu","Lunkai Lin","Zhiqiang Xie","Mingyu Ding","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2504.13059v1.pdf","comment":"CVPR 2025 Highlight. 22 pages. Project page:\n  https://robotwin-benchmark.github.io/"},{"id":"http://arxiv.org/abs/2504.13054v1","updated":"2025-04-17T16:09:57Z","published":"2025-04-17T16:09:57Z","title":"Aspect-Based Summarization with Self-Aspect Retrieval Enhanced\n  Generation","summary":"  Aspect-based summarization aims to generate summaries tailored to specific\naspects, addressing the resource constraints and limited generalizability of\ntraditional summarization approaches. Recently, large language models have\nshown promise in this task without the need for training. However, they rely\nexcessively on prompt engineering and face token limits and hallucination\nchallenges, especially with in-context learning. To address these challenges,\nin this paper, we propose a novel framework for aspect-based summarization:\nSelf-Aspect Retrieval Enhanced Summary Generation. Rather than relying solely\non in-context learning, given an aspect, we employ an embedding-driven\nretrieval mechanism to identify its relevant text segments. This approach\nextracts the pertinent content while avoiding unnecessary details, thereby\nmitigating the challenge of token limits. Moreover, our framework optimizes\ntoken usage by deleting unrelated parts of the text and ensuring that the model\ngenerates output strictly based on the given aspect. With extensive experiments\non benchmark datasets, we demonstrate that our framework not only achieves\nsuperior performance but also effectively mitigates the token limitation\nproblem.\n","authors":["Yichao Feng","Shuai Zhao","Yueqiu Li","Luwei Xiao","Xiaobao Wu","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2504.13054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20973v2","updated":"2025-04-17T16:07:44Z","published":"2025-02-28T11:37:52Z","title":"Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin?","summary":"  In this era of rapid technological advancements, communication continues to\nevolve as new linguistic phenomena emerge. Among these is Arabizi, a hybrid\nform of Arabic that incorporates Latin characters and numbers to represent the\nspoken dialects of Arab communities. Arabizi is widely used on social media and\nallows people to communicate in an informal and dynamic way, but it poses\nsignificant challenges for machine translation due to its lack of formal\nstructure and deeply embedded cultural nuances. This case study arises from a\ngrowing need to translate Arabizi for gisting purposes. It evaluates the\ncapacity of different LLMs to decode and translate Arabizi, focusing on\nmultiple Arabic dialects that have rarely been studied up until now. Using a\ncombination of human evaluators and automatic metrics, this research project\ninvestigates the model's performance in translating Arabizi into both Modern\nStandard Arabic and English. Key questions explored include which dialects are\ntranslated most effectively and whether translations into English surpass those\ninto Arabic.\n","authors":["Perla Al Almaoui","Pierrette Bouillon","Simon Hengchen"],"pdf_url":"https://arxiv.org/pdf/2502.20973v2.pdf","comment":"Accepted to MT Summit 2025 (Track: Implementation and Case Studies)\n  https://mtsummit2025.unige.ch/"},{"id":"http://arxiv.org/abs/2504.13038v1","updated":"2025-04-17T15:51:59Z","published":"2025-04-17T15:51:59Z","title":"How Large Language Models Are Changing MOOC Essay Answers: A Comparison\n  of Pre- and Post-LLM Responses","summary":"  The release of ChatGPT in late 2022 caused a flurry of activity and concern\nin the academic and educational communities. Some see the tool's ability to\ngenerate human-like text that passes at least cursory inspections for factual\naccuracy ``often enough'' a golden age of information retrieval and\ncomputer-assisted learning. Some, on the other hand, worry the tool may lead to\nunprecedented levels of academic dishonesty and cheating. In this work, we\nquantify some of the effects of the emergence of Large Language Models (LLMs)\non online education by analyzing a multi-year dataset of student essay\nresponses from a free university-level MOOC on AI ethics. Our dataset includes\nessays submitted both before and after ChatGPT's release. We find that the\nlaunch of ChatGPT coincided with significant changes in both the length and\nstyle of student essays, mirroring observations in other contexts such as\nacademic publishing. We also observe -- as expected based on related public\ndiscourse -- changes in prevalence of key content words related to AI and LLMs,\nbut not necessarily the general themes or topics discussed in the student\nessays as identified through (dynamic) topic modeling.\n","authors":["Leo Leppänen","Lili Aunimo","Arto Hellas","Jukka K. Nurminen","Linda Mannila"],"pdf_url":"https://arxiv.org/pdf/2504.13038v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.13023v1","updated":"2025-04-17T15:33:17Z","published":"2025-04-17T15:33:17Z","title":"ChatEXAONEPath: An Expert-level Multimodal Large Language Model for\n  Histopathology Using Whole Slide Images","summary":"  Recent studies have made significant progress in developing large language\nmodels (LLMs) in the medical domain, which can answer expert-level questions\nand demonstrate the potential to assist clinicians in real-world clinical\nscenarios. Studies have also witnessed the importance of integrating various\nmodalities with the existing LLMs for a better understanding of complex\nclinical contexts, which are innately multi-faceted by nature. Although studies\nhave demonstrated the ability of multimodal LLMs in histopathology to answer\nquestions from given images, they lack in understanding of thorough clinical\ncontext due to the patch-level data with limited information from public\ndatasets. Thus, developing WSI-level MLLMs is significant in terms of the\nscalability and applicability of MLLMs in histopathology. In this study, we\nintroduce an expert-level MLLM for histopathology using WSIs, dubbed as\nChatEXAONEPath. We present a retrieval-based data generation pipeline using\n10,094 pairs of WSIs and histopathology reports from The Cancer Genome Atlas\n(TCGA). We also showcase an AI-based evaluation protocol for a comprehensive\nunderstanding of the medical context from given multimodal information and\nevaluate generated answers compared to the original histopathology reports. We\ndemonstrate the ability of diagnosing the given histopathology images using\nChatEXAONEPath with the acceptance rate of 62.9% from 1,134 pairs of WSIs and\nreports. Our proposed model can understand pan-cancer WSIs and clinical context\nfrom various cancer types. We argue that our proposed model has the potential\nto assist clinicians by comprehensively understanding complex morphology of\nWSIs for cancer diagnosis through the integration of multiple modalities.\n","authors":["Sangwook Kim","Soonyoung Lee","Jongseong Jang"],"pdf_url":"https://arxiv.org/pdf/2504.13023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11672v3","updated":"2025-04-17T15:10:52Z","published":"2024-04-17T18:13:16Z","title":"MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory","summary":"  While current large language models (LLMs) perform well on many\nknowledge-related tasks, they are limited by relying on their parameters as an\nimplicit storage mechanism. As a result, they struggle with memorizing rare\nevents and with updating their memory as facts change over time. In addition,\nthe uninterpretable nature of parametric memory makes it challenging to prevent\nhallucination. Model editing and augmenting LLMs with parameters specialized\nfor memory are only partial solutions. In this paper, we introduce MemLLM, a\nnovel method of enhancing LLMs by integrating a structured and explicit\nread-and-write memory module. MemLLM tackles the aforementioned challenges by\nenabling dynamic interaction with the memory and improving the LLM's\ncapabilities in using stored knowledge. Our experiments indicate that MemLLM\nenhances the LLM's performance and interpretability, in language modeling in\ngeneral and knowledge-intensive tasks in particular. We see MemLLM as an\nimportant step towards making LLMs more grounded and factual through memory\naugmentation. The project repository is publicly available at\nhttps://github.com/amodaresi/MemLLM\n","authors":["Ali Modarressi","Abdullatif Köksal","Ayyoob Imani","Mohsen Fayyaz","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2404.11672v3.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2504.12996v1","updated":"2025-04-17T15:05:40Z","published":"2025-04-17T15:05:40Z","title":"SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained\n  Unlearning for Large Language Models via Knowledge Isolation","summary":"  Large language models (LLMs) frequently memorize sensitive information during\ntraining, posing risks when deploying publicly accessible models. Current\nmachine unlearning methods struggle to selectively remove specific data\nassociations without degrading overall model capabilities. This paper presents\nour solution to SemEval-2025 Task 4 on targeted unlearning, which introduces a\ntwo-stage methodology that combines causal mediation analysis with\nlayer-specific optimization. Through systematic causal tracing experiments on\nOLMo architectures (1B and 7B parameters), we identify the critical role of the\nfirst few transformer layers (layers 0-5) in storing subject-attribute\nassociations within MLP modules. Building on this insight, we develop a\nconstrained optimization approach that freezes upper layers while applying a\nnovel joint loss function to lower layers-simultaneously maximizing forget set\nloss via output token cross-entropy penalties and minimizing retain set\ndeviation through adaptive regularization. Our method achieves 2nd place in the\n1B model track, demonstrating strong task performance while maintaining 88% of\nbaseline MMLU accuracy. These results establish causal-informed layer\noptimization as a promising paradigm for efficient, precise unlearning in LLMs,\noffering a significant step forward in addressing data privacy concerns in AI\nsystems.\n","authors":["Saransh Agrawal","Kuan-Hao Huang"],"pdf_url":"https://arxiv.org/pdf/2504.12996v1.pdf","comment":"8 pages, In Proceedings of The 19th International Workshop on\n  Semantic Evaluation (SemEval), 2025"},{"id":"http://arxiv.org/abs/2407.14246v3","updated":"2025-04-17T14:57:34Z","published":"2024-07-19T12:28:22Z","title":"Unipa-GPT: Large Language Models for university-oriented QA in Italian","summary":"  This paper illustrates the architecture and training of Unipa-GPT, a chatbot\nrelying on a Large Language Model, developed for assisting students in choosing\na bachelor/master degree course at the University of Palermo. Unipa-GPT relies\non gpt-3.5-turbo, it was presented in the context of the European Researchers'\nNight (SHARPER night). In our experiments we adopted both the Retrieval\nAugmented Generation (RAG) approach and fine-tuning to develop the system. The\nwhole architecture of Unipa-GPT is presented, both the RAG and the fine-tuned\nsystems are compared, and a brief discussion on their performance is reported.\nFurther comparison with other Large Language Models and the experimental\nresults during the SHARPER night are illustrated. Corpora and code are\navailable on GitHub\n","authors":["Irene Siragusa","Roberto Pirrone"],"pdf_url":"https://arxiv.org/pdf/2407.14246v3.pdf","comment":"GitHub repository: https://github.com/CHILab1/UnipaGPT-23"},{"id":"http://arxiv.org/abs/2504.12982v1","updated":"2025-04-17T14:40:31Z","published":"2025-04-17T14:40:31Z","title":"Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards\n  Reliable Response Generation in the Wild","summary":"  The proliferation of large language models (LLMs) has significantly advanced\ninformation retrieval systems, particularly in response generation (RG).\nUnfortunately, LLMs often face knowledge conflicts between internal memory and\nretrievaled external information, arising from misinformation, biases, or\noutdated knowledge. These conflicts undermine response reliability and\nintroduce uncertainty in decision-making. In this work, we analyze how LLMs\nnavigate knowledge conflicts from an information-theoretic perspective and\nreveal that when conflicting and supplementary information exhibit significant\ndifferences, LLMs confidently resolve their preferences. However, when the\ndistinction is ambiguous, LLMs experience heightened uncertainty. Based on this\ninsight, we propose Swin-VIB, a novel framework that integrates a pipeline of\nvariational information bottleneck models into adaptive augmentation of\nretrieved information and guiding LLM preference in response generation.\nExtensive experiments on single-choice, open-ended question-answering (QA), and\nretrieval augmented generation (RAG) validate our theoretical findings and\ndemonstrate the efficacy of Swin-VIB. Notably, our method improves\nsingle-choice task accuracy by at least 7.54\\% over competitive baselines.\n","authors":["Jiatai Wang","Zhiwei Xu","Di Jin","Xuewen Yang","Tao Li"],"pdf_url":"https://arxiv.org/pdf/2504.12982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12977v1","updated":"2025-04-17T14:29:25Z","published":"2025-04-17T14:29:25Z","title":"A Phenomenological Approach to Analyzing User Queries in IT Systems\n  Using Heidegger's Fundamental Ontology","summary":"  This paper presents a novel research analytical IT system grounded in Martin\nHeidegger's Fundamental Ontology, distinguishing between beings (das Seiende)\nand Being (das Sein). The system employs two modally distinct, descriptively\ncomplete languages: a categorical language of beings for processing user inputs\nand an existential language of Being for internal analysis. These languages are\nbridged via a phenomenological reduction module, enabling the system to analyze\nuser queries (including questions, answers, and dialogues among IT\nspecialists), identify recursive and self-referential structures, and provide\nactionable insights in categorical terms. Unlike contemporary systems limited\nto categorical analysis, this approach leverages Heidegger's phenomenological\nexistential analysis to uncover deeper ontological patterns in query\nprocessing, aiding in resolving logical traps in complex interactions, such as\nmetaphor usage in IT contexts. The path to full realization involves\nformalizing the language of Being by a research team based on Heidegger's\nFundamental Ontology; given the existing completeness of the language of\nbeings, this reduces the system's computability to completeness, paving the way\nfor a universal query analysis tool. The paper presents the system's\narchitecture, operational principles, technical implementation, use\ncases--including a case based on real IT specialist dialogues--comparative\nevaluation with existing tools, and its advantages and limitations.\n","authors":["Maksim Vishnevskiy"],"pdf_url":"https://arxiv.org/pdf/2504.12977v1.pdf","comment":"12 pages, no figures"},{"id":"http://arxiv.org/abs/2504.12976v1","updated":"2025-04-17T14:29:18Z","published":"2025-04-17T14:29:18Z","title":"Sparks of Science: Hypothesis Generation Using Structured Paper Data","summary":"  Generating novel and creative scientific hypotheses is a cornerstone in\nachieving Artificial General Intelligence. Large language and reasoning models\nhave the potential to aid in the systematic creation, selection, and validation\nof scientifically informed hypotheses. However, current foundation models often\nstruggle to produce scientific ideas that are both novel and feasible. One\nreason is the lack of a dedicated dataset that frames Scientific Hypothesis\nGeneration (SHG) as a Natural Language Generation (NLG) task. In this paper, we\nintroduce HypoGen, the first dataset of approximately 5500 structured\nproblem-hypothesis pairs extracted from top-tier computer science conferences\nstructured with a Bit-Flip-Spark schema, where the Bit is the conventional\nassumption, the Spark is the key insight or conceptual leap, and the Flip is\nthe resulting counterproposal. HypoGen uniquely integrates an explicit\nChain-of-Reasoning component that reflects the intellectual process from Bit to\nFlip. We demonstrate that framing hypothesis generation as conditional language\nmodelling, with the model fine-tuned on Bit-Flip-Spark and the\nChain-of-Reasoning (and where, at inference, we only provide the Bit), leads to\nimprovements in the overall quality of the hypotheses. Our evaluation employs\nautomated metrics and LLM judge rankings for overall quality assessment. We\nshow that by fine-tuning on our HypoGen dataset we improve the novelty,\nfeasibility, and overall quality of the generated hypotheses. The HypoGen\ndataset is publicly available at\nhuggingface.co/datasets/UniverseTBD/hypogen-dr1.\n","authors":["Charles O'Neill","Tirthankar Ghosal","Roberta Răileanu","Mike Walmsley","Thang Bui","Kevin Schawinski","Ioana Ciucă"],"pdf_url":"https://arxiv.org/pdf/2504.12976v1.pdf","comment":"9 pages, 2 figures. Comments welcome"},{"id":"http://arxiv.org/abs/2504.12972v1","updated":"2025-04-17T14:24:51Z","published":"2025-04-17T14:24:51Z","title":"Estimating Optimal Context Length for Hybrid Retrieval-augmented\n  Multi-document Summarization","summary":"  Recent advances in long-context reasoning abilities of language models led to\ninteresting applications in large-scale multi-document summarization. However,\nprior work has shown that these long-context models are not effective at their\nclaimed context windows. To this end, retrieval-augmented systems provide an\nefficient and effective alternative. However, their performance can be highly\nsensitive to the choice of retrieval context length. In this work, we present a\nhybrid method that combines retrieval-augmented systems with long-context\nwindows supported by recent language models. Our method first estimates the\noptimal retrieval length as a function of the retriever, summarizer, and\ndataset. On a randomly sampled subset of the dataset, we use a panel of LLMs to\ngenerate a pool of silver references. We use these silver references to\nestimate the optimal context length for a given RAG system configuration. Our\nresults on the multi-document summarization task showcase the effectiveness of\nour method across model classes and sizes. We compare against length estimates\nfrom strong long-context benchmarks such as RULER and HELMET. Our analysis also\nhighlights the effectiveness of our estimation method for very long-context LMs\nand its generalization to new classes of LMs.\n","authors":["Adithya Pratapa","Teruko Mitamura"],"pdf_url":"https://arxiv.org/pdf/2504.12972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12951v1","updated":"2025-04-17T13:52:48Z","published":"2025-04-17T13:52:48Z","title":"Are Retrials All You Need? Enhancing Large Language Model Reasoning\n  Without Verbalized Feedback","summary":"  Recent advancements in large language models (LLMs) have catalyzed the\ndevelopment of general-purpose autonomous agents, demonstrating remarkable\nperformance in complex reasoning tasks across various domains. This surge has\nspurred the evolution of a plethora of prompt-based reasoning frameworks. A\nrecent focus has been on iterative reasoning strategies that refine outputs\nthrough self-evaluation and verbalized feedback. However, these strategies\nrequire additional computational complexity to enable models to recognize and\ncorrect their mistakes, leading to a significant increase in their cost. In\nthis work, we introduce the concept of ``retrials without feedback'', an\nembarrassingly simple yet powerful mechanism for enhancing reasoning frameworks\nby allowing LLMs to retry problem-solving attempts upon identifying incorrect\nanswers. Unlike conventional iterative refinement methods, our method does not\nrequire explicit self-reflection or verbalized feedback, simplifying the\nrefinement process. Our findings indicate that simpler retrial-based approaches\noften outperform more sophisticated reasoning frameworks, suggesting that the\nbenefits of complex methods may not always justify their computational costs.\nBy challenging the prevailing assumption that more intricate reasoning\nstrategies inherently lead to better performance, our work offers new insights\ninto how simpler, more efficient approaches can achieve optimal results. So,\nare retrials all you need?\n","authors":["Nearchos Potamitis","Akhil Arora"],"pdf_url":"https://arxiv.org/pdf/2504.12951v1.pdf","comment":"8 pages, 16 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2405.06691"},{"id":"http://arxiv.org/abs/2504.12052v2","updated":"2025-04-17T13:49:27Z","published":"2025-04-16T13:06:24Z","title":"Bayesian dynamic borrowing considering semantic similarity between\n  outcomes for disproportionality analysis in FAERS","summary":"  We present a Bayesian dynamic borrowing (BDB) approach to enhance the\nquantitative identification of adverse events (AEs) in spontaneous reporting\nsystems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior\nwithin a Bayesian hierarchical model and incorporates semantic similarity\nmeasures (SSMs) to enable weighted information sharing from MedDRA Preferred\nTerms (PTs) that are clinically similar to the target PT. This continuous\nsimilarity-based borrowing addresses limitation of rigid hierarchical grouping\nin current disproportionality analysis (DPA).\n  Using data from the FDA Adverse Event Reporting System (FAERS) between 2015\nand 2019, we evaluate this approach - termed IC SSM - against standard\nInformation Component (IC) analysis and IC with borrowing at the MedDRA\nhigh-level group term (HLGT) level. A novel references set (PVLens), derived\nfrom FDA product label updates, enabled prospective evaluation of method\nperformance in identifying AEs prior to official labeling.\n  The IC SSM approach demonstrated improved sensitivity compared to both\ntraditional IC and HLGT-based borrowing, with minor trade-offs in F1 scores and\nYouden's index. IC SSM consistently identified more true positives and detected\nsignals over 5 months sooner than traditional IC. Despite a marginally lower\naggregate Youden's index, IC SSM showed higher performance in the early\npost-marketing period, providing more stable and relevant estimates than\nHLGT-based borrowing and traditional IC.\n  These findings support the use of SSM-informed Bayesian borrowing as a\nscalable and context-aware enhancement to traditional DPA methods. Future\nresearch should validate this approach across other datasets and explore\nadditional similarity metrics and Bayesian inference strategies using\ncase-level data.\n","authors":["François Haguinet","Jeffery L Painter","Gregory E Powell","Andrea Callegaro","Andrew Bate"],"pdf_url":"https://arxiv.org/pdf/2504.12052v2.pdf","comment":"30 pages, 7 figures, 5 supplementary figures"},{"id":"http://arxiv.org/abs/2504.12915v1","updated":"2025-04-17T13:05:14Z","published":"2025-04-17T13:05:14Z","title":"ConExion: Concept Extraction with Large Language Models","summary":"  In this paper, an approach for concept extraction from documents using\npre-trained large language models (LLMs) is presented. Compared with\nconventional methods that extract keyphrases summarizing the important\ninformation discussed in a document, our approach tackles a more challenging\ntask of extracting all present concepts related to the specific domain, not\njust the important ones. Through comprehensive evaluations of two widely used\nbenchmark datasets, we demonstrate that our method improves the F1 score\ncompared to state-of-the-art techniques. Additionally, we explore the potential\nof using prompts within these models for unsupervised concept extraction. The\nextracted concepts are intended to support domain coverage evaluation of\nontologies and facilitate ontology learning, highlighting the effectiveness of\nLLMs in concept extraction tasks. Our source code and datasets are publicly\navailable at https://github.com/ISE-FIZKarlsruhe/concept_extraction.\n","authors":["Ebrahim Norouzi","Sven Hertling","Harald Sack"],"pdf_url":"https://arxiv.org/pdf/2504.12915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12913v1","updated":"2025-04-17T13:02:44Z","published":"2025-04-17T13:02:44Z","title":"MAIN: Mutual Alignment Is Necessary for instruction tuning","summary":"  Instruction tuning has enabled large language models (LLMs) to achieve\nremarkable performance, but its success heavily depends on the availability of\nlarge-scale, high-quality instruction-response pairs. However, current methods\nfor scaling up data generation often overlook a crucial aspect: the alignment\nbetween instructions and responses. We hypothesize that high-quality\ninstruction-response pairs are not defined by the individual quality of each\ncomponent, but by the extent of their alignment with each other. To address\nthis, we propose a Mutual Alignment Framework (MAIN) that ensures coherence\nbetween the instruction and response through mutual constraints. Experiments\ndemonstrate that models such as LLaMA and Mistral, fine-tuned within this\nframework, outperform traditional methods across multiple benchmarks. This\napproach underscores the critical role of instruction-response alignment in\nenabling scalable and high-quality instruction tuning for LLMs.\n","authors":["Fanyi Yang","Jianfeng Liu","Xin Zhang","Haoyu Liu","Xixin Cao","Yuefeng Zhan","Hao Sun","Weiwei Deng","Feng Sun","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.12913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12911v1","updated":"2025-04-17T13:01:38Z","published":"2025-04-17T13:01:38Z","title":"Benchmarking Multi-National Value Alignment for Large Language Models","summary":"  Do Large Language Models (LLMs) hold positions that conflict with your\ncountry's values? Occasionally they do! However, existing works primarily focus\non ethical reviews, failing to capture the diversity of national values, which\nencompass broader policy, legal, and moral considerations. Furthermore, current\nbenchmarks that rely on spectrum tests using manually designed questionnaires\nare not easily scalable.\n  To address these limitations, we introduce NaVAB, a comprehensive benchmark\nto evaluate the alignment of LLMs with the values of five major nations: China,\nthe United States, the United Kingdom, France, and Germany. NaVAB implements a\nnational value extraction pipeline to efficiently construct value assessment\ndatasets. Specifically, we propose a modeling procedure with instruction\ntagging to process raw data sources, a screening process to filter\nvalue-related topics and a generation process with a Conflict Reduction\nmechanism to filter non-conflicting values.We conduct extensive experiments on\nvarious LLMs across countries, and the results provide insights into assisting\nin the identification of misaligned scenarios. Moreover, we demonstrate that\nNaVAB can be combined with alignment techniques to effectively reduce value\nconcerns by aligning LLMs' values with the target country.\n","authors":["Chengyi Ju","Weijie Shi","Chengzhong Liu","Jiaming Ji","Jipeng Zhang","Ruiyuan Zhang","Jia Zhu","Jiajie Xu","Yaodong Yang","Sirui Han","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2504.12911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12898v1","updated":"2025-04-17T12:39:25Z","published":"2025-04-17T12:39:25Z","title":"Information Gain-Guided Causal Intervention for Autonomous Debiasing\n  Large Language Models","summary":"  Despite significant progress, recent studies indicate that current large\nlanguage models (LLMs) may still capture dataset biases and utilize them during\ninference, leading to the poor generalizability of LLMs. However, due to the\ndiversity of dataset biases and the insufficient nature of bias suppression\nbased on in-context learning, the effectiveness of previous prior\nknowledge-based debiasing methods and in-context learning based automatic\ndebiasing methods is limited. To address these challenges, we explore the\ncombination of causal mechanisms with information theory and propose an\ninformation gain-guided causal intervention debiasing (IGCIDB) framework. This\nframework first utilizes an information gain-guided causal intervention method\nto automatically and autonomously balance the distribution of\ninstruction-tuning dataset. Subsequently, it employs a standard supervised\nfine-tuning process to train LLMs on the debiased dataset. Experimental results\nshow that IGCIDB can effectively debias LLM to improve its generalizability\nacross different tasks.\n","authors":["Zhouhao Sun","Xiao Ding","Li Du","Yunpeng Xu","Yixuan Ma","Yang Zhao","Bing Qin","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2504.12898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10090v2","updated":"2025-04-17T12:33:59Z","published":"2025-04-14T10:53:44Z","title":"CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography","summary":"  Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning.\n","authors":["I-Sheng Fang","Jun-Cheng Chen"],"pdf_url":"https://arxiv.org/pdf/2504.10090v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12891v1","updated":"2025-04-17T12:32:18Z","published":"2025-04-17T12:32:18Z","title":"Are AI agents the new machine translation frontier? Challenges and\n  opportunities of single- and multi-agent systems for multilingual digital\n  communication","summary":"  The rapid evolution of artificial intelligence (AI) has introduced AI agents\nas a disruptive paradigm across various industries, yet their application in\nmachine translation (MT) remains underexplored. This paper describes and\nanalyses the potential of single- and multi-agent systems for MT, reflecting on\nhow they could enhance multilingual digital communication. While single-agent\nsystems are well-suited for simpler translation tasks, multi-agent systems,\nwhich involve multiple specialized AI agents collaborating in a structured\nmanner, may offer a promising solution for complex scenarios requiring high\naccuracy, domain-specific knowledge, and contextual awareness. To demonstrate\nthe feasibility of multi-agent workflows in MT, we are conducting a pilot study\nin legal MT. The study employs a multi-agent system involving four specialized\nAI agents for (i) translation, (ii) adequacy review, (iii) fluency review, and\n(iv) final editing. Our findings suggest that multi-agent systems may have the\npotential to significantly improve domain-adaptability and contextual\nawareness, with superior translation quality to traditional MT or single-agent\nsystems. This paper also sets the stage for future research into multi-agent\napplications in MT, integration into professional translation workflows, and\nshares a demo of the system analyzed in the paper.\n","authors":["Vicent Briva-Iglesias"],"pdf_url":"https://arxiv.org/pdf/2504.12891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12882v1","updated":"2025-04-17T12:14:38Z","published":"2025-04-17T12:14:38Z","title":"ViClaim: A Multilingual Multilabel Dataset for Automatic Claim Detection\n  in Videos","summary":"  The growing influence of video content as a medium for communication and\nmisinformation underscores the urgent need for effective tools to analyze\nclaims in multilingual and multi-topic settings. Existing efforts in\nmisinformation detection largely focus on written text, leaving a significant\ngap in addressing the complexity of spoken text in video transcripts. We\nintroduce ViClaim, a dataset of 1,798 annotated video transcripts across three\nlanguages (English, German, Spanish) and six topics. Each sentence in the\ntranscripts is labeled with three claim-related categories: fact-check-worthy,\nfact-non-check-worthy, or opinion. We developed a custom annotation tool to\nfacilitate the highly complex annotation process. Experiments with\nstate-of-the-art multilingual language models demonstrate strong performance in\ncross-validation (macro F1 up to 0.896) but reveal challenges in generalization\nto unseen topics, particularly for distinct domains. Our findings highlight the\ncomplexity of claim detection in video transcripts. ViClaim offers a robust\nfoundation for advancing misinformation detection in video-based communication,\naddressing a critical gap in multimodal analysis.\n","authors":["Patrick Giedemann","Pius von Däniken","Jan Deriu","Alvaro Rodrigo","Anselmo Peñas","Mark Cieliebak"],"pdf_url":"https://arxiv.org/pdf/2504.12882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12879v1","updated":"2025-04-17T12:11:14Z","published":"2025-04-17T12:11:14Z","title":"Building Russian Benchmark for Evaluation of Information Retrieval\n  Models","summary":"  We introduce RusBEIR, a comprehensive benchmark designed for zero-shot\nevaluation of information retrieval (IR) models in the Russian language.\nComprising 17 datasets from various domains, it integrates adapted, translated,\nand newly created datasets, enabling systematic comparison of lexical and\nneural models. Our study highlights the importance of preprocessing for lexical\nmodels in morphologically rich languages and confirms BM25 as a strong baseline\nfor full-document retrieval. Neural models, such as mE5-large and BGE-M3,\ndemonstrate superior performance on most datasets, but face challenges with\nlong-document retrieval due to input size constraints. RusBEIR offers a\nunified, open-source framework that promotes research in Russian-language\ninformation retrieval.\n","authors":["Grigory Kovalev","Mikhail Tikhomirov","Evgeny Kozhevnikov","Max Kornilov","Natalia Loukachevitch"],"pdf_url":"https://arxiv.org/pdf/2504.12879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12867v1","updated":"2025-04-17T11:50:04Z","published":"2025-04-17T11:50:04Z","title":"EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text\n  Prompting","summary":"  Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and modality-of-thought (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released.\n","authors":["Guanrou Yang","Chen Yang","Qian Chen","Ziyang Ma","Wenxi Chen","Wen Wang","Tianrui Wang","Yifan Yang","Zhikang Niu","Wenrui Liu","Fan Yu","Zhihao Du","Zhifu Gao","ShiLiang Zhang","Xie Chen"],"pdf_url":"https://arxiv.org/pdf/2504.12867v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.00046v2","updated":"2025-04-17T11:29:06Z","published":"2025-03-30T22:53:52Z","title":"Multi-Stakeholder Disaster Insights from Social Media Using Large\n  Language Models","summary":"  In recent years, social media has emerged as a primary channel for users to\npromptly share feedback and issues during disasters and emergencies, playing a\nkey role in crisis management. While significant progress has been made in\ncollecting and analyzing social media content, there remains a pressing need to\nenhance the automation, aggregation, and customization of this data to deliver\nactionable insights tailored to diverse stakeholders, including the press,\npolice, EMS, and firefighters. This effort is essential for improving the\ncoordination of activities such as relief efforts, resource distribution, and\nmedia communication. This paper presents a methodology that leverages the\ncapabilities of LLMs to enhance disaster response and management. Our approach\ncombines classification techniques with generative AI to bridge the gap between\nraw user feedback and stakeholder-specific reports. Social media posts shared\nduring catastrophic events are analyzed with a focus on user-reported issues,\nservice interruptions, and encountered challenges. We employ full-spectrum\nLLMs, using analytical models like BERT for precise, multi-dimensional\nclassification of content type, sentiment, emotion, geolocation, and topic.\nGenerative models such as ChatGPT are then used to produce human-readable,\ninformative reports tailored to distinct audiences, synthesizing insights\nderived from detailed classifications. We compare standard approaches, which\nanalyze posts directly using prompts in ChatGPT, to our advanced method, which\nincorporates multi-dimensional classification, sub-event selection, and\ntailored report generation. Our methodology demonstrates superior performance\nin both quantitative metrics, such as text coherence scores and latent\nrepresentations, and qualitative assessments by automated tools and field\nexperts, delivering precise insights for diverse disaster response\nstakeholders.\n","authors":["Loris Belcastro","Cristian Cosentino","Fabrizio Marozzo","Merve Gündüz-Cüre","Sule Öztürk-Birim"],"pdf_url":"https://arxiv.org/pdf/2504.00046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.12599v7","updated":"2025-04-17T11:14:20Z","published":"2023-05-21T23:16:26Z","title":"Abstract Meaning Representation-Based Logic-Driven Data Augmentation for\n  Logical Reasoning","summary":"  Combining large language models with logical reasoning enhances their\ncapacity to address problems in a robust and reliable manner. Nevertheless, the\nintricate nature of logical reasoning poses challenges when gathering reliable\ndata from the web to build comprehensive training datasets, subsequently\naffecting performance on downstream tasks. To address this, we introduce a\nnovel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the\noriginal text into an Abstract Meaning Representation (AMR) graph, a structured\nsemantic representation that encapsulates the logical structure of the\nsentence, upon which operations are performed to generate logically modified\nAMR graphs. The modified AMR graphs are subsequently converted back into text\nto create augmented data. Notably, our methodology is architecture-agnostic and\nenhances both generative large language models, such as GPT-3.5 and GPT-4,\nthrough prompt augmentation, and discriminative large language models through\ncontrastive learning with logic-driven data augmentation. Empirical evidence\nunderscores the efficacy of our proposed method with improvement in performance\nacross seven downstream tasks, such as reading comprehension requiring logical\nreasoning, textual entailment, and natural language inference. Furthermore, our\nmethod leads on the ReClor leaderboard at\nhttps://eval.ai/web/challenges/challenge-page/503/leaderboard/1347. The source\ncode and data are publicly available at\nhttps://github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning.\n","authors":["Qiming Bao","Alex Yuxuan Peng","Zhenyun Deng","Wanjun Zhong","Gael Gendron","Timothy Pistotti","Neset Tan","Nathan Young","Yang Chen","Yonghua Zhu","Paul Denny","Michael Witbrock","Jiamou Liu"],"pdf_url":"https://arxiv.org/pdf/2305.12599v7.pdf","comment":"21 pages, 8 figures, the Findings of ACL 2024"},{"id":"http://arxiv.org/abs/2207.14000v4","updated":"2025-04-17T11:11:51Z","published":"2022-07-28T10:44:46Z","title":"Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study\n  on Out-of-Distribution Generalisation","summary":"  Combining deep learning with symbolic logic reasoning aims to capitalize on\nthe success of both fields and is drawing increasing attention. Inspired by\nDeepLogic, an end-to-end model trained to perform inference on logic programs,\nwe introduce IMA-GloVe-GA, an iterative neural inference network for multi-step\nreasoning expressed in natural language. In our model, reasoning is performed\nusing an iterative memory neural network based on RNN with a gated attention\nmechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES\nV1 and CONCEPTRULES V2. Experimental results show DeepLogic with gated\nattention can achieve higher test accuracy than DeepLogic and other RNN\nbaseline models. Our model achieves better out-of-distribution generalisation\nthan RoBERTa-Large when the rules have been shuffled. Furthermore, to address\nthe issue of unbalanced distribution of reasoning depths in the current\nmulti-step reasoning datasets, we develop PARARULE-Plus, a large dataset with\nmore examples that require deeper reasoning steps. Experimental results show\nthat the addition of PARARULE-Plus can increase the model's performance on\nexamples requiring deeper reasoning depths. The source code and data are\navailable at\nhttps://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.\n","authors":["Qiming Bao","Alex Yuxuan Peng","Tim Hartill","Neset Tan","Zhenyun Deng","Michael Witbrock","Jiamou Liu"],"pdf_url":"https://arxiv.org/pdf/2207.14000v4.pdf","comment":"10 pages, 3 figures, The 2nd International Joint Conference on\n  Learning & Reasoning and 16th International Workshop on Neural-Symbolic\n  Learning and Reasoning (IJCLR-NeSy 2022)"},{"id":"http://arxiv.org/abs/2504.12845v1","updated":"2025-04-17T11:02:35Z","published":"2025-04-17T11:02:35Z","title":"Can LLMs reason over extended multilingual contexts? Towards\n  long-context evaluation beyond retrieval and haystacks","summary":"  Existing multilingual long-context benchmarks, often based on the popular\nneedle-in-a-haystack test, primarily evaluate a model's ability to locate\nspecific information buried within irrelevant texts. However, such a\nretrieval-centric approach is myopic and inherently limited, as successful\nrecall alone does not indicate a model's capacity to reason over extended\ncontexts. Moreover, these benchmarks are susceptible to data leakage,\nshort-circuiting, and risk making the evaluation a priori identifiable. To\naddress these limitations, we introduce MLRBench, a new synthetic benchmark for\nmultilingual long-context reasoning. Unlike existing benchmarks, MLRBench goes\nbeyond surface-level retrieval by including tasks that assess multi-hop\ninference, aggregation, and epistemic reasoning. Spanning seven languages,\nMLRBench is designed to be parallel, resistant to leakage, and scalable to\narbitrary context lengths. Our extensive experiments with an open-weight large\nlanguage model (LLM) reveal a pronounced gap between high- and low-resource\nlanguages, particularly for tasks requiring the model to aggregate multiple\nfacts or predict the absence of information. We also find that, in multilingual\nsettings, LLMs effectively utilize less than 30% of their claimed context\nlength. Although off-the-shelf Retrieval Augmented Generation helps alleviate\nthis to a certain extent, it does not solve the long-context problem. We\nopen-source MLRBench to enable future research in improved evaluation and\ntraining of multilingual LLMs.\n","authors":["Amey Hengle","Prasoon Bajpai","Soham Dan","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2504.12845v1.pdf","comment":"33 Pages in Total - 23 (Main Manuscript) + 10 (Appendix)"},{"id":"http://arxiv.org/abs/2504.12816v1","updated":"2025-04-17T10:21:15Z","published":"2025-04-17T10:21:15Z","title":"SMARTe: Slot-based Method for Accountable Relational Triple extraction","summary":"  Relational Triple Extraction (RTE) is a fundamental task in Natural Language\nProcessing (NLP). However, prior research has primarily focused on optimizing\nmodel performance, with limited efforts to understand the internal mechanisms\ndriving these models. Many existing methods rely on complex preprocessing to\ninduce specific interactions, often resulting in opaque systems that may not\nfully align with their theoretical foundations. To address these limitations,\nwe propose SMARTe: a Slot-based Method for Accountable Relational Triple\nextraction. SMARTe introduces intrinsic interpretability through a slot\nattention mechanism and frames the task as a set prediction problem. Slot\nattention consolidates relevant information into distinct slots, ensuring all\npredictions can be explicitly traced to learned slot representations and the\ntokens contributing to each predicted relational triple. While emphasizing\ninterpretability, SMARTe achieves performance comparable to state-of-the-art\nmodels. Evaluations on the NYT and WebNLG datasets demonstrate that adding\ninterpretability does not compromise performance. Furthermore, we conducted\nqualitative assessments to showcase the explanations provided by SMARTe, using\nattention heatmaps that map to their respective tokens. We conclude with a\ndiscussion of our findings and propose directions for future research.\n","authors":["Xue Wen Tan","Stanley Kok"],"pdf_url":"https://arxiv.org/pdf/2504.12816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12805v1","updated":"2025-04-17T10:10:25Z","published":"2025-04-17T10:10:25Z","title":"Assesing LLMs in Art Contexts: Critique Generation and Theory of Mind\n  Evaluation","summary":"  This study explored how large language models (LLMs) perform in two areas\nrelated to art: writing critiques of artworks and reasoning about mental states\n(Theory of Mind, or ToM) in art-related situations. For the critique generation\npart, we built a system that combines Noel Carroll's evaluative framework with\na broad selection of art criticism theories. The model was prompted to first\nwrite a full-length critique and then shorter, more coherent versions using a\nstep-by-step prompting process. These AI-generated critiques were then compared\nwith those written by human experts in a Turing test-style evaluation. In many\ncases, human subjects had difficulty telling which was which, and the results\nsuggest that LLMs can produce critiques that are not only plausible in style\nbut also rich in interpretation, as long as they are carefully guided. In the\nsecond part, we introduced new simple ToM tasks based on situations involving\ninterpretation, emotion, and moral tension, which can appear in the context of\nart. These go beyond standard false-belief tests and allow for more complex,\nsocially embedded forms of reasoning. We tested 41 recent LLMs and found that\ntheir performance varied across tasks and models. In particular, tasks that\ninvolved affective or ambiguous situations tended to reveal clearer\ndifferences. Taken together, these results help clarify how LLMs respond to\ncomplex interpretative challenges, revealing both their cognitive limitations\nand potential. While our findings do not directly contradict the so-called\nGenerative AI Paradox--the idea that LLMs can produce expert-like output\nwithout genuine understanding--they suggest that, depending on how LLMs are\ninstructed, such as through carefully designed prompts, these models may begin\nto show behaviors that resemble understanding more closely than we might\nassume.\n","authors":["Takaya Arita","Wenxian Zheng","Reiji Suzuki","Fuminori Akiba"],"pdf_url":"https://arxiv.org/pdf/2504.12805v1.pdf","comment":"30 pages, 13 figures, 1 table"},{"id":"http://arxiv.org/abs/2504.06562v2","updated":"2025-04-17T09:49:43Z","published":"2025-04-09T03:51:53Z","title":"FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion","summary":"  Heterogeneous model fusion enhances the performance of LLMs by integrating\nthe knowledge and capabilities of multiple structurally diverse models.\nHowever, existing approaches often rely solely on selecting the best output for\neach prompt from source models, which underutilizes their full potential due to\nlimited source knowledge and results in sparse optimization signals. To address\nthis limitation, we propose FuseRL, a novel two-stage framework comprising\nFuseSFT and FusePO to maximize the utilization of source LLMs. FuseSFT\nestablishes a robust initialization by integrating the strengths of\nheterogeneous source models through weighted supervised fine-tuning (SFT) on\ndiverse outputs for each prompt. FusePO optimizes weighted preferences based on\nthe outputs of multiple source models to enable superior alignment performance.\nExtensive experiments demonstrate the effectiveness of our framework across\nvarious preference alignment methods, including RLOO, DPO, and SimPO. Using\nLlama-3.1-8B-Instruct as the target model, our approach achieves\nstate-of-the-art performance among 8B LLMs on the AlpacaEval-2 and Arena-Hard\nbenchmarks. Further analysis suggests that FuseSFT regularizes the training\nprocess to reduce overfitting, while FusePO introduces dense and diverse\nsignals for preference optimization.\n","authors":["Longguang Zhong","Fanqi Wan","Ziyi Yang","Guosheng Liang","Tianyuan Shi","Xiaojun Quan"],"pdf_url":"https://arxiv.org/pdf/2504.06562v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12778v1","updated":"2025-04-17T09:18:58Z","published":"2025-04-17T09:18:58Z","title":"Towards Lossless Token Pruning in Late-Interaction Retrieval Models","summary":"  Late interaction neural IR models like ColBERT offer a competitive\neffectiveness-efficiency trade-off across many benchmarks. However, they\nrequire a huge memory space to store the contextual representation for all the\ndocument tokens. Some works have proposed using either heuristics or\nstatistical-based techniques to prune tokens from each document. This however\ndoesn't guarantee that the removed tokens have no impact on the retrieval\nscore. Our work uses a principled approach to define how to prune tokens\nwithout impacting the score between a document and a query. We introduce three\nregularization losses, that induce a solution with high pruning ratios, as well\nas two pruning strategies. We study them experimentally (in and out-domain),\nshowing that we can preserve ColBERT's performance while using only 30\\% of the\ntokens.\n","authors":["Yuxuan Zong","Benjamin Piwowarski"],"pdf_url":"https://arxiv.org/pdf/2504.12778v1.pdf","comment":"Accepted at SIGIR 2025 Full Paper Track"},{"id":"http://arxiv.org/abs/2504.12773v1","updated":"2025-04-17T09:13:46Z","published":"2025-04-17T09:13:46Z","title":"Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via\n  Symbolic-Neural Integration","summary":"  Recent advances in Multimodal Large Language Models (MLLMs) have achieved\nremarkable progress in general domains and demonstrated promise in multimodal\nmathematical reasoning. However, applying MLLMs to geometry problem solving\n(GPS) remains challenging due to lack of accurate step-by-step solution data\nand severe hallucinations during reasoning. In this paper, we propose GeoGen, a\npipeline that can automatically generates step-wise reasoning paths for\ngeometry diagrams. By leveraging the precise symbolic reasoning,\n\\textbf{GeoGen} produces large-scale, high-quality question-answer pairs. To\nfurther enhance the logical reasoning ability of MLLMs, we train\n\\textbf{GeoLogic}, a Large Language Model (LLM) using synthetic data generated\nby GeoGen. Serving as a bridge between natural language and symbolic systems,\nGeoLogic enables symbolic tools to help verifying MLLM outputs, making the\nreasoning process more rigorous and alleviating hallucinations. Experimental\nresults show that our approach consistently improves the performance of MLLMs,\nachieving remarkable results on benchmarks for geometric reasoning tasks. This\nimprovement stems from our integration of the strengths of LLMs and symbolic\nsystems, which enables a more reliable and interpretable approach for the GPS\ntask. Codes are available at https://github.com/ycpNotFound/GeoGen.\n","authors":["Yicheng Pan","Zhenrong Zhang","Pengfei Hu","Jiefeng Ma","Jun Du","Jianshu Zhang","Quan Liu","Jianqing Gao","Feng Ma"],"pdf_url":"https://arxiv.org/pdf/2504.12773v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.12767v1","updated":"2025-04-17T09:05:50Z","published":"2025-04-17T09:05:50Z","title":"Out of Sight Out of Mind, Out of Sight Out of Mind: Measuring Bias in\n  Language Models Against Overlooked Marginalized Groups in Regional Contexts","summary":"  We know that language models (LMs) form biases and stereotypes of minorities,\nleading to unfair treatments of members of these groups, thanks to research\nmainly in the US and the broader English-speaking world. As the negative\nbehavior of these models has severe consequences for society and individuals,\nindustry and academia are actively developing methods to reduce the bias in\nLMs. However, there are many under-represented groups and languages that have\nbeen overlooked so far. This includes marginalized groups that are specific to\nindividual countries and regions in the English speaking and Western world, but\ncrucially also almost all marginalized groups in the rest of the world. The UN\nestimates, that between 600 million to 1.2 billion people worldwide are members\nof marginalized groups and in need for special protection. If we want to\ndevelop inclusive LMs that work for everyone, we have to broaden our\nunderstanding to include overlooked marginalized groups and low-resource\nlanguages and dialects.\n  In this work, we contribute to this effort with the first study investigating\noffensive stereotyping bias in 23 LMs for 270 marginalized groups from Egypt,\nthe remaining 21 Arab countries, Germany, the UK, and the US. Additionally, we\ninvestigate the impact of low-resource languages and dialects on the study of\nbias in LMs, demonstrating the limitations of current bias metrics, as we\nmeasure significantly higher bias when using the Egyptian Arabic dialect versus\nModern Standard Arabic. Our results show, LMs indeed show higher bias against\nmany marginalized groups in comparison to dominant groups. However, this is not\nthe case for Arabic LMs, where the bias is high against both marginalized and\ndominant groups in relation to religion and ethnicity.\n  Our results also show higher intersectional bias against Non-binary, LGBTQIA+\nand Black women.\n","authors":["Fatma Elsafoury","David Hartmann"],"pdf_url":"https://arxiv.org/pdf/2504.12767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10305v4","updated":"2025-04-17T08:34:42Z","published":"2023-09-19T04:13:22Z","title":"Baichuan 2: Open Large-scale Language Models","summary":"  Large language models (LLMs) have demonstrated remarkable performance on a\nvariety of natural language tasks based on just a few examples of natural\nlanguage instructions, reducing the need for extensive feature engineering.\nHowever, most powerful LLMs are closed-source or limited in their capability\nfor languages other than English. In this technical report, we present Baichuan\n2, a series of large-scale multilingual language models containing 7 billion\nand 13 billion parameters, trained from scratch, on 2.6 trillion tokens.\nBaichuan 2 matches or outperforms other open-source models of similar size on\npublic benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan\n2 excels in vertical domains such as medicine and law. We will release all\npre-training model checkpoints to benefit the research community in better\nunderstanding the training dynamics of Baichuan 2.\n","authors":["Aiyuan Yang","Bin Xiao","Bingning Wang","Borong Zhang","Ce Bian","Chao Yin","Chenxu Lv","Da Pan","Dian Wang","Dong Yan","Fan Yang","Fei Deng","Feng Wang","Feng Liu","Guangwei Ai","Guosheng Dong","Haizhou Zhao","Hang Xu","Haoze Sun","Hongda Zhang","Hui Liu","Jiaming Ji","Jian Xie","JunTao Dai","Kun Fang","Lei Su","Liang Song","Lifeng Liu","Liyun Ru","Luyao Ma","Mang Wang","Mickel Liu","MingAn Lin","Nuolan Nie","Peidong Guo","Ruiyang Sun","Tao Zhang","Tianpeng Li","Tianyu Li","Wei Cheng","Weipeng Chen","Xiangrong Zeng","Xiaochuan Wang","Xiaoxi Chen","Xin Men","Xin Yu","Xuehai Pan","Yanjun Shen","Yiding Wang","Yiyu Li","Youxin Jiang","Yuchen Gao","Yupeng Zhang","Zenan Zhou","Zhiying Wu"],"pdf_url":"https://arxiv.org/pdf/2309.10305v4.pdf","comment":"Baichuan 2 technical report. Github:\n  https://github.com/baichuan-inc/Baichuan2"},{"id":"http://arxiv.org/abs/2410.10291v4","updated":"2025-04-17T08:31:14Z","published":"2024-10-14T08:45:35Z","title":"Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal\n  Perspective","summary":"  Accurate interpretation and visualization of human instructions are crucial\nfor text-to-image (T2I) synthesis. However, current models struggle to capture\nsemantic variations from word order changes, and existing evaluations, relying\non indirect metrics like text-image similarity, fail to reliably assess these\nchallenges. This often obscures poor performance on complex or uncommon\nlinguistic patterns by the focus on frequent word combinations. To address\nthese deficiencies, we propose a novel metric called SemVarEffect and a\nbenchmark named SemVarBench, designed to evaluate the causality between\nsemantic variations in inputs and outputs in T2I synthesis. Semantic variations\nare achieved through two types of linguistic permutations, while avoiding\neasily predictable literal variations. Experiments reveal that the\nCogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.\nSemantic variations in object relations are less understood than attributes,\nscoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in\nUNet or Transformers plays a crucial role in handling semantic variations, a\nfactor previously overlooked by a focus on textual encoders. Our work\nestablishes an effective evaluation framework that advances the T2I synthesis\ncommunity's exploration of human instruction understanding. Our benchmark and\ncode are available at https://github.com/zhuxiangru/SemVarBench .\n","authors":["Xiangru Zhu","Penglei Sun","Yaoxian Song","Yanghua Xiao","Zhixu Li","Chengyu Wang","Jun Huang","Bei Yang","Xiaoxiao Xu"],"pdf_url":"https://arxiv.org/pdf/2410.10291v4.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2504.12737v1","updated":"2025-04-17T08:27:02Z","published":"2025-04-17T08:27:02Z","title":"Chinese-Vicuna: A Chinese Instruction-following Llama-based Model","summary":"  Chinese-Vicuna is an open-source, resource-efficient language model designed\nto bridge the gap in Chinese instruction-following capabilities by fine-tuning\nMeta's LLaMA architecture using Low-Rank Adaptation (LoRA). Targeting\nlow-resource environments, it enables cost-effective deployment on consumer\nGPUs (e.g., RTX-2080Ti for 7B models) and supports domain-specific adaptation\nin fields like healthcare and law. By integrating hybrid datasets (BELLE and\nGuanaco) and 4-bit quantization (QLoRA), the model achieves competitive\nperformance in tasks such as translation, code generation, and domain-specific\nQ\\&A. The project provides a comprehensive toolkit for model conversion, CPU\ninference, and multi-turn dialogue interfaces, emphasizing accessibility for\nresearchers and developers. Evaluations indicate competitive performance across\nmedical tasks, multi-turn dialogue coherence, and real-time legal updates.\nChinese-Vicuna's modular design, open-source ecosystem, and community-driven\nenhancements position it as a versatile foundation for Chinese LLM\napplications.\n","authors":["Chenghao Fan","Zhenyi Lu","Jie Tian"],"pdf_url":"https://arxiv.org/pdf/2504.12737v1.pdf","comment":"Chinese-Vicuna Technique Report"},{"id":"http://arxiv.org/abs/2504.12734v1","updated":"2025-04-17T08:18:09Z","published":"2025-04-17T08:18:09Z","title":"Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning\n  Across Diverse Structured Knowledge","summary":"  Unified Structured Knowledge Reasoning (USKR) aims to answer natural language\nquestions (NLQs) by using structured sources such as tables, databases, and\nknowledge graphs in a unified way. Existing USKR methods either rely on\nemploying task-specific strategies or custom-defined representations, which\nstruggle to leverage the knowledge transfer between different SKR tasks or\nalign with the prior of LLMs, thereby limiting their performance. This paper\nproposes a novel USKR framework named \\textsc{Pandora}, which takes advantage\nof \\textsc{Python}'s \\textsc{Pandas} API to construct a unified knowledge\nrepresentation for alignment with LLM pre-training. It employs an LLM to\ngenerate textual reasoning steps and executable Python code for each question.\nDemonstrations are drawn from a memory of training examples that cover various\nSKR tasks, facilitating knowledge transfer. Extensive experiments on four\nbenchmarks involving three SKR tasks demonstrate that \\textsc{Pandora}\noutperforms existing unified frameworks and competes effectively with\ntask-specific methods.\n","authors":["Yongrui Chen","Junhao He","Linbo Fu","Shenyu Zhang","Rihui Jin","Xinbang Dai","Jiaqi Li","Dehai Min","Nan Hu","Yuxin Zhang","Guilin Qi","Yi Huang","Tongtong Wu"],"pdf_url":"https://arxiv.org/pdf/2504.12734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12723v1","updated":"2025-04-17T07:57:31Z","published":"2025-04-17T07:57:31Z","title":"KODIS: A Multicultural Dispute Resolution Dialogue Corpus","summary":"  We present KODIS, a dyadic dispute resolution corpus containing thousands of\ndialogues from over 75 countries. Motivated by a theoretical model of culture\nand conflict, participants engage in a typical customer service dispute\ndesigned by experts to evoke strong emotions and conflict. The corpus contains\na rich set of dispositional, process, and outcome measures. The initial\nanalysis supports theories of how anger expressions lead to escalatory spirals\nand highlights cultural differences in emotional expression. We make this\ncorpus and data collection framework available to the community.\n","authors":["James Hale","Sushrita Rakshit","Kushal Chawla","Jeanne M. Brett","Jonathan Gratch"],"pdf_url":"https://arxiv.org/pdf/2504.12723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11257v3","updated":"2025-04-17T07:13:52Z","published":"2025-04-15T14:56:21Z","title":"UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction\n  Synthesis","summary":"  Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation. In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://colmon46.github.io/i2e-bench-leaderboard/ .\n","authors":["Xinyi Liu","Xiaoyi Zhang","Ziyun Zhang","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2504.11257v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01113v2","updated":"2025-04-17T06:57:50Z","published":"2024-12-02T04:35:54Z","title":"Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in\n  Multi-Step Arithmetic Reasoning","summary":"  This study investigates the internal reasoning process of language models\nduring arithmetic multi-step reasoning, motivated by the question of when they\ninternally form their answers during reasoning. Particularly, we inspect\nwhether the answer is determined before or after chain-of-thought (CoT) begins\nto determine whether models follow a post-hoc Think-to-Talk mode or a\nstep-by-step Talk-to-Think mode of explanation. Through causal probing\nexperiments in controlled arithmetic reasoning tasks, we found systematic\ninternal reasoning patterns across models in our case study; for example,\nsingle-step subproblems are solved before CoT begins, and more complicated\nmulti-step calculations are performed during CoT.\n","authors":["Keito Kudo","Yoichi Aoki","Tatsuki Kuribayashi","Shusaku Sone","Masaya Taniguchi","Ana Brassard","Keisuke Sakaguchi","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2412.01113v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12691v1","updated":"2025-04-17T06:34:45Z","published":"2025-04-17T06:34:45Z","title":"Why and How LLMs Hallucinate: Connecting the Dots with Subsequence\n  Associations","summary":"  Large language models (LLMs) frequently generate hallucinations-content that\ndeviates from factual accuracy or provided context-posing challenges for\ndiagnosis due to the complex interplay of underlying causes. This paper\nintroduces a subsequence association framework to systematically trace and\nunderstand hallucinations. Our key insight is that hallucinations arise when\ndominant hallucinatory associations outweigh faithful ones. Through theoretical\nand empirical analyses, we demonstrate that decoder-only transformers\neffectively function as subsequence embedding models, with linear layers\nencoding input-output associations. We propose a tracing algorithm that\nidentifies causal subsequences by analyzing hallucination probabilities across\nrandomized input contexts. Experiments show our method outperforms standard\nattribution techniques in identifying hallucination causes and aligns with\nevidence from the model's training corpus. This work provides a unified\nperspective on hallucinations and a robust framework for their tracing and\nanalysis.\n","authors":["Yiyou Sun","Yu Gai","Lijie Chen","Abhilasha Ravichander","Yejin Choi","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2504.12691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12687v1","updated":"2025-04-17T06:29:28Z","published":"2025-04-17T06:29:28Z","title":"Data-efficient LLM Fine-tuning for Code Generation","summary":"  Large language models (LLMs) have demonstrated significant potential in code\ngeneration tasks. However, there remains a performance gap between open-source\nand closed-source models. To address this gap, existing approaches typically\ngenerate large amounts of synthetic data for fine-tuning, which often leads to\ninefficient training. In this work, we propose a data selection strategy in\norder to improve the effectiveness and efficiency of training for code-based\nLLMs. By prioritizing data complexity and ensuring that the sampled subset\naligns with the distribution of the original dataset, our sampling strategy\neffectively selects high-quality data. Additionally, we optimize the\ntokenization process through a \"dynamic pack\" technique, which minimizes\npadding tokens and reduces computational resource consumption. Experimental\nresults show that when training on 40% of the OSS-Instruct dataset, the\nDeepSeek-Coder-Base-6.7B model achieves an average performance of 66.9%,\nsurpassing the 66.1% performance with the full dataset. Moreover, training time\nis reduced from 47 minutes to 34 minutes, and the peak GPU memory decreases\nfrom 61.47 GB to 42.72 GB during a single epoch. Similar improvements are\nobserved with the CodeLlama-Python-7B model on the Evol-Instruct dataset. By\noptimizing both data selection and tokenization, our approach not only improves\nmodel performance but also improves training efficiency.\n","authors":["Weijie Lv","Xuan Xia","Sheng-Jun Huang"],"pdf_url":"https://arxiv.org/pdf/2504.12687v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2408.02193"},{"id":"http://arxiv.org/abs/2504.11793v2","updated":"2025-04-17T06:24:14Z","published":"2025-04-16T05:59:29Z","title":"Selective Attention Federated Learning: Improving Privacy and Efficiency\n  for Clinical Text Classification","summary":"  Federated Learning (FL) faces major challenges regarding communication\noverhead and model privacy when training large language models (LLMs),\nespecially in healthcare applications. To address these, we introduce Selective\nAttention Federated Learning (SAFL), a novel approach that dynamically\nfine-tunes only those transformer layers identified as attention-critical. By\nemploying attention patterns to determine layer importance, SAFL significantly\nreduces communication bandwidth and enhances differential privacy resilience.\nEvaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and\nMIMIC-III discharge summaries) demonstrate that SAFL achieves competitive\nperformance with centralized models while substantially improving communication\nefficiency and privacy preservation.\n","authors":["Yue Li","Lihong Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.11793v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12682v1","updated":"2025-04-17T06:16:40Z","published":"2025-04-17T06:16:40Z","title":"WebLists: Extracting Structured Information From Complex Interactive\n  Websites Using Executable LLM Agents","summary":"  Most recent web agent research has focused on navigation and transaction\ntasks, with little emphasis on extracting structured data at scale. We present\nWebLists, a benchmark of 200 data-extraction tasks across four common business\nand enterprise use-cases. Each task requires an agent to navigate to a webpage,\nconfigure it appropriately, and extract complete datasets with well-defined\nschemas. We show that both LLMs with search capabilities and SOTA web agents\nstruggle with these tasks, with a recall of 3% and 31%, respectively, despite\nhigher performance on question-answering tasks.\n  To address this challenge, we propose BardeenAgent, a novel framework that\nenables web agents to convert their execution into repeatable programs, and\nreplay them at scale across pages with similar structure. BardeenAgent is also\nthe first LLM agent to take advantage of the regular structure of HTML. In\nparticular BardeenAgent constructs a generalizable CSS selector to capture all\nrelevant items on the page, then fits the operations to extract the data.\n  On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more\nthan doubling the performance of SOTA web agents, and reducing cost per output\nrow by 3x.\n","authors":["Arth Bohra","Manvel Saroyan","Danil Melkozerov","Vahe Karufanyan","Gabriel Maher","Pascal Weinberger","Artem Harutyunyan","Giovanni Campagna"],"pdf_url":"https://arxiv.org/pdf/2504.12682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12681v1","updated":"2025-04-17T06:16:32Z","published":"2025-04-17T06:16:32Z","title":"GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in\n  LLMs","summary":"  Large Language Models (LLMs) trained on extensive datasets often learn\nsensitive information, which raises significant social and legal concerns under\nprinciples such as the \"Right to be forgotten.\" Retraining entire models from\nscratch to remove undesired information is both costly and impractical.\nFurthermore, existing single-domain unlearning methods fail to address\nmulti-domain scenarios, where knowledge is interwoven across domains such as\nprivacy and copyright, creating overlapping representations that lead to\nexcessive knowledge removal or degraded performance. To tackle these issues, we\npropose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain\nunlearning framework. GRAIL leverages gradient information from multiple\ndomains to precisely distinguish the unlearning scope from the retention scope,\nand applies an adaptive parameter-wise localization strategy to selectively\nremove targeted knowledge while preserving critical parameters for each domain.\nExperimental results on unlearning benchmarks show that GRAIL achieves\nunlearning success on par with the existing approaches, while also\ndemonstrating up to 17% stronger knowledge retention success compared to the\nprevious state-of-art method. Our findings establish a new paradigm for\neffectively managing and regulating sensitive information in large-scale\npre-trained language models.\n","authors":["Kun-Woo Kim","Ji-Hoon Park","Ju-Min Han","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2504.12681v1.pdf","comment":"Accepted by IJCNN 2025"},{"id":"http://arxiv.org/abs/2504.12673v1","updated":"2025-04-17T06:05:35Z","published":"2025-04-17T06:05:35Z","title":"ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented\n  Language Models","summary":"  Abstractive compression utilizes smaller langauge models to condense\nquery-relevant context, reducing computational costs in retrieval-augmented\ngeneration (RAG). However,retrieved documents often include information that is\neither irrelevant to answering the query or misleading due to factual incorrect\ncontent, despite having high relevance scores. This behavior indicates that\nabstractive compressors are more likely to omit important information essential\nfor the correct answer, especially in long contexts where attention dispersion\noccurs. To address this issue, we categorize retrieved documents in a more\nfine-grained manner and propose Abstractive Compression Robust against Noise\n(ACoRN), which introduces two novel training steps. First, we use offline data\naugmentation on the training dataset to enhance compressor robustness against\ntwo distinct types of retrieval noise. Second, since the language modelbased\ncompressor cannot fully utilize information from multiple retrieved documents\nand exhibits positional bias, we perform finetuning to generate summaries\ncentered around key information that directly supports the correct answer. Our\nexperiments demonstrate that T5-large, trained with ACoRN as a compressor,\nimproves EM and F1 scores while preserving the answer string, which could serve\nas direct evidence. ACoRN excels on datasets with many accuracy-reducing\ndocuments, making it highly useful in real-world scenarios.\n","authors":["Singon Kim","Gunho Jung","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2504.12673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12663v1","updated":"2025-04-17T05:50:13Z","published":"2025-04-17T05:50:13Z","title":"Persona-judge: Personalized Alignment of Large Language Models via\n  Token-level Self-judgment","summary":"  Aligning language models with human preferences presents significant\nchallenges, particularly in achieving personalization without incurring\nexcessive computational costs. Existing methods rely on reward signals and\nadditional annotated data, limiting their scalability and adaptability to\ndiverse human values. To address these challenges, we introduce Persona-judge,\na novel discriminative paradigm that enables training-free personalized\nalignment with unseen preferences. Instead of optimizing policy parameters\nthrough external reward feedback, Persona-judge leverages the intrinsic\npreference judgment capabilities of the model. Specifically, a draft model\ngenerates candidate tokens conditioned on a given preference, while a judge\nmodel, embodying another preference, cross-validates the predicted tokens\nwhether to be accepted. Experimental results demonstrate that Persona-judge,\nusing the inherent preference evaluation mechanisms of the model, offers a\nscalable and computationally efficient solution to personalized alignment,\npaving the way for more adaptive customized alignment.\n","authors":["Xiaotian Zhang","Ruizhe Chen","Yang Feng","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2504.12663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12661v1","updated":"2025-04-17T05:46:41Z","published":"2025-04-17T05:46:41Z","title":"VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven\n  Prompt Optimization","summary":"  Aligning Vision-Language Models (VLMs) with safety standards is essential to\nmitigate risks arising from their multimodal complexity, where integrating\nvision and language unveils subtle threats beyond the reach of conventional\nsafeguards. Inspired by the insight that reasoning across modalities is key to\npreempting intricate vulnerabilities, we propose a novel direction for VLM\nsafety: multimodal reasoning-driven prompt rewriting. To this end, we introduce\nVLMGuard-R1, a proactive framework that refines user inputs through a\nreasoning-guided rewriter, dynamically interpreting text-image interactions to\ndeliver refined prompts that bolster safety across diverse VLM architectures\nwithout altering their core parameters. To achieve this, we devise a\nthree-stage reasoning pipeline to synthesize a dataset that trains the rewriter\nto infer subtle threats, enabling tailored, actionable responses over generic\nrefusals. Extensive experiments across three benchmarks with five VLMs reveal\nthat VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1\nachieves a remarkable 43.59\\% increase in average safety across five models on\nthe SIUO benchmark.\n","authors":["Menglan Chen","Xianghe Pang","Jingjing Dong","WenHao Wang","Yaxin Du","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2504.12661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12637v1","updated":"2025-04-17T04:46:57Z","published":"2025-04-17T04:46:57Z","title":"Scaling Instruction-Tuned LLMs to Million-Token Contexts via\n  Hierarchical Synthetic Data Generation","summary":"  Large Language Models (LLMs) struggle with long-context reasoning, not only\ndue to the quadratic scaling of computational complexity with sequence length\nbut also because of the scarcity and expense of annotating long-context data.\nThere has been barely any open-source work that systematically ablates\nlong-context data, nor is there any openly available instruction tuning dataset\nwith contexts surpassing 100K tokens. To bridge this gap, we introduce a novel\npost-training synthetic data generation strategy designed to efficiently extend\nthe context window of LLMs while preserving their general task performance. Our\napproach scalably extends to arbitrarily long context lengths, unconstrained by\nthe length of available real-world data, which effectively addresses the\nscarcity of raw long-context data. Through a step-by-step rotary position\nembedding (RoPE) scaling training strategy, we demonstrate that our model, with\na context length of up to 1M tokens, performs well on the RULER benchmark and\nInfiniteBench and maintains robust performance on general language tasks.\n","authors":["Linda He","Jue Wang","Maurice Weber","Shang Zhu","Ben Athiwaratkun","Ce Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.12637v1.pdf","comment":"26 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.03160v4","updated":"2025-04-17T04:46:08Z","published":"2025-04-04T04:41:28Z","title":"DeepResearcher: Scaling Deep Research via Reinforcement Learning in\n  Real-world Environments","summary":"  Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.\n","authors":["Yuxiang Zheng","Dayuan Fu","Xiangkun Hu","Xiaojie Cai","Lyumanshan Ye","Pengrui Lu","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2504.03160v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12633v1","updated":"2025-04-17T04:20:05Z","published":"2025-04-17T04:20:05Z","title":"Towards Characterizing Subjectivity of Individuals through Modeling\n  Value Conflicts and Trade-offs","summary":"  Large Language Models (LLMs) not only have solved complex reasoning problems\nbut also exhibit remarkable performance in tasks that require subjective\ndecision making. Existing studies suggest that LLM generations can be\nsubjectively grounded to some extent, yet exploring whether LLMs can account\nfor individual-level subjectivity has not been sufficiently studied. In this\npaper, we characterize subjectivity of individuals on social media and infer\ntheir moral judgments using LLMs. We propose a framework, SOLAR (Subjective\nGround with Value Abstraction), that observes value conflicts and trade-offs in\nthe user-generated texts to better represent subjective ground of individuals.\nEmpirical results show that our framework improves overall inference results as\nwell as performance on controversial situations. Additionally, we qualitatively\nshow that SOLAR provides explanations about individuals' value preferences,\nwhich can further account for their judgments.\n","authors":["Younghun Lee","Dan Goldwasser"],"pdf_url":"https://arxiv.org/pdf/2504.12633v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2410.12876v3","updated":"2025-04-17T03:51:06Z","published":"2024-10-15T05:01:19Z","title":"In-context KV-Cache Eviction for LLMs via Attention-Gate","summary":"  The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.\n","authors":["Zihao Zeng","Bokai Lin","Tianqi Hou","Hao Zhang","Zhijie Deng"],"pdf_url":"https://arxiv.org/pdf/2410.12876v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10982v3","updated":"2025-04-17T03:27:55Z","published":"2025-04-15T08:46:39Z","title":"Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical\n  Question Answering with Small-Scale LLMs","summary":"  Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages.\n","authors":["Yingjian Chen","Feiyang Li","Xingyu Song","Tianxiao Li","Issey Sukeda","Irene Li"],"pdf_url":"https://arxiv.org/pdf/2504.10982v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2504.12597v1","updated":"2025-04-17T02:46:27Z","published":"2025-04-17T02:46:27Z","title":"GeoSense: Evaluating Identification and Application of Geometric\n  Principles in Multimodal Reasoning","summary":"  Geometry problem-solving (GPS), a challenging task requiring both visual\ncomprehension and symbolic reasoning, effectively measures the reasoning\ncapabilities of multimodal large language models (MLLMs). Humans exhibit strong\nreasoning ability in this task through accurate identification and adaptive\napplication of geometric principles within visual contexts. However, existing\nbenchmarks fail to jointly assess both dimensions of the human-like geometric\nreasoning mechanism in MLLMs, remaining a critical gap in assessing their\nability to tackle GPS. To this end, we introduce GeoSense, the first\ncomprehensive bilingual benchmark designed to systematically evaluate the\ngeometric reasoning abilities of MLLMs through the lens of geometric\nprinciples. GeoSense features a five-level hierarchical framework of geometric\nprinciples spanning plane and solid geometry, an intricately annotated dataset\nof 1,789 problems, and an innovative evaluation strategy. Through extensive\nexperiments on GeoSense with various open-source and closed-source MLLMs, we\nobserve that Gemini-2.0-pro-flash performs best, achieving an overall score of\n$65.3$. Our in-depth analysis reveals that the identification and application\nof geometric principles remain a bottleneck for leading MLLMs, jointly\nhindering their reasoning abilities. These findings underscore GeoSense's\npotential to guide future advancements in MLLMs' geometric reasoning\ncapabilities, paving the way for more robust and human-like reasoning in\nartificial intelligence.\n","authors":["Liangyu Xu","Yingxiu Zhao","Jingyun Wang","Yingyao Wang","Bu Pi","Chen Wang","Mingliang Zhang","Jihao Gu","Xiang Li","Xiaoyong Zhu","Jun Song","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2504.12597v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2406.07791v8","updated":"2025-04-17T02:43:35Z","published":"2024-06-12T01:12:28Z","title":"Judging the Judges: A Systematic Study of Position Bias in\n  LLM-as-a-Judge","summary":"  LLM-as-a-Judge has emerged as a promising alternative to human evaluators\nacross various tasks, yet inherent biases - particularly position bias, the\ntendency to favor solutions based on their position within the prompt -\ncompromise its reliability. This exploratory study evaluates position bias in\nLLM judges across pairwise and list-wise comparison settings, introducing three\nmetrics: repetition stability, position consistency, and preference fairness.\nOur experiments, involving 15 LLM judges across MTBench and DevBench with 22\ntasks and approximately 40 solution-generating models, result in over 150,000\nevaluation instances. We identify Judge-Level, Candidate-Level, and Task-Level\nfactors contributing to bias. The findings confirm that position bias is not\ndue to random chance and varies significantly across judges and tasks. While\nposition bias is weakly influenced by the length of prompt components, it is\nstrongly affected by the quality gap between solutions. Our agreement and\ndisagreement analysis among judges further provides insights into the\ndistribution of judging difficulty across the dataset, and highlights the\npotential for dataset modifications.\n","authors":["Lin Shi","Chiyu Ma","Wenhua Liang","Xingjian Diao","Weicheng Ma","Soroush Vosoughi"],"pdf_url":"https://arxiv.org/pdf/2406.07791v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10673v2","updated":"2025-04-17T02:14:21Z","published":"2025-03-10T16:54:27Z","title":"ZeroSumEval: An Extensible Framework For Scaling LLM Evaluation with\n  Inter-Model Competition","summary":"  We introduce ZeroSumEval, a dynamic, competition-based, and evolving\nevaluation framework for Large Language Models (LLMs) that leverages\ncompetitive games. ZeroSumEval encompasses a diverse suite of games, including\nsecurity challenges (Capture the Flag), classic board games (chess), and\nknowledge tests (MathQuiz). These games are designed to evaluate a range of\ncapabilities such as strategic reasoning, planning, knowledge application,\nsafety, and adaptability. Building upon recent studies that highlight the\neffectiveness of game-based evaluations for LLMs, ZeroSumEval enhances these\napproaches by providing a standardized and extensible framework for easily\nimplementing games and leverages DSPy to provide a better abstraction for LLM\nplayer strategies.\n","authors":["Hisham A. Alyahya","Haidar Khan","Yazeed Alnumay","M Saiful Bari","Bülent Yener"],"pdf_url":"https://arxiv.org/pdf/2503.10673v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12588v1","updated":"2025-04-17T02:06:50Z","published":"2025-04-17T02:06:50Z","title":"Simplifying Graph Transformers","summary":"  Transformers have attained outstanding performance across various modalities,\nemploying scaled-dot-product (SDP) attention mechanisms. Researchers have\nattempted to migrate Transformers to graph learning, but most advanced Graph\nTransformers are designed with major architectural differences, either\nintegrating message-passing or incorporating sophisticated attention\nmechanisms. These complexities prevent the easy adoption of Transformer\ntraining advances. We propose three simple modifications to the plain\nTransformer to render it applicable to graphs without introducing major\narchitectural distortions. Specifically, we advocate for the use of (1)\nsimplified $L_2$ attention to measure the magnitude closeness of tokens; (2)\nadaptive root-mean-square normalization to preserve token magnitude\ninformation; and (3) a relative positional encoding bias with a shared encoder.\nSignificant performance gains across a variety of graph datasets justify the\neffectiveness of our proposed modifications. Furthermore, empirical evaluation\non the expressiveness benchmark reveals noteworthy realized expressiveness in\nthe graph isomorphism.\n","authors":["Liheng Ma","Soumyasundar Pal","Yingxue Zhang","Philip H. S. Torr","Mark Coates"],"pdf_url":"https://arxiv.org/pdf/2504.12588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06691v2","updated":"2025-04-17T02:02:57Z","published":"2024-05-07T09:36:23Z","title":"Fleet of Agents: Coordinated Problem Solving with Large Language Models","summary":"  While numerous frameworks have been developed to enhance the reasoning\nabilities of large language models (LLMs), there is a scarcity of methods that\neffectively balance the trade-off between cost and quality. In this paper, we\nintroduce Fleet of Agents (FoA), a novel and intuitive yet principled framework\nutilizing LLMs as agents to navigate through dynamic tree searches, employing a\ngenetic-type particle filtering approach. FoA spawns a multitude of agents,\neach exploring the search space autonomously, followed by a selection phase\nwhere resampling based on a heuristic value function optimizes the balance\nbetween exploration and exploitation. This mechanism enables dynamic branching,\nadapting the exploration strategy based on discovered solutions. We conduct\nextensive experiments on three benchmark tasks, ``Game of 24'',\n``Mini-Crosswords'', and ``WebShop'', utilizing four different LLMs,\n``GPT-3.5'', ``GPT-4'', ``LLaMA3.2-11B'', and ``LLaMA3.2-90B''. On average\nacross all tasks and LLMs, FoA obtains a quality improvement of ~5% while\nrequiring only ~40% of the cost of previous SOTA methods. Notably, our analyses\nreveal that (1) FoA achieves the best cost-quality trade-off among all\nbenchmarked methods and (2) FoA + LLaMA3.2-11B surpasses the Llama3.2-90B\nmodel. FoA is publicly available at https://github.com/au-clan/FoA.\n","authors":["Nearchos Potamitis","Lars Klein","Roland Aydin","Robert West","Caglar Gulcehre","Akhil Arora"],"pdf_url":"https://arxiv.org/pdf/2405.06691v2.pdf","comment":"28 pages, 68 figures, 8 tables"},{"id":"http://arxiv.org/abs/2504.12585v1","updated":"2025-04-17T02:00:53Z","published":"2025-04-17T02:00:53Z","title":"Identifying and Mitigating the Influence of the Prior Distribution in\n  Large Language Models","summary":"  Large language models (LLMs) sometimes fail to respond appropriately to\ndeterministic tasks -- such as counting or forming acronyms -- because the\nimplicit prior distribution they have learned over sequences of tokens\ninfluences their responses. In this work, we show that, in at least some cases,\nLLMs actually compute the information needed to perform these tasks correctly,\nand we identify some interventions that can allow them to access this\ninformation to improve their performance. First, we show that simply prompting\nthe language model to not rely on its prior knowledge leads to dramatic\nimprovements in prior-dominated tasks. We then use mechanistic interpretability\ntechniques to localize the prior within the LLM and manipulate the extent to\nwhich that prior influences its responses. Specifically, we show that it is\npossible to identify layers of the underlying neural network that correlate\nwith the prior probability of a response and that lightweight finetuning of\nthese layers with basic prompts on prior-dominated tasks achieves high\nperformance on held-out answers. These results suggest that the information\nrequired to produce a correct response is contained within the representations\nof the problems formed by the models. Furthermore, we show that this finetuning\nis significantly more effective for prior-dominated tasks, and that the error\nafter finetuning is no longer correlated with the prior. Our results suggest\nthat it may be possible to define effective methods for manipulating the extent\nto which LLMs rely upon their priors in solving problems, potentially\nincreasing their performance in settings where LLMs hallucinate for reasons\nrelated to the prior probability of token sequences.\n","authors":["Liyi Zhang","Veniamin Veselovsky","R. Thomas McCoy","Thomas L. Griffiths"],"pdf_url":"https://arxiv.org/pdf/2504.12585v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.12579v1","updated":"2025-04-17T01:52:09Z","published":"2025-04-17T01:52:09Z","title":"Provable Secure Steganography Based on Adaptive Dynamic Sampling","summary":"  The security of private communication is increasingly at risk due to\nwidespread surveillance. Steganography, a technique for embedding secret\nmessages within innocuous carriers, enables covert communication over monitored\nchannels. Provably Secure Steganography (PSS) is state of the art for making\nstego carriers indistinguishable from normal ones by ensuring computational\nindistinguishability between stego and cover distributions. However, current\nPSS methods often require explicit access to the distribution of generative\nmodel for both sender and receiver, limiting their practicality in black box\nscenarios. In this paper, we propose a provably secure steganography scheme\nthat does not require access to explicit model distributions for both sender\nand receiver. Our method incorporates a dynamic sampling strategy, enabling\ngenerative models to embed secret messages within multiple sampling choices\nwithout disrupting the normal generation process of the model. Extensive\nevaluations of three real world datasets and three LLMs demonstrate that our\nblackbox method is comparable with existing white-box steganography methods in\nterms of efficiency and capacity while eliminating the degradation of\nsteganography in model generated outputs.\n","authors":["Kaiyi Pang"],"pdf_url":"https://arxiv.org/pdf/2504.12579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11055v2","updated":"2025-04-17T01:37:50Z","published":"2024-09-17T10:31:37Z","title":"Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and\n  Model Size in Large Language Models From Edge to Giant","summary":"  Quantization has gained attention as a promising solution for the\ncost-effective deployment of large and small language models. However, most\nprior work has been limited to perplexity or basic knowledge tasks and lacks a\ncomprehensive evaluation of recent models like Llama-3.3. In this paper, we\nconduct a comprehensive evaluation of instruction-tuned models spanning 1B to\n405B parameters, applying four quantization methods across 13 datasets. Our\nfindings reveal that (1) quantized models generally surpass smaller FP16\nbaselines, yet they often struggle with instruction-following and hallucination\ndetection; (2) FP8 consistently emerges as the most robust option across tasks,\nand AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller\nmodels can suffer severe accuracy drops at 4-bit quantization, while 70B-scale\nmodels maintain stable performance; (4) notably, \\textit{hard} tasks do not\nalways experience the largest accuracy losses, indicating that quantization\nmagnifies a model's inherent weaknesses rather than simply correlating with\ntask difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant\nperformance declines in coding and STEM tasks, though reasoning may sometimes\nimprove.\n","authors":["Jemin Lee","Sihyeong Park","Jinse Kwon","Jihun Oh","Yongin Kwon"],"pdf_url":"https://arxiv.org/pdf/2409.11055v2.pdf","comment":"21 pages, 2 figure"},{"id":"http://arxiv.org/abs/2504.08260v2","updated":"2025-04-17T01:36:57Z","published":"2025-04-11T05:11:40Z","title":"Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in\n  Healthcare","summary":"  Generative agents have been increasingly used to simulate human behaviour in\nsilico, driven by large language models (LLMs). These simulacra serve as\nsandboxes for studying human behaviour without compromising privacy or safety.\nHowever, it remains unclear whether such agents can truly represent real\nindividuals. This work compares survey data from the Understanding America\nStudy (UAS) on healthcare decision-making with simulated responses from\ngenerative agents. Using demographic-based prompt engineering, we create\ndigital twins of survey respondents and analyse how well different LLMs\nreproduce real-world behaviours. Our findings show that some LLMs fail to\nreflect realistic decision-making, such as predicting universal vaccine\nacceptance. However, Llama 3 captures variations across race and Income more\naccurately but also introduces biases not present in the UAS data. This study\nhighlights the potential of generative agents for behavioural research while\nunderscoring the risks of bias from both LLMs and prompting strategies.\n","authors":["Yonchanok Khaokaew","Flora D. Salim","Andreas Züfle","Hao Xue","Taylor Anderson","C. Raina MacIntyre","Matthew Scotch","David J Heslop"],"pdf_url":"https://arxiv.org/pdf/2504.08260v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12563v1","updated":"2025-04-17T01:25:15Z","published":"2025-04-17T01:25:15Z","title":"MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic\n  Data Generation","summary":"  Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data\ngenerated using larger Language models. Questions remain about leveraging\nsynthetic data for other use cases, such as adapting LLMs to specific domains.\nA key limitation of synthetic data is low diversity, which negatively impacts\nits downstream applicability for improving other models. To address this, we\npropose MetaSynth, a method for generating synthetic data that enhances\ndiversity through meta-prompting, where a language model orchestrates multiple\n\"expert\" LLM agents to collaboratively generate data. Using only 25 million\ntokens of synthetic data generated with MetaSynth, we successfully adapt a\nwell-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and\nBiomedicine-without compromising the capabilities of the resulting model in\ngeneral tasks. In addition, we evaluate the diversity of our synthetic data\nusing seven automated metrics, and find that it approaches the diversity of LLM\npre-training corpora.\n  Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms\nthe base LLM, showing improvements of up to 4.08% in Finance and 13.75% in\nBiomedicine. The same model shows degraded performance when trained on data\ngenerated using a template prompt, even when the template includes prior\ngenerations and varying In-Context exemplars of real data. Our findings suggest\nthat a few million tokens of diverse synthetic data without mixing any real\ndata, is sufficient for effective domain adaptation when using MetaSynth.\n","authors":["Haris Riaz","Sourav Bhabesh","Vinayak Arannil","Miguel Ballesteros","Graham Horwood"],"pdf_url":"https://arxiv.org/pdf/2504.12563v1.pdf","comment":"33 pages, 17 figures. Preprint"},{"id":"http://arxiv.org/abs/2504.12562v1","updated":"2025-04-17T01:23:50Z","published":"2025-04-17T01:23:50Z","title":"ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition","summary":"  Evaluating the capabilities of Large Language Models (LLMs) has traditionally\nrelied on static benchmark datasets, human assessments, or model-based\nevaluations - methods that often suffer from overfitting, high costs, and\nbiases. ZeroSumEval is a novel competition-based evaluation protocol that\nleverages zero-sum games to assess LLMs with dynamic benchmarks that resist\nsaturation. ZeroSumEval encompasses a diverse suite of games, including\nsecurity challenges (PyJail), classic games (Chess, Liar's Dice, Poker),\nknowledge tests (MathQuiz), and persuasion challenges (Gandalf, Debate). These\ngames are designed to evaluate a range of AI capabilities such as strategic\nreasoning, planning, knowledge application, and creativity. Building upon\nrecent studies that highlight the effectiveness of game-based evaluations for\nLLMs, ZeroSumEval enhances these approaches by providing a standardized and\nextensible framework. To demonstrate this, we conduct extensive experiments\nwith >7000 simulations across 7 games and 13 models. Our results show that\nwhile frontier models from the GPT and Claude families can play common games\nand answer questions, they struggle to play games that require creating novel\nand challenging questions. We also observe that models cannot reliably\njailbreak each other and fail generally at tasks requiring creativity. We\nrelease our code at https://github.com/facebookresearch/ZeroSumEval.\n","authors":["Haidar Khan","Hisham A. Alyahya","Yazeed Alnumay","M Saiful Bari","Bülent Yener"],"pdf_url":"https://arxiv.org/pdf/2504.12562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12560v1","updated":"2025-04-17T01:15:13Z","published":"2025-04-17T01:15:13Z","title":"CDF-RAG: Causal Dynamic Feedback for Adaptive Retrieval-Augmented\n  Generation","summary":"  Retrieval-Augmented Generation (RAG) has significantly enhanced large\nlanguage models (LLMs) in knowledge-intensive tasks by incorporating external\nknowledge retrieval. However, existing RAG frameworks primarily rely on\nsemantic similarity and correlation-driven retrieval, limiting their ability to\ndistinguish true causal relationships from spurious associations. This results\nin responses that may be factually grounded but fail to establish\ncause-and-effect mechanisms, leading to incomplete or misleading insights. To\naddress this issue, we introduce Causal Dynamic Feedback for Adaptive\nRetrieval-Augmented Generation (CDF-RAG), a framework designed to improve\ncausal consistency, factual accuracy, and explainability in generative\nreasoning. CDF-RAG iteratively refines queries, retrieves structured causal\ngraphs, and enables multi-hop causal reasoning across interconnected knowledge\nsources. Additionally, it validates responses against causal pathways, ensuring\nlogically coherent and factually grounded outputs. We evaluate CDF-RAG on four\ndiverse datasets, demonstrating its ability to improve response accuracy and\ncausal correctness over existing RAG-based methods. Our code is publicly\navailable at https://github.com/ elakhatibi/CDF-RAG.\n","authors":["Elahe Khatibi","Ziyu Wang","Amir M. Rahmani"],"pdf_url":"https://arxiv.org/pdf/2504.12560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12558v1","updated":"2025-04-17T01:13:21Z","published":"2025-04-17T01:13:21Z","title":"Benchmarking LLM-based Relevance Judgment Methods","summary":"  Large Language Models (LLMs) are increasingly deployed in both academic and\nindustry settings to automate the evaluation of information seeking systems,\nparticularly by generating graded relevance judgments. Previous work on\nLLM-based relevance assessment has primarily focused on replicating graded\nhuman relevance judgments through various prompting strategies. However, there\nhas been limited exploration of alternative assessment methods or comprehensive\ncomparative studies. In this paper, we systematically compare multiple\nLLM-based relevance assessment methods, including binary relevance judgments,\ngraded relevance assessments, pairwise preference-based methods, and two\nnugget-based evaluation methods~--~document-agnostic and document-dependent. In\naddition to a traditional comparison based on system rankings using Kendall\ncorrelations, we also examine how well LLM judgments align with human\npreferences, as inferred from relevance grades. We conduct extensive\nexperiments on datasets from three TREC Deep Learning tracks 2019, 2020 and\n2021 as well as the ANTIQUE dataset, which focuses on non-factoid open-domain\nquestion answering. As part of our data release, we include relevance judgments\ngenerated by both an open-source (Llama3.2b) and a commercial (gpt-4o) model.\nOur goal is to \\textit{reproduce} various LLM-based relevance judgment methods\nto provide a comprehensive comparison. All code, data, and resources are\npublicly available in our GitHub Repository at\nhttps://github.com/Narabzad/llm-relevance-judgement-comparison.\n","authors":["Negar Arabzadeh","Charles L. A. Clarke"],"pdf_url":"https://arxiv.org/pdf/2504.12558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12553v1","updated":"2025-04-17T00:50:41Z","published":"2025-04-17T00:50:41Z","title":"ELAB: Extensive LLM Alignment Benchmark in Persian Language","summary":"  This paper presents a comprehensive evaluation framework for aligning Persian\nLarge Language Models (LLMs) with critical ethical dimensions, including\nsafety, fairness, and social norms. It addresses the gaps in existing LLM\nevaluation frameworks by adapting them to Persian linguistic and cultural\ncontexts. This benchmark creates three types of Persian-language benchmarks:\n(i) translated data, (ii) new data generated synthetically, and (iii) new\nnaturally collected data. We translate Anthropic Red Teaming data, AdvBench,\nHarmBench, and DecodingTrust into Persian. Furthermore, we create\nProhibiBench-fa, SafeBench-fa, FairBench-fa, and SocialBench-fa as new datasets\nto address harmful and prohibited content in indigenous culture. Moreover, we\ncollect extensive dataset as GuardBench-fa to consider Persian cultural norms.\nBy combining these datasets, our work establishes a unified framework for\nevaluating Persian LLMs, offering a new approach to culturally grounded\nalignment evaluation. A systematic evaluation of Persian LLMs is performed\nacross the three alignment aspects: safety (avoiding harmful content), fairness\n(mitigating biases), and social norms (adhering to culturally accepted\nbehaviors). We present a publicly available leaderboard that benchmarks Persian\nLLMs with respect to safety, fairness, and social norms at:\nhttps://huggingface.co/spaces/MCILAB/LLM_Alignment_Evaluation.\n","authors":["Zahra Pourbahman","Fatemeh Rajabi","Mohammadhossein Sadeghi","Omid Ghahroodi","Somaye Bakhshaei","Arash Amini","Reza Kazemi","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2504.12553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12549v1","updated":"2025-04-17T00:20:18Z","published":"2025-04-17T00:20:18Z","title":"Memorization: A Close Look at Books","summary":"  To what extent can entire books be extracted from LLMs? Using the Llama 3 70B\nfamily of models, and the \"prefix-prompting\" extraction technique, we were able\nto auto-regressively reconstruct, with a very high level of similarity, one\nentire book (Alice's Adventures in Wonderland) from just the first 500 tokens.\nWe were also able to obtain high extraction rates on several other books,\npiece-wise. However, these successes do not extend uniformly to all books. We\nshow that extraction rates of books correlate with book popularity and thus,\nlikely duplication in the training data.\n  We also confirm the undoing of mitigations in the instruction-tuned Llama\n3.1, following recent work (Nasr et al., 2025). We further find that this\nundoing comes from changes to only a tiny fraction of weights concentrated\nprimarily in the lower transformer blocks. Our results provide evidence of the\nlimits of current regurgitation mitigation strategies and introduce a framework\nfor studying how fine-tuning affects the retrieval of verbatim memorization in\naligned LLMs.\n","authors":["Iris Ma","Ian Domingo","Alberto Krone-Martins","Pierre Baldi","Cristina V. Lopes"],"pdf_url":"https://arxiv.org/pdf/2504.12549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12545v1","updated":"2025-04-17T00:13:04Z","published":"2025-04-17T00:13:04Z","title":"Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven\n  Justice","summary":"  Mass-shooting events pose a significant challenge to public safety,\ngenerating large volumes of unstructured textual data that hinder effective\ninvestigations and the formulation of public policy. Despite the urgency, few\nprior studies have effectively automated the extraction of key information from\nthese events to support legal and investigative efforts. This paper presented\nthe first dataset designed for knowledge acquisition on mass-shooting events\nthrough the application of named entity recognition (NER) techniques. It\nfocuses on identifying key entities such as offenders, victims, locations, and\ncriminal instruments, that are vital for legal and investigative purposes. The\nNER process is powered by Large Language Models (LLMs) using few-shot\nprompting, facilitating the efficient extraction and organization of critical\ninformation from diverse sources, including news articles, police reports, and\nsocial media. Experimental results on real-world mass-shooting corpora\ndemonstrate that GPT-4o is the most effective model for mass-shooting NER,\nachieving the highest Micro Precision, Micro Recall, and Micro F1-scores.\nMeanwhile, o1-mini delivers competitive performance, making it a\nresource-efficient alternative for less complex NER tasks. It is also observed\nthat increasing the shot count enhances the performance of all models, but the\ngains are more substantial for GPT-4o and o1-mini, highlighting their superior\nadaptability to few-shot learning scenarios.\n","authors":["Benign John Ihugba","Afsana Nasrin","Ling Wu","Lin Li","Lijun Qian","Xishuang Dong"],"pdf_url":"https://arxiv.org/pdf/2504.12545v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2504.13181v1","updated":"2025-04-17T17:59:57Z","published":"2025-04-17T17:59:57Z","title":"Perception Encoder: The best visual embeddings are not at the output of\n  the network","summary":"  We introduce Perception Encoder (PE), a state-of-the-art encoder for image\nand video understanding trained via simple vision-language learning.\nTraditionally, vision encoders have relied on a variety of pretraining\nobjectives, each tailored to specific downstream tasks such as classification,\ncaptioning, or localization. Surprisingly, after scaling our carefully tuned\nimage pretraining recipe and refining with our robust video data engine, we\nfind that contrastive vision-language training alone can produce strong,\ngeneral embeddings for all of these downstream tasks. There is only one caveat:\nthese embeddings are hidden within the intermediate layers of the network. To\ndraw them out, we introduce two alignment methods, language alignment for\nmultimodal language modeling, and spatial alignment for dense prediction.\nTogether with the core contrastive checkpoint, our PE family of models achieves\nstate-of-the-art performance on a wide variety of tasks, including zero-shot\nimage and video classification and retrieval; document, image, and video Q&A;\nand spatial tasks such as detection, depth estimation, and tracking. To foster\nfurther research, we are releasing our models, code, and a novel dataset of\nsynthetically and human-annotated videos.\n","authors":["Daniel Bolya","Po-Yao Huang","Peize Sun","Jang Hyun Cho","Andrea Madotto","Chen Wei","Tengyu Ma","Jiale Zhi","Jathushan Rajasegaran","Hanoona Rasheed","Junke Wang","Marco Monteiro","Hu Xu","Shiyu Dong","Nikhila Ravi","Daniel Li","Piotr Dollár","Christoph Feichtenhofer"],"pdf_url":"https://arxiv.org/pdf/2504.13181v1.pdf","comment":"Initial Submission"},{"id":"http://arxiv.org/abs/2504.13179v1","updated":"2025-04-17T17:59:56Z","published":"2025-04-17T17:59:56Z","title":"ViTa-Zero: Zero-shot Visuotactile Object 6D Pose Estimation","summary":"  Object 6D pose estimation is a critical challenge in robotics, particularly\nfor manipulation tasks. While prior research combining visual and tactile\n(visuotactile) information has shown promise, these approaches often struggle\nwith generalization due to the limited availability of visuotactile data. In\nthis paper, we introduce ViTa-Zero, a zero-shot visuotactile pose estimation\nframework. Our key innovation lies in leveraging a visual model as its backbone\nand performing feasibility checking and test-time optimization based on\nphysical constraints derived from tactile and proprioceptive observations.\nSpecifically, we model the gripper-object interaction as a spring-mass system,\nwhere tactile sensors induce attractive forces, and proprioception generates\nrepulsive forces. We validate our framework through experiments on a real-world\nrobot setup, demonstrating its effectiveness across representative visual\nbackbones and manipulation scenarios, including grasping, object picking, and\nbimanual handover. Compared to the visual models, our approach overcomes some\ndrastic failure modes while tracking the in-hand object pose. In our\nexperiments, our approach shows an average increase of 55% in AUC of ADD-S and\n60% in ADD, along with an 80% lower position error compared to FoundationPose.\n","authors":["Hongyu Li","James Akl","Srinath Sridhar","Tye Brady","Taskin Padir"],"pdf_url":"https://arxiv.org/pdf/2504.13179v1.pdf","comment":"Accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2504.13180v1","updated":"2025-04-17T17:59:56Z","published":"2025-04-17T17:59:56Z","title":"PerceptionLM: Open-Access Data and Models for Detailed Visual\n  Understanding","summary":"  Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models.\n","authors":["Jang Hyun Cho","Andrea Madotto","Effrosyni Mavroudi","Triantafyllos Afouras","Tushar Nagarajan","Muhammad Maaz","Yale Song","Tengyu Ma","Shuming Hu","Suyog Jain","Miguel Martin","Huiyu Wang","Hanoona Rasheed","Peize Sun","Po-Yao Huang","Daniel Bolya","Nikhila Ravi","Shashank Jain","Tammy Stark","Shane Moon","Babak Damavandi","Vivian Lee","Andrew Westbury","Salman Khan","Philipp Krähenbühl","Piotr Dollár","Lorenzo Torresani","Kristen Grauman","Christoph Feichtenhofer"],"pdf_url":"https://arxiv.org/pdf/2504.13180v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2504.13177v1","updated":"2025-04-17T17:59:50Z","published":"2025-04-17T17:59:50Z","title":"Single-Shot Shape and Reflectance with Spatial Polarization Multiplexing","summary":"  We propose spatial polarization multiplexing (SPM) for reconstructing object\nshape and reflectance from a single polarimetric image and demonstrate its\napplication to dynamic surface recovery. Although single-pattern structured\nlight enables single-shot shape reconstruction, the reflectance is challenging\nto recover due to the lack of angular sampling of incident light and the\nentanglement of the projected pattern and the surface color texture. We design\na spatially multiplexed pattern of polarization that can be robustly and\nuniquely decoded for shape reconstruction by quantizing the AoLP values. At the\nsame time, our spatial-multiplexing enables single-shot ellipsometry of linear\npolarization by projecting differently polarized light within a local region,\nwhich separates the specular and diffuse reflections for BRDF estimation. We\nachieve this spatial polarization multiplexing with a constrained de Bruijn\nsequence. Unlike single-pattern structured light with intensity and color, our\npolarization pattern is invisible to the naked eye and retains the natural\nsurface appearance which is essential for accurate appearance modeling and also\ninteraction with people. We experimentally validate our method on real data.\nThe results show that our method can recover the shape, the Mueller matrix, and\nthe BRDF from a single-shot polarimetric image. We also demonstrate the\napplication of our method to dynamic surfaces.\n","authors":["Tomoki Ichikawa","Ryo Kawahara","Ko Nishino"],"pdf_url":"https://arxiv.org/pdf/2504.13177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13176v1","updated":"2025-04-17T17:59:47Z","published":"2025-04-17T17:59:47Z","title":"IMAGGarment-1: Fine-Grained Garment Generation for Controllable Fashion\n  Design","summary":"  This paper presents IMAGGarment-1, a fine-grained garment generation (FGG)\nframework that enables high-fidelity garment synthesis with precise control\nover silhouette, color, and logo placement. Unlike existing methods that are\nlimited to single-condition inputs, IMAGGarment-1 addresses the challenges of\nmulti-conditional controllability in personalized fashion design and digital\napparel applications. Specifically, IMAGGarment-1 employs a two-stage training\nstrategy to separately model global appearance and local details, while\nenabling unified and controllable generation through end-to-end inference. In\nthe first stage, we propose a global appearance model that jointly encodes\nsilhouette and color using a mixed attention module and a color adapter. In the\nsecond stage, we present a local enhancement model with an adaptive\nappearance-aware module to inject user-defined logos and spatial constraints,\nenabling accurate placement and visual consistency. To support this task, we\nrelease GarmentBench, a large-scale dataset comprising over 180K garment\nsamples paired with multi-level design conditions, including sketches, color\nreferences, logo placements, and textual prompts. Extensive experiments\ndemonstrate that our method outperforms existing baselines, achieving superior\nstructural stability, color fidelity, and local controllability performance.\nThe code and model are available at https://github.com/muzishen/IMAGGarment-1.\n","authors":["Fei Shen","Jian Yu","Cong Wang","Xin Jiang","Xiaoyu Du","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2504.13176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17385v2","updated":"2025-04-17T17:59:27Z","published":"2024-10-22T19:39:15Z","title":"Do Vision-Language Models Represent Space and How? Evaluating Spatial\n  Frame of Reference Under Ambiguities","summary":"  Spatial expressions in situated communication can be ambiguous, as their\nmeanings vary depending on the frames of reference (FoR) adopted by speakers\nand listeners. While spatial language understanding and reasoning by\nvision-language models (VLMs) have gained increasing attention, potential\nambiguities in these models are still under-explored. To address this issue, we\npresent the COnsistent Multilingual Frame Of Reference Test (COMFORT), an\nevaluation protocol to systematically assess the spatial reasoning capabilities\nof VLMs. We evaluate nine state-of-the-art VLMs using COMFORT. Despite showing\nsome alignment with English conventions in resolving ambiguities, our\nexperiments reveal significant shortcomings of VLMs: notably, the models (1)\nexhibit poor robustness and consistency, (2) lack the flexibility to\naccommodate multiple FoRs, and (3) fail to adhere to language-specific or\nculture-specific conventions in cross-lingual tests, as English tends to\ndominate other languages. With a growing effort to align vision-language models\nwith human cognitive intuitions, we call for more attention to the ambiguous\nnature and cross-cultural diversity of spatial reasoning.\n","authors":["Zheyuan Zhang","Fengyuan Hu","Jayjun Lee","Freda Shi","Parisa Kordjamshidi","Joyce Chai","Ziqiao Ma"],"pdf_url":"https://arxiv.org/pdf/2410.17385v2.pdf","comment":"Accepted to ICLR 2025 (Oral) | Project page:\n  https://spatial-comfort.github.io/"},{"id":"http://arxiv.org/abs/2504.13169v1","updated":"2025-04-17T17:59:22Z","published":"2025-04-17T17:59:22Z","title":"Generate, but Verify: Reducing Hallucination in Vision-Language Models\n  with Retrospective Resampling","summary":"  Vision-Language Models (VLMs) excel at visual understanding but often suffer\nfrom visual hallucinations, where they generate descriptions of nonexistent\nobjects, actions, or concepts, posing significant risks in safety-critical\napplications. Existing hallucination mitigation methods typically follow one of\ntwo paradigms: generation adjustment, which modifies decoding behavior to align\ntext with visual inputs, and post-hoc verification, where external models\nassess and correct outputs. While effective, generation adjustment methods\noften rely on heuristics and lack correction mechanisms, while post-hoc\nverification is complicated, typically requiring multiple models and tending to\nreject outputs rather than refine them. In this work, we introduce REVERSE, a\nunified framework that integrates hallucination-aware training with on-the-fly\nself-verification. By leveraging a new hallucination-verification dataset\ncontaining over 1.3M semi-synthetic samples, along with a novel inference-time\nretrospective resampling technique, our approach enables VLMs to both detect\nhallucinations during generation and dynamically revise those hallucinations.\nOur evaluations show that REVERSE achieves state-of-the-art hallucination\nreduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO\nand 28% on HaloQuest. Our dataset, model, and code are available at:\nhttps://reverse-vlm.github.io.\n","authors":["Tsung-Han Wu","Heekyung Lee","Jiaxin Ge","Joseph E. Gonzalez","Trevor Darrell","David M. Chan"],"pdf_url":"https://arxiv.org/pdf/2504.13169v1.pdf","comment":"Preprint. Project Page: https://reverse-vlm.github.io"},{"id":"http://arxiv.org/abs/2504.13167v1","updated":"2025-04-17T17:59:02Z","published":"2025-04-17T17:59:02Z","title":"ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from\n  Monocular Videos","summary":"  Creating a photorealistic scene and human reconstruction from a single\nmonocular in-the-wild video figures prominently in the perception of a\nhuman-centric 3D world. Recent neural rendering advances have enabled holistic\nhuman-scene reconstruction but require pre-calibrated camera and human poses,\nand days of training time. In this work, we introduce a novel unified framework\nthat simultaneously performs camera tracking, human pose estimation and\nhuman-scene reconstruction in an online fashion. 3D Gaussian Splatting is\nutilized to learn Gaussian primitives for humans and scenes efficiently, and\nreconstruction-based camera tracking and human pose estimation modules are\ndesigned to enable holistic understanding and effective disentanglement of pose\nand appearance. Specifically, we design a human deformation module to\nreconstruct the details and enhance generalizability to out-of-distribution\nposes faithfully. Aiming to learn the spatial correlation between human and\nscene accurately, we introduce occlusion-aware human silhouette rendering and\nmonocular geometric priors, which further improve reconstruction quality.\nExperiments on the EMDB and NeuMan datasets demonstrate superior or on-par\nperformance with existing methods in camera tracking, human pose estimation,\nnovel view synthesis and runtime. Our project page is at\nhttps://eth-ait.github.io/ODHSR.\n","authors":["Zetong Zhang","Manuel kaufmann","Lixin Xue","Jie Song","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2504.13167v1.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2504.13162v1","updated":"2025-04-17T17:58:26Z","published":"2025-04-17T17:58:26Z","title":"Personalized Text-to-Image Generation with Auto-Regressive Models","summary":"  Personalized image synthesis has emerged as a pivotal application in\ntext-to-image generation, enabling the creation of images featuring specific\nsubjects in diverse contexts. While diffusion models have dominated this\ndomain, auto-regressive models, with their unified architecture for text and\nimage modeling, remain underexplored for personalized image generation. This\npaper investigates the potential of optimizing auto-regressive models for\npersonalized image synthesis, leveraging their inherent multimodal capabilities\nto perform this task. We propose a two-stage training strategy that combines\noptimization of text embeddings and fine-tuning of transformer layers. Our\nexperiments on the auto-regressive model demonstrate that this method achieves\ncomparable subject fidelity and prompt following to the leading diffusion-based\npersonalization methods. The results highlight the effectiveness of\nauto-regressive models in personalized image generation, offering a new\ndirection for future research in this area.\n","authors":["Kaiyue Sun","Xian Liu","Yao Teng","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2504.13162v1.pdf","comment":"Project page:\n  https://github.com/KaiyueSun98/T2I-Personalization-with-AR"},{"id":"http://arxiv.org/abs/2504.13159v1","updated":"2025-04-17T17:57:41Z","published":"2025-04-17T17:57:41Z","title":"Digital Twin Generation from Visual Data: A Survey","summary":"  This survey explores recent developments in generating digital twins from\nvideos. Such digital twins can be used for robotics application, media content\ncreation, or design and construction works. We analyze various approaches,\nincluding 3D Gaussian Splatting, generative in-painting, semantic segmentation,\nand foundation models highlighting their advantages and limitations.\nAdditionally, we discuss challenges such as occlusions, lighting variations,\nand scalability, as well as potential future research directions. This survey\naims to provide a comprehensive overview of state-of-the-art methodologies and\ntheir implications for real-world applications. Awesome list:\nhttps://github.com/ndrwmlnk/awesome-digital-twins\n","authors":["Andrew Melnik","Benjamin Alt","Giang Nguyen","Artur Wilkowski","Maciej Stefańczyk","Qirui Wu","Sinan Harms","Helge Rhodin","Manolis Savva","Michael Beetz"],"pdf_url":"https://arxiv.org/pdf/2504.13159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13157v1","updated":"2025-04-17T17:57:05Z","published":"2025-04-17T17:57:05Z","title":"AerialMegaDepth: Learning Aerial-Ground Reconstruction and View\n  Synthesis","summary":"  We explore the task of geometric reconstruction of images captured from a\nmixture of ground and aerial views. Current state-of-the-art learning-based\napproaches fail to handle the extreme viewpoint variation between aerial-ground\nimage pairs. Our hypothesis is that the lack of high-quality, co-registered\naerial-ground datasets for training is a key reason for this failure. Such data\nis difficult to assemble precisely because it is difficult to reconstruct in a\nscalable way. To overcome this challenge, we propose a scalable framework\ncombining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google\nEarth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The\npseudo-synthetic data simulates a wide range of aerial viewpoints, while the\nreal, crowd-sourced images help improve visual fidelity for ground-level images\nwhere mesh-based renderings lack sufficient detail, effectively bridging the\ndomain gap between real images and pseudo-synthetic renderings. Using this\nhybrid dataset, we fine-tune several state-of-the-art algorithms and achieve\nsignificant improvements on real-world, zero-shot aerial-ground tasks. For\nexample, we observe that baseline DUSt3R localizes fewer than 5% of\naerial-ground pairs within 5 degrees of camera rotation error, while\nfine-tuning with our data raises accuracy to nearly 56%, addressing a major\nfailure point in handling large viewpoint changes. Beyond camera estimation and\nscene reconstruction, our dataset also improves performance on downstream tasks\nlike novel-view synthesis in challenging aerial-ground scenarios, demonstrating\nthe practical value of our approach in real-world applications.\n","authors":["Khiem Vuong","Anurag Ghosh","Deva Ramanan","Srinivasa Narasimhan","Shubham Tulsiani"],"pdf_url":"https://arxiv.org/pdf/2504.13157v1.pdf","comment":"Appearing in CVPR 2025. Project page:\n  https://aerial-megadepth.github.io"},{"id":"http://arxiv.org/abs/2504.13153v1","updated":"2025-04-17T17:56:07Z","published":"2025-04-17T17:56:07Z","title":"Training-Free Hierarchical Scene Understanding for Gaussian Splatting\n  with Superpoint Graphs","summary":"  Bridging natural language and 3D geometry is a crucial step toward flexible,\nlanguage-driven scene understanding. While recent advances in 3D Gaussian\nSplatting (3DGS) have enabled fast and high-quality scene reconstruction,\nresearch has also explored incorporating open-vocabulary understanding into\n3DGS. However, most existing methods require iterative optimization over\nper-view 2D semantic feature maps, which not only results in inefficiencies but\nalso leads to inconsistent 3D semantics across views. To address these\nlimitations, we introduce a training-free framework that constructs a\nsuperpoint graph directly from Gaussian primitives. The superpoint graph\npartitions the scene into spatially compact and semantically coherent regions,\nforming view-consistent 3D entities and providing a structured foundation for\nopen-vocabulary understanding. Based on the graph structure, we design an\nefficient reprojection strategy that lifts 2D semantic features onto the\nsuperpoints, avoiding costly multi-view iterative training. The resulting\nrepresentation ensures strong 3D semantic coherence and naturally supports\nhierarchical understanding, enabling both coarse- and fine-grained\nopen-vocabulary perception within a unified semantic field. Extensive\nexperiments demonstrate that our method achieves state-of-the-art\nopen-vocabulary segmentation performance, with semantic field reconstruction\ncompleted over $30\\times$ faster. Our code will be available at\nhttps://github.com/Atrovast/THGS.\n","authors":["Shaohui Dai","Yansong Qu","Zheyan Li","Xinyang Li","Shengchuan Zhang","Liujuan Cao"],"pdf_url":"https://arxiv.org/pdf/2504.13153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13152v1","updated":"2025-04-17T17:55:58Z","published":"2025-04-17T17:55:58Z","title":"St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World","summary":"  Dynamic 3D reconstruction and point tracking in videos are typically treated\nas separate tasks, despite their deep connection. We propose St4RTrack, a\nfeed-forward framework that simultaneously reconstructs and tracks dynamic\nvideo content in a world coordinate frame from RGB inputs. This is achieved by\npredicting two appropriately defined pointmaps for a pair of frames captured at\ndifferent moments. Specifically, we predict both pointmaps at the same moment,\nin the same world, capturing both static and dynamic scene geometry while\nmaintaining 3D correspondences. Chaining these predictions through the video\nsequence with respect to a reference frame naturally computes long-range\ncorrespondences, effectively combining 3D reconstruction with 3D tracking.\nUnlike prior methods that rely heavily on 4D ground truth supervision, we\nemploy a novel adaptation scheme based on a reprojection loss. We establish a\nnew extensive benchmark for world-frame reconstruction and tracking,\ndemonstrating the effectiveness and efficiency of our unified, data-driven\nframework. Our code, model, and benchmark will be released.\n","authors":["Haiwen Feng","Junyi Zhang","Qianqian Wang","Yufei Ye","Pengcheng Yu","Michael J. Black","Trevor Darrell","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2504.13152v1.pdf","comment":"Project page: https://St4RTrack.github.io/"},{"id":"http://arxiv.org/abs/2504.13150v1","updated":"2025-04-17T17:55:34Z","published":"2025-04-17T17:55:34Z","title":"Readable Twins of Unreadable Models","summary":"  Creating responsible artificial intelligence (AI) systems is an important\nissue in contemporary research and development of works on AI. One of the\ncharacteristics of responsible AI systems is their explainability. In the\npaper, we are interested in explainable deep learning (XDL) systems. On the\nbasis of the creation of digital twins of physical objects, we introduce the\nidea of creating readable twins (in the form of imprecise information flow\nmodels) for unreadable deep learning models. The complete procedure for\nswitching from the deep learning model (DLM) to the imprecise information flow\nmodel (IIFM) is presented. The proposed approach is illustrated with an example\nof a deep learning classification model for image recognition of handwritten\ndigits from the MNIST data set.\n","authors":["Krzysztof Pancerz","Piotr Kulicki","Michał Kalisz","Andrzej Burda","Maciej Stanisławski","Jaromir Sarzyński"],"pdf_url":"https://arxiv.org/pdf/2504.13150v1.pdf","comment":"Based on the abstract accepted for ISFS 2025"},{"id":"http://arxiv.org/abs/2504.13143v1","updated":"2025-04-17T17:51:59Z","published":"2025-04-17T17:51:59Z","title":"$\\texttt{Complex-Edit}$: CoT-Like Instruction Generation for\n  Complexity-Controllable Image Editing Benchmark","summary":"  We introduce $\\texttt{Complex-Edit}$, a comprehensive benchmark designed to\nsystematically evaluate instruction-based image editing models across\ninstructions of varying complexity. To develop this benchmark, we harness\nGPT-4o to automatically collect a diverse set of editing instructions at scale.\nOur approach follows a well-structured ``Chain-of-Edit'' pipeline: we first\ngenerate individual atomic editing tasks independently and then integrate them\nto form cohesive, complex instructions. Additionally, we introduce a suite of\nmetrics to assess various aspects of editing performance, along with a\nVLM-based auto-evaluation pipeline that supports large-scale assessments. Our\nbenchmark yields several notable insights: 1) Open-source models significantly\nunderperform relative to proprietary, closed-source models, with the\nperformance gap widening as instruction complexity increases; 2) Increased\ninstructional complexity primarily impairs the models' ability to retain key\nelements from the input images and to preserve the overall aesthetic quality;\n3) Decomposing a complex instruction into a sequence of atomic steps, executed\nin a step-by-step manner, substantially degrades performance across multiple\nmetrics; 4) A straightforward Best-of-N selection strategy improves results for\nboth direct editing and the step-by-step sequential approach; and 5) We observe\na ``curse of synthetic data'': when synthetic data is involved in model\ntraining, the edited images from such models tend to appear increasingly\nsynthetic as the complexity of the editing instructions rises -- a phenomenon\nthat intriguingly also manifests in the latest GPT-4o outputs.\n","authors":["Siwei Yang","Mude Hui","Bingchen Zhao","Yuyin Zhou","Nataniel Ruiz","Cihang Xie"],"pdf_url":"https://arxiv.org/pdf/2504.13143v1.pdf","comment":"Project Page: https://ucsc-vlaa.github.io/Complex-Edit/, Dataset:\n  https://huggingface.co/datasets/UCSC-VLAA/Complex-Edit"},{"id":"http://arxiv.org/abs/2504.13140v1","updated":"2025-04-17T17:50:07Z","published":"2025-04-17T17:50:07Z","title":"PCBEAR: Pose Concept Bottleneck for Explainable Action Recognition","summary":"  Human action recognition (HAR) has achieved impressive results with deep\nlearning models, but their decision-making process remains opaque due to their\nblack-box nature. Ensuring interpretability is crucial, especially for\nreal-world applications requiring transparency and accountability. Existing\nvideo XAI methods primarily rely on feature attribution or static textual\nconcepts, both of which struggle to capture motion dynamics and temporal\ndependencies essential for action understanding. To address these challenges,\nwe propose Pose Concept Bottleneck for Explainable Action Recognition (PCBEAR),\na novel concept bottleneck framework that introduces human pose sequences as\nmotion-aware, structured concepts for video action recognition. Unlike methods\nbased on pixel-level features or static textual descriptions, PCBEAR leverages\nhuman skeleton poses, which focus solely on body movements, providing robust\nand interpretable explanations of motion dynamics. We define two types of\npose-based concepts: static pose concepts for spatial configurations at\nindividual frames, and dynamic pose concepts for motion patterns across\nmultiple frames. To construct these concepts, PCBEAR applies clustering to\nvideo pose sequences, allowing for automatic discovery of meaningful concepts\nwithout manual annotation. We validate PCBEAR on KTH, Penn-Action, and HAA500,\nshowing that it achieves high classification performance while offering\ninterpretable, motion-driven explanations. Our method provides both strong\npredictive performance and human-understandable insights into the model's\nreasoning process, enabling test-time interventions for debugging and improving\nmodel behavior.\n","authors":["Jongseo Lee","Wooil Lee","Gyeong-Moon Park","Seong Tae Kim","Jinwoo Choi"],"pdf_url":"https://arxiv.org/pdf/2504.13140v1.pdf","comment":"This paper is accepted by CVPRW 2025"},{"id":"http://arxiv.org/abs/2504.13131v1","updated":"2025-04-17T17:45:34Z","published":"2025-04-17T17:45:34Z","title":"NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and\n  Enhancement: Methods and Results","summary":"  This paper presents a review for the NTIRE 2025 Challenge on Short-form UGC\nVideo Quality Assessment and Enhancement. The challenge comprises two tracks:\n(i) Efficient Video Quality Assessment (KVQ), and (ii) Diffusion-based Image\nSuper-Resolution (KwaiSR). Track 1 aims to advance the development of\nlightweight and efficient video quality assessment (VQA) models, with an\nemphasis on eliminating reliance on model ensembles, redundant weights, and\nother computationally expensive components in the previous IQA/VQA\ncompetitions. Track 2 introduces a new short-form UGC dataset tailored for\nsingle image super-resolution, i.e., the KwaiSR dataset. It consists of 1,800\nsynthetically generated S-UGC image pairs and 1,900 real-world S-UGC images,\nwhich are split into training, validation, and test sets using a ratio of\n8:1:1. The primary objective of the challenge is to drive research that\nbenefits the user experience of short-form UGC platforms such as Kwai and\nTikTok. This challenge attracted 266 participants and received 18 valid final\nsubmissions with corresponding fact sheets, significantly contributing to the\nprogress of short-form UGC VQA and image superresolution. The project is\npublicly available at https://github.com/lixinustc/KVQE-\nChallengeCVPR-NTIRE2025.\n","authors":["Xin Li","Kun Yuan","Bingchen Li","Fengbin Guan","Yizhen Shao","Zihao Yu","Xijun Wang","Yiting Lu","Wei Luo","Suhang Yao","Ming Sun","Chao Zhou","Zhibo Chen","Radu Timofte","Yabin Zhang","Ao-Xiang Zhang","Tianwu Zhi","Jianzhao Liu","Yang Li","Jingwen Xu","Yiting Liao","Yushen Zuo","Mingyang Wu","Renjie Li","Shengyun Zhong","Zhengzhong Tu","Yufan Liu","Xiangguang Chen","Zuowei Cao","Minhao Tang","Shan Liu","Kexin Zhang","Jingfen Xie","Yan Wang","Kai Chen","Shijie Zhao","Yunchen Zhang","Xiangkai Xu","Hong Gao","Ji Shi","Yiming Bao","Xiugang Dong","Xiangsheng Zhou","Yaofeng Tu","Ying Liang","Yiwen Wang","Xinning Chai","Yuxuan Zhang","Zhengxue Cheng","Yingsheng Qin","Yucai Yang","Rong Xie","Li Song","Wei Sun","Kang Fu","Linhan Cao","Dandan Zhu","Kaiwei Zhang","Yucheng Zhu","Zicheng Zhang","Menghan Hu","Xiongkuo Min","Guangtao Zhai","Zhi Jin","Jiawei Wu","Wei Wang","Wenjian Zhang","Yuhai Lan","Gaoxiong Yi","Hengyuan Na","Wang Luo","Di Wu","MingYin Bai","Jiawang Du","Zilong Lu","Zhenyu Jiang","Hui Zeng","Ziguan Cui","Zongliang Gan","Guijin Tang","Xinglin Xie","Kehuan Song","Xiaoqiang Lu","Licheng Jiao","Fang Liu","Xu Liu","Puhua Chen","Ha Thu Nguyen","Katrien De Moor","Seyed Ali Amirshahi","Mohamed-Chaker Larabi","Qi Tang","Linfeng He","Zhiyong Gao","Zixuan Gao","Guohua Zhang","Zhiye Huang","Yi Deng","Qingmiao Jiang","Lu Chen","Yi Yang","Xi Liao","Nourine Mohammed Nadir","Yuxuan Jiang","Qiang Zhu","Siyue Teng","Fan Zhang","Shuyuan Zhu","Bing Zeng","David Bull","Meiqin Liu","Chao Yao","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.13131v1.pdf","comment":"Challenge Report of NTIRE 2025; Methods from 18 Teams; Accepted by\n  CVPR Workshop; 21 pages"},{"id":"http://arxiv.org/abs/2504.13129v1","updated":"2025-04-17T17:44:19Z","published":"2025-04-17T17:44:19Z","title":"Science-T2I: Addressing Scientific Illusions in Image Synthesis","summary":"  We present a novel approach to integrating scientific knowledge into\ngenerative models, enhancing their realism and consistency in image synthesis.\nFirst, we introduce Science-T2I, an expert-annotated adversarial dataset\ncomprising adversarial 20k image pairs with 9k prompts, covering wide distinct\nscientific knowledge categories. Leveraging Science-T2I, we present SciScore,\nan end-to-end reward model that refines the assessment of generated images\nbased on scientific knowledge, which is achieved by augmenting both the\nscientific comprehension and visual capabilities of pre-trained CLIP model.\nAdditionally, based on SciScore, we propose a two-stage training framework,\ncomprising a supervised fine-tuning phase and a masked online fine-tuning\nphase, to incorporate scientific knowledge into existing generative models.\nThrough comprehensive experiments, we demonstrate the effectiveness of our\nframework in establishing new standards for evaluating the scientific realism\nof generated content. Specifically, SciScore attains performance comparable to\nhuman-level, demonstrating a 5% improvement similar to evaluations conducted by\nexperienced human evaluators. Furthermore, by applying our proposed fine-tuning\nmethod to FLUX, we achieve a performance enhancement exceeding 50% on SciScore.\n","authors":["Jialuo Li","Wenhao Chai","Xingyu Fu","Haiyang Xu","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2504.13129v1.pdf","comment":"Accepted to CVPR 2025. Code, docs, weight, benchmark and training\n  data are all avaliable at https://jialuo-li.github.io/Science-T2I-Web"},{"id":"http://arxiv.org/abs/2504.13123v1","updated":"2025-04-17T17:40:06Z","published":"2025-04-17T17:40:06Z","title":"Low-hallucination Synthetic Captions for Large-Scale Vision-Language\n  Model Pre-training","summary":"  In recent years, the field of vision-language model pre-training has\nexperienced rapid advancements, driven primarily by the continuous enhancement\nof textual capabilities in large language models. However, existing training\nparadigms for multimodal large language models heavily rely on high-quality\nimage-text pairs. As models and data scales grow exponentially, the\navailability of such meticulously curated data has become increasingly scarce\nand saturated, thereby severely limiting further advancements in this domain.\nThis study investigates scalable caption generation techniques for\nvision-language model pre-training and demonstrates that large-scale\nlow-hallucination synthetic captions can serve dual purposes: 1) acting as a\nviable alternative to real-world data for pre-training paradigms and 2)\nachieving superior performance enhancement when integrated into vision-language\nmodels through empirical validation. This paper presents three key\ncontributions: 1) a novel pipeline for generating high-quality,\nlow-hallucination, and knowledge-rich synthetic captions. Our continuous DPO\nmethodology yields remarkable results in reducing hallucinations. Specifically,\nthe non-hallucination caption rate on a held-out test set increases from 48.2%\nto 77.9% for a 7B-size model. 2) Comprehensive empirical validation reveals\nthat our synthetic captions confer superior pre-training advantages over their\ncounterparts. Across 35 vision language tasks, the model trained with our data\nachieves a significant performance gain of at least 6.2% compared to alt-text\npairs and other previous work. Meanwhile, it also offers considerable support\nin the text-to-image domain. With our dataset, the FID score is reduced by 17.1\non a real-world validation benchmark and 13.3 on the MSCOCO validation\nbenchmark. 3) We will release Hunyuan-Recap100M, a low-hallucination and\nknowledge-intensive synthetic caption dataset.\n","authors":["Xinsong Zhang","Yarong Zeng","Xinting Huang","Hu Hu","Runquan Xie","Han Hu","Zhanhui Kang"],"pdf_url":"https://arxiv.org/pdf/2504.13123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13122v1","updated":"2025-04-17T17:39:41Z","published":"2025-04-17T17:39:41Z","title":"VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models","summary":"  Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO.\n","authors":["Haojian Huang","Haodong Chen","Shengqiong Wu","Meng Luo","Jinlan Fu","Xinya Du","Hanwang Zhang","Hao Fei"],"pdf_url":"https://arxiv.org/pdf/2504.13122v1.pdf","comment":"Code and Data: https://github.com/HaroldChen19/VistaDPO"},{"id":"http://arxiv.org/abs/2504.13120v1","updated":"2025-04-17T17:38:18Z","published":"2025-04-17T17:38:18Z","title":"Probing and Inducing Combinational Creativity in Vision-Language Models","summary":"  The ability to combine existing concepts into novel ideas stands as a\nfundamental hallmark of human intelligence. Recent advances in Vision-Language\nModels (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their\noutputs reflect combinational creativity--defined by M. A. Boden (1998) as\nsynthesizing novel ideas through combining existing concepts--or sophisticated\npattern matching of training data. Drawing inspiration from cognitive science,\nwe investigate the combinational creativity of VLMs from the lens of concept\nblending. We propose the Identification-Explanation-Implication (IEI)\nframework, which decomposes creative processes into three levels: identifying\ninput spaces, extracting shared attributes, and deriving novel semantic\nimplications. To validate this framework, we curate CreativeMashup, a\nhigh-quality dataset of 666 artist-generated visual mashups annotated according\nto the IEI framework. Through extensive experiments, we demonstrate that in\ncomprehension tasks, best VLMs have surpassed average human performance while\nfalling short of expert-level understanding; in generation tasks, incorporating\nour IEI framework into the generation pipeline significantly enhances the\ncreative quality of VLMs outputs. Our findings establish both a theoretical\nfoundation for evaluating artificial creativity and practical guidelines for\nimproving creative generation in VLMs.\n","authors":["Yongqian Peng","Yuxi Ma","Mengmeng Wang","Yuxuan Wang","Yizhou Wang","Chi Zhang","Yixin Zhu","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2504.13120v1.pdf","comment":"Project page: https://ppyyqq.github.io/aicc/ The first two authors\n  contribute equally"},{"id":"http://arxiv.org/abs/2504.13109v1","updated":"2025-04-17T17:24:23Z","published":"2025-04-17T17:24:23Z","title":"UniEdit-Flow: Unleashing Inversion and Editing in the Era of Flow Models","summary":"  Flow matching models have emerged as a strong alternative to diffusion\nmodels, but existing inversion and editing methods designed for diffusion are\noften ineffective or inapplicable to them. The straight-line, non-crossing\ntrajectories of flow models pose challenges for diffusion-based approaches but\nalso open avenues for novel solutions. In this paper, we introduce a\npredictor-corrector-based framework for inversion and editing in flow models.\nFirst, we propose Uni-Inv, an effective inversion method designed for accurate\nreconstruction. Building on this, we extend the concept of delayed injection to\nflow models and introduce Uni-Edit, a region-aware, robust image editing\napproach. Our methodology is tuning-free, model-agnostic, efficient, and\neffective, enabling diverse edits while ensuring strong preservation of\nedit-irrelevant regions. Extensive experiments across various generative models\ndemonstrate the superiority and generalizability of Uni-Inv and Uni-Edit, even\nunder low-cost settings. Project page: https://uniedit-flow.github.io/\n","authors":["Guanlong Jiao","Biqing Huang","Kuan-Chieh Wang","Renjie Liao"],"pdf_url":"https://arxiv.org/pdf/2504.13109v1.pdf","comment":"Project page: https://uniedit-flow.github.io/"},{"id":"http://arxiv.org/abs/2501.09012v2","updated":"2025-04-17T17:14:09Z","published":"2025-01-15T18:56:22Z","title":"Multimodal LLMs Can Reason about Aesthetics in Zero-Shot","summary":"  The rapid progress of generative art has democratized the creation of\nvisually pleasing imagery. However, achieving genuine artistic impact - the\nkind that resonates with viewers on a deeper, more meaningful level - requires\na sophisticated aesthetic sensibility. This sensibility involves a\nmulti-faceted reasoning process extending beyond mere visual appeal, which is\noften overlooked by current computational models. This paper pioneers an\napproach to capture this complex process by investigating how the reasoning\ncapabilities of Multimodal LLMs (MLLMs) can be effectively elicited for\naesthetic judgment. Our analysis reveals a critical challenge: MLLMs exhibit a\ntendency towards hallucinations during aesthetic reasoning, characterized by\nsubjective opinions and unsubstantiated artistic interpretations. We further\ndemonstrate that these limitations can be overcome by employing an\nevidence-based, objective reasoning process, as substantiated by our proposed\nbaseline, ArtCoT. MLLMs prompted by this principle produce multi-faceted and\nin-depth aesthetic reasoning that aligns significantly better with human\njudgment. These findings have direct applications in areas such as AI art\ntutoring and as reward models for generative art. Ultimately, our work paves\nthe way for AI systems that can truly understand, appreciate, and generate\nartworks that align with the sensible human aesthetic standard.\n","authors":["Ruixiang Jiang","Changwen Chen"],"pdf_url":"https://arxiv.org/pdf/2501.09012v2.pdf","comment":"WIP, Homepage https://github.com/songrise/MLLM4Art"},{"id":"http://arxiv.org/abs/2504.13099v1","updated":"2025-04-17T17:08:11Z","published":"2025-04-17T17:08:11Z","title":"RF-DETR Object Detection vs YOLOv12 : A Study of Transformer-based and\n  CNN-based Architectures for Single-Class and Multi-Class Greenfruit Detection\n  in Complex Orchard Environments Under Label Ambiguity","summary":"  This study conducts a detailed comparison of RF-DETR object detection base\nmodel and YOLOv12 object detection model configurations for detecting\ngreenfruits in a complex orchard environment marked by label ambiguity,\nocclusions, and background blending. A custom dataset was developed featuring\nboth single-class (greenfruit) and multi-class (occluded and non-occluded\ngreenfruits) annotations to assess model performance under dynamic real-world\nconditions. RF-DETR object detection model, utilizing a DINOv2 backbone and\ndeformable attention, excelled in global context modeling, effectively\nidentifying partially occluded or ambiguous greenfruits. In contrast, YOLOv12\nleveraged CNN-based attention for enhanced local feature extraction, optimizing\nit for computational efficiency and edge deployment. RF-DETR achieved the\nhighest mean Average Precision (mAP50) of 0.9464 in single-class detection,\nproving its superior ability to localize greenfruits in cluttered scenes.\nAlthough YOLOv12N recorded the highest mAP@50:95 of 0.7620, RF-DETR\nconsistently outperformed in complex spatial scenarios. For multi-class\ndetection, RF-DETR led with an mAP@50 of 0.8298, showing its capability to\ndifferentiate between occluded and non-occluded fruits, while YOLOv12L scored\nhighest in mAP@50:95 with 0.6622, indicating better classification in detailed\nocclusion contexts. Training dynamics analysis highlighted RF-DETR's swift\nconvergence, particularly in single-class settings where it plateaued within 10\nepochs, demonstrating the efficiency of transformer-based architectures in\nadapting to dynamic visual data. These findings validate RF-DETR's\neffectiveness for precision agricultural applications, with YOLOv12 suited for\nfast-response scenarios. >Index Terms: RF-DETR object detection, YOLOv12,\nYOLOv13, YOLOv14, YOLOv15, YOLOE, YOLO World, YOLO, You Only Look Once,\nRoboflow, Detection Transformers, CNNs\n","authors":["Ranjan Sapkota","Rahul Harsha Cheppally","Ajay Sharda","Manoj Karkee"],"pdf_url":"https://arxiv.org/pdf/2504.13099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13092v1","updated":"2025-04-17T16:59:04Z","published":"2025-04-17T16:59:04Z","title":"EventVAD: Training-Free Event-Aware Video Anomaly Detection","summary":"  Video Anomaly Detection~(VAD) focuses on identifying anomalies within videos.\nSupervised methods require an amount of in-domain training data and often\nstruggle to generalize to unseen anomalies. In contrast, training-free methods\nleverage the intrinsic world knowledge of large language models (LLMs) to\ndetect anomalies but face challenges in localizing fine-grained visual\ntransitions and diverse events. Therefore, we propose EventVAD, an event-aware\nvideo anomaly detection framework that combines tailored dynamic graph\narchitectures and multimodal LLMs through temporal-event reasoning.\nSpecifically, EventVAD first employs dynamic spatiotemporal graph modeling with\ntime-decay constraints to capture event-aware video features. Then, it performs\nadaptive noise filtering and uses signal ratio thresholding to detect event\nboundaries via unsupervised statistical features. The statistical boundary\ndetection module reduces the complexity of processing long videos for MLLMs and\nimproves their temporal reasoning through event consistency. Finally, it\nutilizes a hierarchical prompting strategy to guide MLLMs in performing\nreasoning before determining final decisions. We conducted extensive\nexperiments on the UCF-Crime and XD-Violence datasets. The results demonstrate\nthat EventVAD with a 7B MLLM achieves state-of-the-art (SOTA) in training-free\nsettings, outperforming strong baselines that use 7B or larger MLLMs.\n","authors":["Yihua Shao","Haojin He","Sijie Li","Siyu Chen","Xinwei Long","Fanhu Zeng","Yuxuan Fan","Muyang Zhang","Ziyang Yan","Ao Ma","Xiaochen Wang","Hao Tang","Yan Wang","Shuyan Li"],"pdf_url":"https://arxiv.org/pdf/2504.13092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13078v1","updated":"2025-04-17T16:45:18Z","published":"2025-04-17T16:45:18Z","title":"Enhancing Person-to-Person Virtual Try-On with Multi-Garment Virtual\n  Try-Off","summary":"  Computer vision is transforming fashion through Virtual Try-On (VTON) and\nVirtual Try-Off (VTOFF). VTON generates images of a person in a specified\ngarment using a target photo and a standardized garment image, while a more\nchallenging variant, Person-to-Person Virtual Try-On (p2p-VTON), uses a photo\nof another person wearing the garment. VTOFF, on the other hand, extracts\nstandardized garment images from clothed individuals. We introduce TryOffDiff,\na diffusion-based VTOFF model. Built on a latent diffusion framework with\nSigLIP image conditioning, it effectively captures garment properties like\ntexture, shape, and patterns. TryOffDiff achieves state-of-the-art results on\nVITON-HD and strong performance on DressCode dataset, covering upper-body,\nlower-body, and dresses. Enhanced with class-specific embeddings, it pioneers\nmulti-garment VTOFF, the first of its kind. When paired with VTON models, it\nimproves p2p-VTON by minimizing unwanted attribute transfer, such as skin\ncolor. Code is available at: https://rizavelioglu.github.io/tryoffdiff/\n","authors":["Riza Velioglu","Petra Bevandic","Robin Chan","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2504.13078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.01632v2","updated":"2025-04-17T16:43:20Z","published":"2025-04-02T11:37:39Z","title":"Benchmarking the Spatial Robustness of DNNs via Natural and Adversarial\n  Localized Corruptions","summary":"  The robustness of DNNs is a crucial factor in safety-critical applications,\nparticularly in complex and dynamic environments where localized corruptions\ncan arise. While previous studies have evaluated the robustness of semantic\nsegmentation (SS) models under whole-image natural or adversarial corruptions,\na comprehensive investigation into the spatial robustness of dense vision\nmodels under localized corruptions remained underexplored. This paper fills\nthis gap by introducing specialized metrics for benchmarking the spatial\nrobustness of segmentation models, alongside with an evaluation framework to\nassess the impact of localized corruptions. Furthermore, we uncover the\ninherent complexity of characterizing worst-case robustness using a single\nlocalized adversarial perturbation. To address this, we propose region-aware\nmulti-attack adversarial analysis, a method that enables a deeper understanding\nof model robustness against adversarial perturbations applied to specific\nregions. The proposed metrics and analysis were exploited to evaluate 14\nsegmentation models in driving scenarios, uncovering key insights into the\neffects of localized corruption in both natural and adversarial forms. The\nresults reveal that models respond to these two types of threats differently;\nfor instance, transformer-based segmentation models demonstrate notable\nrobustness to localized natural corruptions but are highly vulnerable to\nadversarial ones and vice-versa for CNN-based models. Consequently, we also\naddress the challenge of balancing robustness to both natural and adversarial\nlocalized corruptions by means of ensemble models, thereby achieving a broader\nthreat coverage and improved reliability for dense vision tasks.\n","authors":["Giulia Marchiori Pietrosanti","Giulio Rossolini","Alessandro Biondi","Giorgio Buttazzo"],"pdf_url":"https://arxiv.org/pdf/2504.01632v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2504.13077v1","updated":"2025-04-17T16:42:33Z","published":"2025-04-17T16:42:33Z","title":"Effective Dual-Region Augmentation for Reduced Reliance on Large Amounts\n  of Labeled Data","summary":"  This paper introduces a novel dual-region augmentation approach designed to\nreduce reliance on large-scale labeled datasets while improving model\nrobustness and adaptability across diverse computer vision tasks, including\nsource-free domain adaptation (SFDA) and person re-identification (ReID). Our\nmethod performs targeted data transformations by applying random noise\nperturbations to foreground objects and spatially shuffling background patches.\nThis effectively increases the diversity of the training data, improving model\nrobustness and generalization. Evaluations on the PACS dataset for SFDA\ndemonstrate that our augmentation strategy consistently outperforms existing\nmethods, achieving significant accuracy improvements in both single-target and\nmulti-target adaptation settings. By augmenting training data through\nstructured transformations, our method enables model generalization across\ndomains, providing a scalable solution for reducing reliance on manually\nannotated datasets. Furthermore, experiments on Market-1501 and DukeMTMC-reID\ndatasets validate the effectiveness of our approach for person ReID, surpassing\ntraditional augmentation techniques.\n","authors":["Prasanna Reddy Pulakurthi","Majid Rabbani","Celso M. de Melo","Sohail A. Dianat","Raghuveer M. Rao"],"pdf_url":"https://arxiv.org/pdf/2504.13077v1.pdf","comment":"9 pages, 2 figures, Accepted to SPIE DSC 2025 Conference: Synthetic\n  Data for Artificial Intelligence and Machine Learning: Tools, Techniques, and\n  Applications III"},{"id":"http://arxiv.org/abs/2504.13074v1","updated":"2025-04-17T16:37:27Z","published":"2025-04-17T16:37:27Z","title":"SkyReels-V2: Infinite-length Film Generative Model","summary":"  Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2.\n","authors":["Guibin Chen","Dixuan Lin","Jiangping Yang","Chunze Lin","Juncheng Zhu","Mingyuan Fan","Hao Zhang","Sheng Chen","Zheng Chen","Chengchen Ma","Weiming Xiong","Wei Wang","Nuo Pang","Kang Kang","Zhiheng Xu","Yuzhe Jin","Yupeng Liang","Yubing Song","Peng Zhao","Boyuan Xu","Di Qiu","Debang Li","Zhengcong Fei","Yang Li","Yahui Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.13074v1.pdf","comment":"31 pages,10 figures"},{"id":"http://arxiv.org/abs/2504.13072v1","updated":"2025-04-17T16:33:39Z","published":"2025-04-17T16:33:39Z","title":"HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation","summary":"  Scene-level 3D generation represents a critical frontier in multimedia and\ncomputer graphics, yet existing approaches either suffer from limited object\ncategories or lack editing flexibility for interactive applications. In this\npaper, we present HiScene, a novel hierarchical framework that bridges the gap\nbetween 2D image generation and 3D object generation and delivers high-fidelity\nscenes with compositional identities and aesthetic scene content. Our key\ninsight is treating scenes as hierarchical \"objects\" under isometric views,\nwhere a room functions as a complex object that can be further decomposed into\nmanipulatable items. This hierarchical approach enables us to generate 3D\ncontent that aligns with 2D representations while maintaining compositional\nstructure. To ensure completeness and spatial alignment of each decomposed\ninstance, we develop a video-diffusion-based amodal completion technique that\neffectively handles occlusions and shadows between objects, and introduce shape\nprior injection to ensure spatial coherence within the scene. Experimental\nresults demonstrate that our method produces more natural object arrangements\nand complete object instances suitable for interactive applications, while\nmaintaining physical plausibility and alignment with user inputs.\n","authors":["Wenqi Dong","Bangbang Yang","Zesong Yang","Yuan Li","Tao Hu","Hujun Bao","Yuewen Ma","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2504.13072v1.pdf","comment":"Project webpage: https://zju3dv.github.io/hiscene/"},{"id":"http://arxiv.org/abs/2504.13065v1","updated":"2025-04-17T16:19:05Z","published":"2025-04-17T16:19:05Z","title":"EchoWorld: Learning Motion-Aware World Models for Echocardiography Probe\n  Guidance","summary":"  Echocardiography is crucial for cardiovascular disease detection but relies\nheavily on experienced sonographers. Echocardiography probe guidance systems,\nwhich provide real-time movement instructions for acquiring standard plane\nimages, offer a promising solution for AI-assisted or fully autonomous\nscanning. However, developing effective machine learning models for this task\nremains challenging, as they must grasp heart anatomy and the intricate\ninterplay between probe motion and visual signals. To address this, we present\nEchoWorld, a motion-aware world modeling framework for probe guidance that\nencodes anatomical knowledge and motion-induced visual dynamics, while\neffectively leveraging past visual-motion sequences to enhance guidance\nprecision. EchoWorld employs a pre-training strategy inspired by world modeling\nprinciples, where the model predicts masked anatomical regions and simulates\nthe visual outcomes of probe adjustments. Built upon this pre-trained model, we\nintroduce a motion-aware attention mechanism in the fine-tuning stage that\neffectively integrates historical visual-motion data, enabling precise and\nadaptive probe guidance. Trained on more than one million ultrasound images\nfrom over 200 routine scans, EchoWorld effectively captures key\nechocardiographic knowledge, as validated by qualitative analysis. Moreover,\nour method significantly reduces guidance errors compared to existing visual\nbackbones and guidance frameworks, excelling in both single-frame and\nsequential evaluation protocols. Code is available at\nhttps://github.com/LeapLabTHU/EchoWorld.\n","authors":["Yang Yue","Yulin Wang","Haojun Jiang","Pan Liu","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2504.13065v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2504.13061v1","updated":"2025-04-17T16:15:38Z","published":"2025-04-17T16:15:38Z","title":"ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation\n  Models","summary":"  Text-to-image models based on diffusion processes, such as DALL-E, Stable\nDiffusion, and Midjourney, are capable of transforming texts into detailed\nimages and have widespread applications in art and design. As such, amateur\nusers can easily imitate professional-level paintings by collecting an artist's\nwork and fine-tuning the model, leading to concerns about artworks' copyright\ninfringement. To tackle these issues, previous studies either add visually\nimperceptible perturbation to the artwork to change its underlying styles\n(perturbation-based methods) or embed post-training detectable watermarks in\nthe artwork (watermark-based methods). However, when the artwork or the model\nhas been published online, i.e., modification to the original artwork or model\nretraining is not feasible, these strategies might not be viable.\n  To this end, we propose a novel method for data-use auditing in the\ntext-to-image generation model. The general idea of ArtistAuditor is to\nidentify if a suspicious model has been finetuned using the artworks of\nspecific artists by analyzing the features related to the style. Concretely,\nArtistAuditor employs a style extractor to obtain the multi-granularity style\nrepresentations and treats artworks as samplings of an artist's style. Then,\nArtistAuditor queries a trained discriminator to gain the auditing decisions.\nThe experimental results on six combinations of models and datasets show that\nArtistAuditor can achieve high AUC values (> 0.937). By studying\nArtistAuditor's transferability and core modules, we provide valuable insights\ninto the practical implementation. Finally, we demonstrate the effectiveness of\nArtistAuditor in real-world cases by an online platform Scenario. ArtistAuditor\nis open-sourced at https://github.com/Jozenn/ArtistAuditor.\n","authors":["Linkang Du","Zheng Zhu","Min Chen","Zhou Su","Shouling Ji","Peng Cheng","Jiming Chen","Zhikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.13061v1.pdf","comment":"To appear in the ACM Web Conference 2025, Sydney, Australia"},{"id":"http://arxiv.org/abs/2504.13060v1","updated":"2025-04-17T16:14:34Z","published":"2025-04-17T16:14:34Z","title":"Imaging for All-Day Wearable Smart Glasses","summary":"  In recent years smart glasses technology has rapidly advanced, opening up\nentirely new areas for mobile computing. We expect future smart glasses will\nneed to be all-day wearable, adopting a small form factor to meet the\nrequirements of volume, weight, fashionability and social acceptability, which\nputs significant constraints on the space of possible solutions. Additional\nchallenges arise due to the fact that smart glasses are worn in arbitrary\nenvironments while their wearer moves and performs everyday activities. In this\npaper, we systematically analyze the space of imaging from smart glasses and\nderive several fundamental limits that govern this imaging domain. We discuss\nthe impact of these limits on achievable image quality and camera module size\n-- comparing in particular to related devices such as mobile phones. We then\npropose a novel distributed imaging approach that allows to minimize the size\nof the individual camera modules when compared to a standard monolithic camera\ndesign. Finally, we demonstrate the properties of this novel approach in a\nseries of experiments using synthetic data as well as images captured with two\ndifferent prototype implementations.\n","authors":["Michael Goesele","Daniel Andersen","Yujia Chen","Simon Green","Eddy Ilg","Chao Li","Johnson Liu","Grace Kuo","Logan Wan","Richard Newcombe"],"pdf_url":"https://arxiv.org/pdf/2504.13060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13055v1","updated":"2025-04-17T16:10:13Z","published":"2025-04-17T16:10:13Z","title":"NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation","summary":"  Recent advances in reinforcement learning (RL) have strengthened the\nreasoning capabilities of vision-language models (VLMs). However, enhancing\npolicy exploration to more effectively scale test-time compute remains\nunderexplored in VLMs. In addition, VLMs continue to struggle with imperfect\nvisual perception, which in turn affects the subsequent reasoning process. To\nthis end, we propose NoisyRollout, a simple yet effective RL approach that\nmixes trajectories from both clean and moderately distorted images to introduce\ntargeted diversity in visual perception and the resulting reasoning patterns.\nWithout additional training cost, NoisyRollout enhances the exploration\ncapabilities of VLMs by incorporating a vision-oriented inductive bias.\nFurthermore, NoisyRollout employs a noise annealing schedule that gradually\nreduces distortion strength over training, ensuring benefit from noisy signals\nearly while maintaining training stability and scalability in later stages.\nWith just 2.1K training samples, NoisyRollout achieves state-of-the-art\nperformance among open-source RL-tuned models on 5 out-of-domain benchmarks\nspanning both reasoning and perception tasks, while preserving comparable or\neven better in-domain performance.\n","authors":["Xiangyan Liu","Jinjie Ni","Zijian Wu","Chao Du","Longxu Dou","Haonan Wang","Tianyu Pang","Michael Qizhe Shieh"],"pdf_url":"https://arxiv.org/pdf/2504.13055v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2411.07863v3","updated":"2025-04-17T16:05:10Z","published":"2024-11-12T15:22:14Z","title":"CDXLSTM: Boosting Remote Sensing Change Detection with Extended Long\n  Short-Term Memory","summary":"  In complex scenes and varied conditions, effectively integrating\nspatial-temporal context is crucial for accurately identifying changes.\nHowever, current RS-CD methods lack a balanced consideration of performance and\nefficiency. CNNs lack global context, Transformers are computationally\nexpensive, and Mambas face CUDA dependence and local correlation loss. In this\npaper, we propose CDXLSTM, with a core component that is a powerful XLSTM-based\nfeature enhancement layer, integrating the advantages of linear computational\ncomplexity, global context perception, and strong interpret-ability.\nSpecifically, we introduce a scale-specific Feature Enhancer layer,\nincorporating a Cross-Temporal Global Perceptron customized for\nsemantic-accurate deep features, and a Cross-Temporal Spatial Refiner\ncustomized for detail-rich shallow features. Additionally, we propose a\nCross-Scale Interactive Fusion module to progressively interact global change\nrepresentations with spatial responses. Extensive experimental results\ndemonstrate that CDXLSTM achieves state-of-the-art performance across three\nbenchmark datasets, offering a compelling balance between efficiency and\naccuracy. Code is available at https://github.com/xwmaxwma/rschange.\n","authors":["Zhenkai Wu","Xiaowen Ma","Rongrong Lian","Kai Zheng","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.07863v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07664v2","updated":"2025-04-17T16:04:48Z","published":"2024-07-10T13:44:19Z","title":"A Coding-Theoretic Analysis of Hyperspherical Prototypical Learning\n  Geometry","summary":"  Hyperspherical Prototypical Learning (HPL) is a supervised approach to\nrepresentation learning that designs class prototypes on the unit hypersphere.\nThe prototypes bias the representations to class separation in a scale\ninvariant and known geometry. Previous approaches to HPL have either of the\nfollowing shortcomings: (i) they follow an unprincipled optimisation procedure;\nor (ii) they are theoretically sound, but are constrained to only one possible\nlatent dimension. In this paper, we address both shortcomings. To address (i),\nwe present a principled optimisation procedure whose solution we show is\noptimal. To address (ii), we construct well-separated prototypes in a wide\nrange of dimensions using linear block codes. Additionally, we give a full\ncharacterisation of the optimal prototype placement in terms of achievable and\nconverse bounds, showing that our proposed methods are near-optimal.\n","authors":["Martin Lindström","Borja Rodríguez-Gálvez","Ragnar Thobaben","Mikael Skoglund"],"pdf_url":"https://arxiv.org/pdf/2407.07664v2.pdf","comment":"Changes in version 2: Minor formatting changes. Published in the\n  Proceedings of the Geometry-grounded Representation Learning and Generative\n  Modeling Workshop (GRaM), PMLR 251. Available at:\n  https://proceedings.mlr.press/v251/lindstrom24a.html 14 pages: 9 of the main\n  paper, 2 of references, and 3 of appendices.. Code is available at:\n  https://github.com/martinlindstrom/coding_theoretic_hpl"},{"id":"http://arxiv.org/abs/2504.13045v1","updated":"2025-04-17T16:00:06Z","published":"2025-04-17T16:00:06Z","title":"Expert Kernel Generation Network Driven by Contextual Mapping for\n  Hyperspectral Image Classification","summary":"  Deep neural networks face several challenges in hyperspectral image\nclassification, including high-dimensional data, sparse distribution of ground\nobjects, and spectral redundancy, which often lead to classification\noverfitting and limited generalization capability. To more efficiently adapt to\nground object distributions while extracting image features without introducing\nexcessive parameters and skipping redundant information, this paper proposes\nEKGNet based on an improved 3D-DenseNet model, consisting of a context-aware\nmapping network and a dynamic kernel generation module. The context-aware\nmapping module translates global contextual information of hyperspectral inputs\ninto instructions for combining base convolutional kernels, while the dynamic\nkernels are composed of K groups of base convolutions, analogous to K different\ntypes of experts specializing in fundamental patterns across various\ndimensions. The mapping module and dynamic kernel generation mechanism form a\ntightly coupled system - the former generates meaningful combination weights\nbased on inputs, while the latter constructs an adaptive expert convolution\nsystem using these weights. This dynamic approach enables the model to focus\nmore flexibly on key spatial structures when processing different regions,\nrather than relying on the fixed receptive field of a single static\nconvolutional kernel. EKGNet enhances model representation capability through a\n3D dynamic expert convolution system without increasing network depth or width.\nThe proposed method demonstrates superior performance on IN, UP, and KSC\ndatasets, outperforming mainstream hyperspectral image classification\napproaches.\n","authors":["Guandong Li","Mengxia Ye"],"pdf_url":"https://arxiv.org/pdf/2504.13045v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2503.23472"},{"id":"http://arxiv.org/abs/2504.13042v1","updated":"2025-04-17T15:55:41Z","published":"2025-04-17T15:55:41Z","title":"Event-Enhanced Blurry Video Super-Resolution","summary":"  In this paper, we tackle the task of blurry video super-resolution (BVSR),\naiming to generate high-resolution (HR) videos from low-resolution (LR) and\nblurry inputs. Current BVSR methods often fail to restore sharp details at high\nresolutions, resulting in noticeable artifacts and jitter due to insufficient\nmotion information for deconvolution and the lack of high-frequency details in\nLR frames. To address these challenges, we introduce event signals into BVSR\nand propose a novel event-enhanced network, Ev-DeblurVSR. To effectively fuse\ninformation from frames and events for feature deblurring, we introduce a\nreciprocal feature deblurring module that leverages motion information from\nintra-frame events to deblur frame features while reciprocally using global\nscene context from the frames to enhance event features. Furthermore, to\nenhance temporal consistency, we propose a hybrid deformable alignment module\nthat fully exploits the complementary motion information from inter-frame\nevents and optical flow to improve motion estimation in the deformable\nalignment process. Extensive evaluations demonstrate that Ev-DeblurVSR\nestablishes a new state-of-the-art performance on both synthetic and real-world\ndatasets. Notably, on real data, our method is +2.59 dB more accurate and\n7.28$\\times$ faster than the recent best BVSR baseline FMA-Net. Code:\nhttps://github.com/DachunKai/Ev-DeblurVSR.\n","authors":["Dachun Kai","Yueyi Zhang","Jin Wang","Zeyu Xiao","Zhiwei Xiong","Xiaoyan Sun"],"pdf_url":"https://arxiv.org/pdf/2504.13042v1.pdf","comment":"AAAI 2025. Project page:\n  https://dachunkai.github.io/evtexture.github.io/"},{"id":"http://arxiv.org/abs/2408.01934v5","updated":"2025-04-17T15:52:09Z","published":"2024-08-04T05:22:08Z","title":"A Survey and Evaluation of Adversarial Attacks for Object Detection","summary":"  Deep learning models achieve remarkable accuracy in computer vision tasks,\nyet remain vulnerable to adversarial examples--carefully crafted perturbations\nto input images that can deceive these models into making confident but\nincorrect predictions. This vulnerability pose significant risks in high-stakes\napplications such as autonomous vehicles, security surveillance, and\nsafety-critical inspection systems. While the existing literature extensively\ncovers adversarial attacks in image classification, comprehensive analyses of\nsuch attacks on object detection systems remain limited. This paper presents a\nnovel taxonomic framework for categorizing adversarial attacks specific to\nobject detection architectures, synthesizes existing robustness metrics, and\nprovides a comprehensive empirical evaluation of state-of-the-art attack\nmethodologies on popular object detection models, including both traditional\ndetectors and modern detectors with vision-language pretraining. Through\nrigorous analysis of open-source attack implementations and their effectiveness\nacross diverse detection architectures, we derive key insights into attack\ncharacteristics. Furthermore, we delineate critical research gaps and emerging\nchallenges to guide future investigations in securing object detection systems\nagainst adversarial threats. Our findings establish a foundation for developing\nmore robust detection models while highlighting the urgent need for\nstandardized evaluation protocols in this rapidly evolving domain.\n","authors":["Khoi Nguyen Tiet Nguyen","Wenyu Zhang","Kangkang Lu","Yuhuan Wu","Xingjian Zheng","Hui Li Tan","Liangli Zhen"],"pdf_url":"https://arxiv.org/pdf/2408.01934v5.pdf","comment":"Accepted for publication in the IEEE Transactions on Neural Networks\n  and Learning Systems (TNNLS)"},{"id":"http://arxiv.org/abs/2504.13037v1","updated":"2025-04-17T15:46:19Z","published":"2025-04-17T15:46:19Z","title":"Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular\n  Representations for Whole-Heart Assessment and Beyond","summary":"  Cardiac magnetic resonance imaging is the gold standard for non-invasive\ncardiac assessment, offering rich spatio-temporal views of the cardiac anatomy\nand physiology. Patient-level health factors, such as demographics, metabolic,\nand lifestyle, are known to substantially influence cardiovascular health and\ndisease risk, yet remain uncaptured by CMR alone. To holistically understand\ncardiac health and to enable the best possible interpretation of an\nindividual's disease risk, CMR and patient-level factors must be jointly\nexploited within an integrated framework. Recent multi-modal approaches have\nbegun to bridge this gap, yet they often rely on limited spatio-temporal data\nand focus on isolated clinical tasks, thereby hindering the development of a\ncomprehensive representation for cardiac health evaluation. To overcome these\nlimitations, we introduce ViTa, a step toward foundation models that delivers a\ncomprehensive representation of the heart and a precise interpretation of\nindividual disease risk. Leveraging data from 42,000 UK Biobank participants,\nViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling\na complete capture of the cardiac cycle. These imaging data are then fused with\ndetailed tabular patient-level factors, enabling context-aware insights. This\nmulti-modal paradigm supports a wide spectrum of downstream tasks, including\ncardiac phenotype and physiological feature prediction, segmentation, and\nclassification of cardiac and metabolic diseases within a single unified\nframework. By learning a shared latent representation that bridges rich imaging\nfeatures and patient context, ViTa moves beyond traditional, task-specific\nmodels toward a universal, patient-specific understanding of cardiac health,\nhighlighting its potential to advance clinical utility and scalability in\ncardiac analysis.\n","authors":["Yundi Zhang","Paul Hager","Che Liu","Suprosanna Shit","Chen Chen","Daniel Rueckert","Jiazhen Pan"],"pdf_url":"https://arxiv.org/pdf/2504.13037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13035v1","updated":"2025-04-17T15:43:29Z","published":"2025-04-17T15:43:29Z","title":"Prototypes are Balanced Units for Efficient and Effective Partially\n  Relevant Video Retrieval","summary":"  In a retrieval system, simultaneously achieving search accuracy and\nefficiency is inherently challenging. This challenge is particularly pronounced\nin partially relevant video retrieval (PRVR), where incorporating more diverse\ncontext representations at varying temporal scales for each video enhances\naccuracy but increases computational and memory costs. To address this\ndichotomy, we propose a prototypical PRVR framework that encodes diverse\ncontexts within a video into a fixed number of prototypes. We then introduce\nseveral strategies to enhance text association and video understanding within\nthe prototypes, along with an orthogonal objective to ensure that the\nprototypes capture a diverse range of content. To keep the prototypes\nsearchable via text queries while accurately encoding video contexts, we\nimplement cross- and uni-modal reconstruction tasks. The cross-modal\nreconstruction task aligns the prototypes with textual features within a shared\nspace, while the uni-modal reconstruction task preserves all video contexts\nduring encoding. Additionally, we employ a video mixing technique to provide\nweak guidance to further align prototypes and associated textual\nrepresentations. Extensive evaluations on TVR, ActivityNet-Captions, and\nQVHighlights validate the effectiveness of our approach without sacrificing\nefficiency.\n","authors":["WonJun Moon","Cheol-Ho Cho","Woojin Jun","Minho Shim","Taeoh Kim","Inwoong Lee","Dongyoon Wee","Jae-Pil Heo"],"pdf_url":"https://arxiv.org/pdf/2504.13035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13026v1","updated":"2025-04-17T15:37:13Z","published":"2025-04-17T15:37:13Z","title":"TTRD3: Texture Transfer Residual Denoising Dual Diffusion Model for\n  Remote Sensing Image Super-Resolution","summary":"  Remote Sensing Image Super-Resolution (RSISR) reconstructs high-resolution\n(HR) remote sensing images from low-resolution inputs to support fine-grained\nground object interpretation. Existing methods face three key challenges: (1)\nDifficulty in extracting multi-scale features from spatially heterogeneous RS\nscenes, (2) Limited prior information causing semantic inconsistency in\nreconstructions, and (3) Trade-off imbalance between geometric accuracy and\nvisual quality. To address these issues, we propose the Texture Transfer\nResidual Denoising Dual Diffusion Model (TTRD3) with three innovations: First,\na Multi-scale Feature Aggregation Block (MFAB) employing parallel heterogeneous\nconvolutional kernels for multi-scale feature extraction. Second, a Sparse\nTexture Transfer Guidance (STTG) module that transfers HR texture priors from\nreference images of similar scenes. Third, a Residual Denoising Dual Diffusion\nModel (RDDM) framework combining residual diffusion for deterministic\nreconstruction and noise diffusion for diverse generation. Experiments on\nmulti-source RS datasets demonstrate TTRD3's superiority over state-of-the-art\nmethods, achieving 1.43% LPIPS improvement and 3.67% FID enhancement compared\nto best-performing baselines. Code/model: https://github.com/LED-666/TTRD3.\n","authors":["Yide Liu","Haijiang Sun","Xiaowen Zhang","Qiaoyuan Liu","Zhouchang Chen","Chongzhuo Xiao"],"pdf_url":"https://arxiv.org/pdf/2504.13026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13024v1","updated":"2025-04-17T15:34:58Z","published":"2025-04-17T15:34:58Z","title":"Riemannian Patch Assignment Gradient Flows","summary":"  This paper introduces patch assignment flows for metric data labeling on\ngraphs. Labelings are determined by regularizing initial local labelings\nthrough the dynamic interaction of both labels and label assignments across the\ngraph, entirely encoded by a dictionary of competing labeled patches and\nmediated by patch assignment variables. Maximal consistency of patch\nassignments is achieved by geometric numerical integration of a Riemannian\nascent flow, as critical point of a Lagrangian action functional. Experiments\nillustrate properties of the approach, including uncertainty quantification of\nlabel assignments.\n","authors":["Daniel Gonzalez-Alvarado","Fabio Schlindwein","Jonas Cassel","Laura Steingruber","Stefania Petra","Christoph Schnörr"],"pdf_url":"https://arxiv.org/pdf/2504.13024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12080v2","updated":"2025-04-17T15:34:30Z","published":"2025-04-16T13:41:59Z","title":"DC-SAM: In-Context Segment Anything in Images and Videos via Dual\n  Consistency","summary":"  Given a single labeled example, in-context segmentation aims to segment\ncorresponding objects. This setting, known as one-shot segmentation in few-shot\nlearning, explores the segmentation model's generalization ability and has been\napplied to various vision tasks, including scene understanding and image/video\nediting. While recent Segment Anything Models have achieved state-of-the-art\nresults in interactive segmentation, these approaches are not directly\napplicable to in-context segmentation. In this work, we propose the Dual\nConsistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2\nfor in-context segmentation of both images and videos. Our key insights are to\nenhance the features of the SAM's prompt encoder in segmentation by providing\nhigh-quality visual prompts. When generating a mask prior, we fuse the SAM\nfeatures to better align the prompt encoder. Then, we design a cycle-consistent\ncross-attention on fused features and initial visual prompts. Next, a\ndual-branch design is provided by using the discriminative positive and\nnegative prompts in the prompt encoder. Furthermore, we design a simple\nmask-tube training strategy to adopt our proposed dual consistency method into\nthe mask tube. Although the proposed DC-SAM is primarily designed for images,\nit can be seamlessly extended to the video domain with the support of SAM2.\nGiven the absence of in-context segmentation in the video domain, we manually\ncurate and construct the first benchmark from existing video segmentation\ndatasets, named In-Context Video Object Segmentation (IC-VOS), to better assess\nthe in-context capability of the model. Extensive experiments demonstrate that\nour method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on\nPASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our\nsource code and benchmark are available at https://github.com/zaplm/DC-SAM.\n","authors":["Mengshi Qi","Pengfei Zhu","Xiangtai Li","Xiaoyang Bi","Lu Qi","Huadong Ma","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2504.12080v2.pdf","comment":"V1 has been withdrawn due to a template issue, because of the arXiv\n  policy, we can't delete it. Please refer to the newest version v2"},{"id":"http://arxiv.org/abs/2504.13023v1","updated":"2025-04-17T15:33:17Z","published":"2025-04-17T15:33:17Z","title":"ChatEXAONEPath: An Expert-level Multimodal Large Language Model for\n  Histopathology Using Whole Slide Images","summary":"  Recent studies have made significant progress in developing large language\nmodels (LLMs) in the medical domain, which can answer expert-level questions\nand demonstrate the potential to assist clinicians in real-world clinical\nscenarios. Studies have also witnessed the importance of integrating various\nmodalities with the existing LLMs for a better understanding of complex\nclinical contexts, which are innately multi-faceted by nature. Although studies\nhave demonstrated the ability of multimodal LLMs in histopathology to answer\nquestions from given images, they lack in understanding of thorough clinical\ncontext due to the patch-level data with limited information from public\ndatasets. Thus, developing WSI-level MLLMs is significant in terms of the\nscalability and applicability of MLLMs in histopathology. In this study, we\nintroduce an expert-level MLLM for histopathology using WSIs, dubbed as\nChatEXAONEPath. We present a retrieval-based data generation pipeline using\n10,094 pairs of WSIs and histopathology reports from The Cancer Genome Atlas\n(TCGA). We also showcase an AI-based evaluation protocol for a comprehensive\nunderstanding of the medical context from given multimodal information and\nevaluate generated answers compared to the original histopathology reports. We\ndemonstrate the ability of diagnosing the given histopathology images using\nChatEXAONEPath with the acceptance rate of 62.9% from 1,134 pairs of WSIs and\nreports. Our proposed model can understand pan-cancer WSIs and clinical context\nfrom various cancer types. We argue that our proposed model has the potential\nto assist clinicians by comprehensively understanding complex morphology of\nWSIs for cancer diagnosis through the integration of multiple modalities.\n","authors":["Sangwook Kim","Soonyoung Lee","Jongseong Jang"],"pdf_url":"https://arxiv.org/pdf/2504.13023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13022v1","updated":"2025-04-17T15:33:01Z","published":"2025-04-17T15:33:01Z","title":"CompGS++: Compressed Gaussian Splatting for Static and Dynamic Scene\n  Representation","summary":"  Gaussian splatting demonstrates proficiency for 3D scene modeling but suffers\nfrom substantial data volume due to inherent primitive redundancy. To enable\nfuture photorealistic 3D immersive visual communication applications,\nsignificant compression is essential for transmission over the existing\nInternet infrastructure. Hence, we propose Compressed Gaussian Splatting\n(CompGS++), a novel framework that leverages compact Gaussian primitives to\nachieve accurate 3D modeling with substantial size reduction for both static\nand dynamic scenes. Our design is based on the principle of eliminating\nredundancy both between and within primitives. Specifically, we develop a\ncomprehensive prediction paradigm to address inter-primitive redundancy through\nspatial and temporal primitive prediction modules. The spatial primitive\nprediction module establishes predictive relationships for scene primitives and\nenables most primitives to be encoded as compact residuals, substantially\nreducing the spatial redundancy. We further devise a temporal primitive\nprediction module to handle dynamic scenes, which exploits primitive\ncorrelations across timestamps to effectively reduce temporal redundancy.\nMoreover, we devise a rate-constrained optimization module that jointly\nminimizes reconstruction error and rate consumption. This module effectively\neliminates parameter redundancy within primitives and enhances the overall\ncompactness of scene representations. Comprehensive evaluations across multiple\nbenchmark datasets demonstrate that CompGS++ significantly outperforms existing\nmethods, achieving superior compression performance while preserving accurate\nscene modeling. Our implementation will be made publicly available on GitHub to\nfacilitate further research.\n","authors":["Xiangrui Liu","Xinju Wu","Shiqi Wang","Zhu Li","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2504.13022v1.pdf","comment":"Submitted to a journal"},{"id":"http://arxiv.org/abs/2504.08937v2","updated":"2025-04-17T15:31:11Z","published":"2025-04-11T19:33:06Z","title":"Rethinking Few-Shot Image Fusion: Granular Ball Priors Enable\n  General-Purpose Deep Fusion","summary":"  In image fusion tasks, the absence of real fused images as priors presents a\nfundamental challenge. Most deep learning-based fusion methods rely on\nlarge-scale paired datasets to extract global weighting features from raw\nimages, thereby generating fused outputs that approximate real fused images. In\ncontrast to previous studies, this paper explores few-shot training of neural\nnetworks under the condition of having prior knowledge. We propose a novel\nfusion framework named GBFF, and a Granular Ball Significant Extraction\nalgorithm specifically designed for the few-shot prior setting. All pixel pairs\ninvolved in the fusion process are initially modeled as a Coarse-Grained\nGranular Ball. At the local level, Fine-Grained Granular Balls are used to\nslide through the brightness space to extract Non-Salient Pixel Pairs, and\nperform splitting operations to obtain Salient Pixel Pairs. Pixel-wise weights\nare then computed to generate a pseudo-supervised image. At the global level,\npixel pairs with significant contributions to the fusion process are\ncategorized into the Positive Region, while those whose contributions cannot be\naccurately determined are assigned to the Boundary Region. The Granular Ball\nperforms modality-aware adaptation based on the proportion of the positive\nregion, thereby adjusting the neural network's loss function and enabling it to\ncomplement the information of the boundary region. Extensive experiments\ndemonstrate the effectiveness of both the proposed algorithm and the underlying\ntheory. Compared with state-of-the-art (SOTA) methods, our approach shows\nstrong competitiveness in terms of both fusion time and image expressiveness.\nOur code is publicly available at:\n","authors":["Minjie Deng","Yan Wei","Hao Zhai","An Wu","Yuncan Ouyang","Qianyao Peng"],"pdf_url":"https://arxiv.org/pdf/2504.08937v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13021v1","updated":"2025-04-17T15:29:41Z","published":"2025-04-17T15:29:41Z","title":"Pose and Facial Expression Transfer by using StyleGAN","summary":"  We propose a method to transfer pose and expression between face images.\nGiven a source and target face portrait, the model produces an output image in\nwhich the pose and expression of the source face image are transferred onto the\ntarget identity. The architecture consists of two encoders and a mapping\nnetwork that projects the two inputs into the latent space of StyleGAN2, which\nfinally generates the output. The training is self-supervised from video\nsequences of many individuals. Manual labeling is not required. Our model\nenables the synthesis of random identities with controllable pose and\nexpression. Close-to-real-time performance is achieved.\n","authors":["Petr Jahoda","Jan Cech"],"pdf_url":"https://arxiv.org/pdf/2504.13021v1.pdf","comment":"Accepted at CVWW 2024. Presented in Terme Olimia, Slovenia"},{"id":"http://arxiv.org/abs/2503.19325v2","updated":"2025-04-17T15:26:04Z","published":"2025-03-25T03:38:06Z","title":"Long-Context Autoregressive Video Modeling with Next-Frame Prediction","summary":"  Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR.\n","authors":["Yuchao Gu","Weijia Mao","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2503.19325v2.pdf","comment":"Project page at https://farlongctx.github.io/"},{"id":"http://arxiv.org/abs/2504.13015v1","updated":"2025-04-17T15:22:31Z","published":"2025-04-17T15:22:31Z","title":"Hierarchical Feature Learning for Medical Point Clouds via State Space\n  Model","summary":"  Deep learning-based point cloud modeling has been widely investigated as an\nindispensable component of general shape analysis. Recently, transformer and\nstate space model (SSM) have shown promising capacities in point cloud\nlearning. However, limited research has been conducted on medical point clouds,\nwhich have great potential in disease diagnosis and treatment. This paper\npresents an SSM-based hierarchical feature learning framework for medical point\ncloud understanding. Specifically, we down-sample input into multiple levels\nthrough the farthest point sampling. At each level, we perform a series of\nk-nearest neighbor (KNN) queries to aggregate multi-scale structural\ninformation. To assist SSM in processing point clouds, we introduce\ncoordinate-order and inside-out scanning strategies for efficient serialization\nof irregular points. Point features are calculated progressively from short\nneighbor sequences and long point sequences through vanilla and group Point SSM\nblocks, to capture both local patterns and long-range dependencies. To evaluate\nthe proposed method, we build a large-scale medical point cloud dataset named\nMedPointS for anatomy classification, completion, and segmentation. Extensive\nexperiments conducted on MedPointS demonstrate that our method achieves\nsuperior performance across all tasks. The dataset is available at\nhttps://flemme-docs.readthedocs.io/en/latest/medpoints.html. Code is merged to\na public medical imaging platform: https://github.com/wlsdzyzl/flemme.\n","authors":["Guoqing Zhang","Jingyun Yang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2504.13015v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2501.08514v3","updated":"2025-04-17T15:16:30Z","published":"2025-01-15T01:52:54Z","title":"Multimodal Fake News Video Explanation: Dataset, Analysis and Evaluation","summary":"  Multimodal fake news videos are difficult to interpret because they require\ncomprehensive consideration of the correlation and consistency between multiple\nmodes. Existing methods deal with fake news videos as a classification problem,\nbut it's not clear why news videos are identified as fake. Without proper\nexplanation, the end user may not understand the underlying meaning of the\nfalsehood. Therefore, we propose a new problem - Fake news video Explanation\n(FNVE) - given a multimodal news post containing a video and title, our goal is\nto generate natural language explanations to reveal the falsity of the news\nvideo. To that end, we developed FakeVE, a new dataset of 2,672 fake news video\nposts that can definitively explain four real-life fake news video aspects. In\norder to understand the characteristics of fake news video explanation, we\nconducted an exploratory analysis of FakeVE from different perspectives. In\naddition, we propose a Multimodal Relation Graph Transformer (MRGT) based on\nthe architecture of multimodal Transformer to benchmark FakeVE. The empirical\nresults show that the results of the various benchmarks (adopted by FakeVE) are\nconvincing and provide a detailed analysis of the differences in explanation\ngeneration of the benchmark models.\n","authors":["Lizhi Chen","Zhong Qian","Peifeng Li","Qiaoming Zhu"],"pdf_url":"https://arxiv.org/pdf/2501.08514v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12999v1","updated":"2025-04-17T15:10:14Z","published":"2025-04-17T15:10:14Z","title":"GSAC: Leveraging Gaussian Splatting for Photorealistic Avatar Creation\n  with Unity Integration","summary":"  Photorealistic avatars have become essential for immersive applications in\nvirtual reality (VR) and augmented reality (AR), enabling lifelike interactions\nin areas such as training simulations, telemedicine, and virtual collaboration.\nThese avatars bridge the gap between the physical and digital worlds, improving\nthe user experience through realistic human representation. However, existing\navatar creation techniques face significant challenges, including high costs,\nlong creation times, and limited utility in virtual applications. Manual\nmethods, such as MetaHuman, require extensive time and expertise, while\nautomatic approaches, such as NeRF-based pipelines often lack efficiency,\ndetailed facial expression fidelity, and are unable to be rendered at a speed\nsufficent for real-time applications. By involving several cutting-edge modern\ntechniques, we introduce an end-to-end 3D Gaussian Splatting (3DGS) avatar\ncreation pipeline that leverages monocular video input to create a scalable and\nefficient photorealistic avatar directly compatible with the Unity game engine.\nOur pipeline incorporates a novel Gaussian splatting technique with customized\npreprocessing that enables the user of \"in the wild\" monocular video capture,\ndetailed facial expression reconstruction and embedding within a fully rigged\navatar model. Additionally, we present a Unity-integrated Gaussian Splatting\nAvatar Editor, offering a user-friendly environment for VR/AR application\ndevelopment. Experimental results validate the effectiveness of our\npreprocessing pipeline in standardizing custom data for 3DGS training and\ndemonstrate the versatility of Gaussian avatars in Unity, highlighting the\nscalability and practicality of our approach.\n","authors":["Rendong Zhang","Alexandra Watkins","Nilanjan Sarkar"],"pdf_url":"https://arxiv.org/pdf/2504.12999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12997v1","updated":"2025-04-17T15:06:52Z","published":"2025-04-17T15:06:52Z","title":"All-in-One Transferring Image Compression from Human Perception to\n  Multi-Machine Perception","summary":"  Efficiently transferring Learned Image Compression (LIC) model from human\nperception to machine perception is an emerging challenge in vision-centric\nrepresentation learning. Existing approaches typically adapt LIC to downstream\ntasks in a single-task manner, which is inefficient, lacks task interaction,\nand results in multiple task-specific bitstreams. To address these limitations,\nwe propose an asymmetric adaptor framework that supports multi-task adaptation\nwithin a single model. Our method introduces a shared adaptor to learn general\nsemantic features and task-specific adaptors to preserve task-level\ndistinctions. With only lightweight plug-in modules and a frozen base codec,\nour method achieves strong performance across multiple tasks while maintaining\ncompression efficiency. Experiments on the PASCAL-Context benchmark demonstrate\nthat our method outperforms both Fully Fine-Tuned and other Parameter Efficient\nFine-Tuned (PEFT) baselines, and validating the effectiveness of multi-vision\ntransferring.\n","authors":["Jiancheng Zhao","Xiang Ji","Zhuoxiao Li","Zunian Wan","Weihang Ran","Mingze Ma","Muyao Niu","Yifan Zhan","Cheng-Ching Tseng","Yinqiang Zheng"],"pdf_url":"https://arxiv.org/pdf/2504.12997v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.12992v1","updated":"2025-04-17T15:02:27Z","published":"2025-04-17T15:02:27Z","title":"Enhancing Cocoa Pod Disease Classification via Transfer Learning and\n  Ensemble Methods: Toward Robust Predictive Modeling","summary":"  This study presents an ensemble-based approach for cocoa pod disease\nclassification by integrating transfer learning with three ensemble learning\nstrategies: Bagging, Boosting, and Stacking. Pre-trained convolutional neural\nnetworks, including VGG16, VGG19, ResNet50, ResNet101, InceptionV3, and\nXception, were fine-tuned and employed as base learners to detect three disease\ncategories: Black Pod Rot, Pod Borer, and Healthy. A balanced dataset of 6,000\ncocoa pod images was curated and augmented to ensure robustness against\nvariations in lighting, orientation, and disease severity. The performance of\neach ensemble method was evaluated using accuracy, precision, recall, and\nF1-score. Experimental results show that Bagging consistently achieved superior\nclassification performance with a test accuracy of 100%, outperforming Boosting\n(97%) and Stacking (92%). The findings confirm that combining transfer learning\nwith ensemble techniques improves model generalization and reliability, making\nit a promising direction for precision agriculture and automated crop disease\nmanagement.\n","authors":["Devina Anduyan","Nyza Cabillo","Navy Gultiano","Mark Phil Pacot"],"pdf_url":"https://arxiv.org/pdf/2504.12992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15499v3","updated":"2025-04-17T14:56:49Z","published":"2024-12-20T02:25:31Z","title":"A Robust Prototype-Based Network with Interpretable RBF Classifier\n  Foundations","summary":"  Prototype-based classification learning methods are known to be inherently\ninterpretable. However, this paradigm suffers from major limitations compared\nto deep models, such as lower performance. This led to the development of the\nso-called deep Prototype-Based Networks (PBNs), also known as prototypical\nparts models. In this work, we analyze these models with respect to different\nproperties, including interpretability. In particular, we focus on the\nClassification-by-Components (CBC) approach, which uses a probabilistic model\nto ensure interpretability and can be used as a shallow or deep architecture.\nWe show that this model has several shortcomings, like creating contradicting\nexplanations. Based on these findings, we propose an extension of CBC that\nsolves these issues. Moreover, we prove that this extension has robustness\nguarantees and derive a loss that optimizes robustness. Additionally, our\nanalysis shows that most (deep) PBNs are related to (deep) RBF classifiers,\nwhich implies that our robustness guarantees generalize to shallow RBF\nclassifiers. The empirical evaluation demonstrates that our deep PBN yields\nstate-of-the-art classification accuracy on different benchmarks while\nresolving the interpretability shortcomings of other approaches. Further, our\nshallow PBN variant outperforms other shallow PBNs while being inherently\ninterpretable and exhibiting provable robustness guarantees.\n","authors":["Sascha Saralajew","Ashish Rana","Thomas Villmann","Ammar Shaker"],"pdf_url":"https://arxiv.org/pdf/2412.15499v3.pdf","comment":"To appear at AAAI 2025. Includes the Appendix of the AAAI submission.\n  In v2, the font size has been increased in some figures. In v3, an incorrect\n  hyperparameter specification (Table 6; $\\lambda$) has been corrected"},{"id":"http://arxiv.org/abs/2504.07940v2","updated":"2025-04-17T14:35:15Z","published":"2025-04-10T17:51:38Z","title":"Beyond the Frame: Generating 360° Panoramic Videos from Perspective\n  Videos","summary":"  360{\\deg} videos have emerged as a promising medium to represent our dynamic\nvisual world. Compared to the \"tunnel vision\" of standard cameras, their\nborderless field of view offers a more complete perspective of our\nsurroundings. While existing video models excel at producing standard videos,\ntheir ability to generate full panoramic videos remains elusive. In this paper,\nwe investigate the task of video-to-360{\\deg} generation: given a perspective\nvideo as input, our goal is to generate a full panoramic video that is\nconsistent with the original video. Unlike conventional video generation tasks,\nthe output's field of view is significantly larger, and the model is required\nto have a deep understanding of both the spatial layout of the scene and the\ndynamics of objects to maintain spatio-temporal consistency. To address these\nchallenges, we first leverage the abundant 360{\\deg} videos available online\nand develop a high-quality data filtering pipeline to curate pairwise training\ndata. We then carefully design a series of geometry- and motion-aware\noperations to facilitate the learning process and improve the quality of\n360{\\deg} video generation. Experimental results demonstrate that our model can\ngenerate realistic and coherent 360{\\deg} videos from in-the-wild perspective\nvideo. In addition, we showcase its potential applications, including video\nstabilization, camera viewpoint control, and interactive visual question\nanswering.\n","authors":["Rundong Luo","Matthew Wallingford","Ali Farhadi","Noah Snavely","Wei-Chiu Ma"],"pdf_url":"https://arxiv.org/pdf/2504.07940v2.pdf","comment":"Project page: https://red-fairy.github.io/argus/"},{"id":"http://arxiv.org/abs/2504.12970v1","updated":"2025-04-17T14:22:27Z","published":"2025-04-17T14:22:27Z","title":"MathPhys-Guided Coarse-to-Fine Anomaly Synthesis with SQE-Driven\n  Bi-Level Optimization for Anomaly Detection","summary":"  Anomaly detection is a crucial task in computer vision, yet collecting\nreal-world defect images is inherently difficult due to the rarity and\nunpredictability of anomalies. Consequently, researchers have turned to\nsynthetic methods for training data augmentation. However, existing synthetic\nstrategies (e.g., naive cut-and-paste or inpainting) overlook the underlying\nphysical causes of defects, leading to inconsistent, low-fidelity anomalies\nthat hamper model generalization to real-world complexities. In this thesis, we\nintroduced a novel pipeline that generates synthetic anomalies through\nMath-Physics model guidance, refines them via a Coarse-to-Fine approach and\nemploys a bi-level optimization strategy with a Synthesis Quality\nEstimator(SQE). By incorporating physical modeling of cracks, corrosion, and\ndeformation, our method produces realistic defect masks, which are subsequently\nenhanced in two phases. The first stage (npcF) enforces a PDE-based consistency\nto achieve a globally coherent anomaly structure, while the second stage\n(npcF++) further improves local fidelity using wavelet transforms and boundary\nsynergy blocks. Additionally, we leverage SQE-driven weighting, ensuring that\nhigh-quality synthetic samples receive greater emphasis during training. To\nvalidate our approach, we conducted comprehensive experiments on three widely\nadopted industrial anomaly detection benchmarks: MVTec AD, VisA, and BTAD.\nAcross these datasets, the proposed pipeline achieves state-of-the-art (SOTA)\nresults in both image-AUROC and pixel-AUROC, confirming the effectiveness of\nour MaPhC2F and BiSQAD.\n","authors":["Long Qian","Bingke Zhu","Yingying Chen","Ming Tang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2504.12970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12966v1","updated":"2025-04-17T14:19:09Z","published":"2025-04-17T14:19:09Z","title":"Vision and Language Integration for Domain Generalization","summary":"  Domain generalization aims at training on source domains to uncover a\ndomain-invariant feature space, allowing the model to perform robust\ngeneralization ability on unknown target domains. However, due to domain gaps,\nit is hard to find reliable common image feature space, and the reason for that\nis the lack of suitable basic units for images. Different from image in vision\nspace, language has comprehensive expression elements that can effectively\nconvey semantics. Inspired by the semantic completeness of language and\nintuitiveness of image, we propose VLCA, which combine language space and\nvision space, and connect the multiple image domains by using semantic space as\nthe bridge domain. Specifically, in language space, by taking advantage of the\ncompleteness of language basic units, we tend to capture the semantic\nrepresentation of the relations between categories through word vector\ndistance. Then, in vision space, by taking advantage of the intuitiveness of\nimage features, the common pattern of sample features with the same class is\nexplored through low-rank approximation. In the end, the language\nrepresentation is aligned with the vision representation through the multimodal\nspace of text and image. Experiments demonstrate the effectiveness of the\nproposed method.\n","authors":["Yanmei Wang","Xiyao Liu","Fupeng Chu","Zhi Han"],"pdf_url":"https://arxiv.org/pdf/2504.12966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11230v2","updated":"2025-04-17T14:13:37Z","published":"2025-04-15T14:30:26Z","title":"CAP-Net: A Unified Network for 6D Pose and Size Estimation of\n  Categorical Articulated Parts from a Single RGB-D Image","summary":"  This paper tackles category-level pose estimation of articulated objects in\nrobotic manipulation tasks and introduces a new benchmark dataset. While recent\nmethods estimate part poses and sizes at the category level, they often rely on\ngeometric cues and complex multi-stage pipelines that first segment parts from\nthe point cloud, followed by Normalized Part Coordinate Space (NPCS) estimation\nfor 6D poses. These approaches overlook dense semantic cues from RGB images,\nleading to suboptimal accuracy, particularly for objects with small parts. To\naddress these limitations, we propose a single-stage Network, CAP-Net, for\nestimating the 6D poses and sizes of Categorical Articulated Parts. This method\ncombines RGB-D features to generate instance segmentation and NPCS\nrepresentations for each part in an end-to-end manner. CAP-Net uses a unified\nnetwork to simultaneously predict point-wise class labels, centroid offsets,\nand NPCS maps. A clustering algorithm then groups points of the same predicted\nclass based on their estimated centroid distances to isolate each part.\nFinally, the NPCS region of each part is aligned with the point cloud to\nrecover its final pose and size. To bridge the sim-to-real domain gap, we\nintroduce the RGBD-Art dataset, the largest RGB-D articulated dataset to date,\nfeaturing photorealistic RGB images and depth noise simulated from real\nsensors. Experimental evaluations on the RGBD-Art dataset demonstrate that our\nmethod significantly outperforms the state-of-the-art approach. Real-world\ndeployments of our model in robotic tasks underscore its robustness and\nexceptional sim-to-real transfer capabilities, confirming its substantial\npractical utility. Our dataset, code and pre-trained models are available on\nthe project page.\n","authors":["Jingshun Huang","Haitao Lin","Tianyu Wang","Yanwei Fu","Xiangyang Xue","Yi Zhu"],"pdf_url":"https://arxiv.org/pdf/2504.11230v2.pdf","comment":"To appear in CVPR 2025 (Highlight)"},{"id":"http://arxiv.org/abs/2504.12959v1","updated":"2025-04-17T14:05:33Z","published":"2025-04-17T14:05:33Z","title":"Rethinking Temporal Fusion with a Unified Gradient Descent View for 3D\n  Semantic Occupancy Prediction","summary":"  We present GDFusion, a temporal fusion method for vision-based 3D semantic\noccupancy prediction (VisionOcc). GDFusion opens up the underexplored aspects\nof temporal fusion within the VisionOcc framework, focusing on both temporal\ncues and fusion strategies. It systematically examines the entire VisionOcc\npipeline, identifying three fundamental yet previously overlooked temporal\ncues: scene-level consistency, motion calibration, and geometric\ncomplementation. These cues capture diverse facets of temporal evolution and\nmake distinct contributions across various modules in the VisionOcc framework.\nTo effectively fuse temporal signals across heterogeneous representations, we\npropose a novel fusion strategy by reinterpreting the formulation of vanilla\nRNNs. This reinterpretation leverages gradient descent on features to unify the\nintegration of diverse temporal information, seamlessly embedding the proposed\ntemporal cues into the network. Extensive experiments on nuScenes demonstrate\nthat GDFusion significantly outperforms established baselines. Notably, on\nOcc3D benchmark, it achieves 1.4\\%-4.8\\% mIoU improvements and reduces memory\nconsumption by 27\\%-72\\%.\n","authors":["Dubing Chen","Huan Zheng","Jin Fang","Xingping Dong","Xianfei Li","Wenlong Liao","Tao He","Pai Peng","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2504.12959v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2504.12939v1","updated":"2025-04-17T13:37:47Z","published":"2025-04-17T13:37:47Z","title":"Disentangling Polysemantic Channels in Convolutional Neural Networks","summary":"  Mechanistic interpretability is concerned with analyzing individual\ncomponents in a (convolutional) neural network (CNN) and how they form larger\ncircuits representing decision mechanisms. These investigations are challenging\nsince CNNs frequently learn polysemantic channels that encode distinct\nconcepts, making them hard to interpret. To address this, we propose an\nalgorithm to disentangle a specific kind of polysemantic channel into multiple\nchannels, each responding to a single concept. Our approach restructures\nweights in a CNN, utilizing that different concepts within the same channel\nexhibit distinct activation patterns in the previous layer. By disentangling\nthese polysemantic features, we enhance the interpretability of CNNs,\nultimately improving explanatory techniques such as feature visualizations.\n","authors":["Robin Hesse","Jonas Fischer","Simone Schaub-Meyer","Stefan Roth"],"pdf_url":"https://arxiv.org/pdf/2504.12939v1.pdf","comment":"Accepted at CVPR 2025 Workshop on Mechanistic Interpretability for\n  Vision (MIV). Code: https://github.com/visinf/disentangle-channels"},{"id":"http://arxiv.org/abs/2411.00826v2","updated":"2025-04-17T13:24:31Z","published":"2024-10-29T04:29:44Z","title":"Uncertainty Quantification via Hölder Divergence for Multi-View\n  Representation Learning","summary":"  Evidence-based deep learning represents a burgeoning paradigm for uncertainty\nestimation, offering reliable predictions with negligible extra computational\noverheads. Existing methods usually adopt Kullback-Leibler divergence to\nestimate the uncertainty of network predictions, ignoring domain gaps among\nvarious modalities. To tackle this issue, this paper introduces a novel\nalgorithm based on H\\\"older Divergence (HD) to enhance the reliability of\nmulti-view learning by addressing inherent uncertainty challenges from\nincomplete or noisy data. Generally, our method extracts the representations of\nmultiple modalities through parallel network branches, and then employs HD to\nestimate the prediction uncertainties. Through the Dempster-Shafer theory,\nintegration of uncertainty from different modalities, thereby generating a\ncomprehensive result that considers all available representations.\nMathematically, HD proves to better measure the ``distance'' between real data\ndistribution and predictive distribution of the model and improve the\nperformances of multi-class recognition tasks.\n  Specifically, our method surpass the existing state-of-the-art counterparts\non all evaluating benchmarks.\n  We further conduct extensive experiments on different backbones to verify our\nsuperior robustness. It is demonstrated that our method successfully pushes the\ncorresponding performance boundaries. Finally, we perform experiments on more\nchallenging scenarios, \\textit{i.e.}, learning with incomplete or noisy data,\nrevealing that our method exhibits a high tolerance to such corrupted data.\n","authors":["Yan Zhang","Ming Li","Chun Li","Zhaoxia Liu","Ye Zhang","Fei Richard Yu"],"pdf_url":"https://arxiv.org/pdf/2411.00826v2.pdf","comment":"NA"},{"id":"http://arxiv.org/abs/2504.12923v1","updated":"2025-04-17T13:12:39Z","published":"2025-04-17T13:12:39Z","title":"Efficient Masked Image Compression with Position-Indexed Self-Attention","summary":"  In recent years, image compression for high-level vision tasks has attracted\nconsiderable attention from researchers. Given that object information in\nimages plays a far more crucial role in downstream tasks than background\ninformation, some studies have proposed semantically structuring the bitstream\nto selectively transmit and reconstruct only the information required by these\ntasks. However, such methods structure the bitstream after encoding, meaning\nthat the coding process still relies on the entire image, even though much of\nthe encoded information will not be transmitted. This leads to redundant\ncomputations. Traditional image compression methods require a two-dimensional\nimage as input, and even if the unimportant regions of the image are set to\nzero by applying a semantic mask, these regions still participate in subsequent\ncomputations as part of the image. To address such limitations, we propose an\nimage compression method based on a position-indexed self-attention mechanism\nthat encodes and decodes only the visible parts of the masked image. Compared\nto existing semantic-structured compression methods, our approach can\nsignificantly reduce computational costs.\n","authors":["Chengjie Dai","Tiantian Song","Hui Tang","Fangdong Chen","Bowei Yang","Guanghua Song"],"pdf_url":"https://arxiv.org/pdf/2504.12923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12909v1","updated":"2025-04-17T12:57:41Z","published":"2025-04-17T12:57:41Z","title":"Real-time High-fidelity Gaussian Human Avatars with Position-based\n  Interpolation of Spatially Distributed MLPs","summary":"  Many works have succeeded in reconstructing Gaussian human avatars from\nmulti-view videos. However, they either struggle to capture pose-dependent\nappearance details with a single MLP, or rely on a computationally intensive\nneural network to reconstruct high-fidelity appearance but with rendering\nperformance degraded to non-real-time. We propose a novel Gaussian human avatar\nrepresentation that can reconstruct high-fidelity pose-dependence appearance\nwith details and meanwhile can be rendered in real time. Our Gaussian avatar is\nempowered by spatially distributed MLPs which are explicitly located on\ndifferent positions on human body. The parameters stored in each Gaussian are\nobtained by interpolating from the outputs of its nearby MLPs based on their\ndistances. To avoid undesired smooth Gaussian property changing during\ninterpolation, for each Gaussian we define a set of Gaussian offset basis, and\na linear combination of basis represents the Gaussian property offsets relative\nto the neutral properties. Then we propose to let the MLPs output a set of\ncoefficients corresponding to the basis. In this way, although Gaussian\ncoefficients are derived from interpolation and change smoothly, the Gaussian\noffset basis is learned freely without constraints. The smoothly varying\ncoefficients combined with freely learned basis can still produce distinctly\ndifferent Gaussian property offsets, allowing the ability to learn\nhigh-frequency spatial signals. We further use control points to constrain the\nGaussians distributed on a surface layer rather than allowing them to be\nirregularly distributed inside the body, to help the human avatar generalize\nbetter when animated under novel poses. Compared to the state-of-the-art\nmethod, our method achieves better appearance quality with finer details while\nthe rendering speed is significantly faster under novel views and novel poses.\n","authors":["Youyi Zhan","Tianjia Shao","Yin Yang","Kun Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.12909v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.06125v2","updated":"2025-04-17T12:57:37Z","published":"2025-03-08T08:37:20Z","title":"RGB-Phase Speckle: Cross-Scene Stereo 3D Reconstruction via Wrapped\n  Pre-Normalization","summary":"  3D reconstruction garners increasing attention alongside the advancement of\nhigh-level image applications, where dense stereo matching (DSM) serves as a\npivotal technique. Previous studies often rely on publicly available datasets\nfor training, focusing on modifying network architectures or incorporating\nspecialized modules to extract domain-invariant features and thus improve model\nrobustness. In contrast, inspired by single-frame structured-light\nphase-shifting encoding, this study introduces RGB-Speckle, a cross-scene 3D\nreconstruction framework based on an active stereo camera system, designed to\nenhance robustness. Specifically, we propose a novel phase pre-normalization\nencoding-decoding method: first, we randomly perturb phase-shift maps and embed\nthem into the three RGB channels to generate color speckle patterns;\nsubsequently, the camera captures phase-encoded images modulated by objects as\ninput to a stereo matching network. This technique effectively mitigates\nexternal interference and ensures consistent input data for RGB-Speckle,\nthereby bolstering cross-domain 3D reconstruction stability. To validate the\nproposed method, we conduct complex experiments: (1) construct a color speckle\ndataset for complex scenarios based on the proposed encoding scheme; (2)\nevaluate the impact of the phase pre-normalization encoding-decoding technique\non 3D reconstruction accuracy; and (3) further investigate its robustness\nacross diverse conditions. Experimental results demonstrate that the proposed\nRGB-Speckle model offers significant advantages in cross-domain and cross-scene\n3D reconstruction tasks, enhancing model generalization and reinforcing\nrobustness in challenging environments, thus providing a novel solution for\nrobust 3D reconstruction research.\n","authors":["Kai Yang","Zijian Bai","Yang Xiao","Xinyu Li","Xiaohan Shi"],"pdf_url":"https://arxiv.org/pdf/2503.06125v2.pdf","comment":"Submitted to ICCV 2025"},{"id":"http://arxiv.org/abs/2504.12908v1","updated":"2025-04-17T12:57:11Z","published":"2025-04-17T12:57:11Z","title":"Taccel: Scaling Up Vision-based Tactile Robotics via High-performance\n  GPU Simulation","summary":"  Tactile sensing is crucial for achieving human-level robotic capabilities in\nmanipulation tasks. VBTSs have emerged as a promising solution, offering high\nspatial resolution and cost-effectiveness by sensing contact through\ncamera-captured deformation patterns of elastic gel pads. However, these\nsensors' complex physical characteristics and visual signal processing\nrequirements present unique challenges for robotic applications. The lack of\nefficient and accurate simulation tools for VBTS has significantly limited the\nscale and scope of tactile robotics research. Here we present Taccel, a\nhigh-performance simulation platform that integrates IPC and ABD to model\nrobots, tactile sensors, and objects with both accuracy and unprecedented\nspeed, achieving an 18-fold acceleration over real-time across thousands of\nparallel environments. Unlike previous simulators that operate at sub-real-time\nspeeds with limited parallelization, Taccel provides precise physics simulation\nand realistic tactile signals while supporting flexible robot-sensor\nconfigurations through user-friendly APIs. Through extensive validation in\nobject recognition, robotic grasping, and articulated object manipulation, we\ndemonstrate precise simulation and successful sim-to-real transfer. These\ncapabilities position Taccel as a powerful tool for scaling up tactile robotics\nresearch and development. By enabling large-scale simulation and\nexperimentation with tactile sensing, Taccel accelerates the development of\nmore capable robotic systems, potentially transforming how robots interact with\nand understand their physical environment.\n","authors":["Yuyang Li","Wenxin Du","Chang Yu","Puhao Li","Zihang Zhao","Tengyu Liu","Chenfanfu Jiang","Yixin Zhu","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2504.12908v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.12905v1","updated":"2025-04-17T12:52:08Z","published":"2025-04-17T12:52:08Z","title":"Second-order Optimization of Gaussian Splats with Importance Sampling","summary":"  3D Gaussian Splatting (3DGS) is widely used for novel view synthesis due to\nits high rendering quality and fast inference time. However, 3DGS predominantly\nrelies on first-order optimizers such as Adam, which leads to long training\ntimes. To address this limitation, we propose a novel second-order optimization\nstrategy based on Levenberg-Marquardt (LM) and Conjugate Gradient (CG), which\nwe specifically tailor towards Gaussian Splatting. Our key insight is that the\nJacobian in 3DGS exhibits significant sparsity since each Gaussian affects only\na limited number of pixels. We exploit this sparsity by proposing a matrix-free\nand GPU-parallelized LM optimization. To further improve its efficiency, we\npropose sampling strategies for both the camera views and loss function and,\nconsequently, the normal equation, significantly reducing the computational\ncomplexity. In addition, we increase the convergence rate of the second-order\napproximation by introducing an effective heuristic to determine the learning\nrate that avoids the expensive computation cost of line search methods. As a\nresult, our method achieves a $3\\times$ speedup over standard LM and\noutperforms Adam by $~6\\times$ when the Gaussian count is low while remaining\ncompetitive for moderate counts. Project Page:\nhttps://vcai.mpi-inf.mpg.de/projects/LM-IS\n","authors":["Hamza Pehlivan","Andrea Boscolo Camiletto","Lin Geng Foo","Marc Habermann","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2504.12905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02064v2","updated":"2025-04-17T12:49:56Z","published":"2025-01-03T19:17:27Z","title":"ArtCrafter: Text-Image Aligning Style Transfer via Embedding Reframing","summary":"  Recent years have witnessed significant advancements in text-guided style\ntransfer, primarily attributed to innovations in diffusion models. These models\nexcel in conditional guidance, utilizing text or images to direct the sampling\nprocess. However, despite their capabilities, direct conditional guidance\napproaches often face challenges in balancing the expressiveness of textual\nsemantics with the diversity of output results while capturing stylistic\nfeatures. To address these challenges, we introduce ArtCrafter, a novel\nframework for text-to-image style transfer. Specifically, we introduce an\nattention-based style extraction module, meticulously engineered to capture the\nsubtle stylistic elements within an image. This module features a multi-layer\narchitecture that leverages the capabilities of perceiver attention mechanisms\nto integrate fine-grained information. Additionally, we present a novel\ntext-image aligning augmentation component that adeptly balances control over\nboth modalities, enabling the model to efficiently map image and text\nembeddings into a shared feature space. We achieve this through attention\noperations that enable smooth information flow between modalities. Lastly, we\nincorporate an explicit modulation that seamlessly blends multimodal enhanced\nembeddings with original embeddings through an embedding reframing design,\nempowering the model to generate diverse outputs. Extensive experiments\ndemonstrate that ArtCrafter yields impressive results in visual stylization,\nexhibiting exceptional levels of stylistic intensity, controllability, and\ndiversity.\n","authors":["Nisha Huang","Kaer Huang","Yifan Pu","Jiangshan Wang","Jie Guo","Yiqiang Yan","Xiu Li","Tong-Yee Lee"],"pdf_url":"https://arxiv.org/pdf/2501.02064v2.pdf","comment":"13 pages, 17 figures, submitted to a journal"},{"id":"http://arxiv.org/abs/2504.12899v1","updated":"2025-04-17T12:40:33Z","published":"2025-04-17T12:40:33Z","title":"Tree-NeRV: A Tree-Structured Neural Representation for Efficient\n  Non-Uniform Video Encoding","summary":"  Implicit Neural Representations for Videos (NeRV) have emerged as a powerful\nparadigm for video representation, enabling direct mappings from frame indices\nto video frames. However, existing NeRV-based methods do not fully exploit\ntemporal redundancy, as they rely on uniform sampling along the temporal axis,\nleading to suboptimal rate-distortion (RD) performance. To address this\nlimitation, we propose Tree-NeRV, a novel tree-structured feature\nrepresentation for efficient and adaptive video encoding. Unlike conventional\napproaches, Tree-NeRV organizes feature representations within a Binary Search\nTree (BST), enabling non-uniform sampling along the temporal axis.\nAdditionally, we introduce an optimization-driven sampling strategy,\ndynamically allocating higher sampling density to regions with greater temporal\nvariation. Extensive experiments demonstrate that Tree-NeRV achieves superior\ncompression efficiency and reconstruction quality, outperforming prior uniform\nsampling-based methods. Code will be released.\n","authors":["Jiancheng Zhao","Yifan Zhan","Qingtian Zhu","Mingze Ma","Muyao Niu","Zunian Wan","Xiang Ji","Yinqiang Zheng"],"pdf_url":"https://arxiv.org/pdf/2504.12899v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2504.10090v2","updated":"2025-04-17T12:33:59Z","published":"2025-04-14T10:53:44Z","title":"CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography","summary":"  Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning.\n","authors":["I-Sheng Fang","Jun-Cheng Chen"],"pdf_url":"https://arxiv.org/pdf/2504.10090v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18672v4","updated":"2025-04-17T12:26:16Z","published":"2025-03-24T13:44:12Z","title":"Feature Calibration enhanced Parameter Synthesis for CLIP-based\n  Class-incremental Learning","summary":"  Class-Incremental Learning (CIL) enables models to continuously learn new\nclass knowledge while retaining previous classes, facilitating adaptation and\nevolution in dynamic, real-world environments. Traditional CIL methods\nprimarily rely on visual features, which limits their effectiveness in complex,\nmultimodal scenarios. In contrast, VLMs show promising potential for enhancing\nCIL by leveraging pre-trained knowledge and integrating multi-modal semantic\ncues such as text and vision. However, existing approaches struggle to mitigate\ncatastrophic forgetting while preserving the generalization strengths of VLMs\nacross diverse modalities. To address these challenges, we propose a Feature\nCalibration Enhanced Parameter Synthesis (FCPS) framework. Specifically, FCPS\nintroduces a dynamic parameter adjustment mechanism that iteratively calibrates\nthe contribution of original visual features to the final class decision, thus\npreserving the model's intrinsic generalization capability across modalities.\nSimultaneously, parameter integration enables effective knowledge transfer,\nmaintaining a balance between acquiring new class representations and\npreserving old knowledge. Experimental results on popular benchmarks (e.g.,\nCIFAR100 and ImageNet100) validate the superiority of the proposed method.\n","authors":["Juncen Guo","Yang Liu","Xiaoguang Zhu","Lianlong Sun","Liangyu Teng","Jingyi Wu","Di Li","Wei Zhou","Liang Song"],"pdf_url":"https://arxiv.org/pdf/2503.18672v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05803v3","updated":"2025-04-17T12:23:46Z","published":"2025-01-10T09:10:30Z","title":"Test-time Alignment of Diffusion Models without Reward Over-optimization","summary":"  Diffusion models excel in generative tasks, but aligning them with specific\nobjectives while maintaining their versatility remains challenging. Existing\nfine-tuning methods often suffer from reward over-optimization, while\napproximate guidance approaches fail to optimize target rewards effectively.\nAddressing these limitations, we propose a training-free, test-time method\nbased on Sequential Monte Carlo (SMC) to sample from the reward-aligned target\ndistribution. Our approach, tailored for diffusion sampling and incorporating\ntempering techniques, achieves comparable or superior target rewards to\nfine-tuning methods while preserving diversity and cross-reward generalization.\nWe demonstrate its effectiveness in single-reward optimization, multi-objective\nscenarios, and online black-box optimization. This work offers a robust\nsolution for aligning diffusion models with diverse downstream objectives\nwithout compromising their general capabilities. Code is available at\nhttps://github.com/krafton-ai/DAS.\n","authors":["Sunwoo Kim","Minkyu Kim","Dongmin Park"],"pdf_url":"https://arxiv.org/pdf/2501.05803v3.pdf","comment":"ICLR 2025 (Spotlight). The Thirteenth International Conference on\n  Learning Representations. 2025"},{"id":"http://arxiv.org/abs/2412.06235v2","updated":"2025-04-17T12:12:38Z","published":"2024-12-09T06:21:11Z","title":"VariFace: Fair and Diverse Synthetic Dataset Generation for Face\n  Recognition","summary":"  The use of large-scale, web-scraped datasets to train face recognition models\nhas raised significant privacy and bias concerns. Synthetic methods mitigate\nthese concerns and provide scalable and controllable face generation to enable\nfair and accurate face recognition. However, existing synthetic datasets\ndisplay limited intraclass and interclass diversity and do not match the face\nrecognition performance obtained using real datasets. Here, we propose\nVariFace, a two-stage diffusion-based pipeline to create fair and diverse\nsynthetic face datasets to train face recognition models. Specifically, we\nintroduce three methods: Face Recognition Consistency to refine demographic\nlabels, Face Vendi Score Guidance to improve interclass diversity, and\nDivergence Score Conditioning to balance the identity preservation-intraclass\ndiversity trade-off. When constrained to the same dataset size, VariFace\nconsiderably outperforms previous synthetic datasets (0.9200 $\\rightarrow$\n0.9405) and achieves comparable performance to face recognition models trained\nwith real data (Real Gap = -0.0065). In an unconstrained setting, VariFace not\nonly consistently achieves better performance compared to previous synthetic\nmethods across dataset sizes but also, for the first time, outperforms the real\ndataset (CASIA-WebFace) across six evaluation datasets. This sets a new\nstate-of-the-art performance with an average face verification accuracy of\n0.9567 (Real Gap = +0.0097) across LFW, CFP-FP, CPLFW, AgeDB, and CALFW\ndatasets and 0.9366 (Real Gap = +0.0380) on the RFW dataset.\n","authors":["Michael Yeung","Toya Teramoto","Songtao Wu","Tatsuo Fujiwara","Kenji Suzuki","Tamaki Kojima"],"pdf_url":"https://arxiv.org/pdf/2412.06235v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12869v1","updated":"2025-04-17T11:54:12Z","published":"2025-04-17T11:54:12Z","title":"SC3EF: A Joint Self-Correlation and Cross-Correspondence Estimation\n  Framework for Visible and Thermal Image Registration","summary":"  Multispectral imaging plays a critical role in a range of intelligent\ntransportation applications, including advanced driver assistance systems\n(ADAS), traffic monitoring, and night vision. However, accurate visible and\nthermal (RGB-T) image registration poses a significant challenge due to the\nconsiderable modality differences. In this paper, we present a novel joint\nSelf-Correlation and Cross-Correspondence Estimation Framework (SC3EF),\nleveraging both local representative features and global contextual cues to\neffectively generate RGB-T correspondences. For this purpose, we design a\nconvolution-transformer-based pipeline to extract local representative features\nand encode global correlations of intra-modality for inter-modality\ncorrespondence estimation between unaligned visible and thermal images. After\nmerging the local and global correspondence estimation results, we further\nemploy a hierarchical optical flow estimation decoder to progressively refine\nthe estimated dense correspondence maps. Extensive experiments demonstrate the\neffectiveness of our proposed method, outperforming the current\nstate-of-the-art (SOTA) methods on representative RGB-T datasets. Furthermore,\nit also shows competitive generalization capabilities across challenging\nscenarios, including large parallax, severe occlusions, adverse weather, and\nother cross-modal datasets (e.g., RGB-N and RGB-D).\n","authors":["Xi Tong","Xing Luo","Jiangxin Yang","Yanpeng Cao"],"pdf_url":"https://arxiv.org/pdf/2504.12869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12868v1","updated":"2025-04-17T11:53:49Z","published":"2025-04-17T11:53:49Z","title":"Computer-Aided Design of Personalized Occlusal Positioning Splints Using\n  Multimodal 3D Data","summary":"  Contemporary digital technology has a pivotal role in the design of\ncustomized medical appliances, including occlusal splints used in the treatment\nof stomatognathic system dysfunctions. We present an approach to computer-aided\ndesign and precision assessment of positioning occlusal splints, bridging\nclinical concepts with current digital dental practice. In our model, a 3D\nsplint is generated based on a transformation matrix that represents the\ntherapeutic change in mandibular position, defined by a specialist using a\nvirtual patient model reconstructed from intraoral scans, CBCT, 3D facial scans\nand plaster model digitisation. The paper introduces a novel method for\ngenerating splints that accurately reproduce occlusal conditions in the\ntherapeutic position, including a mechanism for resolving surface conflicts\nthrough virtual embossing. We demonstrate how transformation matrices can be\nacquired through clinical tools and intraoral devices, and evaluate the\naccuracy of the designed and printed splints using profile and surface\ndeviation analysis. The proposed method enables reproducible, patient-specific\nsplint fabrication and opens new possibilities in diagnostics, multimodal image\nregistration and quantification of occlusal discrepancies.\n","authors":["Agnieszka Anna Tomaka","Leszek Luchowski","Michał Tarnawski","Dariusz Pojda"],"pdf_url":"https://arxiv.org/pdf/2504.12868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05169v2","updated":"2025-04-17T11:49:56Z","published":"2024-04-08T03:33:01Z","title":"QMix: Quality-aware Learning with Mixed Noise for Robust Retinal Disease\n  Diagnosis","summary":"  Due to the complexity of medical image acquisition and the difficulty of\nannotation, medical image datasets inevitably contain noise. Noisy data with\nwrong labels affects the robustness and generalization ability of deep neural\nnetworks. Previous noise learning methods mainly considered noise arising from\nimages being mislabeled, i.e. label noise, assuming that all mislabeled images\nare of high image quality. However, medical images are prone to suffering\nextreme quality issues, i.e. data noise, where discriminative visual features\nare missing for disease diagnosis. In this paper, we propose a noise learning\nframework, termed as QMix, that learns a robust disease diagnosis model under\nmixed noise. QMix alternates between sample separation and quality-aware\nsemisupervised training in each training epoch. In the sample separation phase,\nwe design a joint uncertainty-loss criterion to effectively separate (1)\ncorrectly labeled images; (2) mislabeled images with high quality and (3)\nmislabeled images with low quality. In the semi-supervised training phase, we\ntrain a disease diagnosis model to learn robust feature representation from the\nseparated samples. Specifically, we devise a sample-reweighing loss to mitigate\nthe effect of mislabeled images with low quality during training. Meanwhile, a\ncontrastive enhancement loss is proposed to further distinguish mislabeled\nimages with low quality from correctly labeled images. QMix achieved\nstate-of-the-art disease diagnosis performance on five public retinal image\ndatasets and exhibited substantial improvement on robustness against mixed\nnoise.\n","authors":["Junlin Hou","Jilan Xu","Rui Feng","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2404.05169v2.pdf","comment":"Accepted to IEEE Transactions on Medical Imaging"},{"id":"http://arxiv.org/abs/2504.12856v1","updated":"2025-04-17T11:23:17Z","published":"2025-04-17T11:23:17Z","title":"3D-PNAS: 3D Industrial Surface Anomaly Synthesis with Perlin Noise","summary":"  Large pretrained vision foundation models have shown significant potential in\nvarious vision tasks. However, for industrial anomaly detection, the scarcity\nof real defect samples poses a critical challenge in leveraging these models.\nWhile 2D anomaly generation has significantly advanced with established\ngenerative models, the adoption of 3D sensors in industrial manufacturing has\nmade leveraging 3D data for surface quality inspection an emerging trend. In\ncontrast to 2D techniques, 3D anomaly generation remains largely unexplored,\nlimiting the potential of 3D data in industrial quality inspection. To address\nthis gap, we propose a novel yet simple 3D anomaly generation method, 3D-PNAS,\nbased on Perlin noise and surface parameterization. Our method generates\nrealistic 3D surface anomalies by projecting the point cloud onto a 2D plane,\nsampling multi-scale noise values from a Perlin noise field, and perturbing the\npoint cloud along its normal direction. Through comprehensive visualization\nexperiments, we demonstrate how key parameters - including noise scale,\nperturbation strength, and octaves, provide fine-grained control over the\ngenerated anomalies, enabling the creation of diverse defect patterns from\npronounced deformations to subtle surface variations. Additionally, our\ncross-category experiments show that the method produces consistent yet\ngeometrically plausible anomalies across different object types, adapting to\ntheir specific surface characteristics. We also provide a comprehensive\ncodebase and visualization toolkit to facilitate future research.\n","authors":["Yifeng Cheng","Juan Du"],"pdf_url":"https://arxiv.org/pdf/2504.12856v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13254v4","updated":"2025-04-17T10:58:56Z","published":"2023-11-22T09:18:49Z","title":"Unified Domain Adaptive Semantic Segmentation","summary":"  Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS) aims to transfer\nthe supervision from a labeled source domain to an unlabeled target domain. The\nmajority of existing UDA-SS works typically consider images whilst recent\nattempts have extended further to tackle videos by modeling the temporal\ndimension. Although the two lines of research share the major challenges --\novercoming the underlying domain distribution shift, their studies are largely\nindependent, resulting in fragmented insights, a lack of holistic\nunderstanding, and missed opportunities for cross-pollination of ideas. This\nfragmentation prevents the unification of methods, leading to redundant efforts\nand suboptimal knowledge transfer across image and video domains. Under this\nobservation, we advocate unifying the study of UDA-SS across video and image\nscenarios, enabling a more comprehensive understanding, synergistic\nadvancements, and efficient knowledge sharing. To that end, we explore the\nunified UDA-SS from a general data augmentation perspective, serving as a\nunifying conceptual framework, enabling improved generalization, and potential\nfor cross-pollination of ideas, ultimately contributing to the overall progress\nand practical impact of this field of research. Specifically, we propose a\nQuad-directional Mixup (QuadMix) method, characterized by tackling distinct\npoint attributes and feature inconsistencies through four-directional paths for\nintra- and inter-domain mixing in a feature space. To deal with temporal shifts\nwith videos, we incorporate optical flow-guided feature aggregation across\nspatial and temporal dimensions for fine-grained domain alignment. Extensive\nexperiments show that our method outperforms the state-of-the-art works by\nlarge margins on four challenging UDA-SS benchmarks. Our source code and models\nwill be released at https://github.com/ZHE-SAPI/UDASS.\n","authors":["Zhe Zhang","Gaochang Wu","Jing Zhang","Xiatian Zhu","Dacheng Tao","Tianyou Chai"],"pdf_url":"https://arxiv.org/pdf/2311.13254v4.pdf","comment":"17 pages,11 figures, 11 tables. Accepted by IEEE Transactions on\n  Pattern Analysis and Machine Intelligence (TPAMI), 2025"},{"id":"http://arxiv.org/abs/2504.12844v1","updated":"2025-04-17T10:58:45Z","published":"2025-04-17T10:58:45Z","title":"High-Fidelity Image Inpainting with Multimodal Guided GAN Inversion","summary":"  Generative Adversarial Network (GAN) inversion have demonstrated excellent\nperformance in image inpainting that aims to restore lost or damaged image\ntexture using its unmasked content. Previous GAN inversion-based methods\nusually utilize well-trained GAN models as effective priors to generate the\nrealistic regions for missing holes. Despite excellence, they ignore a hard\nconstraint that the unmasked regions in the input and the output should be the\nsame, resulting in a gap between GAN inversion and image inpainting and thus\ndegrading the performance. Besides, existing GAN inversion approaches often\nconsider a single modality of the input image, neglecting other auxiliary cues\nin images for improvements. Addressing these problems, we propose a novel GAN\ninversion approach, dubbed MMInvertFill, for image inpainting. MMInvertFill\ncontains primarily a multimodal guided encoder with a pre-modulation and a GAN\ngenerator with F&W+ latent space. Specifically, the multimodal encoder aims to\nenhance the multi-scale structures with additional semantic segmentation edge\ntexture modalities through a gated mask-aware attention module. Afterwards, a\npre-modulation is presented to encode these structures into style vectors. To\nmitigate issues of conspicuous color discrepancy and semantic inconsistency, we\nintroduce the F&W+ latent space to bridge the gap between GAN inversion and\nimage inpainting. Furthermore, in order to reconstruct faithful and\nphotorealistic images, we devise a simple yet effective Soft-update Mean Latent\nmodule to capture more diversified in-domain patterns for generating\nhigh-fidelity textures for massive corruptions. In our extensive experiments on\nsix challenging datasets, we show that our MMInvertFill qualitatively and\nquantitatively outperforms other state-of-the-arts and it supports the\ncompletion of out-of-domain images effectively.\n","authors":["Libo Zhang","Yongsheng Yu","Jiali Yao","Heng Fan"],"pdf_url":"https://arxiv.org/pdf/2504.12844v1.pdf","comment":"Accepted to IJCV. arXiv admin note: text overlap with\n  arXiv:2208.11850"},{"id":"http://arxiv.org/abs/2504.12841v1","updated":"2025-04-17T10:57:29Z","published":"2025-04-17T10:57:29Z","title":"ALT: A Python Package for Lightweight Feature Representation in Time\n  Series Classification","summary":"  We introduce ALT, an open-source Python package created for efficient and\naccurate time series classification (TSC). The package implements the adaptive\nlaw-based transformation (ALT) algorithm, which transforms raw time series data\ninto a linearly separable feature space using variable-length shifted time\nwindows. This adaptive approach enhances its predecessor, the linear law-based\ntransformation (LLT), by effectively capturing patterns of varying temporal\nscales. The software is implemented for scalability, interpretability, and ease\nof use, achieving state-of-the-art performance with minimal computational\noverhead. Extensive benchmarking on real-world datasets demonstrates the\nutility of ALT for diverse TSC tasks in physics and related domains.\n","authors":["Balázs P. Halmos","Balázs Hajós","Vince Á. Molnár","Marcell T. Kurbucz","Antal Jakovác"],"pdf_url":"https://arxiv.org/pdf/2504.12841v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.12833v1","updated":"2025-04-17T10:46:39Z","published":"2025-04-17T10:46:39Z","title":"Image-Editing Specialists: An RLAIF Approach for Diffusion Models","summary":"  We present a novel approach to training specialized instruction-based\nimage-editing diffusion models, addressing key challenges in structural\npreservation with input images and semantic alignment with user prompts. We\nintroduce an online reinforcement learning framework that aligns the diffusion\nmodel with human preferences without relying on extensive human annotations or\ncurating a large dataset. Our method significantly improves the realism and\nalignment with instructions in two ways. First, the proposed models achieve\nprecise and structurally coherent modifications in complex scenes while\nmaintaining high fidelity in instruction-irrelevant areas. Second, they capture\nfine nuances in the desired edit by leveraging a visual prompt, enabling\ndetailed control over visual edits without lengthy textual prompts. This\napproach simplifies users' efforts to achieve highly specific edits, requiring\nonly 5 reference images depicting a certain concept for training. Experimental\nresults demonstrate that our models can perform intricate edits in complex\nscenes, after just 10 training steps. Finally, we showcase the versatility of\nour method by applying it to robotics, where enhancing the visual realism of\nsimulated environments through targeted sim-to-real image edits improves their\nutility as proxies for real-world settings.\n","authors":["Elior Benarous","Yilun Du","Heng Yang"],"pdf_url":"https://arxiv.org/pdf/2504.12833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12826v1","updated":"2025-04-17T10:40:36Z","published":"2025-04-17T10:40:36Z","title":"UncAD: Towards Safe End-to-end Autonomous Driving via Online Map\n  Uncertainty","summary":"  End-to-end autonomous driving aims to produce planning trajectories from raw\nsensors directly. Currently, most approaches integrate perception, prediction,\nand planning modules into a fully differentiable network, promising great\nscalability. However, these methods typically rely on deterministic modeling of\nonline maps in the perception module for guiding or constraining vehicle\nplanning, which may incorporate erroneous perception information and further\ncompromise planning safety. To address this issue, we delve into the importance\nof online map uncertainty for enhancing autonomous driving safety and propose a\nnovel paradigm named UncAD. Specifically, UncAD first estimates the uncertainty\nof the online map in the perception module. It then leverages the uncertainty\nto guide motion prediction and planning modules to produce multi-modal\ntrajectories. Finally, to achieve safer autonomous driving, UncAD proposes an\nuncertainty-collision-aware planning selection strategy according to the online\nmap uncertainty to evaluate and select the best trajectory. In this study, we\nincorporate UncAD into various state-of-the-art (SOTA) end-to-end methods.\nExperiments on the nuScenes dataset show that integrating UncAD, with only a\n1.9% increase in parameters, can reduce collision rates by up to 26% and\ndrivable area conflict rate by up to 42%. Codes, pre-trained models, and demo\nvideos can be accessed at https://github.com/pengxuanyang/UncAD.\n","authors":["Pengxuan Yang","Yupeng Zheng","Qichao Zhang","Kefei Zhu","Zebin Xing","Qiao Lin","Yun-Fu Liu","Zhiguo Su","Dongbin Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.12826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12825v1","updated":"2025-04-17T10:39:52Z","published":"2025-04-17T10:39:52Z","title":"TwoSquared: 4D Generation from 2D Image Pairs","summary":"  Despite the astonishing progress in generative AI, 4D dynamic object\ngeneration remains an open challenge. With limited high-quality training data\nand heavy computing requirements, the combination of hallucinating unseen\ngeometry together with unseen movement poses great challenges to generative\nmodels. In this work, we propose TwoSquared as a method to obtain a 4D\nphysically plausible sequence starting from only two 2D RGB images\ncorresponding to the beginning and end of the action. Instead of directly\nsolving the 4D generation problem, TwoSquared decomposes the problem into two\nsteps: 1) an image-to-3D module generation based on the existing generative\nmodel trained on high-quality 3D assets, and 2) a physically inspired\ndeformation module to predict intermediate movements. To this end, our method\ndoes not require templates or object-class-specific prior knowledge and can\ntake in-the-wild images as input. In our experiments, we demonstrate that\nTwoSquared is capable of producing texture-consistent and geometry-consistent\n4D sequences only given 2D images.\n","authors":["Lu Sang","Zehranaz Canfes","Dongliang Cao","Riccardo Marin","Florian Bernard","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2504.12825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12817v1","updated":"2025-04-17T10:21:30Z","published":"2025-04-17T10:21:30Z","title":"Explainable Scene Understanding with Qualitative Representations and\n  Graph Neural Networks","summary":"  This paper investigates the integration of graph neural networks (GNNs) with\nQualitative Explainable Graphs (QXGs) for scene understanding in automated\ndriving. Scene understanding is the basis for any further reactive or proactive\ndecision-making. Scene understanding and related reasoning is inherently an\nexplanation task: why is another traffic participant doing something, what or\nwho caused their actions? While previous work demonstrated QXGs' effectiveness\nusing shallow machine learning models, these approaches were limited to\nanalysing single relation chains between object pairs, disregarding the broader\nscene context. We propose a novel GNN architecture that processes entire graph\nstructures to identify relevant objects in traffic scenes. We evaluate our\nmethod on the nuScenes dataset enriched with DriveLM's human-annotated\nrelevance labels. Experimental results show that our GNN-based approach\nachieves superior performance compared to baseline methods. The model\neffectively handles the inherent class imbalance in relevant object\nidentification tasks while considering the complete spatial-temporal\nrelationships between all objects in the scene. Our work demonstrates the\npotential of combining qualitative representations with deep learning\napproaches for explainable scene understanding in autonomous driving systems.\n","authors":["Nassim Belmecheri","Arnaud Gotlieb","Nadjib Lazaar","Helge Spieker"],"pdf_url":"https://arxiv.org/pdf/2504.12817v1.pdf","comment":"Workshop \"Advancing Automated Driving in Highly Interactive Scenarios\n  through Behavior Prediction, Trustworthy AI, and Remote Operations\" @ 36th\n  IEEE Intelligent Vehicles Symposium (IV)"},{"id":"http://arxiv.org/abs/2504.12811v1","updated":"2025-04-17T10:16:47Z","published":"2025-04-17T10:16:47Z","title":"AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering","summary":"  Although 3D Gaussian Splatting (3DGS) has revolutionized 3D reconstruction,\nit still faces challenges such as aliasing, projection artifacts, and view\ninconsistencies, primarily due to the simplification of treating splats as 2D\nentities. We argue that incorporating full 3D evaluation of Gaussians\nthroughout the 3DGS pipeline can effectively address these issues while\npreserving rasterization efficiency. Specifically, we introduce an adaptive 3D\nsmoothing filter to mitigate aliasing and present a stable view-space bounding\nmethod that eliminates popping artifacts when Gaussians extend beyond the view\nfrustum. Furthermore, we promote tile-based culling to 3D with screen-space\nplanes, accelerating rendering and reducing sorting costs for hierarchical\nrasterization. Our method achieves state-of-the-art quality on in-distribution\nevaluation sets and significantly outperforms other approaches for\nout-of-distribution views. Our qualitative evaluations further demonstrate the\neffective removal of aliasing, distortions, and popping artifacts, ensuring\nreal-time, artifact-free rendering.\n","authors":["Michael Steiner","Thomas Köhler","Lukas Radl","Felix Windisch","Dieter Schmalstieg","Markus Steinberger"],"pdf_url":"https://arxiv.org/pdf/2504.12811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12809v1","updated":"2025-04-17T10:15:10Z","published":"2025-04-17T10:15:10Z","title":"Saliency-Aware Diffusion Reconstruction for Effective Invisible\n  Watermark Removal","summary":"  As digital content becomes increasingly ubiquitous, the need for robust\nwatermark removal techniques has grown due to the inadequacy of existing\nembedding techniques, which lack robustness. This paper introduces a novel\nSaliency-Aware Diffusion Reconstruction (SADRE) framework for watermark\nelimination on the web, combining adaptive noise injection, region-specific\nperturbations, and advanced diffusion-based reconstruction. SADRE disrupts\nembedded watermarks by injecting targeted noise into latent representations\nguided by saliency masks although preserving essential image features. A\nreverse diffusion process ensures high-fidelity image restoration, leveraging\nadaptive noise levels determined by watermark strength. Our framework is\ntheoretically grounded with stability guarantees and achieves robust watermark\nremoval across diverse scenarios. Empirical evaluations on state-of-the-art\n(SOTA) watermarking techniques demonstrate SADRE's superiority in balancing\nwatermark disruption and image quality. SADRE sets a new benchmark for\nwatermark elimination, offering a flexible and reliable solution for real-world\nweb content. Code is available\non~\\href{https://github.com/inzamamulDU/SADRE}{\\textbf{https://github.com/inzamamulDU/SADRE}}.\n","authors":["Inzamamul Alam","Md Tanvir Islam","Simon S. Woo"],"pdf_url":"https://arxiv.org/pdf/2504.12809v1.pdf","comment":"Accepted at The Web Conference 2025"},{"id":"http://arxiv.org/abs/2504.12807v1","updated":"2025-04-17T10:14:05Z","published":"2025-04-17T10:14:05Z","title":"Hybrid Dense-UNet201 Optimization for Pap Smear Image Segmentation Using\n  Spider Monkey Optimization","summary":"  Pap smear image segmentation is crucial for cervical cancer diagnosis.\nHowever, traditional segmentation models often struggle with complex cellular\nstructures and variations in pap smear images. This study proposes a hybrid\nDense-UNet201 optimization approach that integrates a pretrained DenseNet201 as\nthe encoder for the U-Net architecture and optimizes it using the spider monkey\noptimization (SMO) algorithm. The Dense-UNet201 model excelled at feature\nextraction. The SMO was modified to handle categorical and discrete parameters.\nThe SIPaKMeD dataset was used in this study and evaluated using key performance\nmetrics, including loss, accuracy, Intersection over Union (IoU), and Dice\ncoefficient. The experimental results showed that Dense-UNet201 outperformed\nU-Net, Res-UNet50, and Efficient-UNetB0. SMO Dense-UNet201 achieved a\nsegmentation accuracy of 96.16%, an IoU of 91.63%, and a Dice coefficient score\nof 95.63%. These findings underscore the effectiveness of image preprocessing,\npretrained models, and metaheuristic optimization in improving medical image\nanalysis and provide new insights into cervical cell segmentation methods.\n","authors":["Ach Khozaimi","Isnani Darti","Syaiful Anam","Wuryansari Muharini Kusumawinahyu"],"pdf_url":"https://arxiv.org/pdf/2504.12807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00584v2","updated":"2025-04-17T10:10:16Z","published":"2024-12-31T18:17:05Z","title":"Online Video Understanding: OVBench and VideoChat-Online","summary":"  Multimodal Large Language Models (MLLMs) have significantly progressed in\noffline video understanding. However, applying these models to real-world\nscenarios, such as autonomous driving and human-computer interaction, presents\nunique challenges due to the need for real-time processing of continuous online\nvideo streams. To this end, this paper presents systematic efforts from three\nperspectives: evaluation benchmark, model architecture, and training strategy.\nFirst, we introduce OVBench, a comprehensive question-answering benchmark\ndesigned to evaluate models' ability to perceive, memorize, and reason within\nonline video contexts. It features 6 core task types across three temporal\ncontexts-past, current, and future-forming 16 subtasks from diverse datasets.\nSecond, we propose a new Pyramid Memory Bank (PMB) that effectively retains key\nspatiotemporal information in video streams. Third, we proposed an\noffline-to-online learning paradigm, designing an interleaved dialogue format\nfor online video data and constructing an instruction-tuning dataset tailored\nfor online video training. This framework led to the development of\nVideoChat-Online, a robust and efficient model for online video understanding.\nDespite the lower computational cost and higher efficiency, VideoChat-Online\noutperforms existing state-of-the-art offline and online models across popular\noffline video benchmarks and OVBench, demonstrating the effectiveness of our\nmodel architecture and training strategy. % Our approach surpasses existing\nstate-of-the-art offline models Qwen2-VL 7B and online models Flash-VStream, by\n4.19% and 23.7% on OVBench, respectively.\n","authors":["Zhenpeng Huang","Xinhao Li","Jiaqi Li","Jing Wang","Xiangyu Zeng","Cheng Liang","Tao Wu","Xi Chen","Liang Li","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2501.00584v2.pdf","comment":"CVPR 2025 Camera Ready Version. Project Page:\n  https://videochat-online.github.io"},{"id":"http://arxiv.org/abs/2504.12801v1","updated":"2025-04-17T10:01:59Z","published":"2025-04-17T10:01:59Z","title":"Sign-In to the Lottery: Reparameterizing Sparse Training From Scratch","summary":"  The performance gap between training sparse neural networks from scratch\n(PaI) and dense-to-sparse training presents a major roadblock for efficient\ndeep learning. According to the Lottery Ticket Hypothesis, PaI hinges on\nfinding a problem specific parameter initialization. As we show, to this end,\ndetermining correct parameter signs is sufficient. Yet, they remain elusive to\nPaI. To address this issue, we propose Sign-In, which employs a dynamic\nreparameterization that provably induces sign flips. Such sign flips are\ncomplementary to the ones that dense-to-sparse training can accomplish,\nrendering Sign-In as an orthogonal method. While our experiments and theory\nsuggest performance improvements of PaI, they also carve out the main open\nchallenge to close the gap between PaI and dense-to-sparse training.\n","authors":["Advait Gadhikar","Tom Jacobs","Chao Zhou","Rebekka Burkholz"],"pdf_url":"https://arxiv.org/pdf/2504.12801v1.pdf","comment":"21 pages, 9 figures"},{"id":"http://arxiv.org/abs/2504.12800v1","updated":"2025-04-17T10:00:15Z","published":"2025-04-17T10:00:15Z","title":"CAGE-GS: High-fidelity Cage Based 3D Gaussian Splatting Deformation","summary":"  As 3D Gaussian Splatting (3DGS) gains popularity as a 3D representation of\nreal scenes, enabling user-friendly deformation to create novel scenes while\npreserving fine details from the original 3DGS has attracted significant\nresearch attention. We introduce CAGE-GS, a cage-based 3DGS deformation method\nthat seamlessly aligns a source 3DGS scene with a user-defined target shape.\nOur approach learns a deformation cage from the target, which guides the\ngeometric transformation of the source scene. While the cages effectively\ncontrol structural alignment, preserving the textural appearance of 3DGS\nremains challenging due to the complexity of covariance parameters. To address\nthis, we employ a Jacobian matrix-based strategy to update the covariance\nparameters of each Gaussian, ensuring texture fidelity post-deformation. Our\nmethod is highly flexible, accommodating various target shape representations,\nincluding texts, images, point clouds, meshes and 3DGS models. Extensive\nexperiments and ablation studies on both public datasets and newly proposed\nscenes demonstrate that our method significantly outperforms existing\ntechniques in both efficiency and deformation quality.\n","authors":["Yifei Tong","Runze Tian","Xiao Han","Dingyao Liu","Fenggen Yu","Yan Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.12800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12799v1","updated":"2025-04-17T10:00:09Z","published":"2025-04-17T10:00:09Z","title":"TSGS: Improving Gaussian Splatting for Transparent Surface\n  Reconstruction via Normal and De-lighting Priors","summary":"  Reconstructing transparent surfaces is essential for tasks such as robotic\nmanipulation in labs, yet it poses a significant challenge for 3D\nreconstruction techniques like 3D Gaussian Splatting (3DGS). These methods\noften encounter a transparency-depth dilemma, where the pursuit of\nphotorealistic rendering through standard $\\alpha$-blending undermines\ngeometric precision, resulting in considerable depth estimation errors for\ntransparent materials. To address this issue, we introduce Transparent Surface\nGaussian Splatting (TSGS), a new framework that separates geometry learning\nfrom appearance refinement. In the geometry learning stage, TSGS focuses on\ngeometry by using specular-suppressed inputs to accurately represent surfaces.\nIn the second stage, TSGS improves visual fidelity through anisotropic specular\nmodeling, crucially maintaining the established opacity to ensure geometric\naccuracy. To enhance depth inference, TSGS employs a first-surface depth\nextraction method. This technique uses a sliding window over $\\alpha$-blending\nweights to pinpoint the most likely surface location and calculates a robust\nweighted average depth. To evaluate the transparent surface reconstruction task\nunder realistic conditions, we collect a TransLab dataset that includes complex\ntransparent laboratory glassware. Extensive experiments on TransLab show that\nTSGS achieves accurate geometric reconstruction and realistic rendering of\ntransparent objects simultaneously within the efficient 3DGS framework.\nSpecifically, TSGS significantly surpasses current leading methods, achieving a\n37.3% reduction in chamfer distance and an 8.0% improvement in F1 score\ncompared to the top baseline. The code and dataset will be released at\nhttps://longxiang-ai.github.io/TSGS/.\n","authors":["Mingwei Li","Pu Pang","Hehe Fan","Hua Huang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2504.12799v1.pdf","comment":"Project page: https://longxiang-ai.github.io/TSGS/"},{"id":"http://arxiv.org/abs/2504.12795v1","updated":"2025-04-17T09:56:35Z","published":"2025-04-17T09:56:35Z","title":"EarthGPT-X: Enabling MLLMs to Flexibly and Comprehensively Understand\n  Multi-Source Remote Sensing Imagery","summary":"  Recent advances in the visual-language area have developed natural\nmulti-modal large language models (MLLMs) for spatial reasoning through visual\nprompting. However, due to remote sensing (RS) imagery containing abundant\ngeospatial information that differs from natural images, it is challenging to\neffectively adapt natural spatial models to the RS domain. Moreover, current RS\nMLLMs are limited in overly narrow interpretation levels and interaction\nmanner, hindering their applicability in real-world scenarios. To address those\nchallenges, a spatial MLLM named EarthGPT-X is proposed, enabling a\ncomprehensive understanding of multi-source RS imagery, such as optical,\nsynthetic aperture radar (SAR), and infrared. EarthGPT-X offers zoom-in and\nzoom-out insight, and possesses flexible multi-grained interactive abilities.\nMoreover, EarthGPT-X unifies two types of critical spatial tasks (i.e.,\nreferring and grounding) into a visual prompting framework. To achieve these\nversatile capabilities, several key strategies are developed. The first is the\nmulti-modal content integration method, which enhances the interplay between\nimages, visual prompts, and text instructions. Subsequently, a cross-domain\none-stage fusion training strategy is proposed, utilizing the large language\nmodel (LLM) as a unified interface for multi-source multi-task learning.\nFurthermore, by incorporating a pixel perception module, the referring and\ngrounding tasks are seamlessly unified within a single framework. In addition,\nthe experiments conducted demonstrate the superiority of the proposed\nEarthGPT-X in multi-grained tasks and its impressive flexibility in multi-modal\ninteraction, revealing significant advancements of MLLM in the RS field.\n","authors":["Wei Zhang","Miaoxin Cai","Yaqian Ning","Tong Zhang","Yin Zhuang","He Chen","Jun Li","Xuerui Mao"],"pdf_url":"https://arxiv.org/pdf/2504.12795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.06755v2","updated":"2025-04-17T09:49:23Z","published":"2025-04-09T10:19:35Z","title":"FANeRV: Frequency Separation and Augmentation based Neural\n  Representation for Video","summary":"  Neural representations for video (NeRV) have gained considerable attention\nfor their strong performance across various video tasks. However, existing NeRV\nmethods often struggle to capture fine spatial details, resulting in vague\nreconstructions. In this paper, we present a Frequency Separation and\nAugmentation based Neural Representation for video (FANeRV), which addresses\nthese limitations with its core Wavelet Frequency Upgrade Block. This block\nexplicitly separates input frames into high and low-frequency components using\ndiscrete wavelet transform, followed by targeted enhancement using specialized\nmodules. Finally, a specially designed gated network effectively fuses these\nfrequency components for optimal reconstruction. Additionally, convolutional\nresidual enhancement blocks are integrated into the later stages of the network\nto balance parameter distribution and improve the restoration of high-frequency\ndetails. Experimental results demonstrate that FANeRV significantly improves\nreconstruction performance and excels in multiple tasks, including video\ncompression, inpainting, and interpolation, outperforming existing NeRV\nmethods.\n","authors":["Li Yu","Zhihui Li","Jimin Xiao","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2504.06755v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11054v3","updated":"2025-04-17T09:45:54Z","published":"2024-08-20T17:58:59Z","title":"Near, far: Patch-ordering enhances vision foundation models' scene\n  understanding","summary":"  We introduce NeCo: Patch Neighbor Consistency, a novel self-supervised\ntraining loss that enforces patch-level nearest neighbor consistency across a\nstudent and teacher model. Compared to contrastive approaches that only yield\nbinary learning signals, i.e., 'attract' and 'repel', this approach benefits\nfrom the more fine-grained learning signal of sorting spatially dense features\nrelative to reference patches. Our method leverages differentiable sorting\napplied on top of pretrained representations, such as DINOv2-registers to\nbootstrap the learning signal and further improve upon them. This dense\npost-pretraining leads to superior performance across various models and\ndatasets, despite requiring only 19 hours on a single GPU. This method\ngenerates high-quality dense feature encoders and establishes several new\nstate-of-the-art results such as +5.5% and +6% for non-parametric in-context\nsemantic segmentation on ADE20k and Pascal VOC, +7.2% and +5.7% for linear\nsegmentation evaluations on COCO-Things and -Stuff and improvements in the 3D\nunderstanding of multi-view consistency on SPair-71k, by more than 1.5%.\n","authors":["Valentinos Pariza","Mohammadreza Salehi","Gertjan Burghouts","Francesco Locatello","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2408.11054v3.pdf","comment":"Accepted at ICLR25. The webpage is accessible at:\n  https://vpariza.github.io/NeCo/"},{"id":"http://arxiv.org/abs/2504.12788v1","updated":"2025-04-17T09:37:11Z","published":"2025-04-17T09:37:11Z","title":"ARAP-GS: Drag-driven As-Rigid-As-Possible 3D Gaussian Splatting Editing\n  with Diffusion Prior","summary":"  Drag-driven editing has become popular among designers for its ability to\nmodify complex geometric structures through simple and intuitive manipulation,\nallowing users to adjust and reshape content with minimal technical skill. This\ndrag operation has been incorporated into numerous methods to facilitate the\nediting of 2D images and 3D meshes in design. However, few studies have\nexplored drag-driven editing for the widely-used 3D Gaussian Splatting (3DGS)\nrepresentation, as deforming 3DGS while preserving shape coherence and visual\ncontinuity remains challenging. In this paper, we introduce ARAP-GS, a\ndrag-driven 3DGS editing framework based on As-Rigid-As-Possible (ARAP)\ndeformation. Unlike previous 3DGS editing methods, we are the first to apply\nARAP deformation directly to 3D Gaussians, enabling flexible, drag-driven\ngeometric transformations. To preserve scene appearance after deformation, we\nincorporate an advanced diffusion prior for image super-resolution within our\niterative optimization process. This approach enhances visual quality while\nmaintaining multi-view consistency in the edited results. Experiments show that\nARAP-GS outperforms current methods across diverse 3D scenes, demonstrating its\neffectiveness and superiority for drag-driven 3DGS editing. Additionally, our\nmethod is highly efficient, requiring only 10 to 20 minutes to edit a scene on\na single RTX 3090 GPU.\n","authors":["Xiao Han","Runze Tian","Yifei Tong","Fenggen Yu","Dingyao Liu","Yan Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.12788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12782v1","updated":"2025-04-17T09:29:30Z","published":"2025-04-17T09:29:30Z","title":"Set You Straight: Auto-Steering Denoising Trajectories to Sidestep\n  Unwanted Concepts","summary":"  Ensuring the ethical deployment of text-to-image models requires effective\ntechniques to prevent the generation of harmful or inappropriate content. While\nconcept erasure methods offer a promising solution, existing finetuning-based\napproaches suffer from notable limitations. Anchor-free methods risk disrupting\nsampling trajectories, leading to visual artifacts, while anchor-based methods\nrely on the heuristic selection of anchor concepts. To overcome these\nshortcomings, we introduce a finetuning framework, dubbed ANT, which\nAutomatically guides deNoising Trajectories to avoid unwanted concepts. ANT is\nbuilt on a key insight: reversing the condition direction of classifier-free\nguidance during mid-to-late denoising stages enables precise content\nmodification without sacrificing early-stage structural integrity. This\ninspires a trajectory-aware objective that preserves the integrity of the\nearly-stage score function field, which steers samples toward the natural image\nmanifold, without relying on heuristic anchor concept selection. For\nsingle-concept erasure, we propose an augmentation-enhanced weight saliency map\nto precisely identify the critical parameters that most significantly\ncontribute to the unwanted concept, enabling more thorough and efficient\nerasure. For multi-concept erasure, our objective function offers a versatile\nplug-and-play solution that significantly boosts performance. Extensive\nexperiments demonstrate that ANT achieves state-of-the-art results in both\nsingle and multi-concept erasure, delivering high-quality, safe outputs without\ncompromising the generative fidelity. Code is available at\nhttps://github.com/lileyang1210/ANT\n","authors":["Leyang Li","Shilin Lu","Yan Ren","Adams Wai-Kin Kong"],"pdf_url":"https://arxiv.org/pdf/2504.12782v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2504.11967v2","updated":"2025-04-17T09:25:04Z","published":"2025-04-16T10:58:33Z","title":"Securing the Skies: A Comprehensive Survey on Anti-UAV Methods,\n  Benchmarking, and Future Directions","summary":"  Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure\ninspection, surveillance, and related tasks, yet they also introduce critical\nsecurity challenges. This survey provides a wide-ranging examination of the\nanti-UAV domain, centering on three core objectives-classification, detection,\nand tracking-while detailing emerging methodologies such as diffusion-based\ndata synthesis, multi-modal fusion, vision-language modeling, self-supervised\nlearning, and reinforcement learning. We systematically evaluate\nstate-of-the-art solutions across both single-modality and multi-sensor\npipelines (spanning RGB, infrared, audio, radar, and RF) and discuss\nlarge-scale as well as adversarially oriented benchmarks. Our analysis reveals\npersistent gaps in real-time performance, stealth detection, and swarm-based\nscenarios, underscoring pressing needs for robust, adaptive anti-UAV systems.\nBy highlighting open research directions, we aim to foster innovation and guide\nthe development of next-generation defense strategies in an era marked by the\nextensive use of UAVs.\n","authors":["Yifei Dong","Fengyi Wu","Sanjian Zhang","Guangyu Chen","Yuzhi Hu","Masumi Yano","Jingdong Sun","Siyu Huang","Feng Liu","Qi Dai","Zhi-Qi Cheng"],"pdf_url":"https://arxiv.org/pdf/2504.11967v2.pdf","comment":"Accepted at CVPR Workshop Anti-UAV 2025. 15 pages"},{"id":"http://arxiv.org/abs/2504.12753v1","updated":"2025-04-17T08:45:33Z","published":"2025-04-17T08:45:33Z","title":"Stronger, Steadier & Superior: Geometric Consistency in Depth VFM Forges\n  Domain Generalized Semantic Segmentation","summary":"  Vision Foundation Models (VFMs) have delivered remarkable performance in\nDomain Generalized Semantic Segmentation (DGSS). However, recent methods often\noverlook the fact that visual cues are susceptible, whereas the underlying\ngeometry remains stable, rendering depth information more robust. In this\npaper, we investigate the potential of integrating depth information with\nfeatures from VFMs, to improve the geometric consistency within an image and\nboost the generalization performance of VFMs. We propose a novel fine-tuning\nDGSS framework, named DepthForge, which integrates the visual cues from frozen\nDINOv2 or EVA02 and depth cues from frozen Depth Anything V2. In each layer of\nthe VFMs, we incorporate depth-aware learnable tokens to continuously decouple\ndomain-invariant visual and spatial information, thereby enhancing depth\nawareness and attention of the VFMs. Finally, we develop a depth refinement\ndecoder and integrate it into the model architecture to adaptively refine\nmulti-layer VFM features and depth-aware learnable tokens. Extensive\nexperiments are conducted based on various DGSS settings and five different\ndatsets as unseen target domains. The qualitative and quantitative results\ndemonstrate that our method significantly outperforms alternative approaches\nwith stronger performance, steadier visual-spatial attention, and superior\ngeneralization ability. In particular, DepthForge exhibits outstanding\nperformance under extreme conditions (e.g., night and snow). Code is available\nat https://github.com/anonymouse-xzrptkvyqc/DepthForge.\n","authors":["Siyu Chen","Ting Han","Changshe Zhang","Xin Luo","Meiliu Wu","Guorong Cai","Jinhe Su"],"pdf_url":"https://arxiv.org/pdf/2504.12753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12749v1","updated":"2025-04-17T08:41:23Z","published":"2025-04-17T08:41:23Z","title":"LAD-Reasoner: Tiny Multimodal Models are Good Reasoners for Logical\n  Anomaly Detection","summary":"  Recent advances in industrial anomaly detection have highlighted the need for\ndeeper logical anomaly analysis, where unexpected relationships among objects,\ncounts, and spatial configurations must be identified and explained. Existing\napproaches often rely on large-scale external reasoning modules or elaborate\npipeline designs, hindering practical deployment and interpretability. To\naddress these limitations, we introduce a new task, Reasoning Logical Anomaly\nDetection (RLAD), which extends traditional anomaly detection by incorporating\nlogical reasoning. We propose a new framework, LAD-Reasoner, a customized tiny\nmultimodal language model built on Qwen2.5-VL 3B. Our approach leverages a\ntwo-stage training paradigm that first employs Supervised Fine-Tuning (SFT) for\nfine-grained visual understanding, followed by Group Relative Policy\nOptimization (GRPO) to refine logical anomaly detection and enforce coherent,\nhuman-readable reasoning. Crucially, reward signals are derived from both the\ndetection accuracy and the structural quality of the outputs, obviating the\nneed for building chain of thought (CoT) reasoning data. Experiments on the\nMVTec LOCO AD dataset show that LAD-Reasoner, though significantly smaller,\nmatches the performance of Qwen2.5-VL-72B in accuracy and F1 score, and further\nexcels in producing concise and interpretable rationales. This unified design\nreduces reliance on large models and complex pipelines, while offering\ntransparent and interpretable insights into logical anomaly detection. Code and\ndata will be released.\n","authors":["Weijia Li","Guanglei Chu","Jiong Chen","Guo-Sen Xie","Caifeng Shan","Fang Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.12749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12747v1","updated":"2025-04-17T08:39:32Z","published":"2025-04-17T08:39:32Z","title":"Privacy Protection Against Personalized Text-to-Image Synthesis via\n  Cross-image Consistency Constraints","summary":"  The rapid advancement of diffusion models and personalization techniques has\nmade it possible to recreate individual portraits from just a few publicly\navailable images. While such capabilities empower various creative\napplications, they also introduce serious privacy concerns, as adversaries can\nexploit them to generate highly realistic impersonations. To counter these\nthreats, anti-personalization methods have been proposed, which add adversarial\nperturbations to published images to disrupt the training of personalization\nmodels. However, existing approaches largely overlook the intrinsic multi-image\nnature of personalization and instead adopt a naive strategy of applying\nperturbations independently, as commonly done in single-image settings. This\nneglects the opportunity to leverage inter-image relationships for stronger\nprivacy protection. Therefore, we advocate for a group-level perspective on\nprivacy protection against personalization. Specifically, we introduce\nCross-image Anti-Personalization (CAP), a novel framework that enhances\nresistance to personalization by enforcing style consistency across perturbed\nimages. Furthermore, we develop a dynamic ratio adjustment strategy that\nadaptively balances the impact of the consistency loss throughout the attack\niterations. Extensive experiments on the classical CelebHQ and VGGFace2\nbenchmarks show that CAP substantially improves existing methods.\n","authors":["Guanyu Wang","Kailong Wang","Yihao Huang","Mingyi Zhou","Zhang Qing cnwatcher","Geguang Pu","Li Li"],"pdf_url":"https://arxiv.org/pdf/2504.12747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10291v4","updated":"2025-04-17T08:31:14Z","published":"2024-10-14T08:45:35Z","title":"Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal\n  Perspective","summary":"  Accurate interpretation and visualization of human instructions are crucial\nfor text-to-image (T2I) synthesis. However, current models struggle to capture\nsemantic variations from word order changes, and existing evaluations, relying\non indirect metrics like text-image similarity, fail to reliably assess these\nchallenges. This often obscures poor performance on complex or uncommon\nlinguistic patterns by the focus on frequent word combinations. To address\nthese deficiencies, we propose a novel metric called SemVarEffect and a\nbenchmark named SemVarBench, designed to evaluate the causality between\nsemantic variations in inputs and outputs in T2I synthesis. Semantic variations\nare achieved through two types of linguistic permutations, while avoiding\neasily predictable literal variations. Experiments reveal that the\nCogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.\nSemantic variations in object relations are less understood than attributes,\nscoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in\nUNet or Transformers plays a crucial role in handling semantic variations, a\nfactor previously overlooked by a focus on textual encoders. Our work\nestablishes an effective evaluation framework that advances the T2I synthesis\ncommunity's exploration of human instruction understanding. Our benchmark and\ncode are available at https://github.com/zhuxiangru/SemVarBench .\n","authors":["Xiangru Zhu","Penglei Sun","Yaoxian Song","Yanghua Xiao","Zhixu Li","Chengyu Wang","Jun Huang","Bei Yang","Xiaoxiao Xu"],"pdf_url":"https://arxiv.org/pdf/2410.10291v4.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2504.12739v1","updated":"2025-04-17T08:29:00Z","published":"2025-04-17T08:29:00Z","title":"Mask Image Watermarking","summary":"  We present MaskMark, a simple, efficient and flexible framework for image\nwatermarking. MaskMark has two variants: MaskMark-D, which supports global\nwatermark embedding, watermark localization, and local watermark extraction for\napplications such as tamper detection, and MaskMark-ED, which focuses on local\nwatermark embedding and extraction with enhanced robustness in small regions,\nenabling localized image protection. Built upon the classical Encoder-\nDistortion-Decoder training paradigm, MaskMark-D introduces a simple masking\nmechanism during the decoding stage to support both global and local watermark\nextraction. A mask is applied to the watermarked image before extraction,\nallowing the decoder to focus on selected regions and learn local extraction. A\nlocalization module is also integrated into the decoder to identify watermark\nregions during inference, reducing interference from irrelevant content and\nimproving accuracy. MaskMark-ED extends this design by incorporating the mask\ninto the encoding stage as well, guiding the encoder to embed the watermark in\ndesignated local regions for enhanced robustness. Comprehensive experiments\nshow that MaskMark achieves state-of-the-art performance in global watermark\nextraction, local watermark extraction, watermark localization, and\nmulti-watermark embedding. It outperforms all existing baselines, including the\nrecent leading model WAM for local watermarking, while preserving high visual\nquality of the watermarked images. MaskMark is also flexible, by adjusting the\ndistortion layer, it can adapt to different robustness requirements with just a\nfew steps of fine-tuning. Moreover, our approach is efficient and easy to\noptimize, requiring only 20 hours on a single A6000 GPU with just 1/15 the\ncomputational cost of WAM.\n","authors":["Runyi Hu","Jie Zhang","Shiqian Zhao","Nils Lukas","Jiwei Li","Qing Guo","Han Qiu","Tianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.12739v1.pdf","comment":"23 pages, 18 figures"},{"id":"http://arxiv.org/abs/2412.10273v2","updated":"2025-04-17T08:18:15Z","published":"2024-12-13T16:46:46Z","title":"unPIC: A Geometric Multiview Prior for Image to 3D Synthesis","summary":"  We introduce a hierarchical probabilistic approach to go from a 2D image to\nmultiview 3D: a diffusion \"prior\" predicts the unseen 3D geometry, which then\nconditions a diffusion \"decoder\" to generate novel views of the subject. We use\na pointmap-based geometric representation to coordinate the generation of\nmultiple target views simultaneously. We construct a predictable distribution\nof geometric features per target view to enable learnability across examples,\nand generalization to arbitrary inputs images. Our modular, geometry-driven\napproach to novel-view synthesis (called \"unPIC\") beats competing baselines\nsuch as CAT3D, EscherNet, Free3D, and One-2-3-45 on held-out objects from\nObjaverseXL, as well as unseen real-world objects from Google Scanned Objects,\nAmazon Berkeley Objects, and the Digital Twin Catalog.\n","authors":["Rishabh Kabra","Drew A. Hudson","Sjoerd van Steenkiste","Joao Carreira","Niloy J. Mitra"],"pdf_url":"https://arxiv.org/pdf/2412.10273v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12966v5","updated":"2025-04-17T08:05:10Z","published":"2024-04-19T15:53:27Z","title":"Look Before You Decide: Prompting Active Deduction of MLLMs for\n  Assumptive Reasoning","summary":"  Recently, Multimodal Large Language Models (MLLMs) have achieved significant\nsuccess across multiple disciplines due to their exceptional\ninstruction-following capabilities and extensive world knowledge. However,\nwhether these MLLMs possess human-like compositional reasoning abilities\nremains an open problem. To unveil their reasoning behaviors, we first curate a\n\\textbf{M}ultimodal \\textbf{A}ssumptive \\textbf{R}ea\\textbf{s}oning Benchmark\n(MARS-Bench) in this paper. Interestingly, we find that most prevalent MLLMs\ncan be easily fooled by the introduction of a presupposition into the question,\nwhereas such presuppositions appear naive to human reasoning. Besides, we also\npropose a simple yet effective method, Active Deduction (AD), a novel\nreinforcement learning paradigm to encourage the model to actively perform\ncomposite deduction before reaching a final decision. Equipped with the\nproposed AD method, a MLLM demonstrates significant improvements in assumptive\nreasoning abilities without compromising its general-purpose question-answering\nperformance. We also provide extensive evaluations of both open-source and\nprivate MLLMs on MARS-Bench, along with experimental analyses of the AD method.\n","authors":["Yian Li","Wentao Tian","Yang Jiao","Jingjing Chen","Tianwen Qian","Bin Zhu","Na Zhao","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2404.12966v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12718v1","updated":"2025-04-17T07:48:05Z","published":"2025-04-17T07:48:05Z","title":"TUMLS: Trustful Fully Unsupervised Multi-Level Segmentation for Whole\n  Slide Images of Histology","summary":"  Digital pathology, augmented by artificial intelligence (AI), holds\nsignificant promise for improving the workflow of pathologists. However,\nchallenges such as the labor-intensive annotation of whole slide images (WSIs),\nhigh computational demands, and trust concerns arising from the absence of\nuncertainty estimation in predictions hinder the practical application of\ncurrent AI methodologies in histopathology. To address these issues, we present\na novel trustful fully unsupervised multi-level segmentation methodology\n(TUMLS) for WSIs. TUMLS adopts an autoencoder (AE) as a feature extractor to\nidentify the different tissue types within low-resolution training data. It\nselects representative patches from each identified group based on an\nuncertainty measure and then does unsupervised nuclei segmentation in their\nrespective higher-resolution space without using any ML algorithms. Crucially,\nthis solution integrates seamlessly into clinicians workflows, transforming the\nexamination of a whole WSI into a review of concise, interpretable cross-level\ninsights. This integration significantly enhances and accelerates the workflow\nwhile ensuring transparency. We evaluated our approach using the UPENN-GBM\ndataset, where the AE achieved a mean squared error (MSE) of 0.0016.\nAdditionally, nucleus segmentation is assessed on the MoNuSeg dataset,\noutperforming all unsupervised approaches with an F1 score of 77.46% and a\nJaccard score of 63.35%. These results demonstrate the efficacy of TUMLS in\nadvancing the field of digital pathology.\n","authors":["Walid Rehamnia","Alexandra Getmanskaya","Evgeniy Vasilyev","Vadim Turlapov"],"pdf_url":"https://arxiv.org/pdf/2504.12718v1.pdf","comment":"32 pages, 15 figures, 3 tables, 42 references"},{"id":"http://arxiv.org/abs/2504.12717v1","updated":"2025-04-17T07:46:19Z","published":"2025-04-17T07:46:19Z","title":"Post-pre-training for Modality Alignment in Vision-Language Foundation\n  Models","summary":"  Contrastive language image pre-training (CLIP) is an essential component of\nbuilding modern vision-language foundation models. While CLIP demonstrates\nremarkable zero-shot performance on downstream tasks, the multi-modal feature\nspaces still suffer from a modality gap, which is a gap between image and text\nfeature clusters and limits downstream task performance. Although existing\nworks attempt to address the modality gap by modifying pre-training or\nfine-tuning, they struggle with heavy training costs with large datasets or\ndegradations of zero-shot performance. This paper presents CLIP-Refine, a\npost-pre-training method for CLIP models at a phase between pre-training and\nfine-tuning. CLIP-Refine aims to align the feature space with 1 epoch training\non small image-text datasets without zero-shot performance degradations. To\nthis end, we introduce two techniques: random feature alignment (RaFA) and\nhybrid contrastive-distillation (HyCD). RaFA aligns the image and text features\nto follow a shared prior distribution by minimizing the distance to random\nreference vectors sampled from the prior. HyCD updates the model with hybrid\nsoft labels generated by combining ground-truth image-text pair labels and\noutputs from the pre-trained CLIP model. This contributes to achieving both\nmaintaining the past knowledge and learning new knowledge to align features.\nOur extensive experiments with multiple classification and retrieval tasks show\nthat CLIP-Refine succeeds in mitigating the modality gap and improving the\nzero-shot performance.\n","authors":["Shin'ya Yamaguchi","Dewei Feng","Sekitoshi Kanai","Kazuki Adachi","Daiki Chijiwa"],"pdf_url":"https://arxiv.org/pdf/2504.12717v1.pdf","comment":"Accepted to CVPR 2025; Code: https://github.com/yshinya6/clip-refine"},{"id":"http://arxiv.org/abs/2501.17547v2","updated":"2025-04-17T07:39:35Z","published":"2025-01-29T10:32:08Z","title":"Towards Training-Free Open-World Classification with 3D Generative\n  Models","summary":"  3D open-world classification is a challenging yet essential task in dynamic\nand unstructured real-world scenarios, requiring both open-category and\nopen-pose recognition. To address these challenges, recent wisdom often takes\nsophisticated 2D pre-trained models to provide enriched and stable\nrepresentations. However, these methods largely rely on how 3D objects can be\nprojected into 2D space, which is unfortunately not well solved, and thus\nsignificantly limits their performance. Unlike these present efforts, in this\npaper we make a pioneering exploration of 3D generative models for 3D\nopen-world classification. Drawing on abundant prior knowledge from 3D\ngenerative models, we additionally craft a rotation-invariant feature\nextractor. This innovative synergy endows our pipeline with the advantages of\nbeing training-free, open-category, and pose-invariant, thus well suited to 3D\nopen-world classification. Extensive experiments on benchmark datasets\ndemonstrate the potential of generative models in 3D open-world classification,\nachieving state-of-the-art performance on ModelNet10 and McGill with 32.0% and\n8.7% overall accuracy improvement, respectively.\n","authors":["Xinzhe Xia","Weiguang Zhao","Yuyao Yan","Guanyu Yang","Rui Zhang","Kaizhu Huang","Xi Yang"],"pdf_url":"https://arxiv.org/pdf/2501.17547v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14319v2","updated":"2025-04-17T07:35:40Z","published":"2024-09-22T05:13:11Z","title":"Scene-Text Grounding for Text-Based Video Question Answering","summary":"  Existing efforts in text-based video question answering (TextVideoQA) are\ncriticized for their opaque decisionmaking and heavy reliance on scene-text\nrecognition. In this paper, we propose to study Grounded TextVideoQA by forcing\nmodels to answer questions and spatio-temporally localize the relevant\nscene-text regions, thus decoupling QA from scenetext recognition and promoting\nresearch towards interpretable QA. The task has three-fold significance. First,\nit encourages scene-text evidence versus other short-cuts for answer\npredictions. Second, it directly accepts scene-text regions as visual answers,\nthus circumventing the problem of ineffective answer evaluation by stringent\nstring matching. Third, it isolates the challenges inherited in VideoQA and\nscene-text recognition. This enables the diagnosis of the root causes for\nfailure predictions, e.g., wrong QA or wrong scene-text recognition? To achieve\nGrounded TextVideoQA, we propose the T2S-QA model that highlights a\ndisentangled temporal-to-spatial contrastive learning strategy for\nweakly-supervised scene-text grounding and grounded TextVideoQA. To facilitate\nevaluation, we construct a new dataset ViTXT-GQA which features 52K scene-text\nbounding boxes within 2.2K temporal segments related to 2K questions and 729\nvideos. With ViTXT-GQA, we perform extensive experiments and demonstrate the\nsevere limitations of existing techniques in Grounded TextVideoQA. While T2S-QA\nachieves superior results, the large performance gap with human leaves ample\nspace for improvement. Our further analysis of oracle scene-text inputs posits\nthat the major challenge is scene-text recognition. To advance the research of\nGrounded TextVideoQA, our dataset and code are at\nhttps://github.com/zhousheng97/ViTXT-GQA.git\n","authors":["Sheng Zhou","Junbin Xiao","Xun Yang","Peipei Song","Dan Guo","Angela Yao","Meng Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2409.14319v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12711v1","updated":"2025-04-17T07:35:35Z","published":"2025-04-17T07:35:35Z","title":"NTIRE 2025 Challenge on Day and Night Raindrop Removal for Dual-Focused\n  Images: Methods and Results","summary":"  This paper reviews the NTIRE 2025 Challenge on Day and Night Raindrop Removal\nfor Dual-Focused Images. This challenge received a wide range of impressive\nsolutions, which are developed and evaluated using our collected real-world\nRaindrop Clarity dataset. Unlike existing deraining datasets, our Raindrop\nClarity dataset is more diverse and challenging in degradation types and\ncontents, which includes day raindrop-focused, day background-focused, night\nraindrop-focused, and night background-focused degradations. This dataset is\ndivided into three subsets for competition: 14,139 images for training, 240\nimages for validation, and 731 images for testing. The primary objective of\nthis challenge is to establish a new and powerful benchmark for the task of\nremoving raindrops under varying lighting and focus conditions. There are a\ntotal of 361 participants in the competition, and 32 teams submitting valid\nsolutions and fact sheets for the final testing phase. These submissions\nachieved state-of-the-art (SOTA) performance on the Raindrop Clarity dataset.\nThe project can be found at\nhttps://lixinustc.github.io/CVPR-NTIRE2025-RainDrop-Competition.github.io/.\n","authors":["Xin Li","Yeying Jin","Xin Jin","Zongwei Wu","Bingchen Li","Yufei Wang","Wenhan Yang","Yu Li","Zhibo Chen","Bihan Wen","Robby T. Tan","Radu Timofte","Qiyu Rong","Hongyuan Jing","Mengmeng Zhang","Jinglong Li","Xiangyu Lu","Yi Ren","Yuting Liu","Meng Zhang","Xiang Chen","Qiyuan Guan","Jiangxin Dong","Jinshan Pan","Conglin Gou","Qirui Yang","Fangpu Zhang","Yunlong Lin","Sixiang Chen","Guoxi Huang","Ruirui Lin","Yan Zhang","Jingyu Yang","Huanjing Yue","Jiyuan Chen","Qiaosi Yi","Hongjun Wang","Chenxi Xie","Shuai Li","Yuhui Wu","Kaiyi Ma","Jiakui Hu","Juncheng Li","Liwen Pan","Guangwei Gao","Wenjie Li","Zhenyu Jin","Heng Guo","Zhanyu Ma","Yubo Wang","Jinghua Wang","Wangzhi Xing","Anjusree Karnavar","Diqi Chen","Mohammad Aminul Islam","Hao Yang","Ruikun Zhang","Liyuan Pan","Qianhao Luo"," XinCao","Han Zhou","Yan Min","Wei Dong","Jun Chen","Taoyi Wu","Weijia Dou","Yu Wang","Shengjie Zhao","Yongcheng Huang","Xingyu Han","Anyan Huang","Hongtao Wu","Hong Wang","Yefeng Zheng","Abhijeet Kumar","Aman Kumar","Marcos V. Conde","Paula Garrido","Daniel Feijoo","Juan C. Benito","Guanglu Dong","Xin Lin","Siyuan Liu","Tianheng Zheng","Jiayu Zhong","Shouyi Wang","Xiangtai Li","Lanqing Guo","Lu Qi","Chao Ren","Shuaibo Wang","Shilong Zhang","Wanyu Zhou","Yunze Wu","Qinzhong Tan","Jieyuan Pei","Zhuoxuan Li","Jiayu Wang","Haoyu Bian","Haoran Sun","Subhajit Paul","Ni Tang","Junhao Huang","Zihan Cheng","Hongyun Zhu","Yuehan Wu","Kaixin Deng","Hang Ouyang","Tianxin Xiao","Fan Yang","Zhizun Luo","Zeyu Xiao","Zhuoyuan Li","Nguyen Pham Hoang Le","An Dinh Thien","Son T. Luu","Kiet Van Nguyen","Ronghua Xu","Xianmin Tian","Weijian Zhou","Jiacheng Zhang","Yuqian Chen","Yihang Duan","Yujie Wu","Suresh Raikwar","Arsh Garg"," Kritika","Jianhua Zheng","Xiaoshan Ma","Ruolin Zhao","Yongyu Yang","Yongsheng Liang","Guiming Huang","Qiang Li","Hongbin Zhang","Xiangyu Zheng","A. N. Rajagopalan"],"pdf_url":"https://arxiv.org/pdf/2504.12711v1.pdf","comment":"Challenge Report of CVPR NTIRE 2025; 26 pages; Methods from 32 teams"},{"id":"http://arxiv.org/abs/2411.16619v2","updated":"2025-04-17T07:28:43Z","published":"2024-11-25T17:58:43Z","title":"Human-Activity AGV Quality Assessment: A Benchmark Dataset and an\n  Objective Evaluation Metric","summary":"  AI-driven video generation techniques have made significant progress in\nrecent years. However, AI-generated videos (AGVs) involving human activities\noften exhibit substantial visual and semantic distortions, hindering the\npractical application of video generation technologies in real-world scenarios.\nTo address this challenge, we conduct a pioneering study on human activity AGV\nquality assessment, focusing on visual quality evaluation and the\nidentification of semantic distortions. First, we construct the AI-Generated\nHuman activity Video Quality Assessment (Human-AGVQA) dataset, consisting of\n6,000 AGVs derived from 15 popular text-to-video (T2V) models using 400 text\nprompts that describe diverse human activities. We conduct a subjective study\nto evaluate the human appearance quality, action continuity quality, and\noverall video quality of AGVs, and identify semantic issues of human body\nparts. Based on Human-AGVQA, we benchmark the performance of T2V models and\nanalyze their strengths and weaknesses in generating different categories of\nhuman activities. Second, we develop an objective evaluation metric, named\nAI-Generated Human activity Video Quality metric (GHVQ), to automatically\nanalyze the quality of human activity AGVs. GHVQ systematically extracts\nhuman-focused quality features, AI-generated content-aware quality features,\nand temporal continuity features, making it a comprehensive and explainable\nquality metric for human activity AGVs. The extensive experimental results show\nthat GHVQ outperforms existing quality metrics on the Human-AGVQA dataset by a\nlarge margin, demonstrating its efficacy in assessing the quality of human\nactivity AGVs. The Human-AGVQA dataset and GHVQ metric will be released\npublicly.\n","authors":["Zhichao Zhang","Wei Sun","Xinyue Li","Yunhao Li","Qihang Ge","Jun Jia","Zicheng Zhang","Zhongpeng Ji","Fengyu Sun","Shangling Jui","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2411.16619v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12709v1","updated":"2025-04-17T07:26:11Z","published":"2025-04-17T07:26:11Z","title":"Self-Supervised Pre-training with Combined Datasets for 3D Perception in\n  Autonomous Driving","summary":"  The significant achievements of pre-trained models leveraging large volumes\nof data in the field of NLP and 2D vision inspire us to explore the potential\nof extensive data pre-training for 3D perception in autonomous driving. Toward\nthis goal, this paper proposes to utilize massive unlabeled data from\nheterogeneous datasets to pre-train 3D perception models. We introduce a\nself-supervised pre-training framework that learns effective 3D representations\nfrom scratch on unlabeled data, combined with a prompt adapter based domain\nadaptation strategy to reduce dataset bias. The approach significantly improves\nmodel performance on downstream tasks such as 3D object detection, BEV\nsegmentation, 3D object tracking, and occupancy prediction, and shows steady\nperformance increase as the training data volume scales up, demonstrating the\npotential of continually benefit 3D perception models for autonomous driving.\nWe will release the source code to inspire further investigations in the\ncommunity.\n","authors":["Shumin Wang","Zhuoran Yang","Lidian Wang","Zhipeng Tang","Heng Li","Lehan Pan","Sha Zhang","Jie Peng","Jianmin Ji","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.12709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12704v1","updated":"2025-04-17T07:17:49Z","published":"2025-04-17T07:17:49Z","title":"SmartFreeEdit: Mask-Free Spatial-Aware Image Editing with Complex\n  Instruction Understanding","summary":"  Recent advancements in image editing have utilized large-scale multimodal\nmodels to enable intuitive, natural instruction-driven interactions. However,\nconventional methods still face significant challenges, particularly in spatial\nreasoning, precise region segmentation, and maintaining semantic consistency,\nespecially in complex scenes. To overcome these challenges, we introduce\nSmartFreeEdit, a novel end-to-end framework that integrates a multimodal large\nlanguage model (MLLM) with a hypergraph-enhanced inpainting architecture,\nenabling precise, mask-free image editing guided exclusively by natural\nlanguage instructions. The key innovations of SmartFreeEdit include:(1)the\nintroduction of region aware tokens and a mask embedding paradigm that enhance\nthe spatial understanding of complex scenes;(2) a reasoning segmentation\npipeline designed to optimize the generation of editing masks based on natural\nlanguage instructions;and (3) a hypergraph-augmented inpainting module that\nensures the preservation of both structural integrity and semantic coherence\nduring complex edits, overcoming the limitations of local-based image\ngeneration. Extensive experiments on the Reason-Edit benchmark demonstrate that\nSmartFreeEdit surpasses current state-of-the-art methods across multiple\nevaluation metrics, including segmentation accuracy, instruction adherence, and\nvisual quality preservation, while addressing the issue of local information\nfocus and improving global consistency in the edited image. Our project will be\navailable at https://github.com/smileformylove/SmartFreeEdit.\n","authors":["Qianqian Sun","Jixiang Luo","Dell Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2504.12704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11257v3","updated":"2025-04-17T07:13:52Z","published":"2025-04-15T14:56:21Z","title":"UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction\n  Synthesis","summary":"  Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation. In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://colmon46.github.io/i2e-bench-leaderboard/ .\n","authors":["Xinyi Liu","Xiaoyi Zhang","Ziyun Zhang","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2504.11257v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10741v2","updated":"2025-04-17T07:12:52Z","published":"2024-12-14T08:22:49Z","title":"RegMixMatch: Optimizing Mixup Utilization in Semi-Supervised Learning","summary":"  Consistency regularization and pseudo-labeling have significantly advanced\nsemi-supervised learning (SSL). Prior works have effectively employed Mixup for\nconsistency regularization in SSL. However, our findings indicate that applying\nMixup for consistency regularization may degrade SSL performance by\ncompromising the purity of artificial labels. Moreover, most pseudo-labeling\nbased methods utilize thresholding strategy to exclude low-confidence data,\naiming to mitigate confirmation bias; however, this approach limits the utility\nof unlabeled samples. To address these challenges, we propose RegMixMatch, a\nnovel framework that optimizes the use of Mixup with both high- and\nlow-confidence samples in SSL. First, we introduce semi-supervised RegMixup,\nwhich effectively addresses reduced artificial labels purity by using both\nmixed samples and clean samples for training. Second, we develop a class-aware\nMixup technique that integrates information from the top-2 predicted classes\ninto low-confidence samples and their artificial labels, reducing the\nconfirmation bias associated with these samples and enhancing their effective\nutilization. Experimental results demonstrate that RegMixMatch achieves\nstate-of-the-art performance across various SSL benchmarks.\n","authors":["Haorong Han","Jidong Yuan","Chixuan Wei","Zhongyang Yu"],"pdf_url":"https://arxiv.org/pdf/2412.10741v2.pdf","comment":"Accepted in AAAI Conference on Artificial Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2411.03728v2","updated":"2025-04-17T07:11:34Z","published":"2024-11-06T07:46:34Z","title":"Efficient Fourier Filtering Network with Contrastive Learning for\n  UAV-based Unaligned Bi-modal Salient Object Detection","summary":"  Unmanned aerial vehicle (UAV)-based bi-modal salient object detection (BSOD)\naims to segment salient objects in a scene utilizing complementary cues in\nunaligned RGB and thermal image pairs. However, the high computational expense\nof existing UAV-based BSOD models limits their applicability to real-world UAV\ndevices. To address this problem, we propose an efficient Fourier filter\nnetwork with contrastive learning that achieves both real-time and accurate\nperformance. Specifically, we first design a semantic contrastive alignment\nloss to align the two modalities at the semantic level, which facilitates\nmutual refinement in a parameter-free way. Second, inspired by the fast Fourier\ntransform that obtains global relevance in linear complexity, we propose\nsynchronized alignment fusion, which aligns and fuses bi-modal features in the\nchannel and spatial dimensions by a hierarchical filtering mechanism. Our\nproposed model, AlignSal, reduces the number of parameters by 70.0%, decreases\nthe floating point operations by 49.4%, and increases the inference speed by\n152.5% compared to the cutting-edge BSOD model (i.e., MROS). Extensive\nexperiments on the UAV RGB-T 2400 and seven bi-modal dense prediction datasets\ndemonstrate that AlignSal achieves both real-time inference speed and better\nperformance and generalizability compared to nineteen state-of-the-art models\nacross most evaluation metrics. In addition, our ablation studies further\nverify AlignSal's potential in boosting the performance of existing aligned\nBSOD models on UAV-based unaligned data. The code is available at:\nhttps://github.com/JoshuaLPF/AlignSal.\n","authors":["Pengfei Lyu","Pak-Hei Yeung","Xiaosheng Yu","Xiufei Cheng","Chengdong Wu","Jagath C. Rajapakse"],"pdf_url":"https://arxiv.org/pdf/2411.03728v2.pdf","comment":"Accepted by TGRS 2025"},{"id":"http://arxiv.org/abs/2312.08866v3","updated":"2025-04-17T07:04:50Z","published":"2023-12-14T12:41:08Z","title":"MCANet: Medical Image Segmentation with Multi-Scale Cross-Axis Attention","summary":"  Efficiently capturing multi-scale information and building long-range\ndependencies among pixels are essential for medical image segmentation because\nof the various sizes and shapes of the lesion regions or organs. In this paper,\nwe present Multi-scale Cross-axis Attention (MCA) to solve the above\nchallenging issues based on the efficient axial attention. Instead of simply\nconnecting axial attention along the horizontal and vertical directions\nsequentially, we propose to calculate dual cross attentions between two\nparallel axial attentions to capture global information better. To process the\nsignificant variations of lesion regions or organs in individual sizes and\nshapes, we also use multiple convolutions of strip-shape kernels with different\nkernel sizes in each axial attention path to improve the efficiency of the\nproposed MCA in encoding spatial information. We build the proposed MCA upon\nthe MSCAN backbone, yielding our network, termed MCANet. Our MCANet with only\n4M+ parameters performs even better than most previous works with heavy\nbackbones (e.g., Swin Transformer) on four challenging tasks, including skin\nlesion segmentation, nuclei segmentation, abdominal multi-organ segmentation,\nand polyp segmentation. Code is available at\nhttps://github.com/haoshao-nku/medical_seg.\n","authors":["Hao Shao","Quansheng Zeng","Qibin Hou","Jufeng Yang"],"pdf_url":"https://arxiv.org/pdf/2312.08866v3.pdf","comment":"accept to Machine Intelligence Research.DOI:\n  10.1007/s11633-025-1552-6"},{"id":"http://arxiv.org/abs/2411.07265v4","updated":"2025-04-17T06:59:19Z","published":"2024-11-09T13:13:49Z","title":"ViTOC: Vision Transformer and Object-aware Captioner","summary":"  This paper presents ViTOC (Vision Transformer and Object-aware Captioner), a\nnovel vision-language model for image captioning that addresses the challenges\nof accuracy and diversity in generated descriptions. Unlike conventional\napproaches, ViTOC employs a dual-path architecture based on Vision Transformer\nand object detector, effectively fusing global visual features and local object\ninformation through learnable vectors. The model introduces an innovative\nobject-aware prompting strategy that significantly enhances its capability in\nhandling long-tail data. Experiments on the standard COCO dataset demonstrate\nthat ViTOC outperforms baseline models across all evaluation metrics.\nAdditionally, we propose a reference-free evaluation method based on CLIP to\nfurther validate the model's effectiveness. By utilizing pretrained visual\nmodel parameters, ViTOC achieves efficient end-to-end training.\n","authors":["Feiyang Huang"],"pdf_url":"https://arxiv.org/pdf/2411.07265v4.pdf","comment":"The core idea is too close to what has been published in other\n  journals"},{"id":"http://arxiv.org/abs/2504.12699v1","updated":"2025-04-17T06:57:20Z","published":"2025-04-17T06:57:20Z","title":"Unsupervised Cross-Domain 3D Human Pose Estimation via\n  Pseudo-Label-Guided Global Transforms","summary":"  Existing 3D human pose estimation methods often suffer in performance, when\napplied to cross-scenario inference, due to domain shifts in characteristics\nsuch as camera viewpoint, position, posture, and body size. Among these\nfactors, camera viewpoints and locations {have been shown} to contribute\nsignificantly to the domain gap by influencing the global positions of human\nposes. To address this, we propose a novel framework that explicitly conducts\nglobal transformations between pose positions in the camera coordinate systems\nof source and target domains. We start with a Pseudo-Label Generation Module\nthat is applied to the 2D poses of the target dataset to generate pseudo-3D\nposes. Then, a Global Transformation Module leverages a human-centered\ncoordinate system as a novel bridging mechanism to seamlessly align the\npositional orientations of poses across disparate domains, ensuring consistent\nspatial referencing. To further enhance generalization, a Pose Augmentor is\nincorporated to address variations in human posture and body size. This process\nis iterative, allowing refined pseudo-labels to progressively improve guidance\nfor domain adaptation. Our method is evaluated on various cross-dataset\nbenchmarks, including Human3.6M, MPI-INF-3DHP, and 3DPW. The proposed method\noutperforms state-of-the-art approaches and even outperforms the target-trained\nmodel.\n","authors":["Jingjing Liu","Zhiyong Wang","Xinyu Fan","Amirhossein Dadashzadeh","Honghai Liu","Majid Mirmehdi"],"pdf_url":"https://arxiv.org/pdf/2504.12699v1.pdf","comment":"11 pages, 6 figures, including appendix. This work has been submitted\n  to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2504.12696v1","updated":"2025-04-17T06:49:21Z","published":"2025-04-17T06:49:21Z","title":"Collaborative Perception Datasets for Autonomous Driving: A Review","summary":"  Collaborative perception has attracted growing interest from academia and\nindustry due to its potential to enhance perception accuracy, safety, and\nrobustness in autonomous driving through multi-agent information fusion. With\nthe advancement of Vehicle-to-Everything (V2X) communication, numerous\ncollaborative perception datasets have emerged, varying in cooperation\nparadigms, sensor configurations, data sources, and application scenarios.\nHowever, the absence of systematic summarization and comparative analysis\nhinders effective resource utilization and standardization of model evaluation.\nAs the first comprehensive review focused on collaborative perception datasets,\nthis work reviews and compares existing resources from a multi-dimensional\nperspective. We categorize datasets based on cooperation paradigms, examine\ntheir data sources and scenarios, and analyze sensor modalities and supported\ntasks. A detailed comparative analysis is conducted across multiple dimensions.\nWe also outline key challenges and future directions, including dataset\nscalability, diversity, domain adaptation, standardization, privacy, and the\nintegration of large language models. To support ongoing research, we provide a\ncontinuously updated online repository of collaborative perception datasets and\nrelated literature:\nhttps://github.com/frankwnb/Collaborative-Perception-Datasets-for-Autonomous-Driving.\n","authors":["Naibang Wang","Deyong Shang","Yan Gong","Xiaoxi Hu","Ziying Song","Lei Yang","Yuhan Huang","Xiaoyu Wang","Jianli Lu"],"pdf_url":"https://arxiv.org/pdf/2504.12696v1.pdf","comment":"18pages, 7figures, journal"},{"id":"http://arxiv.org/abs/2504.12689v1","updated":"2025-04-17T06:31:26Z","published":"2025-04-17T06:31:26Z","title":"HSS-IAD: A Heterogeneous Same-Sort Industrial Anomaly Detection Dataset","summary":"  Multi-class Unsupervised Anomaly Detection algorithms (MUAD) are receiving\nincreasing attention due to their relatively low deployment costs and improved\ntraining efficiency. However, the real-world effectiveness of MUAD methods is\nquestioned due to limitations in current Industrial Anomaly Detection (IAD)\ndatasets. These datasets contain numerous classes that are unlikely to be\nproduced by the same factory and fail to cover multiple structures or\nappearances. Additionally, the defects do not reflect real-world\ncharacteristics. Therefore, we introduce the Heterogeneous Same-Sort Industrial\nAnomaly Detection (HSS-IAD) dataset, which contains 8,580 images of\nmetallic-like industrial parts and precise anomaly annotations. These parts\nexhibit variations in structure and appearance, with subtle defects that\nclosely resemble the base materials. We also provide foreground images for\nsynthetic anomaly generation. Finally, we evaluate popular IAD methods on this\ndataset under multi-class and class-separated settings, demonstrating its\npotential to bridge the gap between existing datasets and real factory\nconditions. The dataset is available at\nhttps://github.com/Qiqigeww/HSS-IAD-Dataset.\n","authors":["Qishan Wang","Shuyong Gao","Junjie Hu","Jiawen Yu","Xuan Tong","You Li","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.12689v1.pdf","comment":"Accepted to IEEE ICME 2025"},{"id":"http://arxiv.org/abs/2504.12684v1","updated":"2025-04-17T06:17:24Z","published":"2025-04-17T06:17:24Z","title":"SOPHY: Generating Simulation-Ready Objects with Physical Materials","summary":"  We present SOPHY, a generative model for 3D physics-aware shape synthesis.\nUnlike existing 3D generative models that focus solely on static geometry or 4D\nmodels that produce physics-agnostic animations, our approach jointly\nsynthesizes shape, texture, and material properties related to physics-grounded\ndynamics, making the generated objects ready for simulations and interactive,\ndynamic environments. To train our model, we introduce a dataset of 3D objects\nannotated with detailed physical material attributes, along with an annotation\npipeline for efficient material annotation. Our method enables applications\nsuch as text-driven generation of interactive, physics-aware 3D objects and\nsingle-image reconstruction of physically plausible shapes. Furthermore, our\nexperiments demonstrate that jointly modeling shape and material properties\nenhances the realism and fidelity of generated shapes, improving performance on\ngenerative geometry evaluation metrics.\n","authors":["Junyi Cao","Evangelos Kalogerakis"],"pdf_url":"https://arxiv.org/pdf/2504.12684v1.pdf","comment":"Project page: https://xjay18.github.io/SOPHY"},{"id":"http://arxiv.org/abs/2504.12680v1","updated":"2025-04-17T06:16:11Z","published":"2025-04-17T06:16:11Z","title":"Embodied-R: Collaborative Framework for Activating Embodied Spatial\n  Reasoning in Foundation Models via Reinforcement Learning","summary":"  Humans can perceive and reason about spatial relationships from sequential\nvisual observations, such as egocentric video streams. However, how pretrained\nmodels acquire such abilities, especially high-level reasoning, remains\nunclear. This paper introduces Embodied-R, a collaborative framework combining\nlarge-scale Vision-Language Models (VLMs) for perception and small-scale\nLanguage Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a\nnovel reward system considering think-answer logical consistency, the model\nachieves slow-thinking capabilities with limited computational resources. After\ntraining on only 5k embodied video samples, Embodied-R with a 3B LM matches\nstate-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on\nboth in-distribution and out-of-distribution embodied spatial reasoning tasks.\nEmbodied-R also exhibits emergent thinking patterns such as systematic analysis\nand contextual integration. We further explore research questions including\nresponse length, training on VLM, strategies for reward design, and differences\nin model generalization after SFT (Supervised Fine-Tuning) and RL training.\n","authors":["Baining Zhao","Ziyou Wang","Jianjie Fang","Chen Gao","Fanhang Man","Jinqiang Cui","Xin Wang","Xinlei Chen","Yong Li","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2504.12680v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.12679v1","updated":"2025-04-17T06:15:56Z","published":"2025-04-17T06:15:56Z","title":"TongUI: Building Generalized GUI Agents by Learning from Multimodal Web\n  Tutorials","summary":"  Building Graphical User Interface (GUI) agents is a promising research\ndirection, which simulates human interaction with computers or mobile phones to\nperform diverse GUI tasks. However, a major challenge in developing generalized\nGUI agents is the lack of sufficient trajectory data across various operating\nsystems and applications, mainly due to the high cost of manual annotations. In\nthis paper, we propose the TongUI framework that builds generalized GUI agents\nby learning from rich multimodal web tutorials. Concretely, we crawl and\nprocess online GUI tutorials (such as videos and articles) into GUI agent\ntrajectory data, through which we produce the GUI-Net dataset containing 143K\ntrajectory data across five operating systems and more than 200 applications.\nWe develop the TongUI agent by fine-tuning Qwen2.5-VL-3B/7B models on GUI-Net,\nwhich show remarkable performance improvements on commonly used grounding and\nnavigation benchmarks, outperforming baseline agents about 10\\% on multiple\nbenchmarks, showing the effectiveness of the GUI-Net dataset and underscoring\nthe significance of our TongUI framework. We will fully open-source the code,\nthe GUI-Net dataset, and the trained models soon.\n","authors":["Bofei Zhang","Zirui Shang","Zhi Gao","Wang Zhang","Rui Xie","Xiaojian Ma","Tao Yuan","Xinxiao Wu","Song-Chun Zhu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2504.12679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11509v2","updated":"2025-04-17T06:10:27Z","published":"2025-04-15T09:53:02Z","title":"PATFinger: Prompt-Adapted Transferable Fingerprinting against\n  Unauthorized Multimodal Dataset Usage","summary":"  The multimodal datasets can be leveraged to pre-train large-scale\nvision-language models by providing cross-modal semantics. Current endeavors\nfor determining the usage of datasets mainly focus on single-modal dataset\nownership verification through intrusive methods and non-intrusive techniques,\nwhile cross-modal approaches remain under-explored. Intrusive methods can adapt\nto multimodal datasets but degrade model accuracy, while non-intrusive methods\nrely on label-driven decision boundaries that fail to guarantee stable\nbehaviors for verification. To address these issues, we propose a novel\nprompt-adapted transferable fingerprinting scheme from a training-free\nperspective, called PATFinger, which incorporates the global optimal\nperturbation (GOP) and the adaptive prompts to capture dataset-specific\ndistribution characteristics. Our scheme utilizes inherent dataset attributes\nas fingerprints instead of compelling the model to learn triggers. The GOP is\nderived from the sample distribution to maximize embedding drifts between\ndifferent modalities. Subsequently, our PATFinger re-aligns the adaptive prompt\nwith GOP samples to capture the cross-modal interactions on the carefully\ncrafted surrogate model. This allows the dataset owner to check the usage of\ndatasets by observing specific prediction behaviors linked to the PATFinger\nduring retrieval queries. Extensive experiments demonstrate the effectiveness\nof our scheme against unauthorized multimodal dataset usage on various\ncross-modal retrieval architectures by 30% over state-of-the-art baselines.\n","authors":["Wenyi Zhang","Ju Jia","Xiaojun Jia","Yihao Huang","Xinfeng Li","Cong Wu","Lina Wang"],"pdf_url":"https://arxiv.org/pdf/2504.11509v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12676v1","updated":"2025-04-17T06:07:17Z","published":"2025-04-17T06:07:17Z","title":"Accurate Tracking of Arabidopsis Root Cortex Cell Nuclei in 3D\n  Time-Lapse Microscopy Images Based on Genetic Algorithm","summary":"  Arabidopsis is a widely used model plant to gain basic knowledge on plant\nphysiology and development. Live imaging is an important technique to visualize\nand quantify elemental processes in plant development. To uncover novel\ntheories underlying plant growth and cell division, accurate cell tracking on\nlive imaging is of utmost importance. The commonly used cell tracking software,\nTrackMate, adopts tracking-by-detection fashion, which applies Laplacian of\nGaussian (LoG) for blob detection, and Linear Assignment Problem (LAP) tracker\nfor tracking. However, they do not perform sufficiently when cells are densely\narranged. To alleviate the problems mentioned above, we propose an accurate\ntracking method based on Genetic algorithm (GA) using knowledge of Arabidopsis\nroot cellular patterns and spatial relationship among volumes. Our method can\nbe described as a coarse-to-fine method, in which we first conducted relatively\neasy line-level tracking of cell nuclei, then performed complicated nuclear\ntracking based on known linear arrangement of cell files and their spatial\nrelationship between nuclei. Our method has been evaluated on a long-time live\nimaging dataset of Arabidopsis root tips, and with minor manual rectification,\nit accurately tracks nuclei. To the best of our knowledge, this research\nrepresents the first successful attempt to address a long-standing problem in\nthe field of time-lapse microscopy in the root meristem by proposing an\naccurate tracking method for Arabidopsis root nuclei.\n","authors":["Yu Song","Tatsuaki Goh","Yinhao Li","Jiahua Dong","Shunsuke Miyashima","Yutaro Iwamoto","Yohei Kondo","Keiji Nakajima","Yen-wei Chen"],"pdf_url":"https://arxiv.org/pdf/2504.12676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12667v1","updated":"2025-04-17T05:52:35Z","published":"2025-04-17T05:52:35Z","title":"Two Tasks, One Goal: Uniting Motion and Planning for Excellent End To\n  End Autonomous Driving Performance","summary":"  End-to-end autonomous driving has made impressive progress in recent years.\nFormer end-to-end autonomous driving approaches often decouple planning and\nmotion tasks, treating them as separate modules. This separation overlooks the\npotential benefits that planning can gain from learning out-of-distribution\ndata encountered in motion tasks. However, unifying these tasks poses\nsignificant challenges, such as constructing shared contextual representations\nand handling the unobservability of other vehicles' states. To address these\nchallenges, we propose TTOG, a novel two-stage trajectory generation framework.\nIn the first stage, a diverse set of trajectory candidates is generated, while\nthe second stage focuses on refining these candidates through vehicle state\ninformation. To mitigate the issue of unavailable surrounding vehicle states,\nTTOG employs a self-vehicle data-trained state estimator, subsequently extended\nto other vehicles. Furthermore, we introduce ECSA (equivariant context-sharing\nscene adapter) to enhance the generalization of scene representations across\ndifferent agents. Experimental results demonstrate that TTOG achieves\nstate-of-the-art performance across both planning and motion tasks. Notably, on\nthe challenging open-loop nuScenes dataset, TTOG reduces the L2 distance by\n36.06\\%. Furthermore, on the closed-loop Bench2Drive dataset, our approach\nachieves a 22\\% improvement in the driving score (DS), significantly\noutperforming existing baselines.\n","authors":["Lin Liu","Ziying Song","Hongyu Pan","Lei Yang","Caiyan Jia"],"pdf_url":"https://arxiv.org/pdf/2504.12667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12661v1","updated":"2025-04-17T05:46:41Z","published":"2025-04-17T05:46:41Z","title":"VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven\n  Prompt Optimization","summary":"  Aligning Vision-Language Models (VLMs) with safety standards is essential to\nmitigate risks arising from their multimodal complexity, where integrating\nvision and language unveils subtle threats beyond the reach of conventional\nsafeguards. Inspired by the insight that reasoning across modalities is key to\npreempting intricate vulnerabilities, we propose a novel direction for VLM\nsafety: multimodal reasoning-driven prompt rewriting. To this end, we introduce\nVLMGuard-R1, a proactive framework that refines user inputs through a\nreasoning-guided rewriter, dynamically interpreting text-image interactions to\ndeliver refined prompts that bolster safety across diverse VLM architectures\nwithout altering their core parameters. To achieve this, we devise a\nthree-stage reasoning pipeline to synthesize a dataset that trains the rewriter\nto infer subtle threats, enabling tailored, actionable responses over generic\nrefusals. Extensive experiments across three benchmarks with five VLMs reveal\nthat VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1\nachieves a remarkable 43.59\\% increase in average safety across five models on\nthe SIUO benchmark.\n","authors":["Menglan Chen","Xianghe Pang","Jingjing Dong","WenHao Wang","Yaxin Du","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2504.12661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04991v2","updated":"2025-04-17T05:45:55Z","published":"2025-02-07T15:11:31Z","title":"C2GM: Cascading conditional generative cartography framework for\n  multi-scale tile map generation with geographic feature constraints","summary":"  Multi-scale maps are essential representations of surveying and cartographic\nresults, serving as fundamental components of geographic services. Current\nimage generation networks can quickly produce map tiles from remote-sensing\nimages. However, generative models designed for natural images often focus on\ntexture features, neglecting the unique characteristics of remote-sensing\nfeatures and the scale attributes of tile maps. This limitation in generative\nmodels impairs the accurate representation of geographic information, and the\nquality of tile map generation still needs improvement. Diffusion models have\ndemonstrated remarkable success in various image generation tasks, highlighting\ntheir potential to address this challenge. This paper presents C2GM, a novel\nframework for generating multi-scale tile maps through conditional guided\ndiffusion and multi-scale cascade generation. Specifically, we implement a\nconditional feature fusion encoder to extract object priors from remote sensing\nimages and cascade reference double branch input, ensuring an accurate\nrepresentation of complex features. Low-level generated tiles act as\nconstraints for high-level map generation, enhancing visual continuity.\nMoreover, we incorporate map scale modality information using CLIP to simulate\nthe relationship between map scale and cartographic generalization in tile\nmaps. Extensive experimental evaluations demonstrate that C2GM consistently\nachieves the state-of-the-art (SOTA) performance on all metrics, facilitating\nthe rapid and effective generation of multi-scale large-format maps for\nemergency response and remote mapping applications.\n","authors":["Chenxing Sun","Yongyang Xu","Xuwei Xu","Xixi Fan","Jing Bai","Xiechun Lu","Zhanlong Chen"],"pdf_url":"https://arxiv.org/pdf/2502.04991v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12652v1","updated":"2025-04-17T05:23:07Z","published":"2025-04-17T05:23:07Z","title":"AdaptoVision: A Multi-Resolution Image Recognition Model for Robust and\n  Scalable Classification","summary":"  This paper introduces AdaptoVision, a novel convolutional neural network\n(CNN) architecture designed to efficiently balance computational complexity and\nclassification accuracy. By leveraging enhanced residual units, depth-wise\nseparable convolutions, and hierarchical skip connections, AdaptoVision\nsignificantly reduces parameter count and computational requirements while\npreserving competitive performance across various benchmark and medical image\ndatasets. Extensive experimentation demonstrates that AdaptoVision achieves\nstate-of-the-art on BreakHis dataset and comparable accuracy levels, notably\n95.3\\% on CIFAR-10 and 85.77\\% on CIFAR-100, without relying on any pretrained\nweights. The model's streamlined architecture and strategic simplifications\npromote effective feature extraction and robust generalization, making it\nparticularly suitable for deployment in real-time and resource-constrained\nenvironments.\n","authors":["Md. Sanaullah Chowdhury Lameya Sabrin"],"pdf_url":"https://arxiv.org/pdf/2504.12652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12644v1","updated":"2025-04-17T05:08:08Z","published":"2025-04-17T05:08:08Z","title":"Quantum Computing Supported Adversarial Attack-Resilient Autonomous\n  Vehicle Perception Module for Traffic Sign Classification","summary":"  Deep learning (DL)-based image classification models are essential for\nautonomous vehicle (AV) perception modules since incorrect categorization might\nhave severe repercussions. Adversarial attacks are widely studied cyberattacks\nthat can lead DL models to predict inaccurate output, such as incorrectly\nclassified traffic signs by the perception module of an autonomous vehicle. In\nthis study, we create and compare hybrid classical-quantum deep learning\n(HCQ-DL) models with classical deep learning (C-DL) models to demonstrate\nrobustness against adversarial attacks for perception modules. Before feeding\nthem into the quantum system, we used transfer learning models, alexnet and\nvgg-16, as feature extractors. We tested over 1000 quantum circuits in our\nHCQ-DL models for projected gradient descent (PGD), fast gradient sign attack\n(FGSA), and gradient attack (GA), which are three well-known untargeted\nadversarial approaches. We evaluated the performance of all models during\nadversarial attacks and no-attack scenarios. Our HCQ-DL models maintain\naccuracy above 95\\% during a no-attack scenario and above 91\\% for GA and FGSA\nattacks, which is higher than C-DL models. During the PGD attack, our\nalexnet-based HCQ-DL model maintained an accuracy of 85\\% compared to C-DL\nmodels that achieved accuracies below 21\\%. Our results highlight that the\nHCQ-DL models provide improved accuracy for traffic sign classification under\nadversarial settings compared to their classical counterparts.\n","authors":["Reek Majumder","Mashrur Chowdhury","Sakib Mahmud Khan","Zadid Khan","Fahim Ahmad","Frank Ngeni","Gurcan Comert","Judith Mwakalonge","Dimitra Michalaka"],"pdf_url":"https://arxiv.org/pdf/2504.12644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12643v1","updated":"2025-04-17T05:05:31Z","published":"2025-04-17T05:05:31Z","title":"RoPETR: Improving Temporal Camera-Only 3D Detection by Integrating\n  Enhanced Rotary Position Embedding","summary":"  This technical report introduces a targeted improvement to the StreamPETR\nframework, specifically aimed at enhancing velocity estimation, a critical\nfactor influencing the overall NuScenes Detection Score. While StreamPETR\nexhibits strong 3D bounding box detection performance as reflected by its high\nmean Average Precision our analysis identified velocity estimation as a\nsubstantial bottleneck when evaluated on the NuScenes dataset. To overcome this\nlimitation, we propose a customized positional embedding strategy tailored to\nenhance temporal modeling capabilities. Experimental evaluations conducted on\nthe NuScenes test set demonstrate that our improved approach achieves a\nstate-of-the-art NDS of 70.86% using the ViT-L backbone, setting a new\nbenchmark for camera-only 3D object detection.\n","authors":["Hang Ji","Tao Ni","Xufeng Huang","Tao Luo","Xin Zhan","Junbo Chen"],"pdf_url":"https://arxiv.org/pdf/2504.12643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10976v2","updated":"2025-04-17T04:50:16Z","published":"2025-04-15T08:37:24Z","title":"Adaptive Decision Boundary for Few-Shot Class-Incremental Learning","summary":"  Few-Shot Class-Incremental Learning (FSCIL) aims to continuously learn new\nclasses from a limited set of training samples without forgetting knowledge of\npreviously learned classes. Conventional FSCIL methods typically build a robust\nfeature extractor during the base training session with abundant training\nsamples and subsequently freeze this extractor, only fine-tuning the classifier\nin subsequent incremental phases. However, current strategies primarily focus\non preventing catastrophic forgetting, considering only the relationship\nbetween novel and base classes, without paying attention to the specific\ndecision spaces of each class. To address this challenge, we propose a\nplug-and-play Adaptive Decision Boundary Strategy (ADBS), which is compatible\nwith most FSCIL methods. Specifically, we assign a specific decision boundary\nto each class and adaptively adjust these boundaries during training to\noptimally refine the decision spaces for the classes in each session.\nFurthermore, to amplify the distinctiveness between classes, we employ a novel\ninter-class constraint loss that optimizes the decision boundaries and\nprototypes for each class. Extensive experiments on three benchmarks, namely\nCIFAR100, miniImageNet, and CUB200, demonstrate that incorporating our ADBS\nmethod with existing FSCIL techniques significantly improves performance,\nachieving overall state-of-the-art results.\n","authors":["Linhao Li","Yongzhang Tan","Siyuan Yang","Hao Cheng","Yongfeng Dong","Liang Yang"],"pdf_url":"https://arxiv.org/pdf/2504.10976v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11733v2","updated":"2025-04-17T04:03:27Z","published":"2025-04-16T03:20:28Z","title":"DVLTA-VQA: Decoupled Vision-Language Modeling with Text-Guided\n  Adaptation for Blind Video Quality Assessment","summary":"  Inspired by the dual-stream theory of the human visual system (HVS) - where\nthe ventral stream is responsible for object recognition and detail analysis,\nwhile the dorsal stream focuses on spatial relationships and motion perception\n- an increasing number of video quality assessment (VQA) works built upon this\nframework are proposed. Recent advancements in large multi-modal models,\nnotably Contrastive Language-Image Pretraining (CLIP), have motivated\nresearchers to incorporate CLIP into dual-stream-based VQA methods. This\nintegration aims to harness the model's superior semantic understanding\ncapabilities to replicate the object recognition and detail analysis in ventral\nstream, as well as spatial relationship analysis in dorsal stream. However,\nCLIP is originally designed for images and lacks the ability to capture\ntemporal and motion information inherent in videos.To address the limitation,\nthis paper propose a Decoupled Vision-Language Modeling with Text-Guided\nAdaptation for Blind Video Quality Assessment (DVLTA-VQA), which decouples\nCLIP's visual and textual components, and integrates them into different stages\nof the NR-VQA pipeline. Specifically, a Video-Based Temporal CLIP module is\nproposed to explicitly model temporal dynamics and enhance motion perception,\naligning with the dorsal stream. Additionally, a Temporal Context Module is\ndeveloped to refine inter-frame dependencies, further improving motion\nmodeling. On the ventral stream side, a Basic Visual Feature Extraction Module\nis employed to strengthen detail analysis. Finally, a text-guided adaptive\nfusion strategy is proposed to enable dynamic weighting of features,\nfacilitating more effective integration of spatial and temporal information.\n","authors":["Li Yu","Situo Wang","Wei Zhou","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2504.11733v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12626v1","updated":"2025-04-17T04:02:31Z","published":"2025-04-17T04:02:31Z","title":"Packing Input Frame Context in Next-Frame Prediction Models for Video\n  Generation","summary":"  We present a neural network structure, FramePack, to train next-frame (or\nnext-frame-section) prediction models for video generation. The FramePack\ncompresses input frames to make the transformer context length a fixed number\nregardless of the video length. As a result, we are able to process a large\nnumber of frames using video diffusion with computation bottleneck similar to\nimage diffusion. This also makes the training video batch sizes significantly\nhigher (batch sizes become comparable to image diffusion training). We also\npropose an anti-drifting sampling method that generates frames in inverted\ntemporal order with early-established endpoints to avoid exposure bias (error\naccumulation over iterations). Finally, we show that existing video diffusion\nmodels can be finetuned with FramePack, and their visual quality may be\nimproved because the next-frame prediction supports more balanced diffusion\nschedulers with less extreme flow shift timesteps.\n","authors":["Lvmin Zhang","Maneesh Agrawala"],"pdf_url":"https://arxiv.org/pdf/2504.12626v1.pdf","comment":"https://github.com/lllyasviel/FramePack"},{"id":"http://arxiv.org/abs/2408.15991v3","updated":"2025-04-17T03:58:19Z","published":"2024-08-28T17:58:17Z","title":"Distribution Backtracking Builds A Faster Convergence Trajectory for\n  Diffusion Distillation","summary":"  Accelerating the sampling speed of diffusion models remains a significant\nchallenge. Recent score distillation methods distill a heavy teacher model into\na student generator to achieve one-step generation, which is optimized by\ncalculating the difference between the two score functions on the samples\ngenerated by the student model. However, there is a score mismatch issue in the\nearly stage of the distillation process, because existing methods mainly focus\non using the endpoint of pre-trained diffusion models as teacher models,\noverlooking the importance of the convergence trajectory between the student\ngenerator and the teacher model. To address this issue, we extend the score\ndistillation process by introducing the entire convergence trajectory of\nteacher models and propose Distribution Backtracking Distillation (DisBack).\nDisBask is composed of two stages: Degradation Recording and Distribution\nBacktracking. Degradation Recording is designed to obtain the convergence\ntrajectory of the teacher model, which records the degradation path from the\ntrained teacher model to the untrained initial student generator. The\ndegradation path implicitly represents the teacher model's intermediate\ndistributions, and its reverse can be viewed as the convergence trajectory from\nthe student generator to the teacher model. Then Distribution Backtracking\ntrains a student generator to backtrack the intermediate distributions along\nthe path to approximate the convergence trajectory of teacher models. Extensive\nexperiments show that DisBack achieves faster and better convergence than the\nexisting distillation method and accomplishes comparable generation\nperformance, with FID score of 1.38 on ImageNet 64x64 dataset. Notably, DisBack\nis easy to implement and can be generalized to existing distillation methods to\nboost performance. Our code is publicly available on\nhttps://github.com/SYZhang0805/DisBack.\n","authors":["Shengyuan Zhang","Ling Yang","Zejian Li","An Zhao","Chenye Meng","Changyuan Yang","Guang Yang","Zhiyuan Yang","Lingyun Sun"],"pdf_url":"https://arxiv.org/pdf/2408.15991v3.pdf","comment":"Our code is publicly available on\n  https://github.com/SYZhang0805/DisBack"},{"id":"http://arxiv.org/abs/2504.12619v1","updated":"2025-04-17T03:47:43Z","published":"2025-04-17T03:47:43Z","title":"SAM-Based Building Change Detection with Distribution-Aware Fourier\n  Adaptation and Edge-Constrained Warping","summary":"  Building change detection remains challenging for urban development, disaster\nassessment, and military reconnaissance. While foundation models like Segment\nAnything Model (SAM) show strong segmentation capabilities, SAM is limited in\nthe task of building change detection due to domain gap issues. Existing\nadapter-based fine-tuning approaches face challenges with imbalanced building\ndistribution, resulting in poor detection of subtle changes and inaccurate edge\nextraction. Additionally, bi-temporal misalignment in change detection,\ntypically addressed by optical flow, remains vulnerable to background noises.\nThis affects the detection of building changes and compromises both detection\naccuracy and edge recognition. To tackle these challenges, we propose a new\nSAM-Based Network with Distribution-Aware Fourier Adaptation and\nEdge-Constrained Warping (FAEWNet) for building change detection. FAEWNet\nutilizes the SAM encoder to extract rich visual features from remote sensing\nimages. To guide SAM in focusing on specific ground objects in remote sensing\nscenes, we propose a Distribution-Aware Fourier Aggregated Adapter to aggregate\ntask-oriented changed information. This adapter not only effectively addresses\nthe domain gap issue, but also pays attention to the distribution of changed\nbuildings. Furthermore, to mitigate noise interference and misalignment in\nheight offset estimation, we design a novel flow module that refines building\nedge extraction and enhances the perception of changed buildings. Our\nstate-of-the-art results on the LEVIR-CD, S2Looking and WHU-CD datasets\nhighlight the effectiveness of FAEWNet. The code is available at\nhttps://github.com/SUPERMAN123000/FAEWNet.\n","authors":["Yun-Cheng Li","Sen Lei","Yi-Tao Zhao","Heng-Chao Li","Jun Li","Antonio Plaza"],"pdf_url":"https://arxiv.org/pdf/2504.12619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04649v2","updated":"2025-04-17T03:47:25Z","published":"2025-03-06T17:35:37Z","title":"Transferable Foundation Models for Geometric Tasks on Point Cloud\n  Representations: Geometric Neural Operators","summary":"  We introduce methods for obtaining pretrained Geometric Neural Operators\n(GNPs) that can serve as basal foundation models for use in obtaining geometric\nfeatures. These can be used within data processing pipelines for machine\nlearning tasks and numerical methods. We show how our GNPs can be trained to\nlearn robust latent representations for the differential geometry of\npoint-clouds to provide estimates of metric, curvature, and other shape-related\nfeatures. We demonstrate how our pre-trained GNPs can be used (i) to estimate\nthe geometric properties of surfaces of arbitrary shape and topologies with\nrobustness in the presence of noise, (ii) to approximate solutions of geometric\npartial differential equations (PDEs) on manifolds, and (iii) to solve\nequations for shape deformations such as curvature driven flows. We release\ncodes and weights for using GNPs in the package geo_neural_op. This allows for\nincorporating our pre-trained GNPs as components for reuse within existing and\nnew data processing pipelines. The GNPs also can be used as part of numerical\nsolvers involving geometry or as part of methods for performing inference and\nother geometric tasks.\n","authors":["Blaine Quackenbush","Paul J. Atzberger"],"pdf_url":"https://arxiv.org/pdf/2503.04649v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05584v2","updated":"2025-04-17T03:41:57Z","published":"2024-12-07T08:38:29Z","title":"UMSPU: Universal Multi-Size Phase Unwrapping via Mutual\n  Self-Distillation and Adaptive Boosting Ensemble Segmenters","summary":"  Spatial phase unwrapping is a key technique for extracting phase information\nto obtain 3D morphology and other features. Modern industrial measurement\nscenarios demand high precision, large image sizes, and high speed. However,\nconventional methods struggle with noise resistance and processing speed.\nCurrent deep learning methods are limited by the receptive field size and\nsparse semantic information, making them ineffective for large size images. To\naddress this issue, we propose a mutual self-distillation (MSD) mechanism and\nadaptive boosting ensemble segmenters to construct a universal multi-size phase\nunwrapping network (UMSPU). MSD performs hierarchical attention refinement and\nachieves cross-layer collaborative learning through bidirectional distillation,\nensuring fine-grained semantic representation across image sizes. The adaptive\nboosting ensemble segmenters combine weak segmenters with different receptive\nfields into a strong one, ensuring stable segmentation across spatial\nfrequencies. Experimental results show that UMSPU overcomes image size\nlimitations, achieving high precision across image sizes ranging from 256*256\nto 2048*2048 (an 8 times increase). It also outperforms existing methods in\nspeed, robustness, and generalization. Its practicality is further validated in\nstructured light imaging and InSAR. We believe that UMSPU offers a universal\nsolution for phase unwrapping, with broad potential for industrial\napplications.\n","authors":["Lintong Du","Huazhen Liu","Yijia Zhang","ShuXin Liu","Yuan Qu","Zenghui Zhang","Jiamiao Yang"],"pdf_url":"https://arxiv.org/pdf/2412.05584v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.23752v2","updated":"2025-04-17T03:17:42Z","published":"2025-03-31T06:03:03Z","title":"StrokeFusion: Vector Sketch Generation via Joint Stroke-UDF Encoding and\n  Latent Sequence Diffusion","summary":"  In the field of sketch generation, raster-format trained models often produce\nnon-stroke artifacts, while vector-format trained models typically lack a\nholistic understanding of sketches, leading to compromised recognizability.\nMoreover, existing methods struggle to extract common features from similar\nelements (e.g., eyes of animals) appearing at varying positions across\nsketches. To address these challenges, we propose StrokeFusion, a two-stage\nframework for vector sketch generation. It contains a dual-modal sketch feature\nlearning network that maps strokes into a high-quality latent space. This\nnetwork decomposes sketches into normalized strokes and jointly encodes stroke\nsequences with Unsigned Distance Function (UDF) maps, representing sketches as\nsets of stroke feature vectors. Building upon this representation, our\nframework exploits a stroke-level latent diffusion model that simultaneously\nadjusts stroke position, scale, and trajectory during generation. This enables\nhigh-fidelity sketch generation while supporting stroke interpolation editing.\nExtensive experiments on the QuickDraw dataset demonstrate that our framework\noutperforms state-of-the-art techniques, validating its effectiveness in\npreserving structural integrity and semantic features. Code and models will be\nmade publicly available upon publication.\n","authors":["Jin Zhou","Yi Zhou","Pengfei Xu","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2503.23752v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01506v2","updated":"2025-04-17T03:14:38Z","published":"2024-12-02T13:58:38Z","title":"Structured 3D Latents for Scalable and Versatile 3D Generation","summary":"  We introduce a novel 3D generation method for versatile and high-quality 3D\nasset creation. The cornerstone is a unified Structured LATent (SLAT)\nrepresentation which allows decoding to different output formats, such as\nRadiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a\nsparsely-populated 3D grid with dense multiview visual features extracted from\na powerful vision foundation model, comprehensively capturing both structural\n(geometry) and textural (appearance) information while maintaining flexibility\nduring decoding. We employ rectified flow transformers tailored for SLAT as our\n3D generation models and train models with up to 2 billion parameters on a\nlarge 3D asset dataset of 500K diverse objects. Our model generates\nhigh-quality results with text or image conditions, significantly surpassing\nexisting methods, including recent ones at similar scales. We showcase flexible\noutput format selection and local 3D editing capabilities which were not\noffered by previous models. Code, model, and data will be released.\n","authors":["Jianfeng Xiang","Zelong Lv","Sicheng Xu","Yu Deng","Ruicheng Wang","Bowen Zhang","Dong Chen","Xin Tong","Jiaolong Yang"],"pdf_url":"https://arxiv.org/pdf/2412.01506v2.pdf","comment":"Project Page: https://github.com/Microsoft/TRELLIS"},{"id":"http://arxiv.org/abs/2504.12606v1","updated":"2025-04-17T03:09:22Z","published":"2025-04-17T03:09:22Z","title":"Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution for\n  Robust Scene Graph Generation","summary":"  In this paper, we introduce a novel method named Robo-SGG, i.e.,\nLayout-Oriented Normalization and Restitution for Robust Scene Graph\nGeneration. Compared to the existing SGG setting, the robust scene graph\ngeneration aims to perform inference on a diverse range of corrupted images,\nwith the core challenge being the domain shift between the clean and corrupted\nimages. Existing SGG methods suffer from degraded performance due to\ncompromised visual features e.g., corruption interference or occlusions. To\nobtain robust visual features, we exploit the layout information, which is\ndomain-invariant, to enhance the efficacy of existing SGG methods on corrupted\nimages. Specifically, we employ Instance Normalization(IN) to filter out the\ndomain-specific feature and recover the unchangeable structural features, i.e.,\nthe positional and semantic relationships among objects by the proposed\nLayout-Oriented Restitution. Additionally, we propose a Layout-Embedded Encoder\n(LEE) that augments the existing object and predicate encoders within the SGG\nframework, enriching the robust positional and semantic features of objects and\npredicates. Note that our proposed Robo-SGG module is designed as a\nplug-and-play component, which can be easily integrated into any baseline SGG\nmodel. Extensive experiments demonstrate that by integrating the\nstate-of-the-art method into our proposed Robo-SGG, we achieve relative\nimprovements of 5.6%, 8.0%, and 6.5% in mR@50 for PredCls, SGCls, and SGDet\ntasks on the VG-C dataset, respectively, and achieve new state-of-the-art\nperformance in corruption scene graph generation benchmark (VG-C and GQA-C). We\nwill release our source code and model.\n","authors":["Changsheng Lv","Mengshi Qi","Zijian Fu","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2504.12606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12605v1","updated":"2025-04-17T03:08:27Z","published":"2025-04-17T03:08:27Z","title":"AdaQual-Diff: Diffusion-Based Image Restoration via Adaptive Quality\n  Prompting","summary":"  Restoring images afflicted by complex real-world degradations remains\nchallenging, as conventional methods often fail to adapt to the unique mixture\nand severity of artifacts present. This stems from a reliance on indirect cues\nwhich poorly capture the true perceptual quality deficit. To address this\nfundamental limitation, we introduce AdaQual-Diff, a diffusion-based framework\nthat integrates perceptual quality assessment directly into the generative\nrestoration process. Our approach establishes a mathematical relationship\nbetween regional quality scores from DeQAScore and optimal guidance complexity,\nimplemented through an Adaptive Quality Prompting mechanism. This mechanism\nsystematically modulates prompt structure according to measured degradation\nseverity: regions with lower perceptual quality receive computationally\nintensive, structurally complex prompts with precise restoration directives,\nwhile higher quality regions receive minimal prompts focused on preservation\nrather than intervention. The technical core of our method lies in the dynamic\nallocation of computational resources proportional to degradation severity,\ncreating a spatially-varying guidance field that directs the diffusion process\nwith mathematical precision. By combining this quality-guided approach with\ncontent-specific conditioning, our framework achieves fine-grained control over\nregional restoration intensity without requiring additional parameters or\ninference iterations. Experimental results demonstrate that AdaQual-Diff\nachieves visually superior restorations across diverse synthetic and real-world\ndatasets.\n","authors":["Xin Su","Chen Wu","Yu Zhang","Chen Lyu","Zhuoran Zheng"],"pdf_url":"https://arxiv.org/pdf/2504.12605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.07246v5","updated":"2025-04-17T03:01:41Z","published":"2024-08-14T01:16:40Z","title":"ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area","summary":"  Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B.\n","authors":["Junxian Li","Di Zhang","Xunzhi Wang","Zeying Hao","Jingdi Lei","Qian Tan","Cai Zhou","Wei Liu","Yaotian Yang","Xinrui Xiong","Weiyun Wang","Zhe Chen","Wenhai Wang","Wei Li","Shufei Zhang","Mao Su","Wanli Ouyang","Yuqiang Li","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.07246v5.pdf","comment":"11 pages, updated version"},{"id":"http://arxiv.org/abs/2504.12599v1","updated":"2025-04-17T02:50:52Z","published":"2025-04-17T02:50:52Z","title":"3DResT: A Strong Baseline for Semi-Supervised 3D Referring Expression\n  Segmentation","summary":"  3D Referring Expression Segmentation (3D-RES) typically requires extensive\ninstance-level annotations, which are time-consuming and costly.\nSemi-supervised learning (SSL) mitigates this by using limited labeled data\nalongside abundant unlabeled data, improving performance while reducing\nannotation costs. SSL uses a teacher-student paradigm where teacher generates\nhigh-confidence-filtered pseudo-labels to guide student. However, in the\ncontext of 3D-RES, where each label corresponds to a single mask and labeled\ndata is scarce, existing SSL methods treat high-quality pseudo-labels merely as\nauxiliary supervision, which limits the model's learning potential. The\nreliance on high-confidence thresholds for filtering often results in\npotentially valuable pseudo-labels being discarded, restricting the model's\nability to leverage the abundant unlabeled data. Therefore, we identify two\ncritical challenges in semi-supervised 3D-RES, namely, inefficient utilization\nof high-quality pseudo-labels and wastage of useful information from\nlow-quality pseudo-labels. In this paper, we introduce the first\nsemi-supervised learning framework for 3D-RES, presenting a robust baseline\nmethod named 3DResT. To address these challenges, we propose two novel designs\ncalled Teacher-Student Consistency-Based Sampling (TSCS) and Quality-Driven\nDynamic Weighting (QDW). TSCS aids in the selection of high-quality\npseudo-labels, integrating them into the labeled dataset to strengthen the\nlabeled supervision signals. QDW preserves low-quality pseudo-labels by\ndynamically assigning them lower weights, allowing for the effective extraction\nof useful information rather than discarding them. Extensive experiments\nconducted on the widely used benchmark demonstrate the effectiveness of our\nmethod. Notably, with only 1% labeled data, 3DResT achieves an mIoU improvement\nof 8.34 points compared to the fully supervised method.\n","authors":["Wenxin Chen","Mengxue Qu","Weitai Kang","Yan Yan","Yao Zhao","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2504.12599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17510v4","updated":"2025-04-17T02:46:59Z","published":"2023-11-29T10:35:00Z","title":"StructRe: Rewriting for Structured Shape Modeling","summary":"  Man-made 3D shapes are naturally organized in parts and hierarchies; such\nstructures provide important constraints for shape reconstruction and\ngeneration. Modeling shape structures is difficult, because there can be\nmultiple hierarchies for a given shape, causing ambiguity, and across different\ncategories the shape structures are correlated with semantics, limiting\ngeneralization. We present StructRe, a structure rewriting system, as a novel\napproach to structured shape modeling. Given a 3D object represented by points\nand components, StructRe can rewrite it upward into more concise structures, or\ndownward into more detailed structures; by iterating the rewriting process,\nhierarchies are obtained. Such a localized rewriting process enables\nprobabilistic modeling of ambiguous structures and robust generalization across\nobject categories. We train StructRe on PartNet data and show its\ngeneralization to cross-category and multiple object hierarchies, and test its\nextension to ShapeNet. We also demonstrate the benefits of probabilistic and\ngeneralizable structure modeling for shape reconstruction, generation and\nediting tasks.\n","authors":["Jiepeng Wang","Hao Pan","Yang Liu","Xin Tong","Taku Komura","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2311.17510v4.pdf","comment":"Our project page: https://jiepengwang.github.io/StructRe/"},{"id":"http://arxiv.org/abs/2411.01639v3","updated":"2025-04-17T02:45:08Z","published":"2024-11-03T17:32:00Z","title":"Know Where You're Uncertain When Planning with Multimodal Foundation\n  Models: A Formal Framework","summary":"  Multimodal foundation models offer a promising framework for robotic\nperception and planning by processing sensory inputs to generate actionable\nplans. However, addressing uncertainty in both perception (sensory\ninterpretation) and decision-making (plan generation) remains a critical\nchallenge for ensuring task reliability. We present a comprehensive framework\nto disentangle, quantify, and mitigate these two forms of uncertainty. We first\nintroduce a framework for uncertainty disentanglement, isolating perception\nuncertainty arising from limitations in visual understanding and decision\nuncertainty relating to the robustness of generated plans.\n  To quantify each type of uncertainty, we propose methods tailored to the\nunique properties of perception and decision-making: we use conformal\nprediction to calibrate perception uncertainty and introduce\nFormal-Methods-Driven Prediction (FMDP) to quantify decision uncertainty,\nleveraging formal verification techniques for theoretical guarantees. Building\non this quantification, we implement two targeted intervention mechanisms: an\nactive sensing process that dynamically re-observes high-uncertainty scenes to\nenhance visual input quality and an automated refinement procedure that\nfine-tunes the model on high-certainty data, improving its capability to meet\ntask specifications. Empirical validation in real-world and simulated robotic\ntasks demonstrates that our uncertainty disentanglement framework reduces\nvariability by up to 40% and enhances task success rates by 5% compared to\nbaselines. These improvements are attributed to the combined effect of both\ninterventions and highlight the importance of uncertainty disentanglement,\nwhich facilitates targeted interventions that enhance the robustness and\nreliability of autonomous systems. Fine-tuned models, code, and datasets are\navailable at https://uncertainty-in-planning.github.io/.\n","authors":["Neel P. Bhatt","Yunhao Yang","Rohan Siva","Daniel Milan","Ufuk Topcu","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01639v3.pdf","comment":"Fine-tuned models, code, and datasets are available at\n  https://uncertainty-in-planning.github.io/"},{"id":"http://arxiv.org/abs/2412.01798v3","updated":"2025-04-17T02:23:52Z","published":"2024-12-02T18:46:12Z","title":"SEAL: Semantic Attention Learning for Long Video Representation","summary":"  Long video understanding presents challenges due to the inherent high\ncomputational complexity and redundant temporal information. An effective\nrepresentation for long videos must efficiently process such redundancy while\npreserving essential contents for downstream tasks. This paper introduces\nSEmantic Attention Learning (SEAL), a novel unified representation for long\nvideos. To reduce computational complexity, long videos are decomposed into\nthree distinct types of semantic entities: scenes, objects, and actions,\nallowing models to operate on a compact set of entities rather than a large\nnumber of frames or pixels. To further address redundancy, we propose an\nattention learning module that balances token relevance with diversity,\nformulated as a subset selection optimization problem. Our representation is\nversatile and applicable across various long video understanding tasks.\nExtensive experiments demonstrate that SEAL significantly outperforms\nstate-of-the-art methods in video question answering and temporal grounding\ntasks across diverse benchmarks, including LVBench, MovieChat-1K, and Ego4D.\n","authors":["Lan Wang","Yujia Chen","Du Tran","Vishnu Naresh Boddeti","Wen-Sheng Chu"],"pdf_url":"https://arxiv.org/pdf/2412.01798v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12576v1","updated":"2025-04-17T01:49:46Z","published":"2025-04-17T01:49:46Z","title":"CM3AE: A Unified RGB Frame and Event-Voxel/-Frame Pre-training Framework","summary":"  Event cameras have attracted increasing attention in recent years due to\ntheir advantages in high dynamic range, high temporal resolution, low power\nconsumption, and low latency. Some researchers have begun exploring\npre-training directly on event data. Nevertheless, these efforts often fail to\nestablish strong connections with RGB frames, limiting their applicability in\nmulti-modal fusion scenarios. To address these issues, we propose a novel CM3AE\npre-training framework for the RGB-Event perception. This framework accepts\nmulti-modalities/views of data as input, including RGB images, event images,\nand event voxels, providing robust support for both event-based and RGB-event\nfusion based downstream tasks. Specifically, we design a multi-modal fusion\nreconstruction module that reconstructs the original image from fused\nmulti-modal features, explicitly enhancing the model's ability to aggregate\ncross-modal complementary information. Additionally, we employ a multi-modal\ncontrastive learning strategy to align cross-modal feature representations in a\nshared latent space, which effectively enhances the model's capability for\nmulti-modal understanding and capturing global dependencies. We construct a\nlarge-scale dataset containing 2,535,759 RGB-Event data pairs for the\npre-training. Extensive experiments on five downstream tasks fully demonstrated\nthe effectiveness of CM3AE. Source code and pre-trained models will be released\non https://github.com/Event-AHU/CM3AE.\n","authors":["Wentao Wu","Xiao Wang","Chenglong Li","Bo Jiang","Jin Tang","Bin Luo","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2504.12576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12027v2","updated":"2025-04-17T01:49:31Z","published":"2025-04-16T12:37:08Z","title":"Understanding Attention Mechanism in Video Diffusion Models","summary":"  Text-to-video (T2V) synthesis models, such as OpenAI's Sora, have garnered\nsignificant attention due to their ability to generate high-quality videos from\na text prompt. In diffusion-based T2V models, the attention mechanism is a\ncritical component. However, it remains unclear what intermediate features are\nlearned and how attention blocks in T2V models affect various aspects of video\nsynthesis, such as image quality and temporal consistency. In this paper, we\nconduct an in-depth perturbation analysis of the spatial and temporal attention\nblocks of T2V models using an information-theoretic approach. Our results\nindicate that temporal and spatial attention maps affect not only the timing\nand layout of the videos but also the complexity of spatiotemporal elements and\nthe aesthetic quality of the synthesized videos. Notably, high-entropy\nattention maps are often key elements linked to superior video quality, whereas\nlow-entropy attention maps are associated with the video's intra-frame\nstructure. Based on our findings, we propose two novel methods to enhance video\nquality and enable text-guided video editing. These methods rely entirely on\nlightweight manipulation of the attention matrices in T2V models. The efficacy\nand effectiveness of our methods are further validated through experimental\nevaluation across multiple datasets.\n","authors":["Bingyan Liu","Chengyu Wang","Tongtong Su","Huan Ten","Jun Huang","Kailing Guo","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2504.12027v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14891v2","updated":"2025-04-17T01:47:10Z","published":"2025-02-17T03:20:52Z","title":"CoDiff: Conditional Diffusion Model for Collaborative 3D Object\n  Detection","summary":"  Collaborative 3D object detection holds significant importance in the field\nof autonomous driving, as it greatly enhances the perception capabilities of\neach individual agent by facilitating information exchange among multiple\nagents. However, in practice, due to pose estimation errors and time delays,\nthe fusion of information across agents often results in feature\nrepresentations with spatial and temporal noise, leading to detection errors.\nDiffusion models naturally have the ability to denoise noisy samples to the\nideal data, which motivates us to explore the use of diffusion models to\naddress the noise problem between multi-agent systems. In this work, we propose\nCoDiff, a novel robust collaborative perception framework that leverages the\npotential of diffusion models to generate more comprehensive and clearer\nfeature representations. To the best of our knowledge, this is the first work\nto apply diffusion models to multi-agent collaborative perception.\nSpecifically, we project high-dimensional feature map into the latent space of\na powerful pre-trained autoencoder. Within this space, individual agent\ninformation serves as a condition to guide the diffusion model's sampling. This\nprocess denoises coarse feature maps and progressively refines the fused\nfeatures. Experimental study on both simulated and real-world datasets\ndemonstrates that the proposed framework CoDiff consistently outperforms\nexisting relevant methods in terms of the collaborative object detection\nperformance, and exhibits highly desired robustness when the pose and delay\ninformation of agents is with high-level noise. The code is released at\nhttps://github.com/HuangZhe885/CoDiff\n","authors":["Zhe Huang","Shuo Wang","Yongcai Wang","Lei Wang"],"pdf_url":"https://arxiv.org/pdf/2502.14891v2.pdf","comment":null}]}}